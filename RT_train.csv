text,labels
"Cluster randomized sampling is 1 method for sampling a population. It requires recruiting subgroups of participants from the population of interest (e.g., whole classes from schools) instead of individuals solicited independently. Here, we demonstrate how clusters affect the standard error of the mean. The presence of clusters influences 2 quantities, the variance of the means and the expected variance. Ignoring clustering produces spurious statistical significance and reduces statistical power when effect sizes are moderate to large. Here, we propose a correction factor. It can be used to estimate standard errors and confidence intervals of the mean under cluster randomized sampling. This correction factor is easy to integrate into regular tests of means and effect sizes. It can also be used to determine sample size needed to reach a prespecified power. Finally, this approach is an easy-to-use alternative to linear mixed modeling and hierarchical linear modeling when there are only 2 levels and no covariates.-A correction factor for the impact of cluster randomized sampling and its applications.",1
"Recent success of immunotherapy and other targeted therapies in cancer treatment has led to an unprecedented surge in the number of novel therapeutic agents that need to be evaluated in clinical trials. Traditional phase II clinical trial designs were developed for evaluating one candidate treatment at a time and thus not efficient for this task. We propose a Bayesian phase II platform design, the multi-candidate iterative design with adaptive selection (MIDAS), which allows investigators to continuously screen a large number of candidate agents in an efficient and seamless fashion. MIDAS consists of one control arm, which contains a standard therapy as the control, and several experimental arms, which contain the experimental agents. Patients are adaptively randomized to the control and experimental agents based on their estimated efficacy. During the trial, we adaptively drop inefficacious or overly toxic agents and 'graduate' the promising agents from the trial to the next stage of development. Whenever an experimental agent graduates or is dropped, the corresponding arm opens immediately for testing the next available new agent. Simulation studies show that MIDAS substantially outperforms the conventional approach. The proposed design yields a significantly higher probability for identifying the promising agents and dropping the futile agents. In addition, MIDAS requires only one master protocol, which streamlines trial conduct and substantially decreases the overhead burden. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-MIDAS: a practical Bayesian design for platform trials with molecularly targeted agents.",0
"Clinical trials are unlikely to ever be launched for many comparative effectiveness research (CER) questions. Inferences from hypothetical randomized trials may however be emulated with marginal structural modeling (MSM) using observational data, but success in adjusting for time-dependent confounding and selection bias typically relies on parametric modeling assumptions. If these assumptions are violated, inferences from MSM may be inaccurate. In this article, we motivate the application of a data-adaptive estimation approach called super learning (SL) to avoid reliance on arbitrary parametric assumptions in CER. Using the electronic health records data from adults with new-onset type 2 diabetes, we implemented MSM with inverse probability weighting (IPW) estimation to evaluate the effect of three oral antidiabetic therapies on the worsening of glomerular filtration rate. Inferences from IPW estimation were noticeably sensitive to the parametric assumptions about the associations between both the exposure and censoring processes and the main suspected source of confounding, that is, time-dependent measurements of hemoglobin A1c. SL was successfully implemented to harness flexible confounding and selection bias adjustment from existing machine learning algorithms. Erroneous IPW inference about clinical effectiveness because of arbitrary and incorrect modeling decisions may be avoided with SL.-Super learning to hedge against incorrect inference from arbitrary parametric assumptions in marginal structural modeling.",0
"We investigate the use of permutation tests for the analysis of parallel and stepped-wedge cluster-randomized trials. Permutation tests for parallel designs with exponential family endpoints have been extensively studied. The optimal permutation tests developed for exponential family alternatives require information on intraclass correlation, a quantity not yet defined for time-to-event endpoints. Therefore, it is unclear how efficient permutation tests can be constructed for cluster-randomized trials with such endpoints. We consider a class of test statistics formed by a weighted average of pair-specific treatment effect estimates and offer practical guidance on the choice of weights to improve efficiency. We apply the permutation tests to a cluster-randomized trial evaluating the effect of an intervention to reduce the incidence of hospital-acquired infection. In some settings, outcomes from different clusters may be correlated, and we evaluate the validity and efficiency of permutation test in such settings. Lastly, we propose a permutation test for stepped-wedge designs and compare its performance with mixed-effect modeling and illustrate its superiority when sample sizes are small, the underlying distribution is skewed, or there is correlation across clusters. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-The use of permutation tests for the analysis of parallel and stepped-wedge cluster-randomized trials.",3
"Cluster-randomized trials (CRTs) are increasingly common in pragmatic trials of interventions for older adults, where staff of existing clinics or service agencies deliver interventions. The Adult Day Service (ADS) Plus intervention is delivered by trained staff at adult day service facilities to assist older adults with cognitive impairments and their family caregivers. Because sizable imbalances on important site characteristics might emerge from a simple randomization, we implemented a 3-stage constrained randomization approach to limit imbalance between intervention and usual care control conditions on 5 site characteristics: capacity; % of minority clients; % of clients with dementia; urban, rural or suburban location; and private or public ownership. In stage 1, the Balance Match Weighted (BMW) re-randomization procedure was used to assign 30 sites to ADS Plus or control arms based on the best randomization out of 20 total randomizations for minimizing site imbalance. In stage 2, propensity scores from the BMW logistic regression analysis for reserve sites were used to determine substitutions for randomized sites that opted out of the CRT prior to implementation. In stage 3, a minimization approach was used to add 20 more sites to the trial. A standardized metric based on the half-normal distribution of the absolute value of mean differences was used to assess site imbalance. After stage 3, the remaining imbalance for the 49 enrolled sites was reduced by 75% from what would have been expected from a simple randomization. Optimized randomization procedures with similar imbalance metrics should be used more routinely in pragmatic CRTs.-Application of randomization techniques for balancing site covariates in the adult day service plus pragmatic cluster-randomized trial",1
"Case-cohort study design has been widely used for its cost-effectiveness. In any real study, there are always other important outcomes of interest beside the failure time that the original case-cohort study is based on. How to utilize the available case-cohort data to study the relationship of a secondary outcome with the primary exposure obtained through the case-cohort study is not well studied. In this article, we propose a non-parametric estimated likelihood approach for analyzing a secondary outcome in a case-cohort study. The estimation is based on maximizing a semiparametric likelihood function that is built jointly on both time-to-failure outcome and the secondary outcome. The proposed estimator is shown to be consistent, efficient, and asymptotically normal. Finite sample performance is evaluated via simulation studies. Data from the Sister Study is analyzed to illustrate our method.-Regression analysis for secondary response variable in a case-cohort study.",0
"To estimate the net (i.e., overall) effect of highly active antiretroviral therapy (HAART) on time to acquired immunodeficiency syndrome (AIDS) or death, the authors used inverse probability-of-treatment weighted estimation of a marginal structural model, which can appropriately adjust for time-varying confounders affected by prior treatment or exposure. Human immunodeficiency virus (HIV)-positive men and women (n = 1,498) were followed in two ongoing cohort studies between 1995 and 2002. Sixty-one percent (n = 918) of the participants initiated HAART during 6,763 person-years of follow-up, and 382 developed AIDS or died. Strong confounding by indication for HAART was apparent; the unadjusted hazard ratio for AIDS or death was 0.98. The hazard ratio from a standard time-dependent Cox model that included time-varying CD4 cell count, HIV RNA level, and other time-varying and fixed covariates as regressors was 0.81 (95% confidence interval: 0.61, 1.07). In contrast, the hazard ratio from a marginal structural survival model was 0.54 (robust 95% confidence interval: 0.38, 0.78), suggesting a clinically meaningful net benefit of HAART. Standard Cox analysis failed to detect a clear net benefit, because it does not appropriately adjust for time-dependent covariates, such as HIV RNA level and CD4 cell count, that are simultaneously confounders and intermediate variables.-Effect of highly active antiretroviral therapy on time to acquired immunodeficiency syndrome or death using marginal structural models.",0
"Stepped wedge designs are increasingly commonplace and advantageous for cluster randomized trials when it is both unethical to assign placebo, and it is logistically difficult to allocate an intervention simultaneously to many clusters. We study marginal mean models fit with generalized estimating equations for assessing treatment effectiveness in stepped wedge cluster randomized trials. This approach has advantages over the more commonly used mixed models that (1) the population-average parameters have an important interpretation for public health applications and (2) they avoid untestable assumptions on latent variable distributions and avoid parametric assumptions about error distributions, therefore, providing more robust evidence on treatment effects. However, cluster randomized trials typically have a small number of clusters, rendering the standard generalized estimating equation sandwich variance estimator biased and highly variable and hence yielding incorrect inferences. We study the usual asymptotic generalized estimating equation inferences (i.e., using sandwich variance estimators and asymptotic normality) and four small-sample corrections to generalized estimating equation for stepped wedge cluster randomized trials and for parallel cluster randomized trials as a comparison. We show by simulation that the small-sample corrections provide improvement, with one correction appearing to provide at least nominal coverage even with only 10 clusters per group. These results demonstrate the viability of the marginal mean approach for both stepped wedge and parallel cluster randomized trials. We also study the comparative performance of the corrected methods for stepped wedge and parallel designs, and describe how the methods can accommodate interval censoring of individual failure times and incorporate semiparametric efficient estimators.-Finite-sample corrected generalized estimating equation of population average treatment effects in stepped wedge cluster randomized trials.",3
"Current duration data arise in cross-sectional studies from questions on the length of time from an initiating event to the time of interview. For example, in the National Survey on Family Growth, women who were considered at risk for pregnancy were asked (i) 'Are you currently attempting pregnancy?' and (ii) 'If yes, how many months have you been attempting to get pregnant?' The responses to (ii), referred to as the current durations, are length-biased because women with longer durations are more likely to answer yes to question (i) and therefore be included in the sample. Previous methods to analyze such data include continuous time nonparametric and parametric approaches. In this article, we propose a semiparametric Cox model and a piecewise constant baseline model (used to account for digit preference) to analyze grouped current duration data. We discuss and investigate through simulation studies, the robustness properties of the proposed methods when digit preference is present. Lastly, we present an analysis of the current duration data resulting from the 2002 National Survey on Family Growth. Published 2014. This article is a U.S. Government work and is in the public domain in the USA.-Semiparametric modeling of grouped current duration data with preferential reporting.",0
"The sandwich standard error estimator is commonly used for making inferences about parameter estimates found as solutions to generalized estimating equations (GEE) for clustered data. The sandwich tends to underestimate the variability in the parameter estimates when the number of clusters is small, and reference distributions commonly used for hypothesis testing poorly approximate the distribution of Wald test statistics. Consequently, tests have greater than nominal type I error rates. We propose tests that use bias-reduced linearization, BRL, to adjust the sandwich estimator and Satterthwaite or saddlepoint approximations for the reference distribution of resulting Wald t-tests. We conducted a large simulation study of tests using a variety of estimators (traditional sandwich, BRL, Mancl and DeRouen's BC estimator, and a modification of an estimator proposed by Kott) and approximations to reference distributions under diverse settings that varied the distribution of the explanatory variables, the values of coefficients, and the degree of intra-cluster correlation (ICC). Our new method generally worked well, providing accurate estimates of the variability of fitted coefficients and tests with near-nominal type I error rates when the ICC is small. Our method works less well when the ICC is large, but it continues to out-perform the traditional sandwich and other alternatives.-Improved hypothesis testing for coefficients in generalized estimating equations with small samples of clusters.",1
"In this paper, we describe two types of neuroscience problems which challenge the typical statistical models assumed for analyzing neuronal data. This offers an opportunity for new modeling and statistical inference. In the first problem, the data are spatial neural counts which are often over-dispersed and spatially correlated so that a standard Poisson regression model is inadequate. In the second problem, the data are averaged electroencephalograph signals recorded during muscle fatigue, where a time series AR(1) regression model cannot fully capture all the variation and correlation structure in the data. It is shown that an additional parameter has to be included in the modeling of the correlation structure and that the role of the parameter differs from one channel to the other. We propose appropriate generalized models for these data, develop statistical procedures under the generalized models, and apply these procedures to the real data that motivated this paper. The effect of mis-specification of a correlation structure is also investigated.-Modeling heterogeneity and dependence for analysis of neuronal data.",0
"To assess the impact of the 2004 extension of the CONSORT guidelines on the reporting and methodological quality of cluster randomised trials. Methodological review of 300 randomly sampled cluster randomised trials. Two reviewers independently abstracted 14 criteria related to quality of reporting and four methodological criteria specific to cluster randomised trials. We compared manuscripts published before CONSORT (2000-4) with those published after CONSORT (2005-8). We also investigated differences by journal impact factor, type of journal, and trial setting. A validated Medline search strategy. Eligibility criteria for selecting studies Cluster randomised trials published in English language journals, 2000-8. There were significant improvements in five of 14 reporting criteria: identification as cluster randomised; justification for cluster randomisation; reporting whether outcome assessments were blind; reporting the number of clusters randomised; and reporting the number of clusters lost to follow-up. No significant improvements were found in adherence to methodological criteria. Trials conducted in clinical rather than non-clinical settings and studies published in medical journals with higher impact factor or general medical journals were more likely to adhere to recommended reporting and methodological criteria overall, but there was no evidence that improvements after publication of the CONSORT extension for cluster trials were more likely in trials conducted in clinical settings nor in trials published in either general medical journals or in higher impact factor journals. The quality of reporting of cluster randomised trials improved in only a few aspects since the publication of the extension of CONSORT for cluster randomised trials, and no improvements at all were observed in essential methodological features. Overall, the adherence to reporting and methodological guidelines for cluster randomised trials remains suboptimal, and further efforts are needed to improve both reporting and methodology.-Impact of CONSORT extension for cluster randomised trials on quality of reporting and study methodology: review of random sample of 300 trials, 2000-8.",1
"Pulse detection algorithms and spectral analysis are the two most common methods for analysing pulsatile hormone data. We compared a popular high quality pulse detection algorithm (CLUSTER) to spectral analysis on a data set comparing luteinizing hormone data in depressed and control women. For these data, periodogram analysis methods, in particular Fisher's periodicity test, were superior in distinguishing the groups. Extending the pulse detection method to include measures of intra-individual variability improved its discriminatory performance. The two methods complement each other.-Periodograms and pulse detection methods for pulsatile hormone data.",0
"The continual reassessment method (CRM) is an adaptive design for Phase I trials whose operating characteristics, including appropriate sample size, probability of correctly identifying the maximum tolerated dose, and the expected proportion of participants assigned to each dose, can only be determined via simulation. The actual time to determine a final ""best"" design can take several hours or days, depending on the number of scenarios that are examined. The computational cost increases as the kernel of the one-parameter CRM design is expanded to other settings, including additional parameters, monitoring of both toxicity and efficacy, and studies of combinations of two agents. For a given vector of true DLT probabilities, we have developed an approach that replaces a simulation study of thousands of hypothetical trials with a single simulation. Our approach, which is founded on the consistency of the CRM, very accurately reflects the results produced by the simulation study, but does so in a fraction of time required by the simulation study. Relative to traditional simulations, we extensively examine how our method is able to assess the operating characteristics of a CRM design for a hypothetical trial whose characteristics are based upon a previously published Phase I trial. We also provide a metric of nonconsistency and demonstrate that although nonconsistency can impact the operating characteristics of our method, the degree of over- or under-estimation is unpredictable. As a solution, we provide an algorithm for maintaining the consistency of a chosen CRM design so that our method is applicable for any trial.-A simulation-free approach to assessing the performance of the continual reassessment method.",0
"In some cluster randomization trials, the number of clusters cannot exceed a specified maximum value due to cost constraints or other practical reasons. Donner and Klar [Donner A, and Klar N. Design and analysis of cluster randomization trials in health research. Oxford University Press 2000] provided the sample size formula for the number of subjects required per cluster when the number of clusters cannot exceed a specified maximum value. The sample size formula of Donner and Klar assumes that the number of subjects is the same in each cluster. In practical situations, the number of subjects may be different among clusters. We conducted simulation studies to investigate the effect of the cluster size variability (kappa) and the intracluster correlation coefficient (rho) on the power of the study in which the number of available clusters is fixed in advance. For the balanced case (kappa=1.0), i.e., equal cluster size among clusters, the sample size formula yielded empirical powers close to the nominal level even when the number of available clusters per group (k*) is as small as 10. The sample size formula yielded empirical powers close to the nominal level when the number of available clusters per group (k*) is at least 20 and the imbalance parameter (kappa) is at least 0.8. Empirical powers were close to the nominal level when (rho&lt; or =0.02, kappa&gt; or =0.8, and k*=10) or (rho&lt; or =0.02, kappa=0.8, and k*=20).-Effect of imbalance and intracluster correlation coefficient in cluster randomization trials with binary outcomes when the available number of clusters is fixed in advance.",1
"Mediation analyses supply a principal lens to probe the pathways through which a treatment acts upon an outcome because they can dismantle and test the core components of treatments and test how these components function as a coordinated system or theory of action. Experimental evaluation of mediation effects in addition to total effects has become increasingly common but literature has developed only limited guidance on how to plan mediation studies with multi-tiered hierarchical or clustered structures. In this study, we provide methods for computing the power to detect mediation effects in three-level cluster-randomized designs that examine individual- (level one), intermediate- (level two) or cluster-level (level three) mediators. We assess the methods using a simulation and provide examples of a three-level clinic-randomized study (individuals nested within therapists nested within clinics) probing an individual-, intermediate- or cluster-level mediator using the R package PowerUpR and its Shiny application.-Power and Sample Size Determination for Multilevel Mediation in Three-Level Cluster-Randomized Trials",1
"In medical therapies involving multiple stages, a physician's choice of a subject's treatment at each stage depends on the subject's history of previous treatments and outcomes. The sequence of decisions is known as a dynamic treatment regime or treatment policy. We consider dynamic treatment regimes in settings where each subject's final outcome can be defined as the sum of longitudinally observed values, each corresponding to a stage of the regime. Q-learning, which is a backward induction method, is used to first optimize the last stage treatment then sequentially optimize each previous stage treatment until the first stage treatment is optimized. During this process, model-based expectations of outcomes of late stages are used in the optimization of earlier stages. When the outcome models are misspecified, bias can accumulate from stage to stage and become severe, especially when the number of treatment stages is large. We demonstrate that a modification of standard Q-learning can help reduce the accumulated bias. We provide a computational algorithm, estimators, and closed-form variance formulas. Simulation studies show that the modified Q-learning method has a higher probability of identifying the optimal treatment regime even in settings with misspecified models for outcomes. It is applied to identify optimal treatment regimes in a study for advanced prostate cancer and to estimate and compare the final mean rewards of all the possible discrete two-stage treatment sequences.-Optimization of multi-stage dynamic treatment regimes utilizing accumulated data.",0
"The authors estimated components of variance and intraclass correlation coefficients (ICCs) to aid in the design of complex surveys and community intervention studies by analyzing data from the Health Survey for England 1994. This cross-sectional survey of English adults included data on a range of lifestyle risk factors and health outcomes. For the survey, households were sampled in 720 postal code sectors nested within 177 district health authorities and 14 regional health authorities. Study subjects were adults aged 16 years or more. ICCs and components of variance were estimated from a nested random-effects analysis of variance. Results are presented at the district health authority, postal code sector, and household levels. Between-cluster variation was evident at each level of clustering. In these data, ICCs were inversely related to cluster size, but design effects could be substantial when the cluster size was large. Most ICCs were below 0.01 at the district health authority level, and they were mostly below 0.05 at the postal code sector level. At the household level, many ICCs were in the range of 0.0-0.3. These data may provide useful information for the design of epidemiologic studies in which the units sampled or allocated range in size from households to large administrative areas.-Components of variance and intraclass correlations for the design of community-based surveys and intervention studies: data from the Health Survey for England 1994.",1
"This paper discusses the application of the cluster-randomized trial (CRT) design to evaluate the effectiveness of interventions against infectious diseases. In addition to the usual rationale for this design, there are a number of other advantages that are peculiar to the study of infectious diseases. In particular, CRTs are able to measure the overall effect of an intervention at the population level, capturing both the direct effect of an intervention on an individual's susceptibility to infection, and also the indirect effects due to changes in risks of transmission to other individuals, or to the mass effect or 'herd immunity' resulting from intervening in a large proportion of the population. We briefly review published CRTs of interventions against infectious diseases, most of which have been conducted in the developing countries where such diseases predominate. The focus is on trials in which communities or other large groupings are randomized, and in which impacts on infectious disease incidence or mortality are assessed. We then discuss three issues that are of special relevance to CRTs of infectious diseases. First, issues relating to the definition and size of clusters; secondly, the role of matching or stratification, and the choice of matching factors; and thirdly, the definition of direct and indirect effects of intervention, and methods of assessing these components in a CRT. We conclude by outlining some areas for future research.-Design and analysis issues in cluster-randomized trials of interventions against infectious diseases.",1
"Group interventions are interventions delivered to groups of people rather than to individuals and are used in healthcare for mental health recovery, behaviour change, peer support, self-management and/or health education. Evaluating group interventions in randomised controlled trials (RCTs) presents trialists with a set of practical problems, which are not present in RCTs of one-to-one interventions and which may not be immediately obvious. Case-based approach summarising Sheffield trials unit's experience in the design and implementation of five group interventions. We reviewed participant recruitment and attrition, facilitator training and attrition, attendance at the group sessions, group size and fidelity aspects across five RCTs. Median recruitment across the five trials was 3.2 (range 1.7-21.0) participants per site per month. Group intervention trials involve a delay in starting the intervention for some participants, until sufficient numbers are available to start a group. There was no evidence that the timing of consent, relative to randomisation, affected post-randomisation attrition which was a matter of concern for all trial teams. Group facilitator attrition was common in studies where facilitators were employed by the health system rather than the by the grant holder and led to the early closure of one trial; research sites responded by training 'back-up' and new facilitators. Trials specified that participants had to attend a median of 62.5% (range 16.7%-80%) of sessions, in order to receive a 'therapeutic dose'; a median of 76.7% (range 42.9%-97.8%) received a therapeutic dose. Across the five trials, 75.3% of all sessions went ahead without the pre-specified ideal group size. A variety of methods were used to assess the fidelity of group interventions at a group and individual level across the five trials. This is the first paper to provide an empirical basis for planning group intervention trials. Investigators should expect delays/difficulties in recruiting groups of the optimal size, plan for both facilitator and participant attrition, and consider how group attendance and group size affects treatment fidelity. ISRCTN17993825 registered on 11/10/2016, ISRCTN28645428?registered on 11/04/2012, ISRCTN61215213?registered on 11/05/2011, ISRCTN67209155?registered on 22/03/2012, ISRCTN19447796?registered on 20/03/2014.-Challenges in the design, planning and implementation of trials evaluating group interventions",2
"The use of the stepped-wedge cluster randomised trial (SW-CRT) is on the increase, and although there are still relatively few SW-CRTs currently published its use is bound to show an increase in the near future. An extension of the CONSORT reporting guideline for SW-CRTs has recently been developed. By making reporting guidelines for this innovative design available relatively early in its development, it is possible that the methodological conduct and reporting of future SW-CRTs will not be at the same risk of low-quality of reporting as is the case with many other study designs. We provide a brief overview of this reporting guideline and encourage authors to use it appropriately; and for journal editors to endorse its use.-Introducing the new CONSORT extension for stepped-wedge cluster randomised trials",3
"We propose an extension of the Harezlak and Heckman (J. Comput. Graph. Statist. 2001; 10(4): 713-729) test for detecting local extrema to the longitudinal data setting. We use penalized spline regression techniques (Statist. Sci. 1996; 11:89-102) to provide a computationally efficient method of testing for relatively large data sets. We estimate the p-values of our test, LongCriSP, with a smoothed bootstrap. Our simulation studies indicate that the test is generally conservative and has power exceeding 70 per cent at the alpha = 0.1 nominal level in most considered settings. Finally, we apply our testing procedure to the longitudinal measurements of body mass index of former prisoners of war in Vietnam and conclude that the mean population curve exhibits non-monotone behaviour.-LongCriSP: a test for bump hunting in longitudinal data.",0
"The stepped-wedge cluster randomized trial (SW-CRT) involves the sequential transition of clusters (such as hospitals, public health units or communities) from control to intervention conditions in a randomized order. The use of the SW-CRT is growing rapidly. Yet the SW-CRT is at greater risks of bias compared with the conventional parallel cluster randomized trial (parallel-CRT). For this reason, the CONSORT extension for SW-CRTs requires that investigators provide a clear justification for the choice of study design. In this paper, we argue that all other things being equal, the SW-CRT is at greater risk of bias due to misspecification of the secular trends at the analysis stage. This is particularly problematic for studies randomizing a small number of heterogeneous clusters. We outline the potential conditions under which an SW-CRT might be an appropriate choice. Potentially appropriate and often overlapping justifications for conducting an SW-CRT include: (i) the SW-CRT provides a means to conduct a randomized evaluation which otherwise would not be possible; (ii) the SW-CRT facilitates cluster recruitment as it enhances the acceptability of a randomized evaluation either to cluster gatekeepers or other stakeholders; (iii) the SW-CRT is the only feasible design due to pragmatic and logistical constraints (for example the roll-out of a scare resource); and (iv) the SW-CRT has increased statistical power over other study designs (which will include situations with a limited number of clusters). As the number of arguments in favour of an SW-CRT increases, the likelihood that the benefits of using the SW-CRT, as opposed to a parallel-CRT, outweigh its risks also increases. We argue that the mere popularity and novelty of the SW-CRT should not be a factor in its adoption. In situations when a conventional parallel-CRT is feasible, it is likely to be the preferred design.-Reflection on modern methods: when is a stepped-wedge cluster randomized trial a good study design choice?",3
"To investigate the trade off between performing an individual randomized trial with a Complier Average Causal Effect analysis and accepting the fact that there will be some contamination, with a cluster randomized trial and the subsequent effect on the sample size and power of the trial. Monte Carlo simulations were undertaken to generate trial data where there was some contamination of control participants. The trials were simulated so the null hypothesis was false. Assessments were made of whether a type II error had been committed (i.e., whether the false null had not been rejected). As contamination increases, the power of the study to detect a true difference between the two groups declines. Using a Complier Average Causal Effect approach, unless anticipated contamination exceeds 30%, retains a sample size advantage over the cluster randomized design despite a relatively small cluster (i.e., 10) and reasonably small ICC (i.e., 0.04). If contamination can be measured precisely or estimated then in some circumstances individual allocation has an advantage over cluster randomization in statistical efficiency.-Individual allocation had an advantage over cluster randomization in statistical efficiency in some circumstances.",1
"Individuals with two copies of the apolipoprotein-1 (APOL1) gene risk variants are at high risk (HR) for non-diabetic kidney disease. The presence of these risk variants is highest in West Africa, specifically in Nigeria. However, there is limited availability of dialysis and kidney transplantation in Nigeria, and most individuals will die soon after developing end-stage renal disease. Blocking the renin angiotensin aldosterone system with angiotensin-converting enzyme inhibitors (ACEi) is a well-recognized strategy to slow renal disease progression in patients with diabetes mellitus with chronic kidney disease (CKD) and in patients with HIV-associated nephropathy. We propose to determine whether presence of the APOL1 HR genotype alters or predicts responsiveness to conventional therapy to treat or prevent CKD and if addition of an ACEi to standard combination antiretroviral therapy (ART) reduces the risk of kidney complications among non-diabetic Nigerian adults. We will screen 2600 HIV-positive adults who have received ART to (1) determine the prevalence of APOL1 risk variants and assess whether APOL1 HR status correlates with prevalent albuminuria, estimated glomerular filtration rate (eGFR), and/or prevalent CKD; (2) assess, via a randomized, placebo-controlled trial (RCT) in a subset of these participants with microalbuminura (n = 280) whether addition of the ACEi, lisinopril, compared to standard of care, significantly reduces the incidence or progression of albuminuria; and (3) determine whether the APOL1 HR genotype is associated with worse kidney outcomes (i.e. eGFR slope or regression of albuminuria) among participants in the RCT. This study will examine the increasing prevalence of kidney diseases in HIV-positive adults in a West African population, and the relationship between these diseases and the APOL1 high-risk genotype. By evaluating the addition of an ACEi to the care of individuals with HIV infection who have albuminuria, our trial will provide definitive evidence to guide strategies for management and clinical care in this population, with the goal of reducing HIV-related kidney complications. ClinicalTrials.gov, NCT03201939 . Registered on 26 August 2016.-Optimal management of HIV- positive?adults at risk for kidney disease in Nigeria (Renal Risk Reduction ""R3"" Trial): protocol and study design.",0
"Cluster-level dynamic treatment regimens can be used to guide sequential treatment decision-making at the cluster level in order to improve outcomes at the individual or patient-level. In a cluster-level dynamic treatment regimen, the treatment is potentially adapted and re-adapted over time based on changes in the cluster that could be impacted by prior intervention, including aggregate measures of the individuals or patients that compose it. Cluster-randomized sequential multiple assignment randomized trials can be used to answer multiple open questions preventing scientists from developing high-quality cluster-level dynamic treatment regimens. In a cluster-randomized sequential multiple assignment randomized trial, sequential randomizations occur at the cluster level and outcomes are observed at the individual level. This manuscript makes two contributions to the design and analysis of cluster-randomized sequential multiple assignment randomized trials. First, a weighted least squares regression approach is proposed for comparing the mean of a patient-level outcome between the cluster-level dynamic treatment regimens embedded in a sequential multiple assignment randomized trial. The regression approach facilitates the use of baseline covariates which is often critical in the analysis of cluster-level trials. Second, sample size calculators are derived for two common cluster-randomized sequential multiple assignment randomized trial designs for use when the primary aim is a between-dynamic treatment regimen comparison of the mean of a continuous patient-level outcome. The methods are motivated by the Adaptive Implementation of Effective Programs Trial which is, to our knowledge, the first-ever cluster-randomized sequential multiple assignment randomized trial in psychiatry.-Comparing cluster-level dynamic treatment regimens using sequential, multiple assignment, randomized trials: Regression estimation and sample size considerations.",1
"A cross-over trial design is more powerful than a parallel groups design, but requires that treatment effects do not carry over from one period of the trial to the next. We focus here on interventions in chronic disease populations where the control is routine care: in such cases we cannot assume the intervention effect is easily washed out in crossing over from the experimental intervention back to the control. We introduce an alternative trial design for these situations, and investigate its performance. One group is assessed before and after the experimental intervention, whereas two other groups provide respective, independent treatment comparisons in each period. We call this a dog-leg design because of the pattern of assessments in the three groups. The dog-leg design is reminiscent of a stepped wedge design, but with a reduced schedule of assessments and with the notable difference that not all groups receive the intervention. If the correlation between baseline and follow-up is &lt;0.72, the dog-leg design is more efficient than a parallel groups design with a baseline assessment. The dog-leg design also requires fewer assessments in total than a parallel groups design where participants are only assessed once, at follow-up. The dog-leg design is simple, and has some attractive properties. Though there is a risk of differential attrition in the three arms, the design's good performance relative to alternatives makes it a useful addition to the methodologist's toolkit.-The dog-leg: an alternative to a cross-over design for pragmatic clinical trials in relatively stable populations.",3
"We focus on estimating the average treatment effect in a randomized trial. If baseline variables are correlated with the outcome, then appropriately adjusting for these variables can improve precision. An example is the analysis of covariance (ANCOVA) estimator, which applies when the outcome is continuous, the quantity of interest is the difference in mean outcomes comparing treatment versus control, and a linear model with only main effects is used. ANCOVA is guaranteed to be at least as precise as the standard unadjusted estimator, asymptotically, under no parametric model assumptions and also is locally semiparametric efficient. Recently, several estimators have been developed that extend these desirable properties to more general settings that allow any real-valued outcome (e.g., binary or count), contrasts other than the difference in mean outcomes (such as the relative risk), and estimators based on a large class of generalized linear models (including logistic regression). To the best of our knowledge, we give the first simulation study in the context of randomized trials that compares these estimators. Furthermore, our simulations are not based on parametric models; instead, our simulations are based on resampling data from completed randomized trials in stroke and HIV in order to assess estimator performance in realistic scenarios. We provide practical guidance on when these estimators are likely to provide substantial precision gains and describe a quick assessment method that allows clinical investigators to determine whether these estimators could be useful in their specific trial contexts.-Leveraging prognostic baseline variables to gain precision in randomized trials.",0
"Nesting of patients within therapists in psychotherapy trials creates an additional level within the design. The multilevel nature of this design has implications for the precision, internal and external validity of estimates of the treatment effect. Prior to or during a trial, psychotherapies are allocated to therapists and therapists are assigned to patients such that the therapist becomes part of the causal pathway from the intervention to the patient. It is therefore important to consider not only the relationship between interventions and patients but also relationships between interventions and therapists and between therapists and patients. Research designs comparing the effects of therapeutic approaches, therapist characteristics and packages of the two can be unified by viewing therapists as an important source of variability within psychotherapy outcome studies. Methodological considerations arising from therapist variation will be discussed, drawing together and building upon the associated psychotherapy and statistical literatures. Parallels will also be made with related designs and methods of analysis.-Therapist variation within randomised trials of psychotherapy: implications for precision, internal and external validity.",2
"Analysis of covariance models, which adjust for a baseline covariate, are often used to compare treatment groups in a controlled trial in which individuals are randomized. Such analysis adjusts for any baseline imbalance and usually increases the precision of the treatment effect estimate. We assess the value of such adjustments in the context of a cluster randomized trial with repeated cross-sectional design and a binary outcome. In such a design, a new sample of individuals is taken from the clusters at each measurement occasion, so that baseline adjustment has to be at the cluster level. Logistic regression models are used to analyse the data, with cluster level random effects to allow for different outcome probabilities in each cluster. We compare the estimated treatment effect and its precision in models that incorporate a covariate measuring the cluster level probabilities at baseline and those that do not. In two data sets, taken from a cluster randomized trial in the treatment of menorrhagia, the value of baseline adjustment is only evident when the number of subjects per cluster is large. We assess the generalizability of these findings by undertaking a simulation study, and find that increased precision of the treatment effect requires both large cluster sizes and substantial heterogeneity between clusters at baseline, but baseline imbalance arising by chance in a randomized study can always be effectively adjusted for.-Baseline adjustments for binary data in repeated cross-sectional cluster randomized trials.",1
"Many factors have been identified that influence the recruitment of African Americans into clinical trials; however, the influence of eligibility criteria may not be widely appreciated. We used the experience from the Look AHEAD (Action for Health in Diabetes) trial screening process to examine the differential impact eligibility criteria had on the enrollment of African Americans compared to other volunteers. Look AHEAD is a large randomized clinical trial to examine whether assignment to an intensive lifestyle intervention designed to produce and maintain weight loss reduces the long-term risk of major cardiovascular events in adults with type 2 diabetes. Differences in the screening, eligibility, and enrollment rates between African Americans and members of other racial/ethnic groups were examined to identify possible reasons. Look AHEAD screened 28,735 individuals for enrollment, including 6226 (21.7%) who were self-identified African Americans. Of these volunteers, 12.9% of the African Americans compared to 19.3% of all other screenees ultimately enrolled (p &lt; 0.001). African Americans no more often than others were lost to follow-up or refused to attend clinic visits to establish eligibility. Furthermore, the enrollment rates of individuals with histories of cardiovascular disease and diabetes therapy did not markedly differ between the ethnic groups. Higher prevalence of adverse levels of blood pressure, heart rate, HbA1c, and serum creatinine among African American screenees accounted for the greater proportions excluded (all p &lt; 0.001). Compared to non-African Americans, African American were more often ineligible for the Look AHEAD trial due to comorbid conditions. Monitoring trial eligibility criteria for differential impact, and modifying them when appropriate, may ensure greater enrollment yields.-Factors influencing enrollment of African Americans in the Look AHEAD trial.",0
"Analytical techniques appropriate for cluster randomized trials that utilize a repeated cross-sectional design have not been extensively evaluated. This paper compares methods that can be used to evaluate the impact of an intervention on dichotomous outcomes. The methods are applied to data from a study on the implementation of Cochrane review evidence, in which 25 hospital obstetric units were randomized. Assessments were made for 30 pregnancies in each obstetric unit at baseline, and for 30 separate pregnancies at follow-up. The principal issues addressed are how best to take clustering into account and to allow for baseline imbalance. We compare cluster level analyses, the clustered Woolf method, marginal models based on generalized estimating equations, multilevel models, and methods based on random effects meta-analysis. Analyses which ignored the baseline assessments showed no effect of the intervention. There was substantial baseline imbalance, however, so that analyses taking into account the baseline were necessary. Yet, while analyses of change from baseline showed evidence of an effect of intervention, adjusting for baseline using analysis of covariance did not. Analysis of covariance required the use of cluster level rather than individual level responses, since different pregnancies were evaluated at baseline and follow-up. Also, when analysing change from baseline, we show it is important to allow for variation in the effect of secular trend between clusters in a multilevel model, or use robust variance estimates in a marginal model, for otherwise confidence intervals for the effect of intervention will be too narrow. We conclude however that analyses of change from baseline can be misleading since they are affected too much by baseline results, and that analysis of covariance approaches are preferable. To prevent difficulties in interpreting the results from repeated cross-sectional cluster trial designs, one should either attempt to achieve baseline balance by careful stratification of the clusters prior to randomization, or have sufficiently large samples for precise estimation of the effects of imbalance.-Analysis of cluster randomized trials with repeated cross-sectional binary measurements.",1
"To determine if risky sexual intercourse, sexually transmitted diseases, and sexual intercourse at an early age are associated with psychiatric disorder. Cross sectional study of a birth cohort at age 21 years with assessments presented by computer (for sexual behaviour) and by trained interviewers (for psychiatric disorder). New Zealand in 1993-4. 992 study members (487 women) from the Dunedin multidisciplinary health and development study. Complete data were available on both measures for 930 study members. Psychiatric disorders (anxiety, depression, eating disorder, substance dependence, antisocial disorder, mania, schizophrenia spectrum) and measures of sexual behaviour. Young people diagnosed with substance dependence, schizophrenia spectrum, and antisocial disorders were more likely to engage in risky sexual intercourse, contract sexually transmitted diseases, and have sexual intercourse at an early age (before 16 years). Unexpectedly, so were young people with depressive disorders. Young people with mania were more likely to report risky sexual intercourse and have sexually transmitted diseases. The likelihood of risky behaviour was increased by psychiatric comorbidity. There is a clear association between risky sexual behaviour and common psychiatric disorders. Although the temporal relation is uncertain, the results indicate the need to coordinate sexual medicine with mental health services in the treatment of young people.-Psychiatric disorders and risky sexual behaviour in young adulthood: cross sectional study in birth cohort.",0
"An extensive literature has covered the statistical properties of the Continual Reassessment Method (CRM) and the modifications of this method. While there are some applications of CRM designs in recent Phase I trials, the standard method (SM) of escalating doses after three patients with an option for an additional three patients SM remains very popular, mainly due to its simplicity. From a practical perspective, clinicians are interested in designs that can estimate the MTD using fewer patients for a fixed number of doses, or can test more dose levels for a given sample size. This article compares CRM-based methods with the SM in terms of the number of patients needed to reach the MTD, total sample size required, and trial duration. The comparisons are performed under two alternative schemes: a fixed or a varying sample approach with the implementation of a stopping rule. The stopping rule halts the trial if the confidence interval around the MTD is within a pre-specified bound. Our simulations evaluated several CRM-based methods under different scenarios by varying the number of dose levels from five to eight and the location of the true MTD. CRM and SM are comparable in terms of how fast they reach the MTD and the total sample size required when testing a limited number of dose levels (&lt;or=5), but as the number of dose levels increases, CRM reaches the MTD in fewer patients when used with a fixed sample of 20 patients. However, a sample size of 20-25 patients is not sufficient to achieve a narrow precision around the estimated toxicity rate at the MTD. We focused on methods with practical design features that are of interest to clinicians. However, there are several alternative CRM-based designs that are not investigated in this manuscript, and hence our results are not generalizable to other designs. We show that CRM-based methods are an improvement over the SM in terms of accuracy and optimal dose allocation in almost all cases, except when the true dose is among the lower levels.-A comprehensive comparison of the continual reassessment method to the standard 3 + 3 dose escalation scheme in Phase I dose-finding studies.",0
"There is growing interest and investment in precision medicine as a means to provide the best possible health care. A treatment regime formalizes precision medicine as a sequence of decision rules, one per clinical intervention period, that specify if, when and how current treatment should be adjusted in response to a patient's evolving health status. It is standard to define a regime as optimal if, when applied to a population of interest, it maximizes the mean of some desirable clinical outcome, such as efficacy. However, in many clinical settings, a high-quality treatment regime must balance multiple competing outcomes; eg, when a high dose is associated with substantial symptom reduction but a greater risk of an adverse event. We consider the problem of estimating the most efficacious treatment regime subject to constraints on the risk of adverse events. We combine nonparametric Q-learning with policy-search to estimate a high-quality yet parsimonious treatment regime. This estimator applies to both observational and randomized data, as well as settings with variable, outcome-dependent follow-up, mixed treatment types, and multiple time points. This work is motivated by and framed in the context of dosing for chronic pain; however, the proposed framework can be applied generally to estimate a treatment regime which maximizes the mean of one primary outcome subject to constraints on one or more secondary outcomes. We illustrate the proposed method using data pooled from 5 open-label flexible dosing clinical trials for chronic pain.-Identifying optimal dosage regimes under safety constraints: An application to long term opioid treatment of chronic pain.",0
"This article studies the notion of irrational dose assignment in Phase I clinical trials. This property was recently defined by Zhou and colleagues as a dose assignment that fails to de-escalate the dose when two out of three, three out of six, or four out of six patients have experienced a dose-limiting toxicity event at the current dose level. The authors claimed that a drawback of the well-known continual reassessment method is that it can result in irrational dose assignments. The aim of this article is to examine this definition of irrationality more closely within the conduct of the continual reassessment method. Over a broad range of assumed dose-limiting toxicity probability scenarios for six study dose levels and a variety of target dose-limiting toxicity rates, we simulated 2000 trials of n = 36 patients. For each scenario, we counted the number of irrational dose assignments that were made by the continual reassessment method, according to the definitions of Zhou and colleagues. For each of the irrational decisions made, we classified the dose assignment as an underdose assignment, a target dose assignment, or an overdose assignment based on the true dose-limiting toxicity probability at that dose. Across eight dose-toxicity scenarios, there were a total of 181,581 dose assignments made in the simulation study. Of these assignments, 8165 (4.5%) decisions were made when two out of three, three out of six, or four out of six patients had experienced a dose-limiting toxicity at the current dose. Of these 8165 decisions, 1505 (18.4%) recommended staying at the current dose level and would therefore be classified as irrational by Zhou and colleagues. Among the irrational decisions, 41.2% were misclassified, meaning they were made either at the true target dose (17.9%) or at a true underdose (23.3%). The remaining 58.8% were made at a true overdose and therefore truly irrational. Overall, irrational dose assignments comprised &lt;1% of the total dose assignments made during the simulation study. Similar findings are reported in simulations across 100 randomly generated dose-toxicity scenarios from a recently proposed family of curves. Zhou and colleagues argue that the behavior of the continual reassessment method is disturbing due to its ability to make irrational dose assignments. These definitions are based on rules that mimic the popular 3 + 3 design, which should not be the benchmark used to construct guidelines for trial conduct of modern Phase I methods. Our study illustrates that these dose assignments occur very seldom in the continual reassessment method and that even when they do occur, they can often be considered sensible when accounting for all accumulated data in the study.-Evaluation of irrational dose assignment definitions using the continual reassessment method.",0
"The common factor model assumes that the linear coefficients (intercepts and factor loadings) linking the observed variables to the latent factors are fixed coefficients (i.e., common for all participants). When the observed variables are participants' observed responses to stimuli, such as their responses to the items of a questionnaire, the assumption of common linear coefficients may be too restrictive. For instance, this may occur if participants consistently use the response scale idiosyncratically. To account for this phenomenon, the authors partially relax the fixed coefficients assumption by allowing the intercepts in the factor model to change across participants. The model is attractive when m factors are expected on the basis of substantive theory but m + 1 factors are needed in practice to adequately reproduce the data. Also, this model for single-level data can be fitted with conventional software for structural equation modeling. The authors demonstrate the use of this model with an empirical data set on optimism in which they compare it with competing models such as the bifactor and the correlated trait-correlated method minus 1 models.-Random intercept item factor analysis.",0
"microRNAs (miRNAs) are fundamental to cellular biology. Although only approximately 22 bases long, miRNAs regulate complex processes in health and disease, including human cancer. Because miRNAs are highly stable in circulation when compared with several other classes of nucleic acids, they have generated intense interest as clinical biomarkers in diverse epidemiologic studies. As with other molecular biomarker fields, however, miRNA research has become beleaguered by pitfalls related to terminology and classification; procedural, assay, and study cohort heterogeneity; and methodological inconsistencies. Together, these issues have led to both false-positive and potentially false-negative miRNA associations. In this review, we summarize the biological rationale for studying miRNAs in human disease with a specific focus on circulating miRNAs, which highlight some of the most challenging topics in the field to date. Examples from lung cancer are used to illustrate the potential utility and some of the pitfalls in contemporary miRNA research. Although the field is in its infancy, several important lessons have been learned relating to cohort development, sample preparation, and statistical analysis that should be considered for future studies. The goal of this primer is to equip epidemiologists and clinical researchers with sound principles of study design and analysis when using miRNAs.-Design and Analysis for Studying microRNAs in Human Disease: A Primer on -Omic Technologies.",0
"We review the uses of electronic health care data algorithms, measures of their accuracy, and reasons for prioritizing one measure of accuracy over another. We use real studies to illustrate the variety of uses of automated health care data in epidemiologic and health services research. Hypothetical examples show the impact of different types of misclassification when algorithms are used to ascertain exposure and outcome. High algorithm sensitivity is important for reducing the costs and burdens associated with the use of a more accurate measurement tool, for enhancing study inclusiveness, and for ascertaining common exposures. High specificity is important for classifying outcomes. High positive predictive value is important for identifying a cohort of persons with a condition of interest but that need not be representative of or include everyone with that condition. Finally, a high negative predictive value is important for reducing the likelihood that study subjects have an exclusionary condition. Epidemiologists must often prioritize one measure of accuracy over another when generating an algorithm for use in their study. We recommend researchers publish all tested algorithms-including those without acceptable accuracy levels-to help future studies refine and apply algorithms that are well suited to their objectives.-Tradeoffs between accuracy measures for electronic health care data algorithms.",0
Analysis of cluster randomised trials with an assessment of outcome at baseline.,1
"Treatment response heterogeneity has long been observed in patients affected by chronic diseases. Administering an individualized treatment rule (ITR) offers an opportunity to tailor treatment strategies according to patient-specific characteristics. Overly complex machine learning methods for estimating ITRs may produce treatment rules that have higher benefit but lack transparency and interpretability. In clinical practices, it is desirable to derive a simple and interpretable ITR while maintaining certain optimality that leads to improved benefit in subgroups of patients, if not on the overall sample. In this work, we propose a tree-based robust learning method to estimate optimal piecewise linear ITRs and identify subgroups of patients with a large benefit. We achieve these goals by simultaneously identifying qualitative and quantitative interactions through a tree model, referred to as the composite interaction tree (CITree). We show that it has improved performance compared to existing methods on both overall sample and subgroups via extensive simulation studies. Lastly, we fit CITree to Research Evaluating the Value of Augmenting Medication with Psychotherapy trial for treating patients with major depressive disorders, where we identified both qualitative and quantitative interactions and subgroups of patients with a large benefit.-Composite interaction tree for simultaneous learning of optimal individualized treatment rules and subgroups.",0
"Objective?To assess causation and clinical presentation of major birth defects.Design?Population based case cohort.Setting?Cases of birth defects in children born 2005-09 to resident women, ascertained through Utah's population based surveillance system. All records underwent clinical re-review.Participants?5504 cases among 270 878 births (prevalence 2.03%), excluding mild isolated conditions (such as muscular ventricular septal defects, distal hypospadias).Main outcome measures?The primary outcomes were the proportion of birth defects with a known etiology (chromosomal, genetic, human teratogen, twinning) or unknown etiology, by morphology (isolated, multiple, minors only), and by pathogenesis (sequence, developmental field defect, or known pattern of birth defects).Results?Definite cause was assigned in 20.2% (n=1114) of cases: chromosomal or genetic conditions accounted for 94.4% (n=1052), teratogens for 4.1% (n=46, mostly poorly controlled pregestational diabetes), and twinning for 1.4% (n=16, conjoined or acardiac). The 79.8% (n=4390) remaining were classified as unknown etiology; of these 88.2% (n=3874) were isolated birth defects. Family history (similarly affected first degree relative) was documented in 4.8% (n=266). In this cohort, 92.1% (5067/5504) were live born infants (isolated and non-isolated birth defects): 75.3% (4147/5504) were classified as having an isolated birth defect (unknown or known etiology).Conclusions?These findings underscore the gaps in our knowledge regarding the causes of birth defects. For the causes that are known, such as smoking or diabetes, assigning causation in individual cases remains challenging. Nevertheless, the ongoing impact of these exposures on fetal development highlights the urgency and benefits of population based preventive interventions. For the causes that are still unknown, better strategies are needed. These can include greater integration of the key elements of etiology, morphology, and pathogenesis into epidemiologic studies; greater collaboration between researchers (such as developmental biologists), clinicians (such as medical geneticists), and epidemiologists; and better ways to objectively measure fetal exposures (beyond maternal self reports) and closer (prenatally) to the critical period of organogenesis.-Etiology and clinical presentation of birth defects: population based study.",0
"Apply and compare two methods that identify signals for the need to update systematic reviews, using three Evidence-based Practice Center reports on omega-3 fatty acids as test cases. We applied the RAND method, which uses domain (subject matter) expert guidance, and a modified Ottawa method, which uses quantitative and qualitative signals. For both methods, we conducted focused electronic literature searches of recent studies using the key terms from the original reports. We assessed the agreement between the methods and qualitatively assessed the merits of each system. Agreement between the two methods was ""substantial"" or better (kappa&gt;0.62) in three of the four systematic reviews. Overall agreement between the methods was ""substantial"" (kappa=0.64, 95% confidence interval [CI] 0.45-0.83). The RAND and modified Ottawa methods appear to provide similar signals for the possible need to update systematic reviews in this pilot study. Future evaluation with a broader range of clinical topics and eventual comparisons between signals to update reports and the results of full evidence review updates will be needed. We propose a hybrid approach combining the best features of both methods, which should allow efficient review and assessment of the need to update.-Two methods provide similar signals for the need to update systematic reviews.",0
"In their criticism of B. E. Wampold and R. C. Serlin's analysis of treatment effects in nested designs, M. Siemer and J. Joormann argued that providers of services should be considered a fixed factor because typically providers are neither randomly selected from a population of providers nor randomly assigned to treatments, and statistical power to detect treatment effects is greater in the fixed than in the mixed model. The authors of the present article argue that if providers are considered fixed, conclusions about the treatment must be conditioned on the specific providers in the study, and they show that in this case generalizing beyond these providers incurs inflated Type I error rates.-Should providers of treatment be regarded as a random factor? If it ain't broke, don't ""fix"" it: A comment on Siemer and Joormann (2003)",2
Randomization by cluster.,1
"Building tools to support personalized medicine needs to model medical decision-making. For this purpose, both expert and real world data provide a rich source of information. Currently, machine learning techniques are developing to select relevant variables for decision-making. Rather than using data-driven analysis alone, eliciting prior information from physicians related to their medical decision-making processes can be useful in variable selection. Our framework is electronic health records data on repeated dose adjustment of Irinotecan for the treatment of metastatic colorectal cancer. We propose a method that incorporates elicited expert weights associated with variables involved in dose reduction decisions into the Stochastic Search Variable Selection (SSVS), a Bayesian variable selection method, by using a power prior. Clinician experts were first asked to provide numerical clinical relevance weights to express their beliefs about the importance of each variable in their medical decision making. Then, we modeled the link between repeated dose reduction, patient characteristics, and toxicities by assuming a logistic mixed-effects model. Simulated data were generated based on the elicited weights and combined with the observed dose reduction data via a power prior. We compared the Bayesian power prior-based SSVS performance to the usual SSVS in our case study, including a sensitivity analysis using the power prior parameter. The selected variables differ when using only expert knowledge, only the usual SSVS, or combining both. Our method enables one to select rare variables that may be missed using only the observed data and to discard variables that appear to be relevant based on the data but not relevant from the expert perspective. We introduce an innovative Bayesian variable selection method that adaptively combines elicited expert information and real world data. The method selects a set of variables relevant to model medical decision process.-Integration of elicited expert information via a power prior in Bayesian variable selection: Application to colon cancer data.",0
"Informal caregivers report substantial burden and depressive symptoms which predict higher rates of patient institutionalization. While caregiver education interventions may reduce caregiver distress and decrease the use of long-term institutional care, evidence is mixed. Inconsistent findings across studies may be the result of reporting average treatment effects which do not account for how effects differ by participant characteristics. We apply a machine-learning approach to randomized clinical trial (RCT) data of the Helping Invested Family Members Improve Veteran's Experiences Study (HI-FIVES) intervention to explore how intervention effects vary by caregiver and patient characteristics. We used model-based recursive partitioning models. Caregivers of community-residing older adult US veterans with functional or cognitive impairment at a single VA Medical Center site were randomized to receive HI-FIVES (n?= 118) vs. usual care (n?= 123). The outcomes included cumulative days not in the community and caregiver depressive symptoms assessed at 12 months post intervention. Potential moderating characteristics were: veteran age, caregiver age, caregiver ethnicity and race, relationship satisfaction, caregiver burden, perceived financial strain, caregiver depressive symptoms, and patient risk score. The effect of HI-FIVES on days not at home was moderated by caregiver burden (p?&lt; 0.001); treatment effects were higher for caregivers with a Zarit Burden Scale score ? 28. Caregivers with lower baseline Center for Epidemiologic Studies Depression Scale (CESD-10) scores (? 8) had slightly lower CESD-10 scores at follow-up (p?&lt; 0.001). Family caregiver education interventions may be less beneficial for highly burdened and distressed caregivers; these caregivers may require a more tailored approach that involves assessing caregiver needs and developing personalized approaches. ClinicalTrials.gov, ID:NCT01777490. Registered on 28 January 2013.-Identifying treatment effects of an informal caregiver education intervention to increase days in the community and decrease caregiver distress: a machine-learning secondary analysis of subgroup effects in the HI-FIVES randomized clinical trial.",0
"This paper provides intraclass correlation coefficients (ICCs) for estimation of sample size inflation required in future cluster randomised trials in primary or residential care. Three cluster randomised trials were conducted among middle-aged and older adults in primary care and residential care in Australia and New Zealand between 1995 and 2002. Baseline means or proportions, mean change, and ICCs with their standard errors and 95% confidence intervals are reported for outcome variables used in the three studies. The ICCs were estimated from a one-way random effects model using the analysis of variance method. ICCs for quality of life and psychological variables in the primary care studies were low (below 0.018). ICCs for clinical and physical activity variables ranged from 0 to 0.08. ICCs for health and functional status in residential care for the elderly were high, ranging from 0.025 to 0.514. The magnitude of the intraclass correlation varies with the venue of the trial, the outcome variables used, and the expected effect of the intervention. However, the intraclass correlations provided will be useful for more appropriate planning of residential and primary care-based trials in the future.-Intraclass correlation coefficients from three cluster randomised controlled trials in primary and residential health care.",1
"The aim of this study was to assess the performance and impact of multilevel modelling (MLM) compared with ordinary least squares (OLS) regression in trial-based economic evaluations with clustered data. Three thousand datasets with balanced and unbalanced clusters were simulated with correlation coefficients between costs and effects of - 0.5, 0, and 0.5, and intraclass correlation coefficients (ICCs) varying between 0.05 and 0.30. Each scenario was analyzed using both MLM and OLS. Statistical uncertainty around MLM and OLS estimates was estimated using bootstrapping. Performance measures were estimated and compared between approaches, including bias, root mean squared error (RMSE) and coverage probability. Cost and effect differences, and their corresponding confidence intervals and standard errors, incremental cost-effectiveness ratios, incremental net-monetary benefits and cost-effectiveness acceptability curves were compared. Cost-effectiveness outcomes were similar between OLS and MLM. MLM produced larger statistical uncertainty and coverage probabilities closer to nominal levels than OLS. The higher the ICC, the larger the effect on statistical uncertainty between MLM and OLS. Significant cost-effectiveness outcomes as estimated by OLS became non-significant when estimated by MLM. At all ICCs, MLM resulted in lower probabilities of cost effectiveness than OLS, and this difference became larger with increasing ICCs. Performance measures and cost-effectiveness outcomes were similar across scenarios with varying correlation coefficients between costs and effects. Although OLS produced similar cost-effectiveness outcomes, it substantially underestimated the amount of variation in the data compared with MLM. To prevent suboptimal conclusions and a possible waste of scarce resources, it is important to use MLM in trial-based economic evaluations when data are clustered.-Taking the Analysis of Trial-Based Economic Evaluations to the Next Level: The Importance of Accounting for Clustering.",1
"For major genes known to influence the risk of cancer, an important task is to determine the risks conferred by individual variants, so that one can appropriately counsel carriers of these mutations. This is a challenging task, since new mutations are continually being identified, and there is typically relatively little empirical evidence available about each individual mutation. Hierarchical modeling offers a natural strategy to leverage the collective evidence from these rare variants with sparse data. This can be accomplished when there are available higher-level covariates that characterize the variants in terms of attributes that could distinguish their association with disease. In this article, we explore the use of hierarchical modeling for this purpose using data from a large population-based study of the risks of melanoma conferred by variants in the CDKN2A gene. We employ both a pseudo-likelihood approach and a Bayesian approach using Gibbs sampling. The results indicate that relative risk estimates tend to be primarily influenced by the individual case-control frequencies when several cases and/or controls are observed with the variant under study, but that relative risk estimates for variants with very sparse data are more influenced by the higher-level covariate values, as one would expect. The analysis offers encouragement that we can draw strength from the aggregating power of hierarchical models to provide guidance to medical geneticists when they offer counseling to patients with rare or even hitherto unobserved variants. However, further research is needed to validate the application of asymptotic methods to such sparse data.-The use of hierarchical models for estimating relative risks of individual genetic variants: an application to a study of melanoma.",0
"To estimate the variability in outcomes attributable to therapists in clinical practice, the authors analyzed the outcomes of 6,146 patients seen by approximately 581 therapists in the context of managed care. For this analysis, the authors used multilevel statistical procedures, in which therapists were treated as a random factor. When the initial level of severity was taken into account, about 5% of the variation in outcomes was due to therapists. Patient age, gender, and diagnosis as well as therapist age, gender, experience, and professional degree accounted for little of the variability in outcomes among therapists. Whether or not patients were receiving psychotropic medication concurrently with psychotherapy did affect therapist variability. However, the patients of the more effective therapists received more benefit from medication than did the patients of less effective therapists.-Estimating variability in outcomes attributable to therapists: a naturalistic study of outcomes in managed care",1
"Effect partitioning is almost exclusively performed with multilevel models (MLMs) - so much so that some have considered the two to be synonymous. MLMs are able to provide estimates with desirable statistical properties when data come from a hierarchical structure; but the random effects included in MLMs are not always integral to the analysis. As a result, other methods with relaxed assumptions are viable options in many cases. Through empirical examples and simulations, we show how generalized estimating equations (GEEs) can be used to effectively partition effects without random effects. We show that more onerous steps of MLMs such as determining the number of random effects and the structure for their covariance can be bypassed with GEEs while still obtaining identical or near-identical results. Additionally, violations of distributional assumptions adversely affect estimates with MLMs but have no effect on GEEs because no such assumptions are made. This makes GEEs a flexible alternative to MLMs with minimal assumptions that may warrant consideration. Limitations of GEEs for partitioning effects are also discussed.-Effect Partitioning in Cross-Sectionally Clustered Data Without Multilevel Models.",1
"There is evidence to suggest that the effects of behavioral interventions may be limited to specific types of individuals, but methods for evaluating such outcomes have not been fully developed. This study proposes the use of finite mixture models to evaluate whether interventions, and, specifically, group randomized trials, impact participants with certain characteristics or levels of problem behaviors. This study uses latent classes defined by clustering of individuals based on the targeted behaviors and illustrates the model by testing whether a preventive intervention aimed at reducing problem behaviors affects experimental users of illicit substances differently than problematic substance users or those individuals engaged in more serious problem behaviors. An illustrative example is used to demonstrate the identification of latent classes, specification of random effects in a multilevel mixture model, independent validation of latent classes, and the estimation of power for the proposed models to detect intervention effects. This study proposes specific steps for the estimation of multilevel mixture models and their power and suggests that this model can be applied more broadly to understand the effectiveness of interventions.-Using Multilevel Mixtures to Evaluate Intervention Effects in Group Randomized Trials.",1
"Studies which compare cases to disease-free siblings are useful for assessing association between a genetic locus and a phenotypic trait, as they eliminate the possibility of confounding by population stratification. Many analytic methods for such family-based studies are based on a binary disease model. However, complex diseases have variable age at onset. Consequently, binary-outcome methods can be inefficient or biased. We review methods for analysing censored age-at-onset data from family studies, including stratified Cox regression and genotype-decomposition regression, an unstratified procedure which regresses age-at-onset on between- and within-family genotype components. We also introduce a retrospective likelihood for censored age-at-onset data, which requires an external estimate of the baseline hazard. Stratified Cox regression does not use controls who have not attained the age of their case sibling(s), potentially leading to a loss of efficiency. Both genotype-decomposition regression and the retrospective likelihood use these younger controls. We assess the performance of these methods via simulation studies. Stratified Cox regression and the retrospective likelihood have appropriate type I error rates in almost all situations studied; genotype-decomposition regression is often anti-conservative. Away from the null, confidence intervals for the relative risk derived from stratified Cox regression are anti-conservative when the disease is rare and case-rich families are sampled. The retrospective likelihood is more efficient than stratified Cox regression and its confidence intervals have correct coverage when the disease is rare or the estimate of the baseline hazard is reasonably accurate. These results suggest that when estimating genotype relative risks is the principal analytic goal, stratified Cox regression is appropriate as long as the disease is common; when the disease is rare, the retrospective likelihood may be more appropriate.-Case-sibling gene-association studies for diseases with variable age at onset.",0
Clustering by health professional in individually randomised trials.,2
"Random-effects regression modelling is proposed for analysis of correlated grouped-time survival data. Two analysis approaches are considered. The first treats survival time as an ordinal outcome, which is either right-censored or not. The second approach treats survival time as a set of dichotomous indicators of whether the event occurred for time periods up to the period of the event or censor. For either approach both proportional hazards and proportional odds versions of the random-effects model are developed, while partial proportional hazards and odds generalizations are described for the latter approach. For estimation, a full-information maximum marginal likelihood solution is implemented using numerical quadrature to integrate over the distribution of multiple random effects. The quadrature solution allows some flexibility in the choice of distributions for the random effects; both normal and rectangular distributions are considered in this article. An analysis of a dataset where students are clustered within schools is used to illustrate features of random-effects analysis of clustered grouped-time survival data.-Random-effects regression analysis of correlated grouped-time survival data.",1
"To study significant predictors of condom use in HIV-infected adults, we propose the use of generalized partially linear models and develop a variable selection procedure incorporating a least squares approximation. Local polynomial regression and spline smoothing techniques are used to estimate the baseline nonparametric function. The asymptotic normality of the resulting estimate is established. We further demonstrate that, with the proper choice of the penalty functions and the regularization parameter, the resulting estimate performs as well as an oracle procedure. Finite sample performance of the proposed inference procedure is assessed by Monte Carlo simulation studies. An application to assess condom use by HIV-infected patients gains some interesting results, which cannot be obtained when an ordinary logistic model is used.-Parametric variable selection in generalized partially linear models with an application to assess condom use by HIV-infected patients.",0
Small sample correction for the variance of GEE estimators,1
"The Wilcoxon rank sum test is widely used for two-group comparisons for nonnormal data. An assumption of this test is independence of sampling units both between and within groups. In ophthalmology, data are often collected on two eyes of an individual, which are highly correlated. In ophthalmological clinical trials, randomization is usually performed at the subject level, but the unit of analysis is the eye. If the eye is used as the unit of analysis, then a modification to the usual Wilcoxon rank sum variance formula must be made to account for the within-cluster dependence. For some clustered data designs, where the unit of analysis is the subunit, group membership may be defined at the subunit level. For example, in some randomized ophthalmologic clinical trials, different treatments may be applied to fellow eyes of some patients, while the same treatment may be applied to fellow eyes of other patients. In general, binary eye-specific covariates may be present (scored as exposed or unexposed) and one wishes to compare nonnormally distributed outcomes between exposed and unexposed eyes using the Wilcoxon rank sum test while accounting for the clustering. In this article, we present a corrected variance formula for the Wilcoxon rank sum statistic in the setting of eye (subunit)-specific covariates. We apply it to compare ocular itching scores in ocular allergy patients between eyes treated with active versus placebo eye drops, where some patients receive the same eye drop in both eyes, while other patients receive different eye drops in fellow eyes. We also present comparisons between the clustered Wilcoxon test and each of the signed rank tests and mixed model approaches and show dramatic differences in power in favor of the clustered Wilcoxon test for some designs.-Extension of the rank sum test for clustered data: two-group comparisons with group membership defined at the subunit level.",1
"The early stages of a systematic review set the scope and expectations. This can be particularly challenging for complex interventions given their multidimensional and dynamic nature. This paper builds on concepts introduced in paper 1 of this series. It describes the methodological, practical, and philosophical challenges and potential approaches for formulating the questions and scope of systematic reviews of complex interventions. Furthermore, it discusses the use of theory to help organize reviews of complex interventions. Many interventions in medicine, public health, education, social services, behavioral health, and community programs are complex, and they may not fit neatly within the established paradigm for reviews of straightforward interventions. This paper provides conceptual and operational guidance for these early stages of scope formulation to assist authors of systematic reviews of complex interventions.-AHRQ series on complex intervention systematic reviews-paper 2: defining complexity, formulating scope, and questions.",0
"There continues to be debate about what constitutes a pragmatic trial and how it is distinguished from more traditional explanatory trials. The NIH Pragmatic Trials Collaborative Project, which includes five trials and a coordinating unit, has adopted the Pragmatic-Explanatory Continuum Indicator Summary (PRECIS-2) instrument. The purpose of the study was to collect PRECIS-2 ratings at two points in time to assess whether the tool was sensitive to change in trial design, and to explore with investigators the rationale for rating shifts. A mixed-methods design included sequential collection and analysis of quantitative data (PRECIS-2 ratings) and qualitative data. Ratings were collected at two annual, in-person project meetings, and subsequent interviews conducted with investigators were recorded, transcribed, and coded using NVivo 11 Pro for Windows. Rating shifts were coded as either (1) actual change (reflects a change in procedure or protocol), (2) primarily a rating shift reflecting rater variability, or (3) themes that reflect important concepts about the tool and/or pragmatic trial design. Based on PRECIS-2 ratings, each trial was highly pragmatic at the planning phase and remained so 1?year later in the early phases of trial implementation. Over half of the 45 paired ratings for the nine PRECIS-2 domains indicated a rating change from Time 1 to Time 2 (N = 24, 53%). Of the 24 rating changes, only three represented a true change in the design of the trial. Analysis of rationales for rating shifts identified critical themes associated with the tool or pragmatic trial design more generally. Each trial contributed one or more relevant comments, with Eligibility, Flexibility of Adherence, and Follow-up each accounting for more than one. PRECIS-2 has proved useful for ""framing the conversation"" about trial design among members of the Pragmatic Trials Collaborative Project. Our findings suggest that design elements assessed by the PRECIS-2 tool may represent mostly stable decisions. Overall, there has been a positive response to using PRECIS-2 to guide conversations around trial design, and the project's focus on the use of the tool by this group of early adopters has provided valuable feedback to inform future trainings on the tool.-Framing the conversation: use of PRECIS-2 ratings to advance understanding of pragmatic trial design domains.",0
"In stepped wedge cluster randomized trials, intact clusters of individuals switch from control to intervention from a randomly assigned period onwards. Such trials are becoming increasingly popular in health services research. When a closed cohort is recruited from each cluster for longitudinal follow-up, proper sample size calculation should account for three distinct types of intraclass correlations: the within-period, the inter-period, and the within-individual correlations. Setting the latter two correlation parameters to be equal accommodates cross-sectional designs. We propose sample size procedures for continuous and binary responses within the framework of generalized estimating equations that employ a block exchangeable within-cluster correlation structure defined from the distinct correlation types. For continuous responses, we show that the intraclass correlations affect power only through two eigenvalues of the correlation matrix. We demonstrate that analytical power agrees well with simulated power for as few as eight clusters, when data are analyzed using bias-corrected estimating equations for the correlation parameters concurrently with a bias-corrected sandwich variance estimator.-Sample size determination for GEE analyses of stepped wedge cluster randomized trials.",3
"Most large clinical trials for human immunodeficiency virus (HIV) conducted at university medical centers require intensive real-time monitoring, with clinic visits at least every 4 to 8 weeks. Investigating a reduced frequency visit schedule will help determine whether the current amount of monitoring is needed to ensure subject safety. If we can show that subjects may visit the clinic less often than every 8 weeks and not miss important laboratory-related drug toxicities, then it may be feasible to conduct large studies with simpler designs without compromising the subjects' health. In a retrospective analysis, we examined 3385 study participants who were enrolled in one of four clinical trials conducted by the National Institute of Allergy and Infectious Disease AIDS Clinical Trials Group. Variables examined included age, sex, race, CD4 cell count, HIV antiretroviral use, and medications to treat or prevent selected HIV-associated opportunistic infections. Significantly more than the hypothesized 5% of clinic visits with at least one drug toxicity were missed when visits were either every 16 or 24 weeks instead of every 8 weeks (exact Poisson lower 95% confidence bounds = 9.7% and 9.2%, respectively). In both visit-skipping scenarios, there were no significant differences found in the rates of missed drug toxicities by sex or age. However, entry CD4 cell count, HIV antiretroviral use, and medications to treat or prevent two HIV-associated opportunistic infections significantly affected the expected mean number of missed drug toxicities. Study visits cannot be extended from every 8 weeks to every 16 or 24 weeks without potentially harming the subjects' health.-A reduced frequency visit schedule underreports adverse events that resulted in dose modifications or treatment discontinuations in HIV/AIDS clinical trials: ACTG DACS 207.",0
"The National Institutes of Mental Health's (NIMH) 1985 Treatment of Depression Collaborative Research Program (TDCRP) reported that imipramine hydrochloride with clinical management (IMI-CM) was significantly more beneficial than placebo with clinical management (PLA-CM) for individuals undergoing treatment for depression. Unfortunately, in analyzing the NIMH TDCRP data, researchers ignored the potential effect that psychiatrists have on patient outcomes, thereby assuming that psychiatrists are equally effective. However, this assumption has yet to be supported empirically. Therefore, the purpose of the current study is to examine psychiatrist effects in the NIMH TDCRP study and to compare the variation among psychiatrists to the variation between treatments. Data from 112 patients [IMI-CM (n = 57, 9 psychiatrists); PLA-CM (n = 55, 9 psychiatrists)] from the NIMH TDCRP study were reanalyzed using a multi-level model. The proportion of variance in the BDI scores due to medication was 3.4% (p &lt; .05), while the proportion of variance in BDI scores due to psychiatrists was 9.1% (p &lt; .05). The proportion of variance in the HAM-D scores due to medication was 5.9% (p &lt; .05), while the proportion of variance in HAM-D scores due to psychiatrist was 6.7% (p = .053). Therefore, the psychiatrist effects were greater than the treatment effects. In this study, both psychiatrists and treatments contributed to outcomes in the treatment of depression. However, given that psychiatrists were responsible for more of the variance in outcomes it can be concluded that effective treatment psychiatrists can, in fact, augment the effects of the active ingredients of anti-depressant medication as well as placebo.-Psychiatrist effects in the psychopharmacological treatment of depression",2
"Individually randomized treatments are often administered within a group setting. As a consequence, outcomes for treated individuals may be correlated due to provider effects, common experiences within the group, and/or informal processes of socialization. In contrast, it is often reasonable to regard outcomes for control participants as independent, given that these individuals are not placed into groups. Although this kind of design is common in intervention research, the statistical models applied to evaluate the treatment effects are usually inconsistent with the resulting data structure, potentially leading to biased inferences. This article presents an alternative model that explicitly accounts for the fact that only treated participants are grouped. In addition to providing a useful test of the overall treatment effect, this approach also permits one to formally determine the extent to which treatment effects vary over treatment groups and whether there is evidence that individuals within treatment groups become similar to one another. This strategy is demonstrated with data from the Reconnecting Youth program for high school students at risk of school failure and behavioral disorders.-Evaluating Group-Based Interventions When Control Participants Are Ungrouped.",2
"Cluster randomized controlled trials (cRCTs) are commonly used by clinical researchers. The advantages of cRCTs include preventing treatment contamination, enhancing administrative efficiency, convenience, external validity, ethical considerations, and likelihood of increased compliance by participants. However, when designing a cRCT, clinical researchers are faced with challenges, such as cluster units that may not have an equal number of participants within each. In this Technical Note, we discuss approaches for estimating the sample size, while taking into account unequal cluster sizes, and strategies for optimizing the design of cluster trials.-Sample size estimation for cluster randomized controlled trials.",1
"Randomized intervention trials in which the community is the unit of randomization are increasingly being used to evaluate the impact of public health interventions. In the design of community randomized trials (CRT), the power of the study is likely to be affected by two issues: the matching or stratification of communities, and the number and size of the communities to be randomized. Data from three East African community intervention trials, designed to evaluate the impact of interventions to reduce human immunodeficiency virus (HIV) incidence, are used to compare the efficiency of different trial designs. Compared with an unmatched design, stratification reduced the between-community variation in the Mwanza trial (from 0.51 to 0.24) and in the Masaka trial (from 0.38 to 0.28). The reduction was smaller in the Rakai trial where the selected communities were more homogeneous (from 0.15 to 0.11). For all trials, individual matching of communities produced estimates of between-community variation similar to those from the stratified designs. The linear association between HIV prevalence and incidence was strong in the Mwanza trial (correlation coefficient R = 0.83) and the Masaka trial (R = 0.83), but weak in the Rakai trial (R = 0.28). Unmatched study designs that use smaller communities tend to increase between-community variation, but reduce the design effect and improve study power. These empirical data suggest that selection of homogeneous communities, or stratification of communities prior to randomization, may improve the power of CRT.-The effects of alternative study designs on the power of community randomized trials: evidence from three studies of human immunodeficiency virus prevention in East Africa.",1
"Although HIV interventions and clinical trials increasingly report the use of mixed methods, studies have not reported on the process through which ethnographic or qualitative findings are incorporated into RCT designs. We conducted a community-based ethnography on social and structural factors that may affect the acceptance of and adherence to oral pre-exposure prophylaxis (PrEP) among Black men who have sex with men (BMSM). We then devised the treatment arm of an adherence clinical trial drawing on findings from the community-based ethnography. This article describes how ethnographic findings informed the RCT and identifies distilled themes and findings that could be included as part of an RCT. The enhanced intervention includes in-person support groups, online support groups, peer navigation, and text message reminders. By describing key process-related facilitators and barriers to conducting meaningful mixed methods research, we provide important insights for the practice of designing clinical trials for 'real-world' community settings.-Passing the baton: Community-based ethnography to design a randomized clinical trial on the effectiveness of oral pre-exposure prophylaxis for HIV prevention among Black men who have sex with men.",0
"Data collection and review were identified as major contributors to the cost of randomized clinical trials (RCTs). We proposed and assessed a novel alternative for long-term clinical trial follow-up based on the data captured through an accredited Cancer Registry (CR) that is part of the National Cancer Database (NCDB). Patients from Mayo Clinic, Rochester, enrolled in the North Central Cancer Treatment Group N934653 (COST) trial (98 patients) and the American College of Surgeons Oncology Group Z0030 trial (55 patients) were included in the study. Demographic, treatment, and long-term outcome data were compared between the hospital-based CR and the RCTs' databases. Concordances were used to estimate the agreement between two databases. Kaplan-Meier curves were plotted to examine the consistency of time-to-event long-term outcomes of the CR and RCT databases. High concordances (&gt;95%) were observed for most demographic and treatment variables between the CR data and RCT data. The vital status concordances were 100% and 94.5% between the CR and COST and Z0030 databases, respectively. Three discrepant death dates were observed, one in the COST trial and two in the Z0030 trial. The concordances of disease-free status between the CR and RCT databases were 99.0% and 87.3%, and 15 discrepant disease recurrence cases were identified: 4 for COST and 11 for Z0030. The analysis has been focused on patients from a single site, Mayo Clinic, Rochester, enrolled in two large RCT evaluating surgical treatments. The findings herein need to be confirmed in a broader setting, such as multi-center, multi-registry including nonsurgical trials. CR data were nearly identical to data from two randomized phase III trials in different disease types and conducted by two different cooperative groups. The NCDB Cancer Registries represent a feasible alternative for obtaining long-term follow-up data for large clinical trials.-Cancer registries: a novel alternative to long-term clinical trial follow-up based on results of a comparative study.",0
"The goals of phase II dose-response studies are to prove that the treatment is effective and to choose the dose for further development. Randomized designs with equal allocation to either a high dose and placebo or to each of several doses and placebo are typically used. However, in trials where response is observed relatively quickly, adaptive designs might offer an advantage over equal allocation. We propose an adaptive design for dose-response trials that concentrates the allocation of subjects in one or more areas of interest, for example, near a minimum clinically important effect level, or near some maximal effect level, and also allows for the possibility to stop the trial early if needed. The proposed adaptive design yields higher power to detect a dose-response relationship, higher power in comparison with placebo, and selects the correct dose more frequently compared with a corresponding randomized design with equal allocation to doses.-Adaptive dose finding based on t-statistic for dose-response trials.",0
"Group-randomized trials are characterised by the allocation of identifiable groups rather than individuals to study conditions; members within those groups are then observed to assess the effect of the intervention. It is convenient to categorize the designs employed in group-randomized trials along two dimensions, each with two levels. The first distinguishes between designs having just one or two time intervals and those having three or more intervals. The second distinguishes between nested cohort and nested cross-sectional designs. Following a brief review of the design and analytic issues common to group-randomized trials, and their general solutions, this paper presents the adaptations of the mixed-model analysis of covariance and random coefficients models that are required for the four combinations that result from this categorization scheme. The assumptions, strengths and weaknesses of each model are discussed.-Statistical models appropriate for designs often used in group-randomized trials.",1
"In a randomized controlled clinical trial that assesses treatment efficacy, a common objective is to assess the association of a measured biomarker response endpoint with the primary study endpoint in the active treatment group, using a case-cohort, case-control, or two-phase sampling design. Methods for power and sample size calculations for such biomarker association analyses typically do not account for the level of treatment efficacy, precluding interpretation of the biomarker association results in terms of biomarker effect modification of treatment efficacy, with detriment that the power calculations may tacitly and inadvertently assume that the treatment harms some study participants. We develop power and sample size methods accounting for this issue, and the methods also account for inter-individual variability of the biomarker that is not biologically relevant (e.g., due to technical measurement error). We focus on a binary study endpoint and on a biomarker subject to measurement error that is normally distributed or categorical with two or three levels. We illustrate the methods with preventive HIV vaccine efficacy trials and include an R package implementing the methods. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Power/sample size calculations for assessing correlates of risk in clinical efficacy trials.",0
"Stepped wedge trials (SWTs) are a type of cluster-randomized trial that are commonly used to evaluate health care interventions. Most SWT-related software packages have restrictive assumptions about the study design and correlation structure of the data. The objective of this paper is to present a package and corresponding web-based graphical user interface (GUI) that provide researchers with another, more flexible option for SWT design and analysis. We developed an Rpackage swCRTdesign ('stepped wedge Cluster Randomized Trial design'), which uses a random effects model to account for correlation in the data induced by a SWT design. Possible sources of correlation include clusters, time within clusters, and treatment within clusters. swCRTdesign allows a user to calculate power, simulate SWT data to streamline simulation studies (e.g. to estimate power), and create descriptive summaries and plots. Additionally, a GUI, developed using shiny, is available to calculate power and create power curves and design plots. The swCRTdesign package accommodates a wide variety of SWT designs, and makes it easy to account for some sources of correlation which are not found in other packages. The user-friendly web-based GUI makes some swCRTdesign features accessible to researchers not familiar with R. These two resources will make appropriately complex SWT calculations more accessible to scientists from a wide variety of backgrounds.-swCRTdesign: An RPackage for Stepped Wedge Trial Design and Analysis",3
"Individually-randomized psychotherapy trials are often partially nested. For instance, individuals assigned to a treatment arm may be clustered into therapy groups for purposes of treatment administration, whereas individuals assigned to a wait-list control are unclustered. The past several years have seen rapid expansion and investigation of methods for analyzing partially nested data. Yet partial nesting often remains ignored in psychotherapy trials. This review integrates and disseminates developments in the analysis of partially nested data that are particularly relevant for psychotherapy researchers. First, we differentiate among alternative partially nested designs. Then, we present adaptations of multilevel model specifications that accommodate each design. Next, we address how moderation by treatment as well as mediation of the treatment effect can be investigated in partially nested designs. Model fitting results, annotated software syntax, and illustrative data sets are provided and key methodological issues are discussed. We emphasize that cluster-level variability in the treatment arm need not be considered a nuisance; it can be modeled to yield insights about the treatment process.-Partially nested designs in psychotherapy trials: A review of modeling developments.",2
Marginal Methods for Incomplete Longitudinal Data Arising in Clusters,1
"In this paper, we give focus to cluster randomized trials, also known as group randomized trials, which randomize clusters, or groups, of subjects to different trial arms, such as intervention or control. Outcomes from subjects within the same cluster tend to exhibit an exchangeable correlation measured by the intra-cluster correlation coefficient (ICC). Our primary interest is to test if the intervention has an impact on the marginal mean of an outcome. Using recently developed methods, we propose how to select a working ICC structure with the goal of choosing the structure that results in the smallest standard errors for regression parameter estimates and thus the greatest power for this test. Specifically, we utilize small-sample corrections for the estimation of the covariance matrix of regression parameter estimates. This matrix is incorporated within correlation selection criteria proposed in the generalized estimating equations literature to choose one of multiple working ICC structures under consideration. We demonstrate the potential power and utility of this approach when used in cluster randomized trial settings via a simulation study and application example, and we discuss practical considerations for its use in practice. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Intra-cluster correlation selection for cluster randomized trials.",1
"Cluster randomized trials (CRTs) complicate the interpretation of standard research ethics guidelines for several reasons. For one, the units of allocation, intervention, and observation often may differ within a single trial. In the absence of tailored and internationally accepted ethics guidelines for CRTs, researchers and research ethics committees have no common standard by which to judge ethically appropriate practices in CRTs. Moreover, lack of familiarity with and consideration of the unique features of the CRT design by research ethics committees may cause difficulties in the research ethics review process, and amplify problems such as variability in the requirements and decisions reached by different research ethics committees. We aimed to characterize research ethics review of CRTs, examine investigator experiences with the ethics review process, and assess the need for ethics guidelines for CRTs. An electronic search strategy implemented in MEDLINE was used to identify and randomly sample 300 CRTs published in English language journals from 2000 to 2008. A web-based survey with closed- and open-ended questions was administered to corresponding authors in a series of six contacts. The survey response rate was 64%. Among 182 of 285 eligible respondents, 91% indicated that they had sought research ethics approval for the identified CRT, although only 70% respondents reported research ethics approval in the published article. Nearly one-third (31%) indicated that they have had to meet with ethics committees to explain aspects of their trials, nearly half (46%) experienced variability in the ethics review process in multijurisdictional trials, and 38% experienced negative impacts of the ethics review process on their trials, including delays in trial initiation (28%), increased costs (10%), compromised ability to recruit participants (16%), and compromised methodological quality (9%). Most respondents (74%; 95% confidence interval (CI): 67%-80%) agreed or strongly agreed that there is a need to develop ethics guidelines for CRTs, and (70%; 95% CI: 63%-77%) that ethics committees could be better informed about distinct ethical issues surrounding CRTs. Thirty-six percent of authors did not respond to the survey. Due to the absence of comparable results from a representative sample of authors of individually randomized trials, it is unclear to what extent the reported challenges result from the CRT design. CRT investigators are experiencing challenges in the research ethics review of their trials, including excessive delays, variability in process and outcome, and imposed requirements that can have negative consequences for study conduct. Investigators identified a clear need for ethics guidelines for CRTs and education of research ethics committees about distinct ethical issues in CRTs.-Challenges in the research ethics review of cluster randomized trials: international survey of investigators.",1
"The Childhood Cancer Survivor Study is a retrospective cohort study that was initiated to explore late effects of childhood cancer and its therapies. We evaluated the characteristics of those requiring tracing and factors that influenced tracing success. Medical record review identified 20,051 eligible individuals from 25 institutions in the United States and Canada. Of these, 13,021 had a current address in the medical record at the treating institution, and 7,030 had an incorrect address and required tracing by a commercial firm. Tracing was successful for 4,188 persons (60%). Younger age at contact, shorter length of time since last contact, having a middle initial available, an uncommon last name, and socioeconomic factors were found to predict successful tracing. Compared to those with a current address available in medical records, subjects successfully traced were less likely to have accessed health care during the previous 2 years; and more likely to be current smokers, obese, and to report moderate to severe impairments (pain, functional status, and activity). These findings provide an empirical basis concerning determinants and predictors of tracing success. If tracing had not been performed in this cohort, spurious associations may have been obtained for some health outcomes of interest.-Characteristics of childhood cancer survivors predicted their successful tracing.",0
"Despite widespread Internet adoption, online advertising remains an underutilized tool to recruit participants into clinical trials. Whether online advertising is a cost-effective method to enroll participants compared to other traditional forms of recruitment is not known. Recruitment for the Survivorship Promotion In Reducing IGF-1 Trial, a community-based study of cancer survivors, was conducted from June 2015 through December 2016 via in-person community fairs, advertisements in periodicals, and direct postal mailings. In addition, ""Right Column"" banner ads were purchased from Facebook to direct participants to the Survivorship Promotion In Reducing IGF-1 Trial website. Response rates, costs of traditional and online advertisements, and demographic data were determined and compared across different online and traditional recruitment strategies. Micro-trials optimizing features of online advertisements were also explored. Of the 406 respondents to our overall outreach efforts, 6% (24 of 406) were referred from online advertising. Facebook advertisements were shown over 3 million times (impressions) to 124,476 people, which resulted in 4401 clicks on our advertisement. Of these, 24 people ultimately contacted study staff, 6 underwent prescreening, and 4 enrolled in the study. The cost of online advertising per enrollee was $794 when targeting a general population versus $1426 when accounting for strategies that specifically targeted African Americans or men. By contrast, community fairs, direct mail, or periodicals cost $917, $799, or $436 per enrollee, respectively. Utilization of micro-trials to assess online ads identified subtleties (e.g. use of an advertisement title) that substantially impacted viewer interest in our trial. Online advertisements effectively directed a relevant population to our website, which resulted in new enrollees in the Survivorship Promotion In Reducing IGF-1 Trial at a cost comparable to traditional methods. Costs were substantially greater with online recruitment when targeting under-represented populations, however. Additional research using online micro-trial tools is needed to evaluate means of more precise recruitment to improve yields in under-represented groups. Potential gains from faster recruitment speed remain to be determined.-Use of online recruitment strategies in a randomized trial of cancer survivors.",0
"Sample size calculation for cluster randomized trials (CRTs) with a [Formula: see text] factorial design is complicated due to the combination of nesting (of individuals within clusters) with crossing (of two treatments). Typically, clusters and individuals are allocated across treatment conditions in a balanced fashion, which is optimal under homogeneity of variance. However, the variance is likely to be heterogeneous if there is a treatment effect. An unbalanced allocation is then more efficient, but impractical because the optimal allocation depends on the unknown variances. Focusing on CRTs with a [Formula: see text] design, this paper addresses two questions: How much efficiency is lost by having a balanced design when the outcome variance is heterogeneous? How large must the sample size be for a balanced allocation to have sufficient power under heterogeneity of variance? We consider different scenarios of heterogeneous variance. Within each scenario, we determine the relative efficiency of a balanced design, as a function of the level (cluster, individual, both) and amount of heterogeneity of the variance. We then provide a simple correction of the sample size for the loss of power due to heterogeneity of variance when a balanced allocation is used. The theory is illustrated with an example of a published 2 x2 CRT.-The effect of heterogeneous variance on efficiency and power of cluster randomized trials with a balanced 2???2 factorial design.",1
"In a clinical trial, we may randomize subjects (called clusters) to different treatments (called groups), and make observations from multiple sites (called units) of each subject. In this case, the observations within each subject could be dependent, whereas those from different subjects are independent. If the outcome of interest is the time to an event, we may use the standard rank tests proposed for independent survival data, such as the logrank and Wilcoxon tests, to test the equality of marginal survival distributions, but their standard error should be modified to accommodate the possible intracluster correlation. In this paper we propose a method of calculating the standard error of the rank tests for two-sample clustered survival data. The method is naturally extended to that for K-sample tests under dependence.-Rank tests for clustered survival data.",1
"Alcohol abuse, other substance use disorders, and risk behaviors associated with the human immunodeficiency virus (HIV) represent three of the top 10 modifiable causes of mortality in the US. Despite evidence that continuing care is effective in sustaining recovery from substance use disorders and associated behaviors, patients rarely receive it. Smartphone applications (apps) have been effective in delivering continuing care to patients almost anywhere and anytime. This study tests the effectiveness of two components of such apps: ongoing self-monitoring through Ecological Momentary Assessments (EMAs) and immediate recovery support through Ecological Momentary Interventions (EMIs). The target population, adults enrolled in substance use disorder treatment (n = 400), are being recruited from treatment centers in Chicago and randomly assigned to one of four conditions upon discharge in a 2 ? 2 factorial design. Participants receive (1) EMAs only, (2) EMIs only, (3) combined EMAs + EMIs, or (4) a control condition without EMA or EMI for 6?months. People in the experimental conditions receive smartphones with the apps (EMA and/or EMI) specific to their condition. Phones alert participants in the EMA and EMA + EMI conditions at five random times per day and present participants with questions about people, places, activities, and feelings that they experienced in the past 30?min and whether these factors make them want to use substances, support their recovery, or have no impact. Those in the EMI and EMA + EMI conditions have continual access to a suite of support services. In the EMA + EMI condition, participants are prompted to use the EMI(s) when responses to the EMA(s) indicate risk. All groups have access to recovery support as usual. The primary outcome is days of abstinence from alcohol and other drugs. Secondary outcomes are number of HIV risk behaviors and whether abstinence mediates the effects of EMA, EMI, or EMA + EMI on HIV risk behaviors. This project will enable the field to learn more about the effects of EMAs and EMIs on substance use disorders and HIV risk behaviors, an understanding that could potentially make treatment and recovery more effective and more widely accessible. ClinicalTrials.gov, ID: NCT02132481 . Registered on 5 May 2014.-Using smartphones to decrease substance use via self-monitoring and recovery support: study protocol for a randomized control trial.",0
"Gene expression profiles obtained from samples of diabetic and normal rats with and without treatments can be used to identify genes that distinguish normal and diabetic individuals and also to evaluate the effectiveness of drug treatments. This study examines changes in global gene expression in rat muscle caused by streptozotocin-induced diabetes and vanadyl sulfate treatment. We explored model-based and algorithm-based methods with gene screening measures for microarray gene expression data to classify and predict individuals with high risk of diabetes. Results show that the mixed ANOVA model-based approach provides an efficient way to conduct an investigation of the inherent variability in gene expression data and to estimate the effects of experimental factors such as treatments and diseases and their interactions. The algorithm-based weighted voting and neural network classifiers show good classification performance for the diabetes and treatment groups. Although neural network performs better than weighted voting with higher classification rate, the interpretation of weighted voting is more straightforward. The study indicates that the choice of the gene selection procedure is at least as important as the choice of the classification procedure. We conclude that both mixed model-based and algorithm-based approaches provide the statistical evidence of the biological hypotheses that vanadyl sulfate treatment of diabetic animals restores gene expression patterns to normal. Although model-based and algorithm-based methods provide different strengths and perspective for the analysis of the same set of data, in general both can be considered and developed for analyzing factorial design experiments with multiple groups and factors. This study represents a major step towards the discovery of responsible genes related to diabetes and its treatment.-Model-based or algorithm-based? Statistical evidence for diabetes and treatments using gene expression.",0
"The design and use of cluster randomised controlled trials in evaluating injury prevention interventions: part 2. Design effect, sample size calculations and methods for analysis.",1
The merits of matching in community intervention trials: a cautionary tale.,1
The clustered logrank test is a nonparametric method of significance testing for correlated survival data. Examples of its application include cluster randomized trials where groups of patients rather than individuals are randomized to either a treatment or a control intervention. We describe a SAS macro that implements the 2-sample clustered logrank test for data where the entire cluster is randomized to the same treatment group. We discuss the theory and applications behind this test as well as details of the SAS code.-A SAS macro for a clustered logrank test.,1
"Understanding secondary databases: a commentary on ""Sources of bias for health state characteristics in secondary databases"".",0
"This paper reviews many different estimators of intraclass correlation that have been proposed for binary data and compares them in an extensive simulation study. Some of the estimators are very specific, while others result from general methods such as pseudo-likelihood and extended quasi-likelihood estimation. The simulation study identifies several useful estimators, one of which does not seem to have been considered previously for binary data. Estimators based on extended quasi-likelihood are found to have a substantial bias in some circumstances.-Estimating intraclass correlation for binary data.",1
"Because pragmatic trials are performed to determine if an intervention can improve current practice, they often have a control group receiving 'usual care'. The behaviour of caregivers and patients in this control group should be influenced by the actions of researchers as little as possible. Guidelines for describing the composition and management of a usual care control group are lacking. To explore the variety of approaches to the usual care concept in pragmatic trials, and evaluate the influence of the study design on the behaviour of caregivers and patients in a usual care control group. Review of 73 pragmatic trials in primary care with a usual care control group published between January 2005 and December 2009 in the British Medical Journal, the British Journal of General Practice, and Family Practice. Outcome measures were: description of the factors influencing caregiver and patients in a usual care control group related to an individual randomised design versus cluster randomisation. In total, 38 individually randomised trials and 35 cluster randomised trials were included. In most trials, caregivers had the freedom to treat control patients according to their own insight; in two studies, treatment options were restricted. Although possible influences on the behaviour of control caregivers and control patients were more often identified in individually randomised trials, these influences were also present in cluster randomised trials. The description of instructions and information provided to the control group was often insufficient, which made evaluation of the trials difficult. Researchers in primary care medicine should carefully consider the design of a usual care control group, especially with regard to minimising the risk of study-induced behavioural change. It is recommended that an adequate description of the information is provided to control caregivers and control patients. A proposal is made for an extension to the CONSORT statement that requires authors to specify details of the usual care control group.-How usual is usual care in pragmatic intervention studies in primary care? An overview of recent trials.",1
Rethinking credible evidence synthesis.,0
"Generalized estimating equations (GEE) are commonly used for the analysis of correlated data. However, use of quadratic inference functions (QIFs) is becoming popular because it increases efficiency relative to GEE when the working covariance structure is misspecified. Although shown to be advantageous in the literature, the impacts of covariates and imbalanced cluster sizes on the estimation performance of the QIF method in finite samples have not been studied. This cluster size variation causes QIF's estimating equations and GEE to be in separate classes when an exchangeable correlation structure is implemented, causing QIF and GEE to be incomparable in terms of efficiency. When utilizing this structure and the number of clusters is not large, we discuss how covariates and cluster size imbalance can cause QIF, rather than GEE, to produce estimates with the larger variability. This occurrence is mainly due to the empirical nature of weighting QIF employs, rather than differences in estimating equations classes. We demonstrate QIF's lost estimation precision through simulation studies covering a variety of general cluster randomized trial scenarios and compare QIF and GEE in the analysis of data from a cluster randomized trial.-The effect of cluster size imbalance and covariates on the estimation performance of quadratic inference functions.",1
"The US lags behind &gt;120 countries in implementing graphic warning labels (GWLs) on cigarette packs. US courts prevented implementation of FDA's 2012 rule requiring GWLs citing the need for more evidence on effectiveness. After more research, in 2020, the FDA proposed a revised rule mandating GWLs. This trial will test how the introduction of GWLs influence cognitions and behavior in US smokers. To investigate the ""real-world"" impact of GWLs in US smokers, we are conducting a randomized trial involving a 3-month intervention and 8-month follow-up. The study recruited California smokers between September 2016 through December 2019 and randomly assigned them into 3 groups (1) Blank Pack devoid of any cigarette branding; (2) GWL Pack featuring 1 of 3 rotating images added to blank pack; or (3) their usual Standard US Pack. Throughout the 3-month intervention, participants purchased study-packaged cigarettes and reported daily cognitions and behavior through ecological momentary assessments. We will validate self-reported tobacco use with saliva cotinine concentrations following the 3-month intervention and 8-month follow-up. The trial enrolled 359 participants (average age 39?years; average cigarette consumption half a pack/day). The 3 study groups were balanced on age, gender, race-ethnicity, education and income (17% low income) as well as on smoking related variables. This 3-month real-world randomized trial will test the effect of repackaging cigarettes from standard US packs to GWL plain packs on smokers' perceptions of the risks of smoking, their perception of the appeal of their cigarettes, and on their smoking behavior.-Real-world exposure to graphic warning labels on cigarette packages in US smokers: The CASA randomized trial protocol.",0
"The statistical analysis for a 2-arm randomised controlled trial (RCT) with a baseline outcome followed by a few assessments at fixed follow-up times typically invokes traditional analytic methods (eg, analysis of covariance (ANCOVA), longitudinal data analysis (LDA)). 'Constrained' longitudinal data analysis (cLDA) is a well-established unconditional technique that constrains means of baseline to be equal between arms. We use an analysis of fasting lipid profiles from the Group Medical Clinics (GMC) longitudinal RCT on patients with diabetes to illustrate applications of ANCOVA, LDA and cLDA to demonstrate theoretical concepts of these methods including the impact of missing data. For the analysis of the illustrated example, all models were fit using linear mixed models to participants with only complete data and to participants using all available data. With complete data (n=195), 95% CI coverage are equivalent for ANCOVA and cLDA with an estimated 11.2 mg/dL (95% CI -19.2 to -3.3; p=0.006) lower mean low-density lipoprotein (LDL) cholesterol in GMC compared with usual care. With all available data (n=233), applying the cLDA model yielded an LDL improvement of 8.9 mg/dL (95% CI -16.7 to -1.0; p=0.03) for GMC compared with usual care. The less efficient, LDA analysis yielded an LDL improvement of 7.2 mg/dL (95% CI -17.2 to 2.8; p=0.15) for GMC compared with usual care. Under reasonable missing data assumptions, cLDA will yield efficient treatment effect estimates and robust inferential statistics. It may be regarded as the method of choice over ANCOVA and LDA.-To condition or not condition? Analysing 'change' in longitudinal randomised controlled trials.",1
"The Johns Hopkins Bloomberg School of Public Health has been engaged in public health research and practice in Washington County, Maryland, nearly since its inception a century ago. In 2005, the center housing this work was renamed the George W. Comstock Center for Public Health Research and Prevention to honor its pioneering leader. Principles that guided innovation and translation well in the past included: research synergies and opportunities for translation realized through longstanding connection with the community; integration of training with public health research; lifelong learning, mentorship, and teamwork; and efficiency through economies of scale. These principles are useful to consider as we face the challenges of improving the health of the population over the next 100 years.-The George W. Comstock Center for Public Health Research and Prevention: A Century of Collaboration, Innovation, and Translation.",0
"Pragmatic clinical trials (PCTs) seek to improve the generalizability and increase the statistical power of traditional explanatory trials. They are a major tenet of comparative effectiveness research. While a powerful study design, PCTs have been limited by high cost, modest efficiency, and limited ability to fill relevant evidence gaps. Based on an American Reinvestment and Recovery Act (ARRA) supported meeting of national stakeholders, we propose several innovations and future research that could improve the efficiency and effectiveness of such studies focused in the U.S. Innovations discussed include optimizing the use of community based practices through partnership with Practice Based Research Networks (PBRNs), using information technology to simplify PCT subject recruitment, consent and randomization processes, and utilizing linkages to large administrative databases, such as Medicare, as a mechanism to capture outcomes and other important PCT variables with lower subject and research team burden. Testing and adaptation of such innovations to PCT are anticipated to improve the public health value of these increasingly important studies.-Improving the efficiency and effectiveness of pragmatic clinical trials in older adults in the United States.",0
"Objectives. We assessed the protocols and system processes for colorectal cancer (CRC) screening at federally qualified health centers (FQHCs) in 4 midwestern states. Methods. We identified 49 FQHCs in 4 states. In January 2013, we mailed their medical directors a 49-item questionnaire about policies on CRC screening, use of electronic medical records, types of CRC screening recommended, clinic tracking systems, referrals for colonoscopy, and barriers to providing CRC. Results. Forty-four questionnaires (90%) were returned. Thirty-three of the respondents (75%) estimated the proportion of their patients up-to-date with CRC screening, with a mean of 35%. One major barrier to screening was inability to provide colonoscopy for patients with a positive fecal occult blood test (59%). The correlation of system strategies and estimated percentage of patients up-to-date with CRC screening was 0.43 (P = .01). Conclusions. CRC system strategies were associated with higher CRC screening rates. Implementing system strategies for CRC screening takes time and effort and is important to maintain, to help prevent, or to cure many cases of CRC, the second leading cause of cancer in the United States.-System Strategies for Colorectal Cancer Screening at Federally Qualified Health Centers.",0
"This article is part of a series of papers examining ethical issues in cluster randomized trials (CRTs) in health research. In the introductory paper in this series, Weijer and colleagues set out six areas of inquiry that must be addressed if the cluster trial is to be set on a firm ethical foundation. This paper addresses the third of the questions posed, namely, does clinical equipoise apply to CRTs in health research? The ethical principle of beneficence is the moral obligation not to harm needlessly and, when possible, to promote the welfare of research subjects. Two related ethical problems have been discussed in the CRT literature. First, are control groups that receive only usual care unduly disadvantaged? Second, when accumulating data suggests the superiority of one intervention in a trial, is there an ethical obligation to act?In individually randomized trials involving patients, similar questions are addressed by the concept of clinical equipoise, that is, the ethical requirement that, at the start of a trial, there be a state of honest, professional disagreement in the community of expert practitioners as to the preferred treatment. Since CRTs may not involve physician-researchers and patient-subjects, the applicability of clinical equipoise to CRTs is uncertain. Here we argue that clinical equipoise may be usefully grounded in a trust relationship between the state and research subjects, and, as a result, clinical equipoise is applicable to CRTs. Clinical equipoise is used to argue that control groups receiving only usual care are not disadvantaged so long as the evidence supporting the experimental and control interventions is such that experts would disagree as to which is preferred. Further, while data accumulating during the course of a CRT may favor one intervention over another, clinical equipoise supports continuing the trial until the results are likely to be broadly convincing, often coinciding with the planned completion of the trial. Finally, clinical equipoise provides research ethics committees with formal and procedural guidelines that form an important part of the assessment of the benefits and harms of CRTs in health research.-Does clinical equipoise apply to cluster randomized trials in health research?",1
"Subject attrition is a ubiquitous problem in any type of clinical trial and, thus, needs to be taken into consideration at the design stage particularly to secure adequate statistical power. Here, we focus on longitudinal cluster randomized clinical trials (cluster-RCT) that aim to test the hypothesis that an intervention has an effect on the rate of change in the outcome over time. In this setting, the cluster-RCT assumes a three-level hierarchical data structure in which subjects are nested within a higher level unit such as clinics and are evaluated for outcome repeatedly over the study period. Furthermore, the subject-specific slopes can be modeled in terms of fixed or random coefficients in a mixed-effects linear model. Closed-form sample size formulas for testing the preceding hypothesis have been developed under an assumption of no attrition. In this article, we propose closed-form approximate samples size determinations with anticipated attrition rates by modifying those existing sample size formulas. With extensive simulations, we examine performances of the modified formulas under three attrition mechanisms: attrition completely at random, attrition at random, and attrition not at random. In conclusion, the proposed modification is very effective under fixed-slope models but yields biased, perhaps substantially so, statistical power under random slope models.-Impact of subject attrition on sample size determinations for longitudinal cluster randomized clinical trials.",1
"Due to a shared environment and similarities among workers within a worksite, the strongest analytical design to evaluate the efficacy of an intervention to reduce occupational health or safety hazards is to randomly assign worksites, not workers, to the intervention and comparison conditions. Statistical methods are well described for estimating the sample size when the unit of assignment is a group but these methods have not been applied in the evaluation of occupational health and safety interventions. We review and apply the statistical methods for group-randomized trials in planning a study to evaluate the effectiveness of technical/behavioral interventions to reduce wood dust levels among small woodworking businesses. We conducted a pilot study in five small woodworking businesses to estimate variance components between and within worksites and between and within workers. In each worksite, 8 h time-weighted dust concentrations were obtained for each production employee on between two and five occasions. With these data, we estimated the parameters necessary to calculate the percent change in dust concentrations that we could detect (alpha = 0.05, power = 80%) for a range of worksites per condition, workers per worksite and repeat measurements per worker. The mean wood dust concentration across woodworking businesses was 4.53 mg/m3. The measure of similarity among workers within a woodworking business was large (intraclass correlation = 0.5086). Repeated measurements within a worker were weakly correlated (r = 0.1927) while repeated measurements within a worksite were strongly correlated (r = 0.8925). The dominant factor in the sample size calculation was the number of worksites per condition, with the number of workers per worksite playing a lesser role. We also observed that increasing the number of repeat measurements per person had little benefit given the low within-worker correlation in our data. We found that 30 worksites per condition and 10 workers per worksite would give us 80% power to detect a reduction of approximately 30% in wood dust levels (alpha = 0.05). Our results demonstrate the application of the group-randomized trials methodology to evaluate interventions to reduce occupational hazards. The methodology is widely applicable and not limited to the context of wood dust reduction.-Sample size considerations for studies of intervention efficacy in the occupational setting.",1
"Cluster randomized trials are an increasingly important methodological tool in health research. In cluster randomized trials, intact social units or groups of individuals, such as medical practices, schools, or entire communities--rather than individual themselves--are randomly allocated to intervention or control conditions, while outcomes are then observed on individual cluster members. The substantial methodological differences between cluster randomized trials and conventional randomized trials pose serious challenges to the current conceptual framework for research ethics. The ethical implications of randomizing groups rather than individuals are not addressed in current research ethics guidelines, nor have they even been thoroughly explored. The main objectives of this research are to: (1) identify ethical issues arising in cluster trials and learn how they are currently being addressed; (2) understand how ethics reviews of cluster trials are carried out in different countries (Canada, the USA and the UK); (3) elicit the views and experiences of trial participants and cluster representatives; (4) develop well-grounded guidelines for the ethical conduct and review of cluster trials by conducting an extensive ethical analysis and organizing a consensus process; (5) disseminate the guidelines to researchers, research ethics boards (REBs), journal editors, and research funders. We will use a mixed-methods (qualitative and quantitative) approach incorporating both empirical and conceptual work. Empirical work will include a systematic review of a random sample of published trials, a survey and in-depth interviews with trialists, a survey of REBs, and in-depth interviews and focus group discussions with trial participants and gatekeepers. The empirical work will inform the concurrent ethical analysis which will lead to a guidance document laying out principles, policy options, and rationale for proposed guidelines. An Expert Panel of researchers, ethicists, health lawyers, consumer advocates, REB members, and representatives from low-middle income countries will be appointed. A consensus conference will be convened and draft guidelines will be generated by the Panel; an e-consultation phase will then be launched to invite comments from the broader community of researchers, policy-makers, and the public before a final set of guidelines is generated by the Panel and widely disseminated by the research team.-Ethical and policy issues in cluster randomized trials: rationale and design of a mixed methods research study.",1
"The purpose of this article is to outline multilevel structural equation modeling (MSEM) for mediation analysis of longitudinal data. The introduction of mediating variables can improve experimental and nonexperimental studies of child growth in several ways as discussed throughout this article. Single-mediator individual-level and multilevel mediation models illustrate several current issues in the estimation of mediation with longitudinal data. The strengths of incorporating structural equation modeling (SEM) with multilevel mediation modeling are described. SUMMARY AND KEY MESSAGES: Longitudinal mediation models are pervasive in many areas of research including child growth. Longitudinal mediation models are ideally modeled as repeated measurements clustered within individuals. Further, the combination of MSEM and SEM provides an ideal approach for several reasons, including the ability to assess effects at different levels of analysis, incorporation of measurement error and possible random effects that vary across individuals.-Mediation from multilevel to structural equation modeling.",1
"Seasonal affective disorder (SAD) is a subtype of recurrent depression involving major depressive episodes during the fall and/or winter months that remit in the spring. The central public health challenge in the management of SAD is prevention of winter depression recurrence. Light therapy (LT) is the established and best available acute SAD treatment. However, long-term compliance with daily LT from first symptom through spontaneous springtime remission every fall/winter season is poor. Time-limited alternative treatments with effects that endure beyond the cessation of acute treatment are needed to prevent the annual recurrence of SAD. This is an NIMH-funded R01-level randomized clinical trial to test the efficacy of a novel, SAD-tailored cognitive-behavioral group therapy (CBT) against LT in a head-to-head comparison on next winter outcomes. This project is designed to test for a clinically meaningful difference between CBT and LT on depression recurrence in the next winter (the primary outcome). This is a concurrent two-arm study that will randomize 160 currently symptomatic community adults with major depression, recurrent with seasonal pattern, to CBT or LT. After 6 weeks of treatment in the initial winter, participants are followed in the subsequent summer, the next winter, and two winters later. Key methodological issues surround timing study procedures for a predictably recurrent and time-limited disorder with a focus on long-term outcomes. The chosen design answers the primary question of whether prior exposure to CBT is associated with a substantially lower likelihood of depression recurrence the next winter than LT. This design does not test the relative contributions of the cognitive-behavioral treatment components vs. nonspecific factors to CBT's outcomes and is not adequately powered to test for differences or equivalence between cells at treatment endpoint. Alternative designs addressing these limitations would have required more patients, increased costs, and reduced power to detect a difference in the primary outcome. Clinicaltrials.gov identifier NCT01714050.-Cognitive-behavioral therapy vs. light therapy for preventing winter depression recurrence: study protocol for a randomized controlled trial.",0
"Objectives?To assess self reported outcomes and adverse events after self sourced medical abortion through online telemedicine.Design?Population based study.Setting?Republic of Ireland and Northern Ireland, where abortion is unavailable through the formal healthcare system except in a few restricted circumstances.Population?1000 women who underwent self sourced medical abortion through Women on Web (WoW), an online telemedicine service, between 1 January 2010 and 31 December 2012.Main outcome measures?Successful medical abortion: the proportion of women who reported ending their pregnancy without surgical intervention. Rates of adverse events: the proportion who reported treatment for adverse events, including receipt of antibiotics and blood transfusion, and deaths reported by family members, friends, or the authorities. Care seeking for symptoms of potential complications: the frequency with which women reported experiencing symptoms of a potentially serious complication and the proportion who reported seeking medical attention as advised.Results?In 2010-12, abortion medications (mifepristone and misoprostol) were sent to 1636 women and follow-up information was obtained for 1158 (71%). Among these, 1023 women confirmed use of the medications, and follow-up information was available for 1000. At the time women requested help from WoW, 781 (78%) were &lt;7 weeks pregnant and 219 (22%) were 7-9 weeks pregnant. Overall, 94.7% (95% confidence interval 93.1% to 96.0%) reported successfully ending their pregnancy without surgical intervention. Seven women (0.7%, 0.3% to 1.5%) reported receiving a blood transfusion, and 26 (2.6%, 1.7% to 3.8%) reported receiving antibiotics (route of administration (IV or oral) could not be determined). No deaths resulting from the intervention were reported by family, friends, the authorities, or the media. Ninety three women (9.3%, 7.6% to 11.3%) reported experiencing any symptom for which they were advised to seek medical advice, and, of these, 87 (95%, 87.8% to 98.2%) sought attention. None of the five women who did not seek medical attention reported experiencing an adverse outcome.Conclusions?Self sourced medical abortion using online telemedicine can be highly effective, and outcomes compare favourably with in clinic protocols. Reported rates of adverse events are low. Women are able to self identify the symptoms of potentially serious complications, and most report seeking medical attention when advised. Results have important implications for women worldwide living in areas where access to abortion is restricted.-Self reported outcomes and adverse events after medical abortion through online telemedicine: population based study in the Republic of Ireland and Northern Ireland.",0
"Overdispersion and structural zeros are two major manifestations of departure from the Poisson assumption when modeling count responses using Poisson log-linear regression. As noted in a large body of literature, ignoring such departures could yield bias and lead to wrong conclusions. Different approaches have been developed to tackle these two major problems. In this paper, we review available methods for dealing with overdispersion and structural zeros within a longitudinal data setting and propose a distribution-free modeling approach to address the limitations of these methods by utilizing a new class of functional response models. We illustrate our approach with both simulated and real study data.-Distribution-free models for longitudinal count responses with overdispersion and structural zeros.",0
"While women are under-represented in research on cardiovascular disease (CVD), little is known about the attitudes of men and women with CVD regarding participation in clinical research studies/clinical trials. Patients with CVD (and/or risk factors) and patients with other chronic conditions from Iowa were recruited from a commercial panel. An on-line survey assessed willingness to participate (WTP) and other attitudes towards aspects of clinical research studies. Based on 504 respondents, there were no differences in WTP in patients with CVD compared to patients with other chronic diseases. Across all respondents, men had 14% lower WTP (relative risk (RR) for men, 0.86, 95% CI, 0.72-1.02). Among patients with CVD, there was no significant difference in WTP between women (RR for women = 1) and men (RR for men, 0.96, 95% CI, 0.82-1.14). There were no significant differences based on sex or CVD status for attitudes on randomization, blinding, side effects, conflict of interest, experimental treatments or willingness to talk to one's physician. Women had more favorable attitudes about participants being treated like ""guinea pigs"" (RR for men, 0.84, 95% CI, 0.73-0.98) and clinical trials being associated with terminally ill patients (RR for men, 0.93, 95% CI, 0.86-1.00). The findings reported here suggest that the observed lower levels of participation by women are due to factors other than a lower WTP or to women having more negative attitudes towards aspects of study participation. Patients with CVD have similar attitudes and WTP as patients with other chronic conditions.-Sex and cardiovascular disease status differences in attitudes and willingness to participate in clinical research studies/clinical trials.",0
"In individually randomised trials we might expect interventions delivered in groups or by care providers to result in clustering of outcomes for participants treated in the same group or by the same care provider. In partially nested randomised controlled trials (pnRCTs) this clustering only occurs in one trial arm, commonly the intervention arm. It is important to measure and account for between-cluster variability in trial design and analysis. We compare analysis approaches for pnRCTs with continuous outcomes, investigating the impact on statistical inference of cluster sizes, coding of the non-clustered arm, intracluster correlation coefficient (ICCs), and differential variance between intervention and control arm, and provide recommendations for analysis. We performed a simulation study assessing the performance of six analysis approaches for a two-arm pnRCT with a continuous outcome. These include: linear regression model; fully clustered mixed-effects model with singleton clusters in control arm; fully clustered mixed-effects model with one large cluster in control arm; fully clustered mixed-effects model with pseudo clusters in control arm; partially nested homoscedastic mixed effects model, and partially nested heteroscedastic mixed effects model. We varied the cluster size, number of clusters, ICC, and individual variance between the two trial arms. All models provided unbiased intervention effect estimates. In the partially nested mixed-effects models, methods for classifying the non-clustered control arm had negligible impact. Failure to account for even small ICCs resulted in inflated Type I error rates and over-coverage of confidence intervals. Fully clustered mixed effects models provided poor control of the Type I error rates and biased ICC estimates. The heteroscedastic partially nested mixed-effects model maintained relatively good control of Type I error rates, unbiased ICC estimation, and did not noticeably reduce power even with homoscedastic individual variances across arms. In general, we recommend the use of a heteroscedastic partially nested mixed-effects model, which models the clustering in only one arm, for continuous outcomes similar to those generated under the scenarios of our simulations study. However, with few clusters (3-6), small cluster sizes (5-10), and small ICC (?0.05) this model underestimates Type I error rates and there is no optimal model.-Appropriate statistical methods for analysing partially nested randomised controlled trials with continuous outcomes: a simulation study.",2
"Use of multiple period, cluster randomised, crossover trial designs for comparative effectiveness research",1
"Few studies have assessed the financial impact of cancer diagnosis on patients and caregivers in diverse clinical settings. S1417CD, led by the SWOG Cancer Research Network, is the first prospective longitudinal cohort study assessing financial outcomes conducted in the NCI Community Oncology Research Program (NCORP). We report our experience navigating design and implementation barriers. Patients age???18 within 120?days of metastatic colorectal cancer diagnosis were considered eligible and invited to identify a caregiver to participate in an optional substudy. Measures include 1) patient and caregiver surveys assessing financial status, caregiver burden, and quality of life and 2) patient credit reports obtained from the credit agency TransUnion through a linkage requiring social security numbers and secure data transfer processes. The primary endpoint is incidence of treatment-related financial hardship, defined as one or more of the following: debt accrual, selling or refinancing home, ?20% income decline, or borrowing money. Accrual goal was n?=?374 patients in 3?years. S1417CD activated on Apr 1, 2016 and closed on Feb 1, 2019 after reaching its accrual goal sooner than anticipated. A total of 380 patients (median age 59.7?years) and 155 caregivers enrolled across 548 clinical sites. Credit data were not obtainable for 76 (20%) patients due to early death, lack of credit, or inability to match records. Robust accrual to S1417CD demonstrates patients' and caregivers' willingness to improve understanding of financial toxicity despite perceived barriers such as embarrassment and fears that disclosing financial status could influence treatment recommendations.-Design, data linkage, and implementation considerations in the first cooperative group led study assessing financial outcomes in cancer patients and their informal caregivers.",0
Is there a weekend effect in obstetrics?,0
"Quantifying socioeconomic disparities and understanding the roots of inequalities are growing topics in cancer research. However, socioeconomic differences are challenging to investigate mainly due to the lack of accurate data at individual-level, while aggregate indicators are only partially informative. We implemented a multiple imputation algorithm within a statistical matching framework that combines diverse sources of data to estimate individual-level associations between income and risk of breast and lung cancer, adjusting for potential confounding factors in Italy. The framework is computationally flexible and can be adapted to similar contexts.-Combining individual and aggregated data to investigate the role of socioeconomic disparities on cancer burden in Italy.",0
"Cluster randomized trials (CRTs) involve the random assignment of intact social units rather than independent subjects to intervention groups. Time-to-event outcomes often are endpoints in CRTs. Analyses of such data need to account for the correlation among cluster members. The intracluster correlation coefficient (ICC) is used to assess the similarity among binary and continuous outcomes that belong to the same cluster. However, estimating the ICC in CRTs with time-to-event outcomes is a challenge because of the presence of censored observations. The literature suggests that the ICC may be estimated using either censoring indicators or observed event times. A simulation study explores the effect of administrative censoring on estimating the ICC. Results show that ICC estimators derived from censoring indicators or observed event times are negatively biased. Analytic work further supports these results. Observed event times are preferred to estimate the ICC under minimum frequency of administrative censoring. To our knowledge, the existing literature provides no practical guidance on the estimation of ICC when substantial amount of administrative censoring is present. The results from this study corroborate the need for further methodological research on estimating the ICC for correlated time-to-event outcomes. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-On the estimation of intracluster correlation for time-to-event outcomes in cluster randomized trials.",1
"We explore the potential of Bayesian hierarchical modelling for the analysis of cluster randomized trials with binary outcome data, and apply the methods to a trial randomized by general practice. An approximate relationship is derived between the intracluster correlation coefficient (ICC) and the between-cluster variance used in a hierarchical logistic regression model. By constructing an informative prior for the ICC on the basis of available information, we are thus able implicitly to specify an informative prior for the between-cluster variance. The approach also provides us with a credible interval for the ICC for binary outcome data. Several approaches to constructing informative priors from empirical ICC values are described. We investigate the sensitivity of results to the prior specified and find that the estimate of intervention effect changes very little in this data set, while its interval estimate is more sensitive. The Bayesian approach allows us to assume distributions other than normality for the random effects used to model the clustering. This enables us to gain insight into the robustness of our parameter estimates to the classical normality assumption. In a model with a more complex variance structure, Bayesian methods can provide credible intervals for a difference between two variance components, in order for example to investigate whether the effect of intervention varies across clusters. We compare our results with those obtained from classical estimation, discuss the relative merits of the Bayesian framework, and conclude that the flexibility of the Bayesian approach offers some substantial advantages, although selection of prior distributions is not straightforward.-Bayesian methods of analysis for cluster randomized trials with binary outcome data.",1
"The outbreak of Ebola virus disease in West Africa is the largest ever recorded. Numerous treatment alternatives for Ebola have been considered, including widely available repurposed drugs, but initiation of enrollment into clinical trials has been limited. The proposed trial is an adaptive platform design. Multiple agents and combinations will be investigated simultaneously. Additionally, new agents may enter the trial as they become available, and failing agents may be removed. In order to accommodate the many possible agents and combinations, a critical feature of this design is the use of response adaptive randomization to assign treatment regimens. As the trial progresses, the randomization ratio evolves to favor the arms that are performing better, making the design also suitable for all-cause pandemic preparedness planning. The study was approved by US and Sierra Leone ethics committees, and reviewed by the US Food and Drug Administration. Additionally, data management, drug supply lines, and local sites were prepared. However, in response to the declining epidemic seen in February 2015, the trial was not initiated. Sierra Leone remains ready to rapidly activate the protocol as an emergency response trial in the event of a resurgence of Ebola. (ClinicalTrials.gov Identifier: NCT02380625.) In summary, we have designed a single controlled trial capable of efficiently identifying highly effective or failing regimens among a rapidly evolving list of proposed therapeutic alternatives for Ebola virus disease and to treat the patients within the trial effectively based on accruing data. Provision of these regimens, if found safe and effective, would have a major impact on future epidemics by providing effective treatment options.-A response adaptive randomization platform trial for efficient evaluation of Ebola virus treatments: A model for pandemic response.",0
"The rapid and continuing progress in gene discovery for complex diseases is fuelling interest in the potential application of genetic risk models for clinical and public health practice. The number of studies assessing the predictive ability is steadily increasing, but they vary widely in completeness of reporting and apparent quality. Transparent reporting of the strengths and weaknesses of these studies is important to facilitate the accumulation of evidence on genetic risk prediction. A multidisciplinary workshop sponsored by the Human Genome Epidemiology Network developed a checklist of 25 items recommended for strengthening the reporting of Genetic RIsk Prediction Studies (GRIPS), building on the principles established by prior reporting guidelines. These recommendations aim to enhance the transparency, quality and completeness of study reporting, and thereby to improve the synthesis and application of information from multiple studies that might differ in design, conduct or analysis.-Strengthening the reporting of Genetic RIsk Prediction Studies (GRIPS): explanation and elaboration.",0
Comment on Statistical Power with Group Mean as the Unit of Analysis,1
"Generalized estimating equations (GEE) are an extension of generalized linear models (GLM) in that they allow adjusting for correlations between observations. A major strength of GEE is that they do not require the correct specification of the multivariate distribution but only of the mean structure. Several concerns have been raised about the validity of GEE when applied to dichotomous dependent variables. In this contribution, we summarize the theoretical findings concerning efficiency and validity of GEE. We introduce the GEE in a formal way, summarize general findings on the choice of the working correlation matrix, and show the existence of a dilemma for the optimal choice of the working correlation matrix for dichotomous dependent variables. Biological and statistical arguments for choosing a specific working correlation matrix are given. Three approaches are described for overcoming the range restriction of the correlation coefficient. The three approaches described in this article for overcoming the range restrictions for dichotomous dependent variables in GEE models provide a simple and practical way for use in applications.-Generalized estimating equations. Notes on the choice of the working correlation matrix.",1
"Systematic reviews can include cluster-randomised controlled trials (C-RCTs), which require different analysis compared with standard individual-randomised controlled trials. However, it is not known whether review authors follow the methodological and reporting guidance when including these trials. The aim of this study was to assess the methodological and reporting practice of Cochrane reviews that included C-RCTs against criteria developed from existing guidance. Criteria were developed, based on methodological literature and personal experience supervising review production and quality. Criteria were grouped into four themes: identifying, reporting, assessing risk of bias, and analysing C-RCTs. The Cochrane Database of Systematic Reviews was searched (2nd December 2013), and the 50 most recent reviews that included C-RCTs were retrieved. Each review was then assessed using the criteria. The 50 reviews we identified were published by 26 Cochrane Review Groups between June 2013 and November 2013. For identifying C-RCTs, only 56% identified that C-RCTs were eligible for inclusion in the review in the eligibility criteria. For reporting C-RCTs, only eight (24%) of the 33 reviews reported the method of cluster adjustment for their included C-RCTs. For assessing risk of bias, only one review assessed all five C-RCT-specific risk-of-bias criteria. For analysing C-RCTs, of the 27 reviews that presented unadjusted data, only nine (33%) provided a warning that confidence intervals may be artificially narrow. Of the 34 reviews that reported data from unadjusted C-RCTs, only 13 (38%) excluded the unadjusted results from the meta-analyses. The methodological and reporting practices in Cochrane reviews incorporating C-RCTs could be greatly improved, particularly with regard to analyses. Criteria developed as part of the current study could be used by review authors or editors to identify errors and improve the quality of published systematic reviews incorporating C-RCTs.-Cluster Randomised Trials in Cochrane Reviews: Evaluation of Methodological and Reporting Practice.",1
"The analysis of multiple outcomes is becoming increasingly common in modern biomedical studies. It is well-known that joint statistical models for multiple outcomes are more flexible and more powerful than fitting a separate model for each outcome; they yield more powerful tests of exposure or treatment effects by taking into account the dependence among outcomes and pooling evidence across outcomes. It is, however, unlikely that all outcomes are related to the same subset of covariates. Therefore, there is interest in identifying exposures or treatments associated with particular outcomes, which we term outcome-specific variable selection. In this work, we propose a variable selection approach for multivariate normal responses that incorporates not only information on the mean model, but also information on the variance-covariance structure of the outcomes. The approach effectively leverages evidence from all correlated outcomes to estimate the effect of a particular covariate on a given outcome. To implement this strategy, we develop a Bayesian method that builds a multivariate prior for the variable selection indicators based on the variance-covariance of the outcomes. We show via simulation that the proposed variable selection strategy can boost power to detect subtle effects without increasing the probability of false discoveries. We apply the approach to the Normative Aging Study (NAS) epigenetic data and identify a subset of five genes in the asthma pathway for which gene-specific DNA methylations are associated with exposures to either black carbon, a marker of traffic pollution, or sulfate, a marker of particles generated by power plants.-Multivariate Bayesian variable selection exploiting dependence structure among outcomes: Application to air pollution effects on DNA methylation.",0
Response to Is the R coefficient of interest in cluster randomized trials with a binary outcome?,1
"An emerging approach to public health emergency preparedness and response, community resilience encompasses individual preparedness as well as establishing a supportive social context in communities to withstand and recover from disasters. We examine why building community resilience has become a key component of national policy across multiple federal agencies and discuss the core principles embodied in community resilience theory-specifically, the focus on incorporating equity and social justice considerations in preparedness planning and response. We also examine the challenges of integrating community resilience with traditional public health practices and the importance of developing metrics for evaluation and strategic planning purposes. Using the example of the Los Angeles County Community Disaster Resilience Project, we discuss our experience and perspective from a large urban county to better understand how to implement a community resilience framework in public health practice.-Building community disaster resilience: perspectives from a large urban county department of public health.",0
"Many investigators rely on previously published point estimates of the intraclass correlation coefficient rather than on their associated confidence intervals to determine the required size of a newly planned cluster randomized trial. Although confidence interval methods for the intraclass correlation coefficient that can be applied to community-based trials have been developed for a continuous outcome variable, fewer methods exist for a binary outcome variable. The aim of this study is to evaluate confidence interval methods for the intraclass correlation coefficient applied to binary outcomes in community intervention trials enrolling a small number of large clusters. Existing methods for confidence interval construction are examined and compared to a new ad hoc approach based on dividing clusters into a large number of smaller sub-clusters and subsequently applying existing methods to the resulting data. Monte Carlo simulation is used to assess the width and coverage of confidence intervals for the intraclass correlation coefficient based on Smith's large sample approximation of the standard error of the one-way analysis of variance estimator, an inverted modified Wald test for the Fleiss-Cuzick estimator, and intervals constructed using a bootstrap-t applied to a variance-stabilizing transformation of the intraclass correlation coefficient estimate. In addition, a new approach is applied in which clusters are randomly divided into a large number of smaller sub-clusters with the same methods applied to these data (with the exception of the bootstrap-t interval, which assumes large cluster sizes). These methods are also applied to a cluster randomized trial on adolescent tobacco use for illustration. When applied to a binary outcome variable in a small number of large clusters, existing confidence interval methods for the intraclass correlation coefficient provide poor coverage. However, confidence intervals constructed using the new approach combined with Smith's method provide nominal or close to nominal coverage when the intraclass correlation coefficient is small (&lt;0.05), as is the case in most community intervention trials. This study concludes that when a binary outcome variable is measured in a small number of large clusters, confidence intervals for the intraclass correlation coefficient may be constructed by dividing existing clusters into sub-clusters (e.g. groups of 5) and using Smith's method. The resulting confidence intervals provide nominal or close to nominal coverage across a wide range of parameters when the intraclass correlation coefficient is small (&lt;0.05). Application of this method should provide investigators with a better understanding of the uncertainty associated with a point estimator of the intraclass correlation coefficient used for determining the sample size needed for a newly designed community-based trial.-A comparison of confidence interval methods for the intraclass correlation coefficient in community-based cluster randomization trials with a binary outcome.",1
We obtain the asymptotic sample variance of the intraclass kappa statistic for multinomial outcome data. A modified Wald type procedure based on this theory is then used for confidence interval construction. The results of a simulation study show that the proposed non-iterative approach performs very well in terms of confidence interval coverage and width for samples as small as 50. The procedure is illustrated with two examples from previously published medical studies.-A non-iterative confidence interval estimating procedure for the intraclass kappa statistic with multinomial outcomes.,1
"All agree that informed consent is a process, but past research has focused content analyses on post-consent or on one conversation in the consent series. Our aim was to identify and describe the content of different types of consent conversations. We conducted a secondary analysis of 38 adult oncology phase 1 consent conversations, which were audio-recorded, transcribed, coded, and qualitatively analyzed for type and content. Four types of consent conversations were identified: (1) priming, (2) patient-centered options, (3) trial centered, and (4) decision made. The analysis provided a robust description of the content discussed in each type of conversation. Two themes, supportive care and prognosis, were rarely mentioned. Four themes clustered in the patient-centered (type 2) conversations: affirmation of honesty, comfort, progression, and offer of supportive care. We identified and described four types of consent conversations. Our novel findings include (1) four different types of conversations with one (priming) not mentioned before and (2) a change of focus from describing the content of one phase 1 consent conversation to describing the content of different types. These in-depth descriptions provide the foundation for future research to determine whether the four types of conversations occur in sequence, thus describing the structure of the consent process and providing the basis for coaching interventions to alert physicians to the appropriate content for each type of conversation. A switch from a focus on one conversation to the types of conversations in the process may better align the consent conversations with the iterative process of shared decision making.-Description of the types and content of phase 1 clinical trial consent conversations in practice.",0
"In cluster randomised trials, randomisation increases internal study validity. If enough clusters are randomised, an unadjusted analysis should be unbiased. If a smaller number of clusters are included, stratified or matched randomisation can increase comparability between trial arms. In addition, an adjusted analysis may be required; nevertheless, randomisation removes the possibility for systematically biased allocation and increases transparency. In stepped wedge trials, clusters are randomised to receive an intervention at different start times ('steps'), and all clusters eventually receive it. In a recent study protocol for a 'modified stepped wedge trial', the investigators considered randomisation of the clusters (hospital wards), but decided against it for ethical and logistical reasons, and under the assumption that it would not add much to the rigour of the evaluation. We show that the benefits of randomisation for cluster randomised trials also apply to stepped wedge trials. The biggest additional issue for stepped wedge trials in relation to parallel cluster randomised trials is the need to control for secular trends in the outcome. Analysis of stepped wedge trials can in theory be based on 'horizontal' or 'vertical' comparisons. Horizontal comparisons are based on measurements taken before and after the intervention is introduced in each cluster, and are unbiased if there are no secular trends. Vertical comparisons are based on outcome measurements from clusters that have switched to the intervention condition and those from clusters that have yet to switch, and are unbiased under randomisation since at any time point, which clusters are in intervention and control conditions will have been determined at random. Secular outcome trends are a possibility in many settings. Many stepped wedge trials are analysed with a mixed model, including a random effect for cluster and fixed effects for time period to account for secular trends, thereby combining both vertical and horizontal comparisons of intervention and control clusters. The importance of randomisation in a stepped wedge trial is that the effects of time can be estimated from the data, and bias from secular trends that would otherwise arise can be controlled for, provided the trends are correctly specified in the model.-How important is randomisation in a stepped wedge trial?",3
"This article assesses whether there are methodological problems with child outcome measures that may contribute to the small associations between child care quality and child outcomes found in the literature. Outcome measures used in 65 studies of child care quality published between 1979 and December 2005 were examined, taking the previous review by Vandell and Wolfe (2000) as the starting point. Serious methodological problems were not pervasive for child outcome measures. However, methodological concerns were most prevalent among measures of socioemotional development. Furthermore, psychometric information on outcome measures was often missing from published reports, and health outcomes and approaches to learning were infrequently studied. Future research should address alignment issues between aspects of quality and the specific child outcomes chosen for study.-Child outcome measures in the study of child care quality.",0
"In analysis of binary data from clustered and longitudinal studies, random effect models have been recently developed to accommodate two-level problems such as subjects nested within clusters or repeated classifications within subjects. Unfortunately, these models cannot be applied to three-level problems that occur frequently in practice. For example, multicenter longitudinal clinical trials involve repeated assessments within individuals and individuals are nested within study centers. This combination of clustered and longitudinal data represents the classic three-level problem in biometry. Similarly, in prevention studies, various educational programs designed to minimize risk taking behavior (e.g., smoking prevention and cessation) may be compared where randomization to various design conditions is at the level of the school and the intervention is performed at the level of the classroom. Previous statistical approaches to the three-level problem for binary response data have either ignored one level of nesting, treated it as a fixed effect, or used first- and second-order Taylor series expansions of the logarithm of the conditional likelihood to linearize these models and estimate model parameters using more conventional procedures for measurement data. Recent studies indicate that these approximate solutions exhibit considerable bias and provide little advantage over use of traditional logistic regression analysis ignoring the hierarchical structure. In this paper, we generalize earlier results for two-level random effects probit and logistic regression models to the three-level case. Parameter estimation is based on full-information maximum marginal likelihood estimation (MMLE) using numerical quadrature to approximate the multiple random effects. The model is illustrated using data from 135 classrooms from 28 schools on the effects of two smoking cessation interventions.-Random effects probit and logistic regression models for three-level data.",1
"Community trials remain the only design appropriate for the evaluation of lifestyle interventions that cannot be allocated to individuals. The Minnesota Heart Health Program, conducted in Minnesota and the Dakotas between 1980 and 1993, is one of the largest community trials ever conducted in the United States. That study suggests several lessons that should guide future community trials. Planners should 1) carefully assess the secular trends for their outcomes and be confident that they can demonstrate an intervention effect against those trends; 2) be confident that they have effective programs than can be delivered to a sufficiently large fraction of their target population; 3) avoid differences between study conditions in levels and trends for their outcomes through random allocation of a sufficient number of communities to each condition; 4) develop good estimates of community-level standard errors prior to launching future trials; and 5) take steps to ensure that power will be sufficient to test the hypotheses of interest.-Design and analysis of community trials: lessons from the Minnesota Heart Health Program.",1
"To assess the cost effectiveness of including preadolescent boys in a routine human papillomavirus (HPV) vaccination programme for preadolescent girls. Cost effectiveness analysis from the societal perspective. United States. Girls and boys aged 12 years. HPV vaccination of girls alone and of girls and boys in the context of screening for cervical cancer. Main outcome measure Incremental cost effectiveness ratios, expressed as cost per quality adjusted life year (QALY) gained. With 75% vaccination coverage and an assumption of complete, lifelong vaccine efficacy, routine HPV vaccination of 12 year old girls was consistently less than $50,000 per QALY gained compared with screening alone. Including preadolescent boys in a routine vaccination programme for preadolescent girls resulted in higher costs and benefits and generally had cost effectiveness ratios that exceeded $100,000 per QALY across a range of HPV related outcomes, scenarios for cervical cancer screening, and assumptions of vaccine efficacy and duration. Vaccinating both girls and boys fell below a willingness to pay threshold of $100,000 per QALY only under scenarios of high, lifelong vaccine efficacy against all HPV related diseases (including other non-cervical cancers and genital warts), or scenarios of lower efficacy with lower coverage or lower vaccine costs. Given currently available information, including boys in an HPV vaccination programme generally exceeds conventional thresholds of good value for money, even under favourable conditions of vaccine protection and health benefits. Uncertainty still exists in many areas that can either strengthen or attenuate our findings. As new information emerges, assumptions and analyses will need to be iteratively revised to continue to inform policies for HPV vaccination.-Cost effectiveness analysis of including boys in a human papillomavirus vaccination programme in the United States.",0
"Only 60-70% of fertilized eggs may result in a live birth, and very early fetal loss mainly goes unnoticed. Outcomes that can only be ascertained in live-born children will be missing for those who do not survive till birth. In this article, we illustrate a common bias structure (leading to 'live-birth bias') that arises from studying the effects of prenatal exposure to environmental factors on long-term health outcomes among live births only in pregnancy cohorts. To illustrate this we used prenatal exposure to perfluoroalkyl substances (PFAS) and attention-deficit/hyperactivity disorder (ADHD) in school-aged children as an example. PFAS are persistent organic pollutants that may impact human fecundity and be toxic for neurodevelopment. We simulated several hypothetical scenarios based on characteristics from the Danish National Birth Cohort and found that a weak inverse association may appear even if PFAS do not cause ADHD but have a considerable effect on fetal survival. The magnitude of the negative bias was generally small, and adjusting for common causes of the outcome and fetal loss can reduce the bias. Our example highlights the need to identify the determinants of pregnancy loss and the importance of quantifying bias arising from conditioning on live birth in observational studies.-Bias from conditioning on live birth in pregnancy cohorts: an illustration based on neurodevelopment in children after prenatal exposure to organic pollutants.",0
"In cluster-randomized trials, groups of subjects (clusters) are assigned to treatments, whereas observations are taken on the individual subjects. Since observations on subjects in the same cluster are typically more similar than observations from different clusters, analyses of such data must take intracluster correlation into account rather than assuming independence among all observations. Random effects models are useful for this purpose. The problem becomes more complicated if, in addition, repeated observations are taken on subjects over time. This introduces intraindividual correlation, which is typical for longitudinal studies. The Waterloo Smoking Prevention Project, study 3 (WSPP3), 1989-1996, is a study giving rise to cluster-correlated longitudinal data, where schools were randomized to either a smoking intervention program or to a control condition. Smoking status was assessed on grade 6 students in these schools, with annual follow-up observations throughout elementary and high school years. The authors illustrate the use of a generalized random effects model for analyzing this type of data. This model obtains appropriate estimates and standard errors for both individual-level covariates and those at the level of the cluster.-Application of a generalized random effects regression model for cluster-correlated longitudinal data to a school-based smoking prevention trial.",1
Avoiding biased exclusions in cluster trials,3
"Mixed-effects logistic regression models are described for analysis of longitudinal ordinal outcomes, where observations are observed clustered within subjects. Random effects are included in the model to account for the correlation of the clustered observations. Typically, the error variance and the variance of the random effects are considered to be homogeneous. These variance terms characterize the within-subjects (i.e., error variance) and between-subjects (i.e., random-effects variance) variation in the data. In this article, we describe how covariates can influence these variances, and also extend the standard logistic mixed model by adding a subject-level random effect to the within-subject variance specification. This permits subjects to have influence on the mean, or location, and variability, or (square of the) scale, of their responses. Additionally, we allow the random effects to be correlated. We illustrate application of these models for ordinal data using Ecological Momentary Assessment (EMA) data, or intensive longitudinal data, from an adolescent smoking study. These mixed-effects ordinal location scale models have useful applications in mental health research where outcomes are often ordinal and there is interest in subject heterogeneity, both between- and within-subjects.-A mixed ordinal location scale model for analysis of Ecological Momentary Assessment (EMA) data.",1
"In this paper we investigate the addition of new variables to an existing risk prediction model and the subsequent impact on discrimination quantified by the area under the receiver operating characteristics curve (AUC of ROC). Based on practical experience, concerns have emerged that the significance of association of the variable under study with the outcome in the risk model does not correspond to the significance of the change in AUC: that is, often the variable is significant, but the change in AUC is not. This paper demonstrates that under the assumption of multivariate normality and employing linear discriminant analysis (LDA) to construct the risk prediction tool, statistical significance of the new predictor(s) is equivalent to the statistical significance of the increase in AUC. Under these assumptions the result extends asymptotically to logistic regression. We further show that equality of variance-covariance matrices of predictors within cases and non-cases is not necessary when LDA is used. However, our practical example from the Framingham Heart Study data suggests that the finding might be sensitive to the assumption of normality.-Equivalence of improvement in area under ROC curve and linear discriminant analysis coefficient under assumption of normality.",0
"Regular physical activity (PA) enhances weight-loss and reduces risk of chronic disease. However, as few as 10% of U.S. adults engage in regular PA. Incentive programs to promote PA have shown some promise, but have typically used incentives that are too large to sustain over time and have not demonstrated habit formation or been tested in community settings. This report presents the rationale and design of a randomized pilot study testing the feasibility and preliminary efficacy of small monetary incentives for PA (n=25) versus charitable donations in the same amount (n=25) versus control (n=25) over 12months among 75 low-active but otherwise healthy adults at a local YMCA. Incentives are based on YMCA attendance, which is verified by electronic swipe card data and is the primary study outcome, with self-reported minutes/week of PA assessed as a secondary outcome. Incentives are intentionally small enough-$1/session, maximum of $5/week-such that they could be indefinitely sustained by community organizations, privately-owned health clubs, healthcare organizations, or employers (e.g., employer fitness facilities). Costs of the incentive program for the sponsoring organization may be partially offset by increases in membership resulting from the appeal of the program. Moreover, if efficacious, the charitable donation incentive program may have the added benefit of building social capital for the sponsoring organization and potentially serving as a tax write-off, thus further offsetting the cost of the incentives. Findings will also have implications for the use of financially sustainable community-based incentive programs for other health-related behaviors (e.g., weight loss, smoking).-Small sustainable monetary incentives versus charitable donations to promote exercise: Rationale, design, and baseline data from a randomized pilot study.",0
"To evaluate any association between obesity in middle age, measured by body mass index and skinfold thickness, and risk of dementia later in life. Analysis of prospective data from a multiethnic population based cohort. Kaiser Permanente Northern California Medical Group, a healthcare delivery organisation. 10,276 men and women who underwent detailed health evaluations from 1964 to 1973 when they were aged 40-45 and who were still members of the health plan in 1994. Diagnosis of dementia from January 1994 to April 2003. Time to diagnosis was analysed with Cox proportional hazard models adjusted for age, sex, race, education, smoking, alcohol use, marital status, diabetes, hypertension, hyperlipidaemia, stroke, and ischaemic heart disease. Dementia was diagnosed in 713 (6.9%) participants. Obese people (body mass index &gt; or = 30) had a 74% increased risk of dementia (hazard ratio 1.74, 95% confidence interval 1.34 to 2.26), while overweight people (body mass index 25.0-29.9) had a 35% greater risk of dementia (1.35, 1.14 to 1.60) compared with those of normal weight (body mass index 18.6-24.9). Compared with those in the lowest fifth, men and women in the highest fifth of the distribution of subscapular or tricep skinfold thickness had a 72% and 59% greater risk of dementia, respectively (1.72, 1.36 to 2.18, and 1.59, 1.24 to 2.04). Obesity in middle age increases the risk of future dementia independently of comorbid conditions.-Obesity in middle age and future risk of dementia: a 27 year longitudinal population based study.",0
"A common form of validation study compares alternative methods for collecting data. The Bland-Altman plot pairs observations across methods and plots their mean values vs. their difference. This method provides only limited information, however, when the range of observed values is small relative to the number of observations. This brief report shows how adding a simple bar chart to a Bland-Altman plot adds essential additional information. The methodological approach is illustrated using data from a randomized controlled clinical trial of patients in a U.S. county health system. When the number of unique values is small, a Bland-Altman plot alone may provide inadequate information. Adding a bar chart yields new and essential information about agreement, bias, and heteroscedasticity. Studies validating one data-collection method against another can be performed successfully even when the number of unique values is small.-Bar charts enhance Bland-Altman plots when value ranges are limited.",0
Peer led programme for asthma education in adolescents. Issues of design and analysis are crucial in cluster randomised trials.,1
"Multilevel modeling (MLM) is frequently used to detect group differences, such as an intervention effect in a pre-test-post-test cluster-randomized design. Group differences on the post-test scores are detected by controlling for pre-test scores as a proxy variable for unobserved factors that predict future attributes. The pre-test and post-test scores that are most often used in MLM are summed item responses (or total scores). In prior research, there have been concerns regarding measurement error in the use of total scores in using MLM. To correct for measurement error in the covariate and outcome, a theoretical justification for the use of multilevel structural equation modeling (MSEM) has been established. However, MSEM for binary responses has not been widely applied to detect intervention effects (group differences) in intervention studies. In this article, the use of MSEM for intervention studies is demonstrated and the performance of MSEM is evaluated via a simulation study. Furthermore, the consequences of using MLM instead of MSEM are shown in detecting group differences. Results of the simulation study showed that MSEM performed adequately as the number of clusters, cluster size, and intraclass correlation increased and outperformed MLM for the detection of group differences.-Detecting Intervention Effects in a Cluster-Randomized Design Using Multilevel Structural Equation Modeling for Binary Responses.",1
"Evidence suggests that cluster randomized trials are often poorly designed and analysed. There is little recent research on the methodologic quality of cluster randomized trials and none focuses on primary health care where these trials are increasingly common. We conducted a systematic review of recent cluster randomized trials in primary health care, searching the Cochrane Controlled Trials Register. We also searched for unpublished trials in conference proceedings, and the UK National Research Register. We assess methodologic quality using a checklist, articulate problems facing investigators conducting these trials, and examine the extent to which carrying out a cluster randomized trial (as opposed to an individually randomized trial) in primary care may reduce power. We found 367 trial reports. Many trials were reported more than once. We characterize 152 independent cluster randomized trials in primary health care published between 1997 and 2000, and briefly describe 47 trials unpublished at December 2000. The quality of design and analysis was variable. Of published trials reporting sample size calculations 20% accounted for clustering in these calculations, 59% of published trials accounted for clustering in analyses. Unpublished trials were more recent and of higher quality. Reporting quality was better in journals reporting more cluster randomized trials. Many trial investigators reported problems with adherence to protocol, recruitment and type of intervention. Methodologic quality of cluster randomized trials in primary health care is variable and reporting needs improvement. The use of cluster randomization should be indicated in the title or abstract so these kinds of trials are easier to identify. Communicating appropriate methodology to health care researchers continues to be a challenge. Cluster randomized trials should always be piloted and information from pilots and unsuccessful trials shared more widely.-Lessons for cluster randomized trials in the twenty-first century: a systematic review of trials in primary care.",1
"Public quality reports for Medicare Advantage health plans include 11 measures of patient experiences reported in the annual Consumer Assessment of Healthcare Providers and Systems surveys. Computing summaries at the health plan level (of multiple measures in multiple years) yields an array-structured random variable. To summarize associations among measures and years, we model the variance-covariance matrix governing the plan-level vectors of yearly quality measures as a Kronecker product of an across-measure matrix and an across-year matrix, or a sum of such Kronecker products. This approach extends separable covariance structure to Fay-Herriot models. In addition, we develop linear combinations of Kronecker products similar to principal components for array random variables. To each Kronecker-product term, we apply post hoc analyses suited to the corresponding dimension of the cross-classification: 1-way factor analysis for the across-measure factor and time-series analysis to the across-year factor. These methods draw out key patterns of variation in the quality measures over time and suggest new strategies for reporting quality information to consumers.-Separable covariance models for health care quality measures across years and topics.",0
"A major goal of the National Institutes of Health's Clinical and Translational Science Award program is to facilitate clinical research and enhance the transition of basic to clinical research. As such, a number of Clinical and Translational Science Award centers have developed services to facilitate the conduct of clinical research, including support with fulfilling regulatory requirements. The University of Kentucky sought to establish an institutional semi-independent monitoring committee to provide oversight for clinical research studies per National Institutes of Health requirements and recommendations. Our semi-independent monitoring committee was initiated in 2010. Since the inception of our semi-independent monitoring committee we have restructured its operations and protocols to improve efficiency. This article discusses our experiences with semi-independent monitoring committee creation and growth. This article summarizes our experience in creating and maturing an institutional data monitoring committee.-Creation of an institutional semi-independent data monitoring committee.",0
"In a randomized test of mixed-mode data collection strategies, 386 participants in the Raising Healthy Children (RHC) Project were either (a) asked to complete a survey via the Internet and later offered the opportunity to complete the survey in person or (b) first offered an in-person survey, with the Web follow-up. The Web-first condition resulted in cost savings although the overall completion rates for the 2 conditions were similar. On average, in-person-first condition participants completed surveys earlier in the field period than Web-first condition participants. Based on intent-to-treat analyses, little evidence of condition effects on response bias, with respect to rates or levels of reported behavior, was found.-Use of web and in-person survey modes to gather data from young adults on sex and drug use: an evaluation of cost, time, and survey error based on a randomized mixed-mode design.",0
Stepped Wedge Cluster Randomized Controlled Trials Are the Way Forward to Assess Innovative Electronic Hand Hygiene Monitoring Technologies.,3
Helping smokers to quit.,0
"Cluster randomized trials (CRTs) are widely used in different areas of medicine and public health. Recently, with increasing complexity of medical therapies and technological advances in monitoring multiple outcomes, many clinical trials attempt to evaluate multiple co-primary endpoints. In this study, we present a power analysis method for CRTs with K ? 2  binary co-primary endpoints. It is developed based on the GEE (generalized estimating equation) approach, and three types of correlations are considered: inter-subject correlation within each endpoint, intra-subject correlation across endpoints, and inter-subject correlation across endpoints. A closed-form joint distribution of the K test statistics is derived, which facilitates the evaluation of power and type I error for arbitrarily constructed hypotheses. We further present a theorem that characterizes the relationship between various correlations and testing power. We assess the performance of the proposed power analysis method based on extensive simulation studies. An application example to a real clinical trial is?presented.-Power analysis for cluster randomized trials with multiple binary co-primary endpoints",1
Semiparametric mixed-effects models for clustered failure time data,1
"Cluster randomized clinical trials (cluster-RCT), where the community entities serve as clusters, often yield data with three hierarchy levels. For example, interventions are randomly assigned to the clusters (level three unit). Health care professionals (level two unit) within the same cluster are trained with the randomly assigned intervention to provide care to subjects (level one unit). In this study, we derived a closed form power function and formulae for sample size determination required to detect an intervention effect on outcomes at the subject's level. In doing so, we used a test statistic based on maximum likelihood estimates from a mixed-effects linear regression model for three level data. A simulation study follows and verifies that theoretical power estimates based on the derived formulae are nearly identical to empirical estimates based on simulated data. Recommendations at the design stage of a cluster-RCT are discussed.-Statistical power and sample size requirements for three level hierarchical cluster randomized trials.",1
"Approximately 39% of the global diarrhoea deaths in children aged 5 years may be attributable to rotavirus infection. Two rotavirus vaccines were recently introduced to the market, with evidence of efficacy in the USA, Europe and Latin America. We sought to estimate the effectiveness of these vaccines against rotavirus morbidity and mortality. We conducted a systematic review of published efficacy and effectiveness trials of rotavirus vaccines. Study descriptors and outcome measures were abstracted into standardized tables and the quality of each study was graded. We performed meta-analyses for any outcome with two or more data points, and used child health epidemiology reference group (CHERG) Rules for Evidence Review to estimate the effect of the vaccine on rotavirus mortality. We identified six papers for abstraction, reporting results from four studies. No studies reported diarrhoea or rotavirus deaths, but all studies showed reductions in hospitalizations due to rotavirus or diarrhoea of any aetiology, severe and any rotavirus infections and diarrhoea episodes of any aetiology in children who received rotavirus vaccine compared with placebo. Effectiveness against very severe rotavirus infection best approximated effectiveness against the fraction of diarrhoea deaths attributable to rotavirus, and was estimated to be 74% (95% confidence interval: 35-90%). Rotavirus vaccines are efficacious against rotavirus morbidity and mortality and have the potential to substantially reduce child mortality in low-income countries if implemented appropriately.-The effect of rotavirus vaccine on diarrhoea mortality.",0
"The convention in clinical trials is to regard outcomes as independently distributed, possibly conditional on covariates, but in some situations they may be correlated. For example, in infectious diseases, correlation may be induced if participants have contact with a common infectious source, or share hygienic tips that prevent infection. This paper discusses the design and analysis of randomized clinical trials that allow arbitrary correlation among all randomized participants. This perspective generalizes the traditional perspective of strata, where patients are exchangeable within strata, and independent across strata. For theoretical work, we focus on the test of no treatment effect ?(1)-?(0)=0 when the n dimensional vector of outcomes follows a Gaussian distribution with known n ? n covariance matrix ?, where the half randomized to treatment (placebo) have mean response ?(1)(?(0)). We show how the new test corresponds to familiar tests in simple situations for independent, exchangeable, paired, and clustered data. We also discuss the design of trials where ? is known before or during randomization of patients and evaluate randomization schemes based on such knowledge. We provide two complex examples to illustrate the method, one for a study of 23 family clusters with cardiomyopathy, and the other where the malaria attack rates vary within households and clusters of households in a Malian village.-On the design and analysis of clinical trials with correlated outcomes.",1
"The aim of this study is to describe and compare three statistical methods to allow for therapist effects in individually randomised controlled trials. In an individually randomised controlled trial where the intervention is delivered by a health professional it seems likely that the effectiveness of the intervention, independent of any treatment effect, could depend on the skill of the health professional delivering it. This leads to a potential clustering of the outcomes for the patients being treated by the same health professional. Retrospective statistical analysis of outcomes from four example randomised controlled trial datasets with potential clustering by health professional. Three methods to allow for clustering are described: cluster level analysis; random effects models and marginal models. These models were fitted to continuous outcome data from four example randomised controlled trial datasets with potential clustering by health professional. The cluster level models produced the widest confidence intervals. Little difference was found between the estimates of the regression coefficients for the treatment effect and confidence intervals between the individual patient level models for the datasets. The conclusions reached for each dataset match those published in the original papers. The intracluster correlation coefficient ranged from &lt;0.001-0.04 for the outcomes, which shows only minor levels of clustering within the datasets. The models, which use individual level data are to be preferred. Treatment coefficients from these models have different interpretations. The choice of model should depend on the scientific question being asked. We recommend that researchers should be aware of any potential clustering, by health professional, in their randomised controlled trial and use appropriate methods to account for this clustering in the statistical analysis of the data.-Therapist effects in randomised controlled trials: what to do about them.",2
"Appropriate quantification of added usefulness offered by new markers included in risk prediction algorithms is a problem of active research and debate. Standard methods, including statistical significance and c statistic are useful but not sufficient. Net reclassification improvement (NRI) offers a simple intuitive way of quantifying improvement offered by new markers and has been gaining popularity among researchers. However, several aspects of the NRI have not been studied in sufficient detail. In this paper we propose a prospective formulation for the NRI which offers immediate application to survival and competing risk data as well as allows for easy weighting with observed or perceived costs. We address the issue of the number and choice of categories and their impact on NRI. We contrast category-based NRI with one which is category-free and conclude that NRIs cannot be compared across studies unless they are defined in the same manner. We discuss the impact of differing event rates when models are applied to different samples or definitions of events and durations of follow-up vary between studies. We also show how NRI can be applied to case-control data. The concepts presented in the paper are illustrated in a Framingham Heart Study example. In conclusion, NRI can be readily calculated for survival, competing risk, and case-control data, is more objective and comparable across studies using the category-free version, and can include relative costs for classifications. We recommend that researchers clearly define and justify the choices they make when choosing NRI for their application.-Extensions of net reclassification improvement calculations to measure usefulness of new biomarkers.",0
"Clinical trial AIN503/A5217 investigates whether a period of early treatment with antiretroviral therapy might lower the viral setpoint in subjects recently infected with HIV-1. We consider two statistical issues. First, even under the null hypothesis control arm subjects are more likely than treatment arm subjects to be missing final outcome data because of disease progression. The analysis must adjust for this missing data, or it may be unacceptably biased. Second, comparing outcomes between treatment and control arms at identical times post-randomization gives different information than comparing outcomes at the same amount of time off-therapy, as measured post-randomization. This may make interpretation of results problematic. We formulate the null hypothesis of the study as exchangeability under a time-shift between arms, which we call ""time delay"" between the study arms. This captures clinically relevant information, and allows us to formalize a two-stage hypothesis test in which stage one is a comparison between arms at identical times post-randomization, and stage two is a comparison between arms at identical times off-therapy, as measured post-randomization. Importantly, within this framework we can show that the two-stage test can be adjusted for the missing data using a simple worst-rank substitution.-Robust analysis of biomarker data with informative missingness using a two-stage hypothesis test in an HIV treatment interruption trial: AIEDRP AIN503/ACTG A5217.",0
"Loss to follow-up threatens internal and external validity yet little research has examined ways to limit participant attrition. We conducted a systematic review of studies with a primary focus on strategies to retain participants in health care research. We completed searches of PubMed, CINAHL, CENTRAL, Cochrane Methodology Register, and EMBASE (August 2005). We also examined reference lists of eligible articles and relevant reviews. A data-driven thematic analysis of the retention strategies identified common themes. We retrieved 3,068 citations, 21 studies were eligible for inclusion. We abstracted 368 strategies and from these identified 12 themes. The studies reported a median of 17 strategies across a median of six themes. The most commonly reported strategies were systematic methods of participant contact and scheduling. Studies with retention rates lower than the mean rate (86%) reported fewer strategies. There was no difference in the number of different themes used. Available evidence suggests that investigators should consider using a number of retention strategies across several themes to maximize the retention of participants. Further research, including explicit evaluation of the effectiveness of different strategies, is needed.-Systematic review identifies number of strategies important for retaining study participants.",0
"Earlier studies suggest a protective association between vitamin K antagonist (VKA) anticoagulants and the incidence of cancer. The authors examined the associations between VKA therapy and incidence of 24 site-specific cancers with a Danish population-based cohort study, using heart valve replacement as an instrumental variable. The authors enrolled 9,727 Danish residents who received a replacement heart valve between 1989 and 2006. The heart valve recipients were matched with 95,481 unexposed individuals on age and sex. The authors used the heart valve replacement instrument to estimate rate ratios associating VKA therapy with incidence of the 24 site-specific cancers using Poisson regression models. Direct associations between VKA therapy and incidence of the 24 cancers were estimated in a prescription validation subset. The instrumental variable associations were plotted according to the inverse normal of rank percentile and subjected to semi-Bayes shrinkage adjustment for multiple comparisons. The pattern of associations was consistent with a null-centered Gaussian distribution. No individual cancer site showed a substantial positive or negative association with VKA therapy in the prescription validation subset, the instrumental variable analysis, or the analysis with semi-Bayes adjustment. These results do not support the existing hypothesis that VKA therapy is associated with reduced cancer risk.-The association between vitamin K antagonist therapy and site-specific cancer incidence estimated by using heart valve replacement as an instrumental variable.",0
"Missing covariates often occur in biomedical studies with survival outcomes. Multiple imputation via chained equations (MICE) is a semi-parametric and flexible approach that imputes multivariate data by a series of conditional models, one for each incomplete variable. When applying MICE, practitioners tend to specify the conditional models in a simple fashion largely dictated by the software, which could lead to suboptimal results. Practical guidelines for specifying appropriate conditional models in MICE are lacking. Motivated by a study of time to hip fractures in the Women's Health Initiative Observational Study using accelerated failure time models, we propose and experiment with some rationales leading to appropriate MICE specifications. This strategy starts with specifying a joint model for the variables involved. We first derive the conditional distribution of each variable under the joint model, then approximate these conditional distributions to the extent which can be characterized by commonly used regression models. We propose to fit separate models to impute incomplete variables by the failure status, which is key to generating appropriate MICE specifications for survival outcomes. The proposed strategy can be conveniently implemented with all available imputation software that uses fully conditional specifications. Our simulation results show that some commonly used simple MICE specifications can produce suboptimal results, while those based on the proposed strategy appear to perform well and be robust toward model misspecifications. Hence, we warn against a mechanical use of MICE and suggest careful modeling of the conditional distributions of variables to ensure proper performance.-Strategies for imputing missing covariates in accelerated failure time models.",0
"Rates of Helicobacter pylori infection are traditionally higher in developing countries than in developed countries, but the specific reasons for these differences are not fully clear. While chronic diseases resulting from H. pylori are generally of adult onset, chronic infection usually begins in childhood. In this cross-sectional study (1998-2000), the authors compared prevalences of H. pylori infection among children under age 6 years on both sides of the Rio Grande. Participants included 264 children of women from low-income families who were receiving services at health clinics in Juarez, Mexico, or El Paso, Texas, from April 1998 through October 2000. Data were collected through personal interviews and serologic testing for H. pylori antibodies. The crude odds ratio for H. pylori prevalence among Mexican children as compared with US children was 3.94 (95% confidence interval: 1.72, 9.06). After adjustment for covariates, the odds ratio decreased to 1.70 (95% confidence interval: 0.64, 4.52). The adjustments that produced the greatest reduction in the odds ratio for location were those for household crowding and maternal education. This study identified specific factors that may explain geographic variation in H. pylori prevalence among children.-Determinants of geographic variation in Helicobacter pylori infection among children on the US-Mexico border.",0
"Because of the central role of the general practice in the delivery of British primary care, intervention trials in primary care often use the practice as the unit of randomization. The creation of primary care groups (PCGs) in April 1999 changed the organization of primary care and the commissioning of secondary care services. PCGs will directly affect the organization and delivery of primary, secondary and social care services. The PCG therefore becomes an appropriate target for organizational and educational interventions. Trials testing these interventions should involve randomization by PCG. This paper discusses the sample size required for a trial in primary care assessing the effect of a falls prevention programme among older people. In this trial PCGs will be randomized. The sample size calculations involve estimating intra-PCG correlation in primary outcome: fractured femur rate for those 65 years and over. No data on fractured femur rate were available at PCG level. PCGs are, however, similar in size and often coterminous with local authorities. Therefore, intra-PCG correlation in fractured femur rate was estimated from the intra-local authority correlation calculated from routine data. Three alternative trial designs are considered. In the first design, PCGs are selected for inclusion in the trial from the total population of England (eight regions). In the second design, PCGs are selected from two regions only. The third design is similar to the second except that PCGs are stratified by region and baseline value of fracture rate. Intracluster correlation is estimated for each of these designs using two methods: an approximation which assumes cluster sizes are equal and an alternative method which takes account of the fact that cluster sizes vary. Estimates of sample size required vary between 26 and 7 PCGs in each intervention group, depending on the trial design and the method used to calculate sample size. Not unexpectedly, stratification by baseline value of the outcome variable decreases the sample size required. In our analyses, geographic restriction of the population to be sampled reduces between-cluster variability in the primary outcome. This leads to an increase in precision. When allowance for variable cluster size is made, the increase in precision is not as great as would be expected with equal cluster sizes. This paper highlights the usefulness of routine data in work of this kind, and establishes one of the essential prerequisites for our proposed trial and other trials using primary outcomes with similar between-PCG variation: a feasible sample size.-Sample size calculations for intervention trials in primary care randomizing by primary care group: an empirical illustration from one proposed intervention trial.",1
"This study compares methods for analyzing correlated survival data from physician-randomized trials of health care quality improvement interventions. Several proposed methods adjust for correlated survival data; however the most suitable method is unknown. Applying the characteristics of our study example, we performed three simulation studies to compare conditional, marginal, and non-parametric methods for analyzing clustered survival data. We simulated 1000 datasets using a shared frailty model with (1) fixed cluster size, (2) variable cluster size, and (3) non-lognormal random effects. Methods of analyses included: the nonlinear mixed model (conditional), the marginal proportional hazards model with robust standard errors, the clustered logrank test, and the clustered permutation test (non-parametric). For each method considered we estimated Type I error, power, mean squared error, and the coverage probability of the treatment effect estimator. We observed underestimated Type I error for the clustered logrank test. The marginal proportional hazards method performed well even when model assumptions were violated. Nonlinear mixed models were only advantageous when the distribution was correctly specified.-A comparison of statistical approaches for physician-randomized trials with survival outcomes.",1
"In this article, we present a discussion of two general ways in which the traditional randomized trial can be modified or adapted in response to the data being collected. We use the term adaptive design to refer to a trial in which characteristics of the study itself, such as the proportion assigned to active intervention versus control, change during the trial in response to data being collected. The term adaptive sequence of trials refers to a decision-making process that fundamentally informs the conceptualization and conduct of each new trial with the results of previous trials. Our discussion below investigates the utility of these two types of adaptations for public health evaluations. Examples are provided to illustrate how adaptation can be used in practice. From these case studies, we discuss whether such evaluations can or should be analyzed as if they were formal randomized trials, and we discuss practical as well as ethical issues arising in the conduct of these new-generation trials.-Adaptive designs for randomized trials in public health.",1
"In profiling studies, the analysis of a single dataset often leads to unsatisfactory results because of the small sample size. Multi-dataset analysis utilizes information of multiple independent datasets and outperforms single-dataset analysis. Among the available multi-dataset analysis methods, integrative analysis methods aggregate and analyze raw data and outperform meta-analysis methods, which analyze multiple datasets separately and then pool summary statistics. In this study, we conduct integrative analysis and marker selection under the heterogeneity structure, which allows different datasets to have overlapping but not necessarily identical sets of markers. Under certain scenarios, it is reasonable to expect some similarity of identified marker sets - or equivalently, similarity of model sparsity structures - across multiple datasets. However, the existing methods do not have a mechanism to explicitly promote such similarity. To tackle this problem, we develop a sparse boosting method. This method uses a BIC/HDBIC criterion to select weak learners in boosting and encourages sparsity. A new penalty is introduced to promote the similarity of model sparsity structures across datasets. The proposed method has a intuitive formulation and is broadly applicable and computationally affordable. In numerical studies, we analyze right censored survival data under the accelerated failure time model. Simulation shows that the proposed method outperforms alternative boosting and penalization methods with more accurate marker identification. The analysis of three breast cancer prognosis datasets shows that the proposed method can identify marker sets with increased similarity across datasets and improved prediction performance. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Promoting similarity of model sparsity structures in integrative analysis of cancer genetic data.",0
"Measurement of individuals' costs and outcomes in randomized trials allows uncertainty about cost effectiveness to be quantified. Uncertainty is expressed as probabilities that an intervention is cost effective, and confidence intervals of incremental cost effectiveness ratios. Randomizing clusters instead of individuals tends to increase uncertainty but such data are often analysed incorrectly in published studies. We used data from a cluster randomized trial to demonstrate five appropriate analytic methods: 1) joint modeling of costs and effects with two-stage non-parametric bootstrap sampling of clusters then individuals, 2) joint modeling of costs and effects with Bayesian hierarchical models and 3) linear regression of net benefits at different willingness to pay levels using a) least squares regression with Huber-White robust adjustment of errors, b) a least squares hierarchical model and c) a Bayesian hierarchical model. All five methods produced similar results, with greater uncertainty than if cluster randomization was not accounted for. Cost effectiveness analyses alongside cluster randomized trials need to account for study design. Several theoretically coherent methods can be implemented with common statistical software.-Methods for analyzing cost effectiveness data from cluster randomized trials.",1
"In assessing causal mediation effects in randomized studies, a challenge is that the direct and indirect effects can vary across participants due to different measured and unmeasured characteristics. In that case, the population effect estimated from standard approaches implicitly averages over and does not estimate the heterogeneous direct and indirect effects. We propose a Bayesian semiparametric method to estimate heterogeneous direct and indirect effects via clusters, where the clusters are formed by both individual covariate profiles and individual effects due to unmeasured characteristics. These cluster-specific direct and indirect effects can be estimated through a set of regression models where specific coefficients are clustered by a stick-breaking prior. To let clustering be appropriately informed by individual direct and indirect effects, we specify a data-dependent prior. We conduct simulation studies to assess performance of the proposed method compared to other methods. We use this approach to estimate heterogeneous causal direct and indirect effects of an expressive writing intervention for patients with renal cell carcinoma.-A Bayesian semiparametric latent variable approach to causal mediation.",0
"Dairy consumption, systolic blood pressure, and risk of hypertension.",0
"In recent years researchers have paid substantial attention to the issue of college students' alcohol use. One limitation to the current literature is an over reliance on retrospective, self-report survey data. This article presents field methodologies for measuring college students' alcohol consumption in natural drinking environments. Specifically, we present the methodology from a large field study of student drinking environments along with some illustrative data from the same study. Field surveys, observational methods, sampling issues, and breath alcohol concentration sample collection are detailed.-Measuring college students' alcohol consumption in natural drinking environments: field methodologies for bars and parties.",0
"To compare exposure to secondhand smoke and respiratory health in bar staff in the Republic of Ireland and Northern Ireland before and after the introduction of legislation for smoke-free workplaces in the Republic. Comparisons before and after the legislation in intervention and control regions. Public houses in three areas in the Republic (intervention) and one area in Northern Ireland (control). 329 bar staff enrolled in baseline survey; 249 (76%) followed up one year later. Of these, 158 were non-smokers both at baseline and follow-up. Salivary cotinine concentration, self reported exposure to secondhand smoke, and respiratory and sensory irritation symptoms. In bar staff in the Republic who did not themselves smoke, salivary cotinine concentrations dropped by 80% after the smoke-free law (from median 29.0 nmol/l (95% confidence interval 18.2 to 43.2 nmol/l)) to 5.1 nmol/l (2.8 to 13.1 nmol/l) in contrast with a 20% decline in Northern Ireland over the same period (from median 25.3 nmol/l (10.4 to 59.2 nmol/l) to 20.4 nmol/l (13.2 to 33.8 nmol/l)). Changes in self reported exposure to secondhand smoke were consistent with the changes in cotinine concentrations. Reporting any respiratory symptom declined significantly in the Republic (down 16.7%, -26.1% to -7.3%) but not in Northern Ireland (0% difference, -32.7% to 32.7%). After adjustment for confounding, respiratory symptoms declined significantly more in the Republic than in Northern Ireland and the decline in cotinine concentration was twice as great. The smoke-free law in the Republic of Ireland protects non-smoking bar workers from exposure to secondhand smoke.-Legislation for smoke-free workplaces and health of bar workers in Ireland: before and after study.",0
"The validation of intermediate markers as surrogate markers (S) for the true outcome of interest (T) in clinical trials offers the possibility for trials to be run more quickly and cheaply by using the surrogate endpoint in place of the true endpoint. Working within a principal stratification framework, we propose causal quantities to evaluate surrogacy using a Gaussian copula model for an ordinal surrogate and time-to-event final outcome. The methods are applied to data from four colorectal cancer clinical trials, where S is tumor response and T is overall survival. For the Gaussian copula model, a Bayesian estimation strategy is used and, as some parameters are not identifiable from the data, we explore the use of informative priors that are consistent with reasonable assumptions in the surrogate marker setting to aid in estimation. While there is some bias in the estimation of the surrogacy quantities of interest, the estimation procedure does reasonably well at distinguishing between poor and good surrogate markers. Some of the parameters of the proposed model are not identifiable from the data, and therefore, assumptions must be made in order to aid in their estimation. The proposed quantities can be used in combination to provide evidence about the validity of S as a surrogate marker for T.-Surrogacy assessment using principal stratification with multivariate normal and Gaussian copula models.",0
"Cluster-randomized trials (CRTs) of infectious disease preventions often yield correlated, interval-censored data: dependencies may exist between observations from the same cluster, and event occurrence may be assessed only at intermittent study visits. This data structure must be accounted for when conducting interim monitoring and futility assessment for CRTs. In this article, we propose a flexible framework for conditional power estimation when outcomes are correlated and interval-censored. Under the assumption that the survival times follow a shared frailty model, we first characterize the correspondence between the marginal and cluster-conditional survival functions, and then use this relationship to semiparametrically estimate the cluster-specific survival distributions from the available interim data. We incorporate assumptions about changes to the event process over the remainder of the trial-as well as estimates of the dependency among observations in the same cluster-to extend these survival curves through the end of the study. Based on these projected survival functions, we generate correlated interval-censored observations, and then calculate the conditional power as the proportion of times (across multiple full-data generation steps) that the null hypothesis of no treatment effect is rejected. We evaluate the performance of the proposed method through extensive simulation studies, and illustrate its use on a large cluster-randomized HIV prevention?trial.-Estimation of conditional power for cluster-randomized trials with interval-censored endpoints",1
"For a heart transplant patient, the risk of graft rejection and risk of death are likely to be associated. Two fully specified Bayesian models for recurrent events with dependent termination are applied to investigate the potential relationships between these two types of risk as well as association with risk factors. We particularly focus on the choice of priors, selection of the appropriate prediction model, and prediction methods for these two types of risk for an individual patient. Our prediction tools can be easily implemented and helpful to physicians for setting heart transplant patients' biopsy schedule.-Bayesian analysis of recurrent event with dependent termination: an application to a heart transplant study.",0
"There is currently a lot of interest in pilot studies conducted in preparation for randomised controlled trials. This paper focuses on sample size requirements for external pilot studies for cluster randomised trials. We consider how large an external pilot study needs to be to assess key parameters for input to the main trial sample size calculation when the primary outcome is continuous, and to estimate rates, for example recruitment rates, with reasonable precision. We used simulation to provide the distribution of the expected number of clusters for the main trial under different assumptions about the natural cluster size, intra-cluster correlation, eventual cluster size in the main trial, and various decisions made at the piloting stage. We chose intra-cluster correlation values and pilot study size to reflect those commonly reported in the literature. Our results show that estimates of sample size required for the main trial are likely to be biased downwards and very imprecise unless the pilot study includes large numbers of clusters and individual participants. We conclude that pilot studies will usually be too small to estimate parameters required for estimating a sample size for a main cluster randomised trial (e.g. the intra-cluster correlation coefficient) with sufficient precision and too small to provide reliable estimates of rates for process measures such as recruitment or follow-up rates.-How big should the pilot study for my cluster randomised trial be?",1
"Despite advances in individual and combined treatments for major depression, issues with non-response and partial-response remain relatively common, motivating the search for new treatment strategies. This study aims to develop one such novel treatment. In this proof-of-concept study, we are investigating whether the treatment enhancing effects of d-cycloserine (DCS) administration can be extended outside the extinction-learning paradigms where they have been primarily examined. Using uniform delivery of cognitive behavioral therapy (CBT) content via computer-administered interventions for depression, we are assessing the value of pre-session administrations of DCS for retention of therapeutic learning. Recall of this information is evaluated in conjunction with performance on standardized tests of memory recall with both emotional and non-emotional stimuli. Specifically, in a randomized, double-blind trial we will compare the benefits of two pre-session administrations of DCS augmentation to those achieved by similar administrations of modafinil or placebo. Because modafinil is associated with a number of discriminable effects in addition to cognitive enhancement (e.g., feelings of vigor, alertness, positive mood); whereas these effects would not be expected with DCS, we will assess drug context effects in relation to memory augmentation effects.-Examining the efficacy of d-cycloserine to augment therapeutic learning in depression.",0
The stepped wedge cluster randomised trial: what it is and when it should be used,3
"Generalised estimating equations with the sandwich standard-error estimator provide a promising method of analysis for stepped wedge cluster randomised trials. However, they have inflated type-one error when used with a small number of clusters, which is common for stepped wedge cluster randomised trials. We present a large simulation study of binary outcomes comparing bias-corrected standard errors from Fay and Graubard; Mancl and DeRouen; Kauermann and Carroll; Morel, Bokossa, and Neerchal; and Mackinnon and White with an independent and exchangeable working correlation matrix. We constructed 95% confidence intervals using a t-distribution with degrees of freedom including clusters minus parameters (DFC-P), cluster periods minus parameters, and estimators from Fay and Graubard (DFFG), and Pan and Wall. Fay and Graubard and an approximation to Kauermann and Carroll (with simpler matrix inversion) were unbiased in a wide range of scenarios with an independent working correlation matrix and more than 12 clusters. They gave confidence intervals with close to 95% coverage with DFFG with 12 or more clusters, and DFC-P with 18 or more clusters. Both standard errors were conservative with fewer clusters. With an exchangeable working correlation matrix, approximated Kauermann and Carroll and Fay and Graubard had a small degree of under-coverage.-Comparison of small-sample standard-error corrections for generalised estimating equations in stepped wedge cluster randomised trials with a binary outcome: A simulation study",3
"Adequate reporting of randomized, controlled trials (RCTs) is necessary to allow accurate critical appraisal of the validity and applicability of the results. The CONSORT (Consolidated Standards of Reporting Trials) Statement, a 22-item checklist and flow diagram, is intended to address this problem by improving the reporting of RCTs. However, some specific issues that apply to trials of nonpharmacologic treatments (for example, surgery, technical interventions, devices, rehabilitation, psychotherapy, and behavioral intervention) are not specifically addressed in the CONSORT Statement. Furthermore, considerable evidence suggests that the reporting of nonpharmacologic trials still needs improvement. Therefore, the CONSORT group developed an extension of the CONSORT Statement for trials assessing nonpharmacologic treatments. A consensus meeting of 33 experts was organized in Paris, France, in February 2006, to develop an extension of the CONSORT Statement for trials of nonpharmacologic treatments. The participants extended 11 items from the CONSORT Statement, added 1 item, and developed a modified flow diagram. To allow adequate understanding and implementation of the CONSORT extension, the CONSORT group developed this elaboration and explanation document from a review of the literature to provide examples of adequate reporting. This extension, in conjunction with the main CONSORT Statement and other CONSORT extensions, should help to improve the reporting of RCTs performed in this field.-Extending the CONSORT statement to randomized trials of nonpharmacologic treatment: explanation and elaboration.",2
"A fundamental goal of epidemiologic research is to investigate the relationship between exposures and disease risk. Cases of the disease are often considered a single outcome and assumed to share a common etiology. However, evidence indicates that many human diseases arise and evolve through a range of heterogeneous molecular pathologic processes, influenced by diverse exposures. Pathogenic heterogeneity has been considered in various neoplasms such as colorectal, lung, prostate, and breast cancers, leukemia and lymphoma, and non-neoplastic diseases, including obesity, type II diabetes, glaucoma, stroke, cardiovascular disease, autism, and autoimmune disease. In this article, we discuss analytic options for studying disease subtype heterogeneity, emphasizing methods for evaluating whether the association of a potential risk factor with disease varies by disease subtype. Methods are described for scenarios where disease subtypes are categorical and ordinal and for cohort studies, matched and unmatched case-control studies, and case-case study designs. For illustration, we apply the methods to a molecular pathological epidemiology study of alcohol intake and colon cancer risk by tumor LINE-1 methylation subtypes. User-friendly software to implement the methods is publicly available.-Statistical methods for studying disease subtype heterogeneity.",0
"Commentary: Reflections on G. M. Lower and colleagues' 1979 study associating slow acetylator phenotype with urinary bladder cancer: meta-analysis, historical refinements of the hypothesis, and lessons learned.",0
"In clinical studies, assessing agreement of multiple readings on the same subject plays an important role in the evaluation of continuous measurement scale. The multiple readings within a subject may be replicated readings by using the same method or/and readings by using several methods (e.g. different technologies or several raters). The traditional agreement data for a given subject often consist of either replicated readings from only one method or multiple readings from several methods where only one reading is taken from each of these methods. In the first case, only intra-method agreement can be evaluated. In the second case, traditional agreement indices such as intra-class correlation (ICC) or concordance correlation coefficient (CCC) is often reported as inter-method agreement. We argue that these indices are in fact measures of total agreement that contains both inter and intra agreement. Only if there are replicated readings from several methods for a given subject, then one can assess intra, inter and total agreement simultaneously. In this paper, we present new inter-method agreement index, inter-CCC, and total agreement index, total-CCC, for agreement data with replicated readings from several methods where the ICCs within methods are used to assess intra-method agreement for each of the several methods. The relationship of the total-CCC with the inter-CCC and the ICCs is investigated. We propose a generalized estimating equations approach for estimation and inference. Simulation studies are conducted to assess the performance of the proposed approach and data from a carotid stenosis screening study is used for illustration.-Assessing intra, inter and total agreement with replicated readings.",0
"Maximum likelihood estimation techniques for subject-specific (SS) generalized linear mixed models and generalized estimating equations for marginal or population-averaged (PA) models are often used for the analysis of cluster-unit intervention trials. Although both classes of procedures account for the presence of within-cluster correlations, the interpretations of fixed effects including intervention effect parameters differ in SS and PA models. Furthermore, closed-form mathematical expressions relating SS and PA parameters from the two respective approaches are generally lacking. This paper investigates the special case of correlated Poisson responses where, for a log-linear model with normal random effects, exact relationships are available. Equivalent PA model representations of two SS models commonly used in the analysis of nested cross-sectional cluster trials with count data are derived. The mathematical results are illustrated with count data from a large non-randomized cluster trial to reduce underage drinking. Knowledge of relationships among parameters in the respective mean and covariance models is essential to understanding empirical comparisons of the two approaches.-Comparison of subject-specific and population averaged models for count data from cluster-unit intervention trials.",1
"Prompted by several recent papers on the inference on median and mean residual life time, we note that the testing involving the mean or median residual life function in censored survival data can be obtained by an easy application of the general empirical likelihood ratio test. This approach has several advantages: (1) There is no need to estimate the variance/covariance at all, which may become prohibitively complicated for other procedures that require the estimation of such. (2) When inverting the tests to obtain confidence regions/intervals, this procedure inherits all the good properties of a likelihood ratio test. (3) Free software implementation of the test is readily available.-Empirical likelihood ratio test for median and mean residual lifetime.",0
"Trials in which intact communities are the units of randomization are increasingly being used to evaluate interventions which are more naturally administered at the community level, or when there is a substantial risk of treatment contamination. In this article we focus on the planning of community intervention trials in which k communities (for example, medical practices, worksites, or villages) are to be randomly allocated to each of an intervention and a control group, and fixed cohorts of m individuals enrolled in each community prior to randomization. Formulas to determine k or m may be obtained by adjusting standard sample size formulas to account for the intracluster correlation coefficient rho. In the presence of individual-level attrition however, observed cohort sizes are likely to vary. We show that conventional approaches of accounting for potential attrition, such as dividing standard sample size formulas by the anticipated follow-up rate pi or using the average anticipated cohort size m pi, may, respectively, overestimate or underestimate the required sample size when cluster follow-up rates are highly variable, and m or rho are large. We present new sample size estimation formulas for the comparison of two means or two proportions, which appropriately account for variation among cluster follow-up rates. These formulas are derived by specifying a model for the binary missingness indicators under the population-averaged approach, assuming an exchangeable intracluster correlation coefficient, denoted by tau. To aid in the planning of future trials, we recommend that estimates for tau be reported in published community intervention trials.-Accounting for expected attrition in the planning of community intervention trials.",1
"It can be difficult to conduct pediatric clinical trials because there is often a low incidence of the disease in children, making accrual slow or infeasible. In addition, low mortality and morbidity in this population make it impractical to achieve adequate power. In this case, the only evidence for treatment efficacy comes from adult trials. Since pediatric care providers are accustomed to relying on evidence from adult studies, it is natural to consider borrowing information from adult trials. The goal of this article is to propose a Bayesian approach to the design and analysis of pediatric trials to allow borrowing strength from previous or simultaneous adult trials. We apply a hierarchical model for which the efficacy parameter from the adult trial and that of the pediatric trail are considered to be draws from a normal distribution. The choice of (the variance of) this distribution is guided by discussion with medical experts. We show that with this information, one can calculate the sample size required for the pediatric trial. We discuss how inference of these studies in pediatric populations depends on the parameter that captures the similarity of the treatment efficacy in adults compared to children. The Bayesian approach can substantially increase the power of a pediatric clinical trial (or equivalently decrease the number of subjects required) by formally leveraging the data from the adult trial. Our method relies on obtaining a value for the inter-study variability, nu, which may be difficult to describe to a clinical investigator. The Bayesian approach has the potential of making pediatric clinical trials feasible because it has the effect of borrowing strength from adult trials, thus requiring a smaller pediatric trial to show efficacy of a drug in children.-Bayesian design using adult data to augment pediatric trials.",0
"The purpose of the present study was to continue the investigation of the membrane transport mechanisms of 20-(S)-camptothecin (CPT) in order to understand the possible role of membrane transporters on its oral bioavailability and disposition. The intestinal transport kinetics of CPT were characterized using Caco-2 cells, MDCKII wild-type cells and MDCKII cells transfected with human P-glycoprotein (PGP) (ABCB1) or human multidrug resistance protein 2 (MRP2) (ABCC2). The effects of drug concentration, inhibitors and temperature on CPT directional permeability were determined. The absorptive (apical to basolateral) and secretory (basolateral to apical) permeabilities of CPT were found to be saturable. Reduced secretory CPT permeabilities with decreasing temperatures suggests the involvement of an active, transporter-mediated secretory pathway. In the presence of etoposide, the CPT secretory permeability decreased 25.6%. However, inhibition was greater in the presence of PGP and of the breast cancer resistant protein inhibitor, GF120918 (52.5%). The involvement of additional secretory transporters was suggested since the basolateral to apical permeability of CPT was not further reduced in the presence of increasing concentrations of GF120918. To investigate the involvement of specific apically-located secretory membrane transporters, CPT transport studies were conducted using MDCKII/PGP cells and MDCKII/MRP2 cells. CPT carrier-mediated permeability was approximately twofold greater in MDCKII/PGP cells and MDCKII/MRP2 cells than in MDCKII/wild-type cells, while the apparent Km values were comparable in all three cell lines. The efflux ratio of CPT in MDCKII/PGP in the presence of 0.2 microM GF120918 was not completely reversed (3.36 to 1.49). However, the decrease in the efflux ratio of CPT in MDCKII/MRP2 cells (2.31 to 1.03) suggests that CPT efflux was completely inhibited by MK571, a potent inhibitor of the Multidrug Resistance Protein transporter family. The current results provide evidence that PGP and MRP2 mediate the secretory transport of CPT in vitro. However, the involvement of other transporters cannot be ruled out based on these studies. Since these transporters are expressed in the intestine, liver and kidney variations in their expression levels and/or regulation may be responsible for the erratic oral absorption and biliary excretion of CPT observed in human subjects.-Membrane transport of camptothecin: facilitation by human P-glycoprotein (ABCB1) and multidrug resistance protein 2 (ABCC2).",0
"Global mental health is a relatively new field that has focused on disparities in mental health services across different settings, and on innovative ways to provide feasible, acceptable, and effective services in poorly-resourced settings. Neuroethics, too, is a relatively new field, lying at the intersection of bioethics and neuroscience; it has studied the implications of neuroscientific findings for age-old questions in philosophy, as well as questions about the ethics of novel neuroscientific methods and interventions. In this essay, we address a number of issues that lie at the intersection of these two fields: an emphasis on a naturalist and empirical position, a concern with both disease and wellness, the importance of human rights in neuropsychiatric care, and the value of social inclusion and patient empowerment. These different disciplines share a number of perspectives, and future dialogue between the two should be encouraged.-Global mental health and neuroethics.",0
"Response data in longitudinal studies and group randomized trials are gathered on units that belong to clusters, within which data are usually positively correlated. Therefore, estimates and confidence intervals for intraclass correlation or variance components are helpful when designing a longitudinal study or group randomized trial. Data simulated from both study designs are used to investigate the estimation of variance and covariance parameters from the following procedures: for continuous outcomes, restricted maximum likelihood (REML) and estimating equations (EE); for binary outcomes, restricted pseudo-likelihood (REPL) and estimating equations (EE). We evaluate these procedures to see which provide valid and precise estimates as well as correct standard errors for the intraclass correlation coefficient or variance components. REML seems the better choice for estimating terms related to correlation for models with normal outcomes, especially in group randomized trial situations. Results for REML and EE are mixed when outcomes are continuous and non-normal. With binary outcomes neither REPL nor EE provides satisfactory estimation or inference in longitudinal study situations, while REPL is preferable for group randomized trials.-A comparison of generalized linear mixed model procedures with estimating equations for variance and covariance parameter estimation in longitudinal studies and group randomized trials.",1
"A methodological review is presented of 16 non-therapeutic intervention trials published over the last decade which have randomized intact clusters rather than individuals in treatment groups. Each of the trials was surveyed as to the information supplied on six methodological criteria. Although there is increasing recognition of the methodological issues associated with cluster randomization, many investigators are still not aware of the impact of this design on sample size requirements and analysis considerations. Investigators are urged to publish the cluster-specific event rates observed in their trials as a guide for the planning of future studies.-A methodological review of non-therapeutic intervention trials employing cluster randomization, 1979-1989.",1
"This article reports results from a feasibility study of a community effort to reduce the availability of legal products that youth can use to get high. The study evaluated the potential of youth purchase attempts to detect actual changes in retail availability of harmful legal products. These results were triangulated with self-reports from retailers about their own policies and practices. Before the intervention, less than half of retailers reported using any of six possible strategies identified as ways to reduce youth access to harmful products, and less than 8% of baseline youth attempts to purchase potentially harmful legal products were refused or questioned. After the low-dosage intervention, retailers reported increased use of three strategies and a statistically significant increase in the percentage of purchase attempts that were either questioned or refused by retail clerks. These findings (a) demonstrate the potential feasibility of retailer-focused environmental strategies and (b) support continued use of youth purchase attempts as a measure of actual retailer behavior.-Evaluating retailer behavior in preventing youth access to harmful legal products a feasibility test.",0
"The presence of a non-zero intracluster correlation coefficient in cluster randomized trial data has well-known statistical implications for trial design, in particular, inflating the required sample size for given specifications. However, problems in recruitment are common in such trials and there may be different costs of recruitment resulting from different recruitment strategies. Examples of how such differences arise are taken from cluster randomized trials and more intuitive methods of describing clustering are summarized which, in conjunction with such cost issues, may provide a framework that enables triallists to consider explicitly the trade-off between power and cost that is often inherent in such trials.-Recruitment strategies in a cluster randomized trial--cost implications.",1
"The number of clusters in a cluster randomized trial is often low. It is therefore likely random assignment of clusters to treatment conditions results in covariate imbalance. There are no studies that quantify the consequences of covariate imbalance in cluster randomized trials on parameter and standard error bias and on power to detect treatment effects. The consequences of covariance imbalance in unadjusted and adjusted linear mixed models are investigated by means of a simulation study. The factors in this study are the degree of imbalance, the covariate effect size, the cluster size and the intraclass correlation coefficient. The covariate is binary and measured at the cluster level; the outcome is continuous and measured at the individual level. The results show covariate imbalance results in negligible parameter bias and small standard error bias in adjusted linear mixed models. Ignoring the possibility of covariate imbalance while calculating the sample size at the cluster level may result in a loss in power of at most 25?% in the adjusted linear mixed model. The results are more severe for the unadjusted linear mixed model: parameter biases up to 100?% and standard error biases up to 200?% may be observed. Power levels based on the unadjusted linear mixed model are often too low. The consequences are most severe for large clusters and/or small intraclass correlation coefficients since then the required number of clusters to achieve a desired power level is smallest. The possibility of covariate imbalance should be taken into account while calculating the sample size of a cluster randomized trial. Otherwise more sophisticated methods to randomize clusters to treatments should be used, such as stratification or balance algorithms. All relevant covariates should be carefully identified, be actually measured and included in the statistical model to avoid severe levels of parameter and standard error bias and insufficient power levels.-How large are the consequences of covariate imbalance in cluster randomized trials: a simulation study with a continuous outcome and a binary covariate at the cluster level.",1
"Breast cancer patients may experience ipsilateral breast tumor relapse (IBTR) after breast conservation therapy. IBTR is classified as either true local recurrence or new ipsilateral primary tumor. The correct classification of IBTR status has significant implications in therapeutic decision-making and patient management. However, the diagnostic tests to classify IBTR are imperfect and prone to misclassification. In addition, some observed survival data (e.g., time to relapse, time from relapse to death) are strongly correlated with IBTR status. We present a Bayesian approach to model the potentially misclassified IBTR status and the correlated survival information. We conduct the inference using a Bayesian framework via Markov chain Monte Carlo simulation implemented in WinBUGS. Extensive simulation shows that the proposed method corrects biases and provides more efficient estimates for the covariate effects on the probability of IBTR and the diagnostic test accuracy. Moreover, our method provides useful subject-specific patient prognostic information. Our method is motivated by, and applied to, a dataset of 397 breast cancer patients.-A Bayesian model for misclassified binary outcomes and correlated survival data with applications to breast cancer.",0
"Cluster randomization trials are increasingly being used in primary care research. The main feature of these trials is that patients are nested within large clusters such as physician practices or communities and the intervention is applied to the cluster. This study design necessitates calculation of intraclass correlation coefficients in order to determine the required sample size. The purpose of this study is to determine intraclass correlation coefficients for a number of outcome measures at the primary care practice level. The CEART study is a randomized trial testing the effectiveness of translating ATP III guidelines into clinical practice, with primary care physician practices as the unit of randomization and patients as the unit of data collection. The intraclass correlation coefficient (ICC) was&lt;0.02 and the design effect ranged from 1.0 to 2.3, respectively, for weight, total cholesterol, LDL, non-HDL, glucose, creatinine, and % at non-HDL goal. For smoking status, body mass index, systolic blood pressure, HDL cholesterol triglycerides, total cholesterol/HDL ratio and % at LDL goal, the ICC was 0.02-0.047 and the design effect was 2.6-4.1. The largest ICCs (0.05-0.12) and design effects (4.4-9.4) were found for height and diastolic blood pressure. These findings suggest that cluster randomization may substantially increase the sample size necessary to maintain adequate statistical power for selected outcomes such as diastolic blood pressure studies compared with simple randomization for most outcomes evaluated in this study where the design effect is small to moderate. Overall, the ICCs presented will be useful in calculating sample sizes at the primary care level.-Intraclass correlation coefficients for cluster randomized trials in primary care: the cholesterol education and research trial (CEART).",1
"Persons with human immunodeficiency virus (HIV) have higher risks for myocardial infarction (MI) than the general population. This is driven in part by higher type 2 MI (T2MI, due to coronary supply-demand mismatch) rates among persons with HIV (PWH). In the general population, T2MI has higher mortality than type 1 MI (T1MI, spontaneous and generally due to plaque rupture and thrombosis). PWH have a greater burden of comorbidities and may therefore have an even greater excess risk for complication and death in the setting of T2MI. However, mortality patterns after T1MI and T2MI in HIV are unknown. We analyzed mortality after MI among PWH enrolled in the multicenter, US-based Centers for AIDS Research Network of Integrated Clinical Systems (CNICS) cohort (N = 28,186). Incident MIs occurring between January 1, 1996, and December 31, 2014, were centrally adjudicated and classified as T1MI or T2MI. We first compared mortality following T1MI vs. T2MI among PWH. Cox survival analyses and Bayesian model averaging were then used to evaluate pre-MI covariates associated with mortality following T1MI and T2MI. Among the 596 out of 28,186 PWH who experienced MI (2.1%; 293 T1MI and 303 T2MI), mortality rates were significantly greater after T2MI (22.2/100 person-years; 1-, 3-, and 5-year mortality 39%, 52%, and 62%) than T1MI (8.2/100 person-years; 1-, 3-, and 5-year mortality 15%, 22%, and 30%). Significant mortality predictors after T1MI were higher HIV viral load, renal dysfunction, and older age. Significant predictors of mortality after T2MI were low body-mass index (BMI) and detectable HIV viral load. Mortality is high following MI for PWH and substantially greater after T2MI than T1MI. Predictors of death after MI differed by type of MI, reinforcing the different clinical scenarios associated with each MI type and the importance of considering MI types separately.-Mortality following myocardial infarction among HIV-infected persons: the Center for AIDS Research Network Of Integrated Clinical Systems (CNICS).",0
"Most information on the prevalence of drug use comes from self-report surveys. The sensitivity of such information is cause for concern about the accuracy of self-report measures. In this study, self-reported drug use in the last 48 hr is compared to results from biological assays of saliva samples from 371 young adults entering clubs. The relationship between self-reports and drug presence in oral fluid was determined for three substances as follows: cocaine, marijuana, and amphetamine. Forty-one percent of the participants with drugs detected in their oral fluids reported no use in the last 48 hr. The significance of these results is discussed.-Predicting drug use at electronic music dance events: self-reports and biological measurement.",0
"Ignoring a nested factor can influence the validity of statistical decisions about treatment effectiveness. Previous discussions have centered on consequences of ignoring nested factors versus treating them as random factors on Type I errors and measures of effect size (B. E. Wampold &amp; R. C. Serlin). The authors (a) discuss circumstances under which the treatment of nested provider effects as fixed as opposed to random is appropriate; (b) present 2 formulas for the correct estimation of effect sizes when nested factors are fixed; (c) present the results of Monte Carlo simulations of the consequences of treating providers as fixed versus random on effect size estimates, Type I error rates, and power; and (d) discuss implications of mistaken considerations of provider effects for the study of differential treatment effects in psychotherapy research.-Power and measures of effect size in analysis of variance with fixed versus random nested factors",2
"We investigated the impact of demographic and disease related factors on non-participation and dropout in a cluster-randomised behavioural trial in cancer patients with measurements taken between hospitalisation and 6?months thereafter. The percentages of non-participation and dropout were documented at each time point. Factors considered to be potentially related with non-participation and dropout were as follows: age, sex, marital status, education, income, employment status, tumour site and stage of disease. Of 1,338 eligible patients, 24% declined participation at baseline. Non-participation was higher in older patients (Odds Ratio [OR] 2.1, CI: 0.6-0.9) and those with advanced disease (OR 2.0, CI: 0.1-1.3). Dropout by 6 months was 25%. Dropout was more frequent with increased age (OR 2.8, CI: 0.8-1.2), advanced disease (OR 3.0, CI: 1.0-1.2), being married (OR 2.4, CI 0.7-1.1) and less frequent with university education (OR 0.4, CI -1.3 to -0.8) and middle income (OR 0.4, CI -0.9 to -0.7). When planning clinical trials, it is important to be aware of patient groups at high risk of non-participation or dropout, for example older patients or those with advanced disease. Trial designs should consider their special needs to increase their rate of participation.-Factors associated with non-participation and dropout among cancer patients in a cluster-randomised controlled trial.",1
"Manatunga and Chen [A.K. Manatunga, S. Chen, Sample size estimation for survival outcomes in cluster-randomized studies with small cluster sizes, Biometrics 56 (2000) 616-621] proposed a method to estimate sample size and power for cluster-randomized studies where the primary outcome variable was survival time. The sample size formula was constructed by considering a bivariate marginal distribution (Clayton-Oakes model) with univariate exponential marginal distributions. In this paper, a user-friendly FORTRAN 90 program was provided to implement this method and a simple example was used to illustrate the features of the program.-MSurvPow: a FORTRAN program to calculate the sample size and power for cluster-randomized clinical trials with survival outcomes.",1
"Health status is an important marker of the impact of disease on function among patients with chronic heart failure (CHF). However, the prognostic value of CHF-specific health status on long-term mortality has not been adequately evaluated. Our objective was to assess CHF-specific health status and 5-year mortality among outpatients with CHF. We analyzed data from 494 Veterans Affairs outpatients with diagnoses of CHF and objective evidence of left ventricular dysfunction who enrolled in a quality improvement intervention. We extracted information about comorbid diagnoses, severity of illness (Charlson index), health care utilization, drug therapy, laboratory, and vital sign data along with generic and CHF-specific health status. We then identified multivariate correlates of subsequent mortality at 5 years. Five-year mortality was 44%. Age (chi2=26.1, hazard ratio [HR]=1.63, confidence interval [CI]: 1.35, 1.97; P&lt;0.0001) and Charlson index (chi2=12.9, HR=1.39, CI: 1.16, 1.67; P=0.0003) were significantly associated with 5-year mortality. Controlling for clinical, lab, medication, and administrative data, a single-item assessing change in CHF-specific health status was independently associated with 5-year mortality (chi2=11.4, HR=0.87, CI: 0.80, 0.94, P=0.0007). Given the strength of the association with mortality, health care providers should routinely assess this single-item change in health status among outpatients with CHF to identify higher risk patients and guide therapy.-A single health status question had important prognostic value among outpatients with chronic heart failure.",0
School-level intraclass correlation for physical activity in adolescent girls: Estimates and applications,1
Cluster Randomization Trials in Epidemiology - Theory and Application,1
"Estimating causal effects in psychiatric clinical trials is often complicated by treatment non-compliance and missing outcomes. While new estimators have recently been proposed to address these problems, they do not allow for inclusion of continuous covariates. We propose estimators that adjust for continuous covariates in addition to non-compliance and missing data. Using simulations, we compare mean squared errors for the new estimators with those of previously established estimators. We then illustrate our findings in a study examining the efficacy of clozapine versus haloperidol in the treatment of refractory schizophrenia. For data with continuous or binary outcomes in the presence of non-compliance, non-ignorable missing data, and a covariate effect, the new estimators generally performed better than the previously established estimators. In the clozapine trial, the new estimators gave point and interval estimates similar to established estimators. We recommend the new estimators as they are unbiased even when outcomes are not missing at random and they are more efficient than established estimators in the presence of covariate effects under the widest variety of circumstances.-Covariate adjustment in clinical trials with non-ignorable missing data and non-compliance.",0
"The cluster randomized trial with a concurrent economic evaluation is considered the gold standard evaluative design for the conduct of implementation research evaluating different strategies to promote the transfer of research findings into clinical practice. This has implications for the planning of such studies, as information is needed on the effects of clustering on both effectiveness and efficiency outcomes. This paper describes the design considerations specific to implementation research studies, focusing particularly on the estimation of sample size requirements and on the need for reliable information on intracluster correlation coefficients for both effectiveness and efficiency outcomes.-Cluster trials in implementation research: estimation of intracluster correlation coefficients and sample size.",1
Partially Nested Randomized Controlled Trials in Education Research: A Guide to Design and Analysis,2
"Inflammation has been implicated in ovarian carcinogenesis. However, studies investigating the association between pelvic inflammatory disease (PID) and ovarian cancer risk are few and inconsistent. We investigated the association between PID and the risk of epithelial ovarian cancer according to tumor behavior and histotype. We pooled data from 13 case-control studies, conducted between 1989 and 2009, from the Ovarian Cancer Association Consortium (OCAC), including 9,162 women with ovarian cancers, 2,354 women with borderline tumors, and 14,736 control participants. Study-specific odds ratios were estimated and subsequently combined into a pooled odds ratio using a random-effects model. A history of PID was associated with an increased risk of borderline tumors (pooled odds ratio (pOR) =?1.32, 95% confidence interval (CI): 1.10, 1.58). Women with at least 2 episodes of PID had a 2-fold increased risk of borderline tumors (pOR?=?2.14, 95% CI: 1.08, 4.24). No association was observed between PID and ovarian cancer risk overall (pOR?=?0.99, 95% CI: 0.83, 1.19); however, a statistically nonsignificantly increased risk of low-grade serous tumors (pOR?=?1.48, 95% CI: 0.92, 2.38) was noted. In conclusion, PID was associated with an increased risk of borderline ovarian tumors, particularly among women who had had multiple episodes of PID. Although our results indicated a histotype-specific association with PID, the association of PID with ovarian cancer risk is still somewhat uncertain and requires further investigation.-Pelvic Inflammatory Disease and the Risk of Ovarian Cancer and Borderline Ovarian Tumors: A Pooled Analysis of 13 Case-Control Studies.",0
"Background Cluster randomized trials have been utilized to evaluate the effectiveness of HIV prevention strategies on reducing incidence. Design of such studies must take into account possible correlation of outcomes within randomized units. Purpose To discuss power and sample size considerations for cluster randomized trials of combination HIV prevention, using an HIV prevention study in Botswana as an illustration. Methods We introduce a new agent-based model to simulate the community-level impact of a combination prevention strategy and investigate how correlation structure within a community affects the coefficient of variation - an essential parameter in designing a cluster randomized trial. Results We construct collections of sexual networks and then propagate HIV on them to simulate the disease epidemic. Increasing level of sexual mixing between intervention and standard-of-care (SOC) communities reduces the difference in cumulative incidence in the two sets of communities. Fifteen clusters per arm and 500 incidence cohort members per community provide 95% power to detect the projected difference in cumulative HIV incidence between SOC and intervention communities (3.93% and 2.34%) at the end of the third study year, using a coefficient of variation 0.25. Although available formulas for calculating sample size for cluster randomized trials can be derived by assuming an exchangeable correlation structure within clusters, we show that deviations from this assumption do not generally affect the validity of such formulas. Limitations We construct sexual networks based on data from Likoma Island, Malawi, and base disease progression on longitudinal estimates from an incidence cohort in Botswana and in Durban as well as a household survey in Mochudi, Botswana. Network data from Botswana and larger sample sizes to estimate rates of disease progression would be useful in assessing the robustness of our model results. Conclusion Epidemic modeling plays a critical role in planning and evaluating interventions for prevention. Simulation studies allow us to take into consideration available information on sexual network characteristics, such as mixing within and between communities as well as coverage levels for different prevention modalities in the combination prevention package.-Sample size considerations in the design of cluster randomized trials of combination HIV prevention.",1
"Cluster randomized and multicentre trials evaluate the effect of a treatment on persons nested within clusters, for instance, patients within clinics or pupils within schools. Optimal sample sizes at the cluster (centre) and person level have been derived under the restrictive assumption of equal sample sizes per cluster. This paper addresses the relative efficiency of unequal versus equal cluster sizes in case of cluster randomization and person randomization within clusters. Starting from maximum likelihood parameter estimation, the relative efficiency is investigated numerically for a range of cluster size distributions. An approximate formula is presented for computing the relative efficiency as a function of the mean and variance of cluster size and the intraclass correlation, which can be used for adjusting the sample size. The accuracy of this formula is checked against the numerical results and found to be quite good. It is concluded that the loss of efficiency due to variation of cluster sizes rarely exceeds 10 per cent and can be compensated by sampling 11 per cent more clusters.-Relative efficiency of unequal versus equal cluster sizes in cluster randomized and multicentre trials.",1
"and This article describes several ethical, legal, and social issues typical of international genetics biobanking, as encountered in the Type 1 Diabetes Genetics Consortium (T1DGC). By studying the examples set and lessons learned from other international biobanking studies and by devoting considerable time and resources to identifying, addressing, and continually monitoring ethical and regulatory concerns, T1DGC was able to minimize the problems reported by some earlier studies. Several important conclusions can be drawn based on the experience in this study: (1) Basic international standards for research ethics review and informed consent are broadly consistent across developed countries. (2) When consent forms are adapted locally and translated into different languages, discrepancies are inevitable and therefore require prompt central review and resolution before research is initiated. (3) Providing separate 'check-box' consent for different elements of a study creates confusion and may not be essential. (4) Creating immortalized cell lines to aid future research is broadly acceptable, both in the US and internationally. (5) Imposing some limits on the use of stored samples aids in obtaining ethics approvals worldwide. (6) Allowing potential commercial uses of donated samples is controversial in some Asian countries. (7) Obtaining government approvals can be labor-intensive and time-consuming, and can require legal and diplomatic skills.-Biobanking, consent, and commercialization in international genetics research: the Type 1 Diabetes Genetics Consortium.",0
"In many studies, it is of interest to predict the future trajectory of subjects based on their historical data, referred to as dynamic prediction. Mixed effects models have traditionally been used for dynamic prediction. However, the commonly used random intercept and slope model is often not sufficiently flexible for modeling subject-specific trajectories. In addition, there may be useful exposures/predictors of interest that are measured concurrently with the outcome, complicating dynamic prediction. To address these problems, we propose a dynamic functional concurrent regression model to handle the case where both the functional response and the functional predictors are irregularly measured. Currently, such a model cannot be fit by existing software. We apply the model to dynamically predict children's length conditional on prior length, weight, and baseline covariates. Inference on model parameters and subject-specific trajectories is conducted using the mixed effects representation of the proposed model. An extensive simulation study shows that the dynamic functional regression model provides more accurate estimation and inference than existing methods. Methods are supported by fast, flexible, open source software that uses heavily tested smoothing techniques.-Dynamic prediction in functional concurrent regression with an application to child growth.",0
"Clinical trials increasingly use results of diagnostic tests as surrogate outcomes. Our objective was to answer the following questions: (1) is the parameter measured by the reference standard a valid surrogate? (2) How does the tests accuracy influence the estimate of the treatment benefit on surrogate? (3) Is it possible to correct the measured treatment effect given by results of inaccurate tests? We reviewed the literature on asymptomatic deep venous thrombosis (DVT), detected by the reference standard and other imaging techniques, as surrogate for venous thromboembolism. The influence of test inaccuracy on the measurement of treatment benefit was calculated as a function of the patient baseline risk, the treatment effect model, and test performances. We show that: (1) asymptomatic DVT is correlated with clinical outcomes but is yet to be established as a surrogate; (2) inaccurate diagnostic test underestimates the treatment effect on surrogate; (3) the prevalence of the disease, the treatment effect model, and the accuracy of the test and the reference standard used to evaluate it need to be known to correct this underestimation. Even when the surrogate end point is valid, without a reliable study of the diagnostic test we cannot quantify the true treatment effect.-The true treatment benefit is unpredictable in clinical trials using surrogate outcome measured with diagnostic tests.",0
"Cluster-randomized experiments that assign intact groups such as schools or school districts to treatment conditions are increasingly common in educational research. Such experiments are inherently multilevel designs whose sensitivity (statistical power and precision of estimates) depends on the variance decomposition across levels. This variance decomposition is usually summarized by the intraclass correlation (ICC) structure and, if covariates are used, the effectiveness of the covariates in explaining variation at each level of the design. This article provides a compilation of school- and district-level ICC values of academic achievement and related covariate effectiveness based on state longitudinal data systems. These values are designed to be used for planning group-randomized experiments in education. The use of these values to compute statistical power and plan two- and three-level group-randomized experiments is illustrated. We fit several hierarchical linear models to state data by grade and subject to estimate ICCs and covariate effectiveness. The total sample size is over 4.8 million students. We then compare our average of state estimates with the national work by Hedges and Hedberg.-Intraclass Correlations and Covariate Outcome Correlations for Planning Two- and Three-Level Cluster-Randomized Experiments in Education.",1
"The World Health Organization and collaborating institutions in four developing countries have conducted a multi-centre randomized controlled trial, in which clinics were allocated at random to two antenatal care (ANC) models. These were the standard 'Western' ANC model and a 'new' ANC model consisting of tests, clinical procedures and follow-up actions scientifically demonstrated to be effective in improving maternal and newborn outcomes. The two models were compared using the equivalence approach. This paper discusses the implications of the equivalence approach in the sample size calculation, analysis and interpretation of results of this cluster randomized trial. It reviews the ethical aspects regarding informed consent, concluding that the Zelen design has a place in cluster randomization trials. It describes the estimation of the intracluster correlation coefficient (ICC) in a stratified cluster randomized trial using two methods and reports estimates of the ICC obtained for many maternal, newborn and perinatal outcomes. Finally, it discusses analytical problems that arose: issues encountered using a composite index, heterogeneity of the intervention effect across sites, the choice of the method of analysis and the importance of efficacy analyses. The choice of the clustered Woolf estimator and the generalized estimating equations (GEE) as the methods of analysis applied is discussed.-Methodological considerations on the design and analysis of an equivalence stratified cluster randomization trial.",1
"Insufficient sleep duration and obstructive sleep apnea, two common causes of sleep deficiency in adults, can result in excessive sleepiness, a well-recognized cause of motor vehicle crashes, although their contribution to crash risk in the general population remains uncertain. The objective of this study was to evaluate the relation of sleep apnea, sleep duration, and excessive sleepiness to crash risk in a community-dwelling population. This was a prospective observational cohort study nested within the Sleep Heart Health Study, a community-based study of the health consequences of sleep apnea. The participants were 1745 men and 1456 women aged 40-89 years. Sleep apnea was measured by home polysomnography and questionnaires were used to assess usual sleep duration and daytime sleepiness. A follow-up questionnaire 2 years after baseline ascertained driving habits and motor vehicle crash history. Logistic regression analysis was used to examine the relation of sleep apnea and sleep duration at baseline to the occurrence of motor vehicle crashes during the year preceding the follow-up visit, adjusting for relevant covariates. The population-attributable fraction of motor vehicle crashes was estimated from the sample proportion of motor vehicle crashes and the adjusted odds ratios for motor vehicle crash within each exposure category. Among 3201 evaluable participants, 222 (6.9%) reported at least one motor vehicle crash during the prior year. A higher apnea-hypopnea index (p &lt; 0.01), fewer hours of sleep (p = 0.04), and self-reported excessive sleepiness (p &lt; 0.01) were each significantly associated with crash risk. Severe sleep apnea was associated with a 123% increased crash risk, compared to no sleep apnea. Sleeping 6 hours per night was associated with a 33% increased crash risk, compared to sleeping 7 or 8 hours per night. These associations were present even in those who did not report excessive sleepiness. The population-attributable fraction of motor vehicle crashes was 10% due to sleep apnea and 9% due to sleep duration less than 7 hours. Sleep deficiency due to either sleep apnea or insufficient sleep duration is strongly associated with motor vehicle crashes in the general population, independent of self-reported excessive sleepiness.-Sleep deficiency and motor vehicle crash risk in the general population: a prospective cohort study.",0
"Cluster randomized and multicenter trials sometimes combine two treatments A and B in a factorial design, with conditions such as A, B, A and B, or none. This results in a two-way nested design. The usual issue of sample size and power now arises for various clinically relevant contrast hypotheses. Assuming a fixed total sample size at each level (number of clusters or centers, number of patients), we derive the optimal proportion of the total sample to be allocated to each treatment arm. We consider treatment assignment first at the highest level (cluster randomized trial) and then at the lowest level (multicenter trial). We derive the optimal allocation ratio for various sets of clinically relevant hypotheses. We then evaluate the efficiency of each allocation and show that the popular balanced design is optimal or highly efficient for a range of research questions except for contrasting one treatment arm with all other treatment arms. We finally present simple equations for the total sample size needed to test each effect of interest in a balanced design, as a function of effect size, power and type I error ?. All results are illustrated on a cluster-randomized trial on smoking prevention in primary schools and on a multicenter trial on lifestyle improvement in general practices.-Efficient treatment allocation in two-way nested designs.",1
"Cluster randomized trials, where individuals are randomized in groups are increasingly being used in healthcare evaluation. The adoption of a clustered design has implications for design, conduct and analysis of studies. In particular, standard sample sizes have to be inflated for cluster designs, as outcomes for individuals within clusters may be correlated; inflation can be achieved either by increasing the cluster size or by increasing the number of clusters in the study. A sample size calculator is presented for calculating appropriate sample sizes for cluster trials, whilst allowing the implications of both methods of inflation to be considered.-Sample size calculator for cluster randomized trials.",1
"Clustered data are common in many fields. Some prominent examples of clustering are employees clustered within supervisors, students within classrooms, and clients within therapists. Many methods exist that explicitly consider the dependency introduced by a clustered data structure, but the multitude of available options has resulted in rigid disciplinary preferences. For example, those working in the psychological, organizational behavior, medical, and educational fields generally prefer mixed effects models, whereas those working in economics, behavioral finance, and strategic management generally prefer fixed effects models. However, increasingly interdisciplinary research has caused lines that separate the fields grounded in psychology and those grounded in economics to blur, leading to researchers encountering unfamiliar statistical methods commonly found in other disciplines. Persistent discipline-specific preferences can be particularly problematic because (a) each approach has certain limitations that can restrict the types of research questions that can be appropriately addressed, and (b) analyses based on the statistical modeling decisions common in one discipline can be difficult to understand for researchers trained in alternative disciplines. This can impede cross-disciplinary collaboration and limit the ability of scientists to make appropriate use of research from adjacent fields. This article discusses the differences between mixed effects and fixed effects models for clustered data, reviews each approach, and helps to identify when each approach is optimal. We then discuss the within-between specification, which blends advantageous properties of each framework into a single model. (PsycINFO Database Record (c) 2019 APA, all rights reserved).-Fixed effects models versus mixed effects models for clustered data: Reviewing the approaches, disentangling the differences, and making recommendations.",1
"To identify features of clinical decision support systems critical for improving clinical practice. Systematic review of randomised controlled trials. Literature searches via Medline, CINAHL, and the Cochrane Controlled Trials Register up to 2003; and searches of reference lists of included studies and relevant reviews. Studies had to evaluate the ability of decision support systems to improve clinical practice. Studies were assessed for statistically and clinically significant improvement in clinical practice and for the presence of 15 decision support system features whose importance had been repeatedly suggested in the literature. Seventy studies were included. Decision support systems significantly improved clinical practice in 68% of trials. Univariate analyses revealed that, for five of the system features, interventions possessing the feature were significantly more likely to improve clinical practice than interventions lacking the feature. Multiple logistic regression analysis identified four features as independent predictors of improved clinical practice: automatic provision of decision support as part of clinician workflow (P &lt; 0.00001), provision of recommendations rather than just assessments (P = 0.0187), provision of decision support at the time and location of decision making (P = 0.0263), and computer based decision support (P = 0.0294). Of 32 systems possessing all four features, 30 (94%) significantly improved clinical practice. Furthermore, direct experimental justification was found for providing periodic performance feedback, sharing recommendations with patients, and requesting documentation of reasons for not following recommendations. Several features were closely correlated with decision support systems' ability to improve patient care significantly. Clinicians and other stakeholders should implement clinical decision support systems that incorporate these features whenever feasible and appropriate.-Improving clinical practice using clinical decision support systems: a systematic review of trials to identify features critical to success.",0
"Cluster randomization trials are randomized controlled trials (RCTs) in which intact clusters of subjects are randomized to either the intervention or to the control. Cluster randomization trials require different statistical methods of analysis than do conventional randomized controlled trials due to the potential presence of within-cluster homogeneity in responses. A variety of statistical methods have been proposed in the literature for the analysis of cluster randomization trials with binary outcomes. However, little is known about the relative statistical power of these methods to detect a statistically significant intervention effect. We conducted a series of Monte Carlo simulations to examine the statistical power of three methods that compare cluster-specific response rates between arms of the trial: the t-test, the Wilcoxon rank sum test, and the permutation test; and three methods that compare subject-level response rates: an adjusted chi-square test, a logistic-normal random effects model, and a generalized estimating equations (GEE) method. In our simulations we allowed the number of clusters, the number of subjects per cluster, the intraclass correlation coefficient and the magnitude of the intervention effect to vary. We demonstrated that the GEE approach tended to have the highest power for detecting a statistically significant intervention effect. However, in most of the 240 scenarios examined, the differences between the competing statistical methods were negligible. The largest mean difference in power between any two different statistical methods across the 240 scenarios was 0.02. The largest observed difference in power between two different statistical methods across the 240 scenarios and 15 pair-wise comparisons of methods was 0.14.-A comparison of the statistical power of different methods for the analysis of cluster randomization trials with binary outcomes.",1
"In recent years, the availability of infectious disease counts in time and space has increased, and consequently, there has been renewed interest in model formulation for such data. In this paper, we describe a model that was motivated by the need to analyze hand, foot, and mouth disease surveillance data in China. The data are aggregated by geographical areas and by week, with the aims of the analysis being to gain insight into the space-time dynamics and to make short-term predictions, which will aid in the implementation of public health campaigns in those areas with a large predicted disease burden. The model we develop decomposes disease-risk into marginal spatial and temporal components and a space-time interaction piece. The latter is the crucial element, and we use a tensor product spline model with a Markov random field prior on the coefficients of the basis functions. The model can be formulated as a Gaussian Markov random field and so fast computation can be carried out using the integrated nested Laplace approximation approach. A simulation study shows that the model can pick up complex space-time structure and our analysis of hand, foot, and mouth disease data in the central north region of China provides new insights into the dynamics of the disease.-Bayesian penalized spline models for the analysis of spatio-temporal count data.",0
"Treatment non-adherence in randomised trials refers to situations where some participants do not receive their allocated treatment as intended. For cluster randomised trials, where the unit of randomisation is a group of participants, non-adherence may occur at the cluster or individual level. When non-adherence occurs, randomisation no longer guarantees that the relationship between treatment receipt and outcome is unconfounded, and the power to detect the treatment effects in intention-to-treat analysis may be reduced. Thus, recording adherence and estimating the causal treatment effect adequately are of interest for clinical trials. To assess the extent of reporting of non-adherence issues in published cluster trials and to establish which methods are currently being used for addressing non-adherence, if any, and whether clustering is accounted for in these. We systematically reviewed 132 cluster trials published in English in 2011 previously identified through a search in PubMed. One-hundred and twenty three cluster trials were included in this systematic review. Non-adherence was reported in 56 cluster trials. Among these, 19 reported a treatment efficacy estimate: per protocol in 15 and as treated in 4. No study discussed the assumptions made by these methods, their plausibility or the sensitivity of the results to deviations from these assumptions. The year of publication of the cluster trials included in this review (2011) could be considered a limitation of this study; however, no new guidelines regarding the reporting and the handling of non-adherence for cluster trials have been published since. In addition, a single reviewer undertook the data extraction. To mitigate this, a second reviewer conducted a validation of the extraction process on 15 randomly selected reports. Agreement was satisfactory (93%). Despite the recommendations of the Consolidated Standards of Reporting Trials statement extension to cluster randomised trials, treatment adherence is under-reported. Among the trials providing adherence information, there was substantial variation in how adherence was defined, handled and reported. Researchers should discuss the assumptions required for the results to be interpreted causally and whether these are scientifically plausible in their studies. Sensitivity analyses to study the robustness of the results to departures from these assumptions should be performed.-Reporting non-adherence in cluster randomised trials: A systematic review.",1
"Emergent principles for the design, implementation, and analysis of cluster-based experiments in social science",1
"This study reports intraclass correlation (ICC) for dependent variables used in group-randomized trials (GRTs). The authors also document the effect of two methods suggested to reduce the impact of ICC in GRTs; these two methods are modeling time and regression adjustment for covariates. They coded and analyzed 1,188 ICC estimates from 17 published, in press, and unpublished articles representing 21 studies. Findings confirm that both methods can improve the efficiency of analyses shown to be valid across conditions common in GRTs. Investigators planning GRTs should obtain ICC estimates matched to their planned analysis so that they can size their studies properly.-Methods to reduce the impact of intraclass correlation in group-randomized trials.",1
"We address estimation of intervention effects in experimental designs in which (a) interventions are assigned at the cluster level; (b) clusters are selected to form pairs, matched on observed characteristics; and (c) intervention is assigned to one cluster at random within each pair. One goal of policy interest is to estimate the average outcome if all clusters in all pairs are assigned control versus if all clusters in all pairs are assigned to intervention. In such designs, inference that ignores individual level covariates can be imprecise because cluster-level assignment can leave substantial imbalance in the covariate distribution between experimental arms within each pair. However, most existing methods that adjust for covariates have estimands that are not of policy interest. We propose a methodology that explicitly balances the observed covariates among clusters in a pair, and retains the original estimand of interest. We demonstrate our approach through the evaluation of the Guided Care program.-Estimation of treatment effects in matched-pair cluster randomized trials by calibrating covariate imbalance between clusters.",1
"Patient outcomes can depend on the treating centre, or health professional, delivering the intervention. A health professional's skill in delivery improves with experience, meaning that outcomes may be associated with learning. Considering differences in intervention delivery at trial design will ensure that any appropriate adjustments can be made during analysis. This work aimed to establish practice for the allowance of clustering and learning effects in the design and analysis of randomised multicentre trials. A survey that drew upon quotes from existing guidelines, references to relevant publications and example trial scenarios was delivered. Registered UK Clinical Research Collaboration Registered Clinical Trials Units were invited to participate. Forty-four Units participated (N?= 50). Clustering was managed through design by stratification, more commonly by centre than by treatment provider. Managing learning by design through defining a minimum expertise level for treatment provider was common (89%). One-third reported experience in expertise-based designs. The majority of Units had adjusted for clustering during analysis, although approaches varied. Analysis of learning was rarely performed for the main analysis (n?= 1), although it was explored by other means. The insight behind the approaches used within and reasons for, or against, alternative approaches were provided. Widespread awareness of challenges in designing and analysing multicentre trials is identified. Approaches used, and opinions on these, vary both across and within Units, indicating that approaches are dependent on the type of trial. Agreeing principles to guide trial design and analysis across a range of realistic clinical scenarios should be considered.-Managing clustering effects and learning effects in the design and analysis of multicentre randomised trials: a survey to establish current practice",2
"In multicentre trials, randomisation is often carried out using permuted blocks stratified by centre. It has previously been shown that stratification variables used in the randomisation process should be adjusted for in the analysis to obtain correct inference. For continuous outcomes, the two primary methods of accounting for centres are fixed-effects and random-effects models. We discuss the differences in interpretation between these two models and the implications that each pose for analysis. We then perform a large simulation study comparing the performance of these analysis methods in a variety of situations. In total, we assessed 378 scenarios. We found that random centre effects performed as well or better than fixed-effects models in all scenarios. Random centre effects models led to increases in power and precision when the number of patients per centre was small (e.g. 10 patients or less) and, in some scenarios, when there was an imbalance between treatments within centres, either due to the randomisation method or to the distribution of patients across centres. With small samples sizes, random-effects models maintained nominal coverage rates when a degree-of-freedom (DF) correction was used. We assessed the robustness of random-effects models when assumptions regarding the distribution of the centre effects were incorrect and found this had no impact on results. We conclude that random-effects models offer many advantages over fixed-effects models in certain situations and should be used more often in practice.-Analysis of multicentre trials with continuous outcomes: when and how should we account for centre effects?",1
"Environmental factors, including infectious agents, are speculated to play a role in the rising prevalence and the geographic distribution of celiac disease, an autoimmune disorder. In the USA and Sweden where the regional variation in the frequency of celiac disease has been studied, a similarity with the geographic distribution of Lyme disease, an emerging multisystemic infection caused by Borrelia burgdorferi spirochetes, has been found, thus raising the possibility of a link. We aimed to determine if infection with Borrelia contributes to an increased risk of celiac disease. Biopsy reports from all of Sweden's pathology departments were used to identify 15,769 individuals with celiac disease. Through linkage to the nationwide Patient Register, we compared the rate of earlier occurrence of Lyme disease in the patients with celiac disease to that in 78,331 matched controls. To further assess the temporal relationship between Borrelia infection and celiac disease, we also examined the risk of subsequent Lyme disease in patients with a diagnosis of celiac disease. Twenty-five individuals (0.16%) with celiac disease had a prior diagnosis of Lyme disease, whereas 79 (0.5%) had a subsequent diagnosis of Lyme disease. A modest association between Lyme disease and celiac disease was seen both before (odds ratio, 1.61; 95% confidence interval (CI), 1.06-2.47) and after the diagnosis of celiac disease (hazard ratio, 1.82; 95% CI, 1.40-2.35), with the risk of disease being highest in the first year of follow-up. Only a minor fraction of the celiac disease patient population had a prior diagnosis of Lyme disease. The similar association between Lyme disease and celiac disease both before and after the diagnosis of celiac disease is strongly suggestive of surveillance bias as a likely contributor. Taken together, the data indicate that Borrelia infection is not a substantive risk factor in the development of celiac disease.-Borrelia infection and risk of celiac disease.",0
"Vaccination in populations can have several kinds of effects. Establishing that vaccination produces population-level effects beyond the direct effects in the vaccinated individuals can have important consequences for public health policy. Formal methods have been developed for study designs and analysis that can estimate the different effects of vaccination. However, implementing field studies to evaluate the different effects of vaccination can be expensive, of limited generalizability, or unethical. It would be advantageous to use routinely collected data to estimate the different effects of vaccination. We consider how different types of data are needed to estimate different effects of vaccination. The examples include rotavirus vaccination of young children, influenza vaccination of elderly adults, and a targeted influenza vaccination campaign in schools. Directions for future research are discussed. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-Estimating population effects of vaccination using large, routinely collected data.",0
"To examine the relation between health and several dimensions of sexuality and to estimate years of sexually active life across sex and health groups in middle aged and older adults. Cross sectional study. Two samples representative of the US population: MIDUS (the national survey of midlife development in the United States, 1995-6) and NSHAP (the national social life, health and ageing project, 2005-6). 3032 adults aged 25 to 74 (1561 women, 1471 men) from the midlife cohort (MIDUS) and 3005 adults aged 57 to 85 (1550 women, 1455 men) from the later life cohort (NSHAP). Sexual activity, quality of sexual life, interest in sex, and average remaining years of sexually active life, referred to as sexually active life expectancy. Overall, men were more likely than women to be sexually active, report a good quality sex life, and be interested in sex. These gender differences increased with age and were greatest among the 75 to 85 year old group: 38.9% of men compared with 16.8% of women were sexually active, 70.8% versus 50.9% of those who were sexually active had a good quality sex life, and 41.2% versus 11.4% were interested in sex. Men and women reporting very good or excellent health were more likely to be sexually active compared with their peers in poor or fair health: age adjusted odds ratio 2.2 (P&lt;0.01) for men and 1.6 (P&lt;0.05) for women in the midlife study and 4.6 (P&lt;0.001) for men and 2.8 (P&lt;0.001) for women in the later life study. Among sexually active people, good health was also significantly associated with frequent sex (once or more weekly) in men (adjusted odds ratio 1.6 to 2.1), with a good quality sex life among men and women in the midlife cohort (adjusted odds ratio 1.7), and with interest in sex. People in very good or excellent health were 1.5 to 1.8 times more likely to report an interest in sex than those in poorer health. At age 30, sexually active life expectancy was 34.7 years for men and 30.7 years for women compared with 14.9 to 15.3 years for men and 10.6 years for women at age 55. This gender disparity attenuated for people with a spouse or other intimate partner. At age 55, men in very good or excellent health on average gained 5-7 years of sexually active life compared with their peers in poor or fair health. Women in very good or excellent health gained 3-6 years compared with women in poor or fair health. Sexual activity, good quality sexual life, and interest in sex were higher for men than for women and this gender gap widened with age. Sexual activity, quality of sexual life, and interest in sex were positively associated with health in middle age and later life. Sexually active life expectancy was longer for men, but men lost more years of sexually active life as a result of poor health than women.-Sex, health, and years of sexually active life gained due to good health: evidence from two US population based cross sectional surveys of ageing.",0
"In group randomized studies, the sample size calculations are complicated by within group (worksite, community, etc.) correlation. We compare by simulation the moment method and the more standard ANOVA method of estimating the intraclass correlation. We find the former is less biased for a small to moderate number of clusters but the difference disappears when the appropriate degree of freedom is used for the ANOVA estimator. We propose a simulation approach for sample size determination and illustrate it with an example.-Correlated binomial variates: properties of estimator of intraclass correlation and its effect on sample size calculation.",1
"Consider a comparative, randomized clinical study with a specific event time as the primary end point. In the presence of censoring, standard methods of summarizing the treatment difference are based on Kaplan-Meier curves, the logrank test, and the point and interval estimates via Cox's procedure. Moreover, for designing and monitoring the study, one usually utilizes an event-driven scheme to determine the sample sizes and interim analysis time points. When the proportional hazards (PHs) assumption is violated, the logrank test may not have sufficient power to detect the difference between two event time distributions. The resulting hazard ratio estimate is difficult, if not impossible, to interpret as a treatment contrast. When the event rates are low, the corresponding interval estimate for the 'hazard ratio' can be quite large due to the fact that the interval length depends on the observed numbers of events. This may indicate that there is not enough information for making inferences about the treatment comparison even when there is no difference between two groups. This situation is quite common for a postmarketing safety study. We need an alternative way to quantify the group difference. Instead of quantifying the treatment group difference using the hazard ratio, we consider an easily interpretable and model-free parameter, the integrated survival rate difference over a prespecified time interval, as an alternative. We present the inference procedures for such a treatment contrast. This approach is purely nonparametric and does not need any model assumption such as the PHs. Moreover, when we deal with equivalence or noninferiority studies and the event rates are low, our procedure would provide more information about the treatment difference. We used a cardiovascular trial data set to illustrate our approach. The results using the integrated event rate differences have a heuristic interpretation for the treatment difference even when the PHs assumption is not valid. When the event rates are low, for example, for the cardiovascular study discussed in this article, the procedure for the integrated event rate difference provides tight interval estimates in contrast to those based on the event-driven inference method. The design of a trial with the integrated event rate difference may be more complicated than that using the event-driven procedure. One may use simulation to determine the sample size and the estimated duration of the study. The procedure discussed in this article can be a useful alternative to the standard PHs method in the survival analysis.-Utilizing the integrated difference of two survival functions to quantify the treatment contrast for designing, monitoring, and analyzing a comparative clinical study.",0
"The Poisson regression model using a sandwich variance estimator has become a viable alternative to the logistic regression model for the analysis of prospective studies with independent binary outcomes. The primary advantage of this approach is that it readily provides covariate-adjusted risk ratios and associated standard errors. In this article, the model is extended to studies with correlated binary outcomes as arise in longitudinal or cluster randomization studies. The key step involves a cluster-level grouping strategy for the computation of the middle term in the sandwich estimator. For a single binary exposure variable without covariate adjustment, this approach results in risk ratio estimates and standard errors that are identical to those found in the survey sampling literature. Simulation results suggest that it is reliable for studies with correlated binary data, provided the total number of clusters is at least 50. Data from observational and cluster randomized studies are used to illustrate the methods.-Extension of the modified Poisson regression model to prospective studies with correlated binary data.",1
"Sample size requirements are provided for designs of studies in which clusters are randomized within each of several strata, where cluster size itself may be a stratifying factor. The approach generalizes a formula derived by Woolson et al., which provides sample size requirements for the Cochran-Mantel-Haenszel statistic. Issues of data analysis are also discussed.-Sample size requirements for stratified cluster randomization designs.",1
"In lifestyle intervention trials, where the goal is to change a participant's weight or modify their eating behavior, self-reported diet is a longitudinal outcome variable that is subject to measurement error. We propose a statistical framework for correcting for measurement error in longitudinal self-reported dietary data by combining intervention data with auxiliary data from an external biomarker validation study where both self-reported and recovery biomarkers of dietary intake are available. In this setting, dietary intake measured without error in the intervention trial is missing data and multiple imputation is used to fill in the missing measurements. Since most validation studies are cross-sectional, they do not contain information on whether the nature of the measurement error changes over time or differs between treatment and control groups. We use sensitivity analyses to address the influence of these unverifiable assumptions involving the measurement error process and how they affect inferences regarding the effect of treatment. We apply our methods to self-reported sodium intake from the PREMIER study, a multi-component lifestyle intervention trial.-Measurement error correction and sensitivity analysis in longitudinal dietary intervention studies using an external validation study.",0
[Cluster randomised trials].,1
"Multilevel statistical models have become increasingly popular among public health researchers over the past decade. Yet the enthusiasm with which these models are being adopted may obscure rather than solve some problems of statistical and substantive inference. We discuss the three most common applications of multilevel models in public health: (a) cluster-randomized trials, (b) observational studies of the multilevel etiology of health and disease, and (c) assessments of health care provider performance. In each area of investigation, we describe how multilevel models are being applied, comment on the validity of the statistical and substantive inferences being drawn, and suggest ways in which the strengths of multilevel models might be more fully exploited. We conclude with a call for more careful thinking about multilevel causal inference.-Statistical and substantive inferences in public health: issues in the application of multilevel models.",1
"A diet rich in fruit, vegetables and dietary fibre and low in fat is associated with reduced risk of chronic disease. This review aimed to estimate the effectiveness of interventions to promote healthy diet for primary prevention among participants attending primary care. A systematic review of trials using individual or cluster randomisation of interventions delivered in primary care to promote dietary change over 12 months in healthy participants free from chronic disease or defined high risk states. Outcomes were change in fruit and vegetable intake, consumption of total fat and fibre and changes in serum cholesterol concentration. Ten studies were included with 12,414 participants. The design and delivery of interventions were diverse with respect to grounding in behavioural theory and intervention intensity. A meta-analysis of three studies showed an increase in fruit consumption of 0.25 (0.01 to 0.49) servings per day, with an increase in vegetable consumption of 0.25 (0.06 to 0.44) serving per day. A further three studies that reported on fruit and vegetable consumption together showed a pooled increment of 0.50 (0.13 to 0.87) servings per day. The pooled effect on consumption of dietary fibre, from four studies, was estimated to be 1.97 (0.43 to 3.52) gm fibre per day. Data from five studies showed a mean decrease in total fat intake of 5.2% of total energy (1.5 to 8.8%). Data from three studies showed a mean decrease in serum cholesterol of 0.10 (-0.19 to 0.00) mmol/L. Presently-reported interventions to promote healthy diet for primary prevention in primary care, which illustrate a diverse range of intervention methods, may yield small beneficial changes in consumption of fruit, vegetables, fibre and fat over 12 months. The present results do not exclude the possibility that more effective intervention strategies might be developed.-Effectiveness of interventions to promote healthy diet in primary care: systematic review and meta-analysis of randomised controlled trials.",1
"We obtain closed-form asymptotic variance formulae for three point estimators of the intraclass correlation coefficient that may be applied to binary outcome data arising in clusters of variable size. Our results include as special cases those that have previously appeared in the literature (Fleiss and Cuzick, 1979, Applied Psychological Measurement 3, 537-542; Bloch and Kraemer, 1989, Biometrics 45, 269-287; Altaye, Donner, and Klar, 2001, Biometrics 57, 584-588). Simulation results indicate that confidence intervals based on the estimator proposed by Fleiss and Cuzick provide coverage levels close to nominal over a wide range of parameter combinations. Two examples are presented.-Confidence interval estimation of the intraclass correlation coefficient for binary outcome data.",1
"Methodological work on randomized trials has largely concerned pharmacological interventions in which the effects of the attending health professional may be regarded as minor. In other clinical settings, such as surgery, talk or physical therapies, staff specific variation may make generalization problematic, undermining the value of the trial. Such variation has been the basis of some objections to controlled trial methodology and non-acceptance of trial results. The implication of this source of variation will be considered for studies in which different types of health professional deliver the intervention in each arm of the trial. Such a trial may involve individual patient or group randomization. Whichever method is used, it is argued that variation in outcome between health professionals may lead to design effects. These issues will be illustrated using data from a large trial comparing primary care service delivered by two types of medical doctor. Random effect models are most suitable for analyzing this type of trial, as they allow adjustment for patient characteristics whilst controlling for design effects. This type of model illustrates that there can be substantial variation in the performance within each category of doctor.-The implications of variation in outcome between health professionals for the design and analysis of randomized controlled trials",2
"Dynamic treatment regimes (DTRs) adaptively prescribe treatments based on patients' intermediate responses and evolving health status over multiple treatment stages. Data from sequential multiple assignment randomization trials (SMARTs) are recommended to be used for learning DTRs. However, due to re-randomization of the same patients over multiple treatment stages and a prolonged follow-up period, SMARTs are often difficult to implement and costly to manage, and patient adherence is always a concern in practice. To lessen such practical challenges, we propose an alternative approach to learn optimal DTRs by synthesizing independent trials over different stages. Specifically, at each stage, data from a single randomized trial along with patients' natural medical history and health status in previous stages are used. We use a backward learning method to estimate optimal treatment decisions at a particular stage, where patients' future optimal outcome increments are estimated using data observed from independent trials with future stages' information. Under some conditions, we show that the proposed method yields consistent estimation of the optimal DTRs and we obtain the same learning rates as those from SMARTs. We conduct simulation studies to demonstrate the advantage of the proposed method. Finally, we learn optimal DTRs for treating major depressive disorder (MDD) by stagewise synthesis of two randomized trials. We perform a validation study on independent subjects and show that the synthesized DTRs lead to the greatest MDD symptom reduction compared to alternative methods.-Synthesizing independent stagewise trials for optimal dynamic treatment regimes.",0
"Slow recruitment in clinical trials leads to increased costs and resource utilization, which includes both the clinic staff and patient volunteers. Careful planning and monitoring of the accrual process can prevent the unnecessary loss of these resources. We propose two hierarchical extensions to the existing Bayesian constant accrual model: the accelerated prior and the hedging prior. The new proposed priors are able to adaptively utilize the researcher's previous experience and current accrual data to produce the estimation of trial completion time. The performance of these models, including prediction precision, coverage probability, and correct decision-making ability, is evaluated using actual studies from our cancer center and simulation. The results showed that a constant accrual model with strongly informative priors is very accurate when accrual is on target or slightly off, producing smaller mean squared error, high percentage of coverage, and a high number of correct decisions as to whether or not continue the trial, but it is strongly biased when off target. Flat or weakly informative priors provide protection against an off target prior but are less efficient when the accrual is on target. The accelerated prior performs similar to a strong prior. The hedging prior performs much like the weak priors when the accrual is extremely off target but closer to the strong priors when the accrual is on target or only slightly off target. We suggest improvements in these models and propose new models for future research.-Modeling and validating Bayesian accrual models on clinical data and simulations using adaptive priors.",0
"To evaluate the comparability and responsiveness of Patient-Reported Outcomes Measurement Information System (PROMIS) fatigue item bank across six chronic conditions. Individuals (n?=?1,430) with chronic obstructive pulmonary disease (n?=?125), chronic heart failure (n?=?60), chronic back pain (n?=?218), major depressive disorder (n?=?196), rheumatoid arthritis (n?=?521), and cancer (n?=?310) completed assessments from the PROMIS fatigue item bank at baseline and a clinically relevant follow-up. The cancer and arthritis samples were followed in observational studies; the other four groups were enrolled immediately before a planned clinical intervention. All participants completed global ratings of change at follow-up. Linear mixed-effects models and standardized response means were estimated to examine clinical validity and responsiveness to change. All patient groups reported more fatigue than the general population (range?=?0.2-1.29 standard deviation worse). The four clinical groups with pretreatment baseline data experienced significant improvement in fatigue at follow-up (effect size range?=?0.25-0.91). Individuals reporting better overall health usually experienced larger fatigue changes than those reporting worse overall health. The results support the PROMIS fatigue measures's responsiveness to change in six different chronic conditions. In addition, these results support the ability of the PROMIS fatigue measures to compare differences in fatigue across a range of chronic conditions, thereby enabling comparative effectiveness research.-PROMIS Fatigue Item Bank had Clinical Validity across Diverse Chronic Conditions.",0
Sample size calculation for dichotomous outcomes in cluster randomization trials with varying cluster size,1
"In response to the ever increasing threat of radiological and nuclear terrorism, active development of nontoxic new drugs and other countermeasures to protect against and/or mitigate adverse health effects of radiation is ongoing. Although the classical LD(50) study used for many decades as a first step in preclinical toxicity testing of new drugs has been largely replaced by experiments that use fewer animals, the need to evaluate the radioprotective efficacy of new drugs necessitates the conduct of traditional LD(50) comparative studies (FDA, 2002, Federal Register 67, 37988-37998). There is, however, no readily available method to determine the number of animals needed for establishing efficacy in these comparative potency studies. This article presents a sample-size formula based on Student's t for comparative potency testing. It is motivated by the U.S. Food and Drug Administration's (FDA's) requirements for robust efficacy data in the testing of response modifiers in total body irradiation experiments where human studies are not ethical or feasible. Monte Carlo simulation demonstrated the formula's performance for Student's t, Wald, and likelihood ratio tests in both logistic and probit models. Importantly, the results showed clear potential for justifying the use of substantially fewer animals than are customarily used in these studies. The present article may thus initiate a dialogue among researchers who use animals for radioprotection survival studies, institutional animal care and use committees, and drug regulatory bodies to reach a consensus on the number of animals needed to achieve statistically robust results for demonstrating efficacy of radioprotective drugs.-Determination of sample sizes for demonstrating efficacy of radiation countermeasures.",0
"Prostate cancer (PrCA) is the most common malignancy in men and a leading cause of cancer mortality among males in the United States. Large geographical variation and racial disparities exist in both the incidence of PrCA and the survival rate after diagnosis. In this population-based study, a joint spatial survival model is constructed to investigate factors that affect the age at diagnosis of PrCA and the subsequent survival. The joint model for these two time-to-event outcomes is specified through parametric models for age at diagnosis and survival time conditional on diagnosis age. To account for possible correlation in these outcomes among men from the same geographical region, frailty terms are included in the survival model. Both spatially correlated and uncorrelated frailties are incorporated in each model considered. The deviance information criterion is used to select a best-fitting model within the Bayesian framework. The results from our final best-fitting model indicate that race, marital status at diagnosis, and cancer stage are significantly associated with both of the two time-to-event outcomes. No pattern emerged in the geographical distribution of age at PrCA diagnosis. In contrast, a spatially clustered pattern was observed in the geographic distribution of survival experience post diagnosis.-Joint spatial survival modeling for the age at diagnosis and the vital outcome of prostate cancer.",0
"To examine trends in and correlates of fighting and violence among youths from the 3 largest racial/ethnic groups in the United States. We derived race/ethnicity-specific prevalence estimates for fighting, group fighting, and attacks with intent to harm from the National Survey on Drug Use and Health, a population-based study of youths aged 12 to 17 years. The prevalence of youth fighting and violence decreased significantly in all racial/ethnic groups over the study period (2002-2014), dropping from a high of 33.6% in 2003 to a low of 23.7% in 2014, reflecting a 29% decrease in the relative proportion of young people involved in these behaviors. However, there was also a clear severity gradient in which year-by-year point estimates for fighting and violence were consistently highest among non-Hispanic African American youths, followed by Hispanic and then non-Hispanic White youths. Although fighting and violence are on the decline among young people in general and across racial/ethnic subgroups, there is a stable pattern of disparities in youth involvement in these behaviors.-Trends in Fighting and Violence Among Adolescents in the United States, 2002-2014.",0
"Alternating logistic regressions is an estimating equations procedure used to model marginal means of correlated binary outcomes while simultaneously specifying a within-cluster association model for log odds ratios of outcome pairs. A recent generalization of alternating logistic regressions, known as orthogonalized residuals, is extended to incorporate finite sample adjustments in the estimation of the log odds ratio model parameters for when there is a moderately small number of clusters. Bias adjustments are made both in the sandwich variance estimators and in the estimating equations for the association parameters. The proposed methods are demonstrated in a repeated cross-sectional cluster trial to reduce underage drinking in the United States, and in an analysis of dental caries incidence in a cluster randomized trial of 30 aboriginal communities in the Northern Territory of Australia. A simulation study demonstrates improved performance with respect to bias and coverage of their estimators relative to those based on the uncorrected orthogonalized residuals procedure.-Alternating logistic regressions with improved finite sample properties.",1
"We consider several designs from the family of up-and-down rules for the sequential allocation of dose levels to subjects in a dose-response study. We show that an up-and-down design can be improved by using more information than the most recent response. For example, the k-in-a-row rule uses up to the k most recent responses. We introduce a new design, the Narayana rule, which uses a local estimate of the probability of toxicity calculated from all previous responses. For the Narayana rule, as the sample size gets large, the probability of assignment goes to zero for dose levels not among the two (or three) closest to the target. Different estimators of the target dose are compared. We find that the isotonic regression estimator is superior to other estimators for small to moderate sample sizes.-Improved up-and-down designs for phase I trials.",0
"This paper considers the methods used in design and analysis of recent clinical trials of topical fluoride interventions designed to prevent the development of dental caries in children, with particular consideration given to issues related to cluster-randomized trials. Studies which met the inclusion criteria were recent clinical trials of topical fluoride interventions published since 1990, conducted in children under 16 years of age, with caries as the outcome variable. Papers not published in English were translated. Information was extracted from the published trial reports on the units of randomization and analysis. The papers were also studied to assess if reporting allowed the assessment of potential consent bias in cluster-randomized trials and the reproduction of sample size calculations. Fifteen trials published since 1990 were included, of which five were cluster randomized. Only 1 of the 5 accounted for the clustering in the analysis. For the other four trials, it was possible to calculate that values from 0.002 (for DMFS) and 0.08 (for being caries free) for the intracluster correlation coefficient within schools could result in statistically non-significant findings. 3 of the 5 cluster-randomized trials did not report the consenting procedure in enough detail to judge whether consent bias could be present. Only 1 of the total 15 trials reported a sample size calculation. In summary, researchers should be aware of the importance of correctly analyzing cluster-randomized data and thorough reporting of clinical trials according to the CONSORT guidelines.-Statistical aspects of design and analysis of clinical trials for the prevention of caries.",1
"To estimate and compare sample average treatment effects (SATE) and population average treatment effects (PATE) of a resident duty hour policy change on patient and resident outcomes using data from the Flexibility in Duty Hour Requirements for Surgical Trainees Trial (""FIRST Trial""). Secondary data from the National Surgical Quality Improvement Program and the FIRST Trial (2014-2015). The FIRST Trial was a cluster-randomized pragmatic noninferiority trial designed to evaluate the effects of a resident work hour policy change to permit greater flexibility in scheduling on patient and resident outcomes. We estimated hierarchical logistic regression models to estimate the SATE of a policy change on outcomes within an intent-to-treat framework. Propensity score-based poststratification was used to estimate PATE. This study was a secondary analysis of previously collected data. Although SATE estimates suggested noninferiority of outcomes under flexible duty hour policy versus standard policy, the noninferiority of a policy change was inconclusively noninferior based on PATE estimates due to imprecision. Propensity score-based poststratification can be valuable tools to address trial generalizability but may yield imprecise estimates of PATE when sparse strata exist.-Estimation of Population Average Treatment Effects in the FIRST Trial: Application of a Propensity Score-Based Stratification Approach.",1
"To evaluate clinical validity, including responsiveness, of Patient-Reported Outcomes Measurement Information System (PROMIS) pain interference (PROMIS-PI) and pain behavior (PROMIS-PB) T-scores. Data were aggregated from longitudinal studies of cancer, chronic low back pain (cLBP), rheumatoid arthritis, chronic obstructive pulmonary disease (COPD), and major depressive disorder (MDD). Linear mixed-effects models were used to compare baseline score differences and score changes over time. We calculated standardized response means (SRMs) for subgroups defined by self-reported change in general health and pain. A total of 1,357 individuals participated at baseline and 1,225 at follow-up. Hypotheses of significant change in PROMIS-PI and PROMIS-PB scores were supported in the intervention groups (cLBP and MDD). Differences in baseline scores for COPD exacerbators compared to stable COPD patients were in the hypothesized direction but were not statistically significant. Subgroups reporting better health showed corresponding negative SRM values supporting responsiveness of T-scores to improvement. Responsiveness to decrements was supported in some but not all clinical groups and varied by anchor. More congruent values were obtained when using a pain-specific anchor. This study provides evidence that PROMIS-PI and PROMIS-PB scores are sensitive to changes in pain in studies of interventions expected to impact pain. The results inform estimation of meaningful change and support power analyses for comparative effectiveness research.-Evidence from diverse clinical populations supported clinical validity of PROMIS pain interference and pain behavior.",0
"Multilevel logistic regression models are increasingly being used to analyze clustered data in medical, public health, epidemiological, and educational research. Procedures for estimating the parameters of such models are available in many statistical software packages. There is currently little evidence on the minimum number of clusters necessary to reliably fit multilevel regression models. We conducted a Monte Carlo study to compare the performance of different statistical software procedures for estimating multilevel logistic regression models when the number of clusters was low. We examined procedures available in BUGS, HLM, R, SAS, and Stata. We found that there were qualitative differences in the performance of different software procedures for estimating multilevel logistic models when the number of clusters was low. Among the likelihood-based procedures, estimation methods based on adaptive Gauss-Hermite approximations to the likelihood (glmer in R and xtlogit in Stata) or adaptive Gaussian quadrature (Proc NLMIXED in SAS) tended to have superior performance for estimating variance components when the number of clusters was small, compared to software procedures based on penalized quasi-likelihood. However, only Bayesian estimation with BUGS allowed for accurate estimation of variance components when there were fewer than 10 clusters. For all statistical software procedures, estimation of variance components tended to be poor when there were only five subjects per cluster, regardless of the number of clusters.-Estimating multilevel logistic regression models when the number of clusters is low: a comparison of different statistical software procedures.",1
The impact of E.F. Lindquist's text on cluster randomisation.,1
"We examined risk behaviors of female drug users, comparing those who reported recently having had sex with women (recent WSW), those who reported previously having had sex with women (former WSW), and those who reported never having had sex with women (never WSW). We used data from the Risk Evaluation and Assessment of Community Health III Study. Adjusted odds for predictors of WSW status were determined via multinomial logistic regression analyses. Of the participants, 75% were never WSW, 12% were former WSW, and 13% were recent WSW. In comparison with never WSW status, significant predictors of recent WSW status were living away from one's parents as a child (adjusted odds ratio [OR]=3.05; 95% confidence interval [CI]=1.07, 8.67) and recently having been paid for sex by men (adjusted OR=4.02; 95% CI=1.67, 9.68). Also, recently having been paid for sex by men was a significant predictor of former WSW status as opposed to never WSW status (adjusted OR=3.97; 95% CI=1.65, 9.59). The recency with which they had sex with women is one of the facets influencing the risk profile of WSW. The diverse characteristics of the WSW population need to be incorporated into future studies and risk interventions targeting this group.-Sexual and drug risk behaviors among women who have sex with women.",0
"Introduction Cluster randomized trials (CRTs) are now the gold standard in health services research, including pharmacy-based interventions. Studies of behaviour, epidemiology, lifestyle modifications, educational programs, and health care models are utilizing the strengths of cluster randomized analyses. Methodology The key property of CRTs is the unit of randomization (clusters), which may be different from the unit of analysis (individual). Subject sample size and, ideally, the number of clusters is determined by the relationship of between-cluster and within-cluster variability. The correlation among participants recruited from the same cluster is known as the intraclass correlation coefficient (ICC). Generally, having more clusters with smaller ICC values will lead to smaller sample sizes. When selecting clusters, stratification before randomization may be useful in decreasing imbalances between study arms. Participant recruitment methods can differ from other types of randomized trials, as blinding a behavioural intervention cannot always be done. When to use CRTs can yield results that are relevant for making ""real world"" decisions. CRTs are often used in non-therapeutic intervention studies (e.g. change in practice guidelines). The advantages of CRT design in pharmacy research have been avoiding contamination and the generalizability of the results. A large CRT that studied physician-pharmacist collaborative management of hypertension is used in this manuscript as a CRT example. The trial, entitled Collaboration Among Pharmacists and physicians To Improve Outcomes Now (CAPTION), was implemented in primary care offices in the United States for hypertensive patients. Limitations CRT design limitations include the need for a large number of clusters, high costs, increased training, increased monitoring, and statistical complexity.-Cluster randomized trials for pharmacy practice research.",1
"We introduce a new study design in which patients are evaluated early in their treatment for disease progression. Our design is appropriate when lack of progression, both early and late, is the criterion for treatment success. An initial cohort of n(1) patients is followed until the last one has been evaluated. If enough of these patients are progression free (PF) at an early time point, t(1) after arrival, a second cohort is recruited until n(2) total patients are evaluable for PF survival at the final time t(2). Otherwise, the trial is terminated for futility both early in time and with a minimal number of patients. Patients in the initial cohort who are PF at t(1) continue on study and are again evaluated at t(2). The design permits early stopping for rapid progression of disease, an indication of futility both for cytotoxic and newer non-cytotoxic targeted therapies. The design tests the composite hypothesis of a probability p(1) of being PF at t(1) and p(2) of being PF at t(2) given PF at t(1). Power and type I error are maintained at design point levels over a wide range of parameters p(1) and p(2). No distributional assumptions are needed other than the binomial, so the design provides rigorous power analysis for this type of study. Tables of optimal designs are supplied for a broad range of requirements.-Early stopping designs based on progression-free survival at an early time point in the initial cohort.",0
Reply to taguri and matsuyama.,0
"We review recent developments in the design and analysis of group-randomized trials (GRTs). Regarding design, we summarize developments in estimates of intraclass correlation, power analysis, matched designs, designs involving one group per condition, and designs in which individuals are randomized to receive treatments in groups. Regarding analysis, we summarize developments in marginal and conditional models, the sandwich estimator, model-based estimators, binary data, survival analysis, randomization tests, survey methods, latent variable methods and nonlinear mixed models, time series methods, global tests for multiple endpoints, mediation effects, missing data, trial reporting, and software. We encourage investigators who conduct GRTs to become familiar with these developments and to collaborate with methodologists who can strengthen the design and analysis of their trials.-Design and analysis of group-randomized trials: a review of recent methodological developments.",1
"Eating Disorders (EDs) are serious psychiatric illnesses marked by psychiatric comorbidity, medical complications, and functional impairment. Research indicates that female athletes are often at greater risk for developing ED pathology versus non-athlete females. The Female Athlete Body (FAB) study is a three-site, randomized controlled trial (RCT) designed to assess the efficacy of a behavioral ED prevention program for female collegiate athletes when implemented by community providers. This paper describes the design, intervention, and participant baseline characteristics. Future papers will discuss outcomes. Female collegiate athletes (N=481) aged 17-21 were randomized by site, team, and sport type to either FAB or a waitlist control group. FAB consisted of three sessions (1.3h each) of a behavioral ED prevention program. Assessments were conducted at baseline (pre-intervention), post-intervention (3weeks), and six-, 12-, and 18-month follow-ups. This study achieved 96% (N=481) of target recruitment (N=500). Few group differences emerged at baseline. Total sample analyses revealed moderately low baseline instances of ED symptoms and clinical cases. Health risks associated with EDs necessitate interventions for female athletes. The FAB study is the largest existing RCT for female athletes aimed at both reduction of ED risk factors and ED prevention. The methods presented and population recruited for this study represent an ideal intervention for assessing the effects of FAB on both the aforementioned outcomes. We anticipate that findings of this study (reported in future papers) will make a significant contribution to the ED risk factor reduction and prevention literature.-The Female Athlete Body (FAB) study: Rationale, design, and baseline characteristics.",0
"Determine the feasibility of using a physical-activity behavior-change (PABC) intervention for increasing physical activity and reducing disability in Veterans 1-5years following dysvascular lower-limb amputation (LLA). Cross-over, feasibility trial SETTING: VA Geriatric Research Education and Clinical Center and Veterans Homes PARTICIPANTS: 32 Veterans with dysvascular LLA (1-5years after major LLA) INTERVENTION: The home-based study, using telerehabilitation technology, is intended to reduce participant burden by removing transportation and time barriers. Participants will be randomized into two participation periods of three months (Months 1-3 and 4-6). PABC intervention will occur Months 1-3 for GROUP1 and Months 4-6 for GROUP2. During PABC Intervention, participants engage in weekly video interaction with a physical therapist, who uses a collaborative approach to develop self-monitoring, barrier identification, problem solving and action planning skills to improve physical activity. GROUP2 will participate in a no physical activity intervention, attention control in Months 1-3. GROUP1 will have a no contact, intervention ""wash-out"" period in Months 4-6. Feasibility will be determined using measures of 1) participant retention, 2) dose goal attainment, 3) participant acceptability, 4) safety, and 5) initial effect size. Effect size will be based on accelerometer-based physical activity and self-report disability using the Late-Life Function and Disability Index. This study focuses on a prevalent and understudied population with low physical activity and high levels of disability due to dysvascular LLA. The results of this study will guide future development of targeted rehabilitation research to improve long term physical activity and disability outcomes.-Physical activity behavior change for older veterans after dysvascular amputation.",0
"Recently, there has been much work on early phase cancer designs that incorporate both toxicity and efficacy data, called phase I-II designs because they combine elements of both phases. However, they do not explicitly address the phase II hypothesis test of H0 : p ? p0 , where p is the probability of efficacy at the estimated maximum tolerated dose ? from phase I and p0 is the baseline efficacy rate. Standard practice for phase II remains to treat p as a fixed, unknown parameter and to use Simon's two-stage design with all patients dosed at ?. We propose a phase I-II design that addresses the uncertainty in the estimate p=p(?) in H0 by using sequential generalized likelihood theory. Combining this with a phase I design that incorporates efficacy data, the phase I-II design provides a common framework that can be used all the way from the first dose of phase I through the final accept/reject decision about H0 at the end of phase II, utilizing both toxicity and efficacy data throughout. Efficient group sequential testing is used in phase II that allows for early stopping to show treatment effect or futility. The proposed phase I-II design thus removes the artificial barrier between phase I and phase II and fulfills the objectives of searching for the maximum tolerated dose and testing if the treatment has an acceptable response rate to enter into a phase III trial.-A new approach to designing phase I-II cancer trials for cytotoxic chemotherapies.",0
"Community intervention trials are often characterized by the allocation of intact social units to different intervention groups. The assessment of adequate sample size for such trials must take into account the statistical dependencies among responses observed within an allocated unit. However, the small numbers of units typically involved in such trials imply that many methods of analysis that have been proposed for analyzing correlated data, particularly in the case of a dichotomous outcome variable, are not applicable to such designs. In this article we investigate this issue and determine the minimum number of units required per group, for the case of both a dichotomous and a continuous outcome variable, needed to provide adequate statistical power for detecting various levels of treatment effect. The use of significance testing as a method of detecting intracluster correlation is also investigated, and, in general, discouraged.-Statistical considerations in the design and analysis of community intervention trials.",1
"Laboratory experiments often involve two groups of subjects, with a linear phenomenon observed in each subject. Simple linear regression as propounded in standard textbooks is inadequate to treat this experimental design, particularly when it comes to dealing with random variation of slopes and intercepts among subjects. The author describes several techniques that can be used to compare two independent families of lines and illustrates their use with laboratory data. The methods are described tutorially, compared, and discussed in the context of more sophisticated and more naive approaches to this common data-analytic problem. Technical details are supplied in APPENDIX A.-Families of lines: random effects in linear regression analysis.",1
"The need for clear reporting of randomized controlled trials has been emphasized recently. The CONSORT Statement has made evidence-based suggestions for a checklist and a patient flow diagram. Adapting this for cluster randomized controlled trials presents particular challenges. Simple changes in the checklist and diagram for the completely randomized two level cluster randomized trials are suggested for discussion. An example taken from an unpublished trial demonstrates that these changes are less simple to implement, although extensions to electronic publications may be helpful. These suggestions should be formally evaluated. Further work is required to consider the cases of more levels and of stratified or pair-matched cluster randomized trials.-Extending the CONSORT statement to cluster randomized trials: for discussion.",1
"Hierarchical data sets arise when the data for lower units (e.g., individuals such as students, clients, and citizens) are nested within higher units (e.g., groups such as classes, hospitals, and regions). In data collection for experimental research, estimating the required sample size beforehand is a fundamental question for obtaining sufficient statistical power and precision of the focused parameters. The present research extends previous research from Heo and Leon (2008) and Usami (2011b), by deriving closed-form formulas for determining the required sample size to test effects in experimental research with hierarchical data, and by focusing on both multisite-randomized trials (MRTs) and cluster-randomized trials (CRTs). These formulas consider both statistical power and the width of the confidence interval of a standardized effect size, on the basis of estimates from a random-intercept model for three-level data that considers both balanced and unbalanced designs. These formulas also address some important results, such as the lower bounds of the needed units at the highest levels.-Generalized sample size determination formulas for experimental research with hierarchical data.",1
"In cluster randomized crossover (CRXO) trials, groups of participants (i.e., clusters) are randomly allocated to receive a sequence of interventions over time (i.e., cluster periods). CRXO trials are becoming more comment when they are feasible, as they require fewer clusters than parallel group cluster randomized trials. However, CRXO trials have not been frequently used in orthopedic fracture trials and represent a novel methodological application within the field. To disseminate the early knowledge gained from our experience initiating two cluster randomized crossover trials, we describe our process for the identification and selection of the orthopedic practices (i.e., clusters) participating in the PREP-IT program and present data to describe their key characteristics. The PREP-IT program comprises two ongoing pragmatic cluster randomized crossover trials (Aqueous-PREP and PREPARE) which compare the effect of iodophor versus chlorhexidine solutions on surgical site infection and unplanned fracture-related reoperations in patients undergoing operative fracture management. We describe the process we used to identify and select orthopedic practices (clusters) for the PREP-IT trials, along with their characteristics. We identified 58 potential orthopedic practices for inclusion in the PREP-IT trials. After screening each practice for eligibility, we selected 30 practices for participation and randomized each to a sequence of interventions (15 for Aqueous-PREP and 20 for PREPARE). The majority of orthopedic practices included in the Aqueous-PREP and PREPARE trials were situated in level I trauma centers (100% and 87%, respectively). Orthopedic practices in the Aqueous-PREP trial operatively treated a median of 149 open fracture patients per year, included a median of 11 orthopedic surgeons, and had access to a median of 5 infection preventionists. Orthopedic practices in the PREPARE trial treated a median of 142 open fracture and 1090 closed fracture patients per year, included a median of 7.5 orthopedic surgeons, and had access to a median of 6 infection preventionists. The PREP-IT trials provide an example of how to follow the reporting standards for cluster randomized crossover trials by providing a clear definition of the cluster unit, a thorough description of the cluster identification and selection process, and sufficient description of key cluster characteristics. Both trials are registered at ClinicalTrials.gov (A-PREP: NCT03385304 December 28, 2017, and PREPARE: NCT03523962 May 14, 2018).-Cluster identification, selection, and description in cluster randomized crossover trials: the PREP-IT trials",1
"The health effects of daily activity behaviours (physical activity, sedentary time and sleep) are widely studied. While previous research has largely examined activity behaviours in isolation, recent studies have adjusted for multiple behaviours. However, the inclusion of all activity behaviours in traditional multivariate analyses has not been possible due to the perfect multicollinearity of 24-h time budget data. The ensuing lack of adjustment for known effects on the outcome undermines the validity of study findings. We describe a statistical approach that enables the inclusion of all daily activity behaviours, based on the principles of compositional data analysis. Using data from the International Study of Childhood Obesity, Lifestyle and the Environment, we demonstrate the application of compositional multiple linear regression to estimate adiposity from children's daily activity behaviours expressed as isometric log-ratio coordinates. We present a novel method for predicting change in a continuous outcome based on relative changes within a composition, and for calculating associated confidence intervals to allow for statistical inference. The compositional data analysis presented overcomes the lack of adjustment that has plagued traditional statistical methods in the field, and provides robust and reliable insights into the health effects of daily activity behaviours.-Compositional data analysis for physical activity, sedentary time and sleep research.",0
Statistics notes. Trials randomised in clusters.,1
"Gastric emptying studies are of great interest in human and veterinary medical research to evaluate effects of medications or diets for promoting gastrointestinal motility and to examine unintended side-effects of new or existing medications, diets, or procedures. Summarizing gastric emptying data is important to allow easier comparison between treatments or groups of subjects and comparisons of results among studies. The standard method for assessing gastric emptying is by using scintigraphy and summarizing the nonlinear emptying of the radioisotope. A popular model for fitting gastric emptying data is the power exponential model. This model can only describes a globally decreasing pattern and thus has the limitation of poorly describing localized intragastric events that can occur during emptying. Hence, we develop a new model for gastric emptying studies to improve population and individual inferences using a mixture of nonlinear mixed effects models. One mixture component is based on a power exponential model which captures globally decreasing patterns. The other is based on a locally extended power exponential model which captures both local bumping and rapid decay. We refer to this mixture model as a two-component nonlinear mixed effects model. The parameters in our model have clear graphical interpretations that provide a more accurate representation and summary of the curves of gastric emptying pattern. Two methods are developed to fit our proposed model: one is the mixture of an Expectation Maximization algorithm and a global two-stage method and the other is the mixture of an Expectation Maximization algorithm and the Monte Carlo Expectation Maximization algorithm. We compare our methods using simulation, showing that the two approaches are comparable to one another. For estimating the variance and covariance matrix, the second approach appears approximately more efficient and is also numerically more stable in some cases. Our new model and approaches are applicable for assessing gastric emptying in human and veterinary medical research and in many other biomedical fields such as pharmacokinetics, toxicokinetics, and physiological research. An example of gastric emptying data from equine medicine is used to demonstrate the advantage of our approaches.-A two-component nonlinear mixed effects model for longitudinal data, with application to gastric emptying studies.",0
"Randomized controlled trials provide the best method of determining which of two comparable treatments is preferable. Unfortunately, contemporary randomized trials have become increasingly expensive, complex and burdened by regulation, so much so that many trials are of doubtful feasibility. Here we present a proposal for a novel, streamlined approach to randomized trials: the ""clinically-integrated randomized trial"". The key aspect of our methodology is that the clinical experience of the patient and doctor is virtually indistinguishable whether or not the patient is randomized, primarily because outcome data are obtained from routine clinical data, or from short, web-based questionnaires. Integration of a randomized trial into routine clinical practice also implies that there should be an attempt to randomize every patient, a corollary of which is that eligibility criteria are minimized. The similar clinical experience of patients on- and off-study also entails that the marginal cost of putting an additional patient on trial is negligible. We propose examples of how the clinically-integrated randomized trial might be applied in four distinct areas of medicine: comparisons of surgical techniques, ""me too"" drugs, rare diseases and lifestyle interventions. Barriers to implementing clinically-integrated randomized trials are discussed. The proposed clinically-integrated randomized trial may allow us to enlarge dramatically the number of clinical questions that can be addressed by randomization.-The clinically-integrated randomized trial: proposed novel method for conducting large trials at low cost.",0
"We use a hierarchical model for a meta-analysis that combines information from autopsy studies of adenoma prevalence and counts. The studies we included reported findings using a variety of adenoma prevalence groupings and age categories. We use a non-homogeneous Poisson model for multinomial bin probabilities. The Poisson model allows risk to depend on age and sex, and incorporates extra-Poisson variability. We evaluate model fit using the posterior predicted distribution of adenoma prevalence reported by the studies included in our analyses and validate our model using adenoma prevalence reported by more recent colonoscopy studies. For 1990, the estimated adenoma prevalence among Americans at age 60 is 40.3 per cent for men compared to 29.2 per cent for women.-A hierarchical non-homogenous Poisson model for meta-analysis of adenoma counts.",0
"To evaluate the associations of dietary fiber after myocardial infarction (MI) and changes in dietary fiber intake from before to after MI with all cause and cardiovascular mortality. Prospective cohort study. Two large prospective cohort studies of US women and men with repeated dietary measurements: the Nurses' Health Study and the Health Professionals Follow-Up Study. 2258 women and 1840 men who were free of cardiovascular disease, stroke, or cancer at enrollment, survived a first MI during follow-up, were free of stroke at the time of initial onset of MI, and provided food frequency questionnaires pre-MI and at least one post-MI. Associations of dietary fiber post-MI and changes from before to after MI with all cause and cardiovascular mortality using Cox proportional hazards models, adjusting for drug use, medical history, and lifestyle factors. Higher post-MI fiber intake was significantly associated with lower all cause mortality (comparing extreme fifths, pooled hazard ratio 0.75, 95% confidence interval 0.58 to 0.97). Greater intake of cereal fiber was more strongly associated with all cause mortality (pooled hazard ratio 0.73, 0.58 to 0.91) than were other sources of dietary fiber. Increased fiber intake from before to after MI was significantly associated with lower all cause mortality (pooled hazard ratio 0.69, 0.55 to 0.87). In this prospective study of patients who survived MI, a greater intake of dietary fiber after MI, especially cereal fiber, was inversely associated with all cause mortality. In addition, increasing consumption of fiber from before to after MI was significantly associated with lower all cause and cardiovascular mortality.-Dietary fiber intake and mortality among survivors of myocardial infarction: prospective cohort study.",0
"The known metabolic cardiovascular disease risk factors associated with insulin resistance syndrome (IRS) do not adequately explain the excess cardiovascular disease risk attributed to this syndrome, and abnormalities in hemostatic variables may contribute to this excess risk. Using data from 322 nondiabetic elderly men and women (aged 65-100 years) participating in the Cardiovascular Health Study during 1989-1990, the authors performed factor analysis on 10 metabolic risk factors associated with IRS and 11 procoagulation, inflammation, and fibrinolysis variables to examine the clustering of the metabolic and hemostatic risk markers. Factor analysis of the metabolic variables confirmed four uncorrelated factors: body mass, insulin/glucose, lipids, and blood pressure. Adding the hemostatic variables yielded three new factors interpreted as inflammation, vitamin K-dependent proteins, and procoagulant activity. Plasminogen activator inhibitor-1 clustered with the body mass factor, supporting the hypothesis that obesity is related to impaired fibrinolysis. Fibrinogen clustered with the inflammation summary factor rather than procoagulant activity, supporting the position that fibrinogen principally reflects underlying inflammation rather than procoagulant potential. The authors conclude that should hemostatic variables be shown to contribute to IRS-related cardiovascular disease, apart from plasminogen activator inhibitor-1, they may do so independently of the established metabolic abnormalities.-Clustering of procoagulation, inflammation, and fibrinolysis variables with metabolic factors in insulin resistance syndrome.",0
"Kang, Janes and Huang propose an interesting boosting method to combine biomarkers for treatment selection. The method requires modeling the treatment effects using markers. We discuss an alternative method, outcome weighted learning. This method sidesteps the need for modeling the outcomes, and thus can be more robust to model misspecification.-Discussion of combining biomarkers to optimize patient treatment recommendations.",0
"It is known that the existence of publication bias can influence the conclusions of a meta-analysis. Some methods have been developed to deal with publication bias, but issues remain. One particular method called 'trim and fill' is designed to adjust for publication bias. The method, which is intuitively appealing and comprehensible by non-statisticians, is based on a simple and popular graphical tool called the funnel plot. We present a simulation study designed to evaluate the behaviour of this method. Our results indicate that when the studies are heterogeneous (that is, when they estimate different effects), trim and fill may inappropriately adjust for publication bias where none exists. We found that trim and fill may spuriously adjust for non-existent bias if (i) the variability among studies causes some precisely estimated studies to have effects far from the global mean or (ii) an inverse relationship between treatment efficacy and sample size is introduced by the studies' a priori power calculations. The results suggest that the funnel plot itself is inappropriate for heterogeneous meta-analyses. Selection modelling is an alternative method warranting further study. It performed better than trim and fill in our simulations, although its frequency of convergence varied, depending on the simulation parameters.-Adjusting for publication bias in the presence of heterogeneity.",0
The cluster randomized cross-over design has been proposed in particular because it prevents an imbalance that may bring into question the internal validity of parallel group cluster trials. We derived a sample size formula for continuous outcomes that takes into account both the intraclass correlation coefficient (representing the clustering effect) and the interperiod correlation (induced by the cross-over design).-Sample size calculation for cluster randomized cross-over trials.,1
"Critical illness increases the risk for poor mental health outcomes among both patients and their informal caregivers, especially their surrogate decision-makers. Surrogates who must make life-and-death medical decisions on behalf of incapacitated patients may experience additional distress. EMPOWER (Enhancing &amp; Mobilizing the POtential for Wellness &amp; Emotional Resilience) is a novel cognitive-behavioral, acceptance-based intervention delivered in the intensive care unit (ICU) setting to surrogate decision-makers designed to improve both patients' quality of life and death and dying as well as surrogates' mental health. Clinician stakeholder and surrogate participant feedback (n = 15), as well as results from an open trial (n = 10), will be used to refine the intervention, which will then be evaluated through a multisite randomized controlled trial (RCT) (n = 60) to examine clinical superiority to usual care. Feasibility, tolerability, and acceptability of the intervention will be evaluated through self-report assessments. Hierarchical linear modeling will be used to adjust for clustering within interventionists to determine the effect of EMPOWER on surrogate differences in the primary outcome, peritraumatic stress. Secondary outcomes will include symptoms of post-traumatic stress disorder, prolonged grief disorder, and experiential avoidance. Exploratory outcomes will include symptoms of anxiety, depression, and decision regret, all measured at 1 and 3 months from post-intervention assessment. Linear regression models will examine the effects of assignment to EMPOWER versus the enhanced usual care group on patient quality of life or quality of death and intensity of care the patient received during the indexed ICU stay assessed at the time of the post-intervention assessment. Participant exit interviews will be conducted at the 3-month assessment time point and will be analyzed using qualitative thematic data analysis methods. The EMPOWER study is unique in its application of evidence-based psychotherapy targeting peritraumatic stress to improve patient and caregiver outcomes in the setting of critical illness. The experimental intervention will be strengthened through the input of a variety of ICU stakeholders, including behavioral health clinicians, physicians, bereaved informal caregivers, and open trial participants. Results of the RCT will be submitted for publication in a peer-reviewed journal and serve as preliminary data for a larger, multisite RCT grant application. ClinicalTrials.gov, NCT03276559 . Retrospectively registered on 8 September 2017.-Enhancing &amp; Mobilizing the POtential for Wellness &amp; Emotional Resilience (EMPOWER)?among Surrogate Decision-Makers of ICU Patients: study protocol for a randomized controlled trial.",0
"Multiple indicators, multiple causes (MIMIC) models are often employed by researchers studying the effects of an unobservable latent variable on a set of outcomes, when causes of the latent variable are observed. There are times, however, when the causes of the latent variable are not observed because measurements of the causal variable are contaminated by measurement error. The objectives of this paper are as follows: (i) to develop a novel model by extending the classical linear MIMIC model to allow both Berkson and classical measurement errors, defining the MIMIC measurement error (MIMIC ME) model; (ii) to develop likelihood-based estimation methods for the MIMIC ME model; and (iii) to apply the newly defined MIMIC ME model to atomic bomb survivor data to study the impact of dyslipidemia and radiation dose on the physical manifestations of dyslipidemia. As a by-product of our work, we also obtain a data-driven estimate of the variance of the classical measurement error associated with an estimate of the amount of radiation dose received by atomic bomb survivors at the time of their exposure.-Multiple indicators, multiple causes measurement error models.",0
"Cluster randomised trials with unequal sized clusters often have lower precision than with clusters of equal size. To allow for this, sample sizes are inflated by a modified version of the design effect for clustering. These inflation factors are valid under the assumption that randomisation is stratified by cluster size. We investigate the impact of unequal cluster size when that constraint is relaxed, with particular focus on the stepped-wedge cluster randomised trial, where this is more difficult to achieve. Assuming a multi-level mixed effect model with exchangeable correlation structure for a cross-sectional design, we use simulation methods to compare the precision for a trial with clusters of unequal size to a trial with clusters of equal size (relative efficiency). For a range of scenarios we illustrate the impact of various design features (the cluster-mean correlation - a function of the intracluster correlation and the cluster size, the number of clusters, number of randomisation sequences) on the average and distribution of the relative efficiency. Simulations confirm that the average reduction in precision, due to varying cluster sizes, is smaller in a stepped-wedge trial compared to the parallel trial. However, the variance of the distribution of the relative efficiency is large; and is larger under the stepped-wedge design compared to the parallel design. This can result in large variations in actual power, depending on the allocation of clusters to sequences. Designs with larger variations in cluster sizes, smaller number of clusters and studies with smaller cluster-mean correlations (smaller cluster sizes or smaller intra-cluster correlation) are particularly at risk. The actual realised power in a stepped-wedge trial might be substantially higher or lower than that estimated. This is particularly important when there are a small number of clusters or the variability in cluster sizes is large. Constraining the randomisation on cluster size, where feasible, might mitigate this effect.-The impact of varying cluster size in cross-sectional stepped-wedge cluster randomised trials",3
"The stepped-wedge design for pragmatic clinical trials has received increased attention in health service-related research seeking to evaluate the effect of interventions. Compared with the parallel design, the stepped-wedge design is preferred when there is prior knowledge supporting the effectiveness and harmlessness of the intervention, and/or when practical or financial constraints exist such that the intervention can only be implemented sequentially on a fraction of clusters. In some health service studies, the study period may consist of two parts: an active implementation followed by a sustainability phase, where the intervention effect is possibly reduced. There is a gap in current literature of the stepped-wedge design for cluster randomization trials for dealing with this specific scenario. We aim to provide an analytical formula for power analysis under this situation to aid the stepped-wedge design of an ongoing PREVENT trial.-Power calculation in stepped-wedge cluster randomized trial with reduced intervention sustainability effect",3
"The pharmaceutical industry and regulatory agencies are increasingly interested in conducting bridging studies in order to bring an approved drug product from the original region (eg, United States or European Union) to a new region (eg, Asian-Pacific countries). In this article, we provide a new methodology for the design and analysis of bridging studies by assuming prior knowledge on how the null and alternative hypotheses in the original, foreign study are related to the null and alternative hypotheses in the bridging study and setting the type I error for the bridging study according to the strength of the foreign-study evidence. The new methodology accounts for randomness in the foreign-study evidence and controls the average type I error of the bridging study over all possibilities of the foreign-study evidence. In addition, the new methodology increases statistical power, when compared to approaches that do not use foreign-study evidence, and it allows for the possibility of not conducting the bridging study when the foreign-study evidence is unfavorable. Finally, we conducted extensive simulation studies to demonstrate the usefulness of the proposed?methodology.-Design and analysis of bridging studies with prior probabilities on the null and alternative hypotheses.",0
"We consider regression analysis when covariate variables are the underlying regression coefficients of another linear mixed model. A naive approach is to use each subject's repeated measurements, which are assumed to follow a linear mixed model, and obtain subject-specific estimated coefficients to replace the covariate variables. However, directly replacing the unobserved covariates in the primary regression by these estimated coefficients may result in a significantly biased estimator. The aforementioned problem can be evaluated as a generalization of the classical additive error model where repeated measures are considered as replicates. To correct for these biases, we investigate a pseudo-expected estimating equation (EEE) estimator, a regression calibration (RC) estimator, and a refined version of the RC estimator. For linear regression, the first two estimators are identical under certain conditions. However, when the primary regression model is a nonlinear model, the RC estimator is usually biased. We thus consider a refined regression calibration estimator whose performance is close to that of the pseudo-EEE estimator but does not require numerical integration. The RC estimator is also extended to the proportional hazards regression model. In addition to the distribution theory, we evaluate the methods through simulation studies. The methods are applied to analyze a real dataset from a child growth study.-Regression analysis when covariates are regression parameters of a random effects model for observed longitudinal measurements.",0
"The use and development of mobile interventions are experiencing rapid growth. In ""just-in-time"" mobile interventions, treatments are provided via a mobile device, and they are intended to help an individual make healthy decisions 'in the moment,' and thus have a proximal, near future impact. Currently, the development of mobile interventions is proceeding at a much faster pace than that of associated data science methods. A first step toward developing data-based methods is to provide an experimental design for testing the proximal effects of these just-in-time treatments. In this paper, we propose a 'micro-randomized' trial design for this purpose. In a micro-randomized trial, treatments are sequentially randomized throughout the conduct of the study, with the result that each participant may be randomized at the 100s or 1000s of occasions at which a treatment might be provided. Further, we develop a test statistic for assessing the proximal effect of a treatment as well as an associated sample size calculator. We conduct simulation evaluations of the sample size calculator in various settings. Rules of thumb that might be used in designing a micro-randomized trial are discussed. This work is motivated by our collaboration on the HeartSteps mobile application designed to increase physical activity. Copyright ? 2015 John Wiley &amp; Sons, Ltd.-Sample size calculations for micro-randomized trials in mHealth.",0
"In settings like the Ebola epidemic, where proof-of-principle trials have provided evidence of efficacy but questions remain about the effectiveness of different possible modes of implementation, it may be useful to conduct trials that not only generate information about intervention effects but also themselves provide public health benefit. Cluster randomized trials are of particular value for infectious disease prevention research by virtue of their ability to capture both direct and indirect effects of intervention, the latter of which depends heavily on the nature of contact networks within and across clusters. By leveraging information about these networks-in particular the degree of connection across randomized units, which can be obtained at study baseline-we propose a novel class of connectivity-informed cluster trial designs that aim both to improve public health impact (speed of epidemic control) and to preserve the ability to detect intervention effects. We several designs for cluster randomized trials with staggered enrollment, in each of which the order of enrollment is based on the total number of ties (contacts) from individuals within a cluster to individuals in other clusters. Our designs can accommodate connectivity based either on the total number of external connections at baseline or on connections only to areas yet to receive the intervention. We further consider a ""holdback"" version of the designs in which control clusters are held back from re-randomization for some time interval. We investigate the performance of these designs in terms of epidemic control outcomes (time to end of epidemic and cumulative incidence) and power to detect intervention effect, by simulating vaccination trials during an SEIR-type epidemic outbreak using a network-structured agent-based model. We compare results to those of a traditional Stepped Wedge trial. In our simulation studies, connectivity-informed designs lead to a 20% reduction in cumulative incidence compared to comparable traditional study designs, but have little impact on epidemic length. Power to detect intervention effect is reduced in all connectivity-informed designs, but ""holdback"" versions provide power that is very close to that of a traditional Stepped Wedge approach. Incorporating information about cluster connectivity in the design of cluster randomized trials can increase their public health impact, especially in acute outbreak settings. Using this information helps control outbreaks-by minimizing the number of cross-cluster infections-with very modest cost in terms of power to detect effectiveness.-Leveraging contact network structure in the design of cluster randomized trials.",1
"A small number of clusters and substantial variation between clusters increase the chance of unbalanced randomization in cluster randomized trials. Baseline imbalances between groups may distort intervention effects. When adjusting for imbalances in the cluster-level analysis, this results in loss of degrees of freedom. Variance reduction that can be achieved through stratification and blocking is limited. Restricted randomization is an alternative approach that ensures balanced allocation. We present the randomization scheme used in the ZAMSTAR trial of tuberculosis control interventions in Southern Africa. We used stratification and restriction to randomize 24 clusters (16 Zambian, 8 South African) into four intervention groups in a 2 x 2 factorial design. Stratification was by country and tuberculous infection prevalence and restriction by tuberculous infection prevalence, HIV prevalence, urban/rural, social context, and geographical location. Balance was defined in terms of covariate-specific tolerance thresholds for the measure of imbalance. For binary (0/1) covariates we defined imbalance = max(S(i)) - min(S(i)), where, S(i) was the number of 1s in group i = 1,2,3,4. For continuous covariates we defined imbalance = (max(M(i)) - min(M(i)))/ min(M(i) ), where, M( i) was the average in group i = 1,2,3,4.We used simulation to estimate the restriction factor (proportion of unacceptable allocations) both for individual covariates and overall. Simulation was also used to investigate the validity of the restricted randomization design, with the use of the validity matrix, by monitoring the probability that any given pair of clusters is allocated to the same intervention group. There were 3 657 930 400 possible ways of allocating the 24 clusters to the four groups after stratification. With a combined restriction factor of 0.998 this still left 7 million acceptable allocations. The final allocation was selected at a public ceremony from a randomly-generated list of acceptable allocations. The design of the allocation process was observed to be valid. The restricted randomization scheme significantly decreased the total number of available allocations of clusters into intervention groups. Our restricted randomization was successful in that it achieved good balance while preserving the impartiality and validity of the trial.-Restricted randomization of ZAMSTAR: a 2 x 2 factorial cluster randomized trial.",1
"Tobacco intervention studies that employ a community trial design require adjustment to the usual analytic methods to account for the allocation of intact social groups to study conditions and the positive intraclass correlation (p) that is inevitable in such a design. In the absence of valid estimates of the relevant p, investigators seeking to establish an appropriate sample size could only guess about the magnitude of the problem. We recently published estimates of p for common measures of adolescent tobacco use, but those estimates were unadjusted for potential covariates and so represented an upper limit on the magnitude of p. This report demonstrates how estimates of intraclass correlation may be substantially reduced through regression adjustment for easily measured covariates. Results show that both the p and the residual variance can be reduced, by an average of 20 and 11%, respectively, offering greater efficiency for investigators who plan future studies and who are able to measure those covariates in their studies. Future work should seek both to replicate this work and to extend it; for example, to cohort designs where the improvements might be even greater.-Intraclass correlation among measures related to tobacco use by adolescents: estimates, correlates, and applications in intervention studies.",1
"Recently developed methodology is applied to the analysis of data arising from a design in which an experimental intervention is assigned at random to one of two clusters in a matched pair. Standard statistical techniques do not apply to such data, since they do not take into account variation arising from differences among clusters as well as within clusters. The methodology presented provides a significance test over kappa pairs of clusters with respect to a dichotomous outcome variable while controlling for confounding. The investigation of interaction effects is also discussed.-Statistical methodology for paired cluster designs.",1
"The authors conducted a pilot study in preparation for a larger investigation that will rely on telephone surveys to assess select health behaviors of teens and their parents, with a focus on indoor tanning. This study used a randomized design to assess the impact of a presurvey letter on response rates to a telephone survey, as well as prevalence estimates of ever having used an indoor tanning facility. The letter did not have a significant effect on response rates or prevalence estimates in the two cities under study. Findings suggest that researchers should not assume that a letter sent in advance of a telephone survey will necessarily increase response rates enough to justify their use.-Effects of an introductory letter on response rates to a teen/parent telephone health survey.",0
"To assess the quality of reported consent processes of cluster-randomised trials conducted in residential facilities for older people and to explore whether the focus on improving the general conduct and reporting of cluster-randomised trials influenced the quality of conduct and reporting of ethical processes in these trials. Systematic review of cluster-randomised trials reports, published up to the end of 2010. National Library of Medicine (Medline) via PubMed, hand-searches of BMJ, Journal of the American Medical Association, BMC Health Services Research, Age and Ageing and Journal of the American Geriatrics Society, reference search in Web of Knowledge and consultation with experts. Published cluster-randomised trials where the unit of randomisation is a part or the whole of a residential facility for older people, without language or year of publication restrictions. We included 73 trials. Authors reported ethical approval in 59, obtaining individual consent in 51, and using proxies for this consent in 37, but the process to assess residents' capacity to consent was clearly reported in only eight. We rated only six trials high for the quality of consent processes. We considered that individual informed consent could have been waived legitimately in 14? of 22 trials not reporting obtaining consent. The proportions reporting ethical approval and quality of consent processes were higher in recent trials. Recently published international recommendations regarding ethical conduct in cluster-randomised trials are much needed. In relation to consent processes when cognitively impaired individuals are included in these trials, we provide a six-point checklist and recommend the minimum information to be reported. Those who lack capacity in trials with complex designs should be afforded the same care in relation to consent as competent adults in trials with simpler designs.-Consent processes in cluster-randomised trials in residential facilities for older adults: a systematic review of reporting practices and proposed guidelines.",1
"Total and abdominal obesity, as well as metabolic factors such as type 2 diabetes, have been associated with a higher risk of endometrial cancer in white women. It remains unclear to what extent these factors influence the risk of endometrial cancer in black women. We followed 47,557 participants from the Black Women's Health Study for incident endometrial cancer from 1995 through 2013 (n = 274). We used Cox regression models to estimate incidence rate ratios and 95% confidence intervals while accounting for potential confounders. Incidence rate ratios for body mass indices (weight (kg)/height (m)(2)) of 25.0-29.9, 30.0-34.9, 35.0-39.9, and ?40.0 versus those &lt;25.0 were 1.00 (95% confidence interval (CI): 0.67, 1.48), 1.49 (95% CI: 0.97, 2.30), 2.16 (95% CI: 1.34, 3.49), and 3.60 (95% CI: 2.24, 5.78), respectively (Ptrend &lt;0.0001). A high weight-to-height ratio was also associated with a higher risk (for the highest quartile vs. the lowest, incidence rate ratio = 2.83, 95% CI: 1.77, 4.53), as was type 2 diabetes mellitus (incidence rate ratio = 1.52, 95% CI: 1.04, 2.21). Positive associations with measures of central adiposity (waist circumference, waist-to-hip ratio, and waist-to-height ratio) and hypertension were attenuated after we controlled for body mass index. Total adiposity was an independent risk factor for endometrial cancer among black women and appeared to explain most of the associations seen with other adiposity measures and metabolic factors.-Body Size, Metabolic Factors, and Risk of Endometrial Cancer in Black Women.",0
"Community trials involve the assignment of intact social groups to study conditions and are becoming increasingly common in epidemiologic research. In both the design and analysis of these studies, whether cross-sectional or cohort, allowance must be made for the dependence of elements within intact groups if variances are to be properly estimated. In the design phase, the statistician needs estimates of the level of dependence likely to be encountered. In the analysis phase, external estimates of the level of dependence may be useful in preventing the erosion of power associated with small numbers of intact groups assigned to each condition. We report the intraclass correlation coefficients of the city-year component of variance as estimated in the Minnesota Heart Health Program for a variety of community survey variables and illustrate their use in both design and analysis. Of 23 variables assessed, all but two showed positive estimates of city-year intraclass correlations. In these data, estimates of intraclass correlation coefficients generally were in the range 0.002-0.012.-Parameters to aid in the design and analysis of community trials: intraclass correlations from the Minnesota Heart Health Program.",1
"There is growing interest in the ethics of cluster trials, but no literature on the uncertainties in defining communities in relation to the scientific notion of the cluster in collaborative biomedical research. The views of participants in a community-based cluster randomised trial (CRT) in Mumbai, India, were solicited regarding their understanding and views on community. We conducted two focus group discussions with local residents and 20 semi-structured interviews with different respondent groups. On average, ten participants took part in each focus group, most of them women aged 18-55. We conducted semi-structured interviews with ten residents (nine women and one man) lasting approximately an hour each and seven individuals (five men and two women) identified by residents as local leaders or decision-makers. In addition, we interviewed two Municipal Corporators (locally elected government officials involved in urban planning and development) and one representative of a political party located in a slum community. Residents' sense of community largely matched the scientific notion of the cluster, defined by the investigators as a geographic area, but their perceived needs were not entirely met by the trial. We examined whether the possibility of a conceptual mismatch between 'clusters' and 'communities' is likely to have methodological implications for a study or to lead to potential social disharmony because of the research interventions, arguing that it is important to take social factors into account as well as statistical efficiency when choosing the size and type of clusters and designing a trial. One method of informing such a design would be to use existing forums for community engagement to explore individuals' primary sense of community or social group and, where possible, to fit clusters around them. ISRCTN Register: ISRCTN56183183 Clinical Trials Registry of India: CTRI/2012/09/003004 .-Reconstructing communities in cluster trials?",1
"Proper estimation of sample size requirements for cluster-based studies requires estimates of the intra-cluster correlation coefficient (ICC) for the variables of interest. We calculated the ICC for 112 variables measured as part of the Vermont Diabetes Information System, a cluster-randomized study of adults with diabetes from 73 primary care practices (the clusters) in Vermont and surrounding areas. ICCs varied widely around a median value of 0.0185 (Inter-quartile range: 0.006, 0.037). Some characteristics (such as the proportion having a recent creatinine measurement) were highly associated with the practice (ICC = 0.288), while others (prevalence of some comorbidities and complications and certain aspects of quality of life) varied much more across patients with only small correlation within practices (ICC&lt;0.001). The ICC values reported here may be useful in designing future studies that use clustered sampling from primary care practices.-Intra-cluster correlation coefficients in adults with diabetes in primary care practices: the Vermont Diabetes Information System field survey.",1
"Cluster-randomized controlled trials are the gold standard for assessing efficacy of community-level interventions, such as vector-control strategies against dengue. We describe a novel cluster-randomized trial methodology with a test-negative design (CR-TND), which offers advantages over traditional approaches. This method uses outcome-based sampling of patients presenting with a syndrome consistent with the disease of interest, who are subsequently classified as test-positive cases or test-negative controls on the basis of diagnostic testing. We used simulations of a cluster trial to demonstrate validity of efficacy estimates under the test-negative approach. We demonstrated that, provided study arms are balanced for both test-negative and test-positive illness at baseline and that other test-negative design assumptions are met, the efficacy estimates closely match true efficacy. Analytical considerations for an odds ratio-based effect estimate arising from clustered data and potential approaches to analysis are also discussed briefly. We concluded that application of the test-negative design to certain cluster-randomized trials could increase their efficiency and ease of implementation.-Cluster-Randomized Test-Negative Design Trials: A Novel and Efficient Method to Assess the Efficacy of Community-Level Dengue Interventions.",1
"In two-armed trials with clustered observations the arms may differ in terms of (i) the intraclass correlation, (ii) the outcome variance, (iii) the average cluster size, and (iv) the number of clusters. For a linear mixed model analysis of the treatment effect, this paper examines the expected efficiency loss due to varying cluster sizes based upon the asymptotic relative efficiency of varying versus constant cluster sizes. Simple, but nearly cost-optimal, correction factors are derived for the numbers of clusters to repair this efficiency loss. In an extensive Monte Carlo simulation, the accuracy of the asymptotic relative efficiency and its Taylor approximation are examined for small sample sizes. Practical guidelines are derived to correct the numbers of clusters calculated under constant cluster sizes (within each treatment) when planning a study. Because of the variety of simulation conditions, these guidelines can be considered conservative but safe in many realistic situations. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Repairing the efficiency loss due to varying cluster sizes in two-level two-armed randomized trials with heterogeneous clustering.",1
"Large-scale public health trials are often randomized by geographic or administrative clusters, for reasons of financial or organizational exigency. In this paper, we deal with the situation where the dependent variable is a count of events, such as mortality from, or incidence of a given disease. Simulation results show that this design may decrease power by more than 50 per cent. The lost power can largely be replaced by incorporating information on the dependent variable, within clusters, before the start of the trial. The pretrial and trial data can be analysed by negative trinomial models.-Cluster randomization in large public health trials: the importance of antecedent data.",1
"Cluster randomized trial design, where groups of participants are randomized instead of individual participants, is increasingly being used in long-term care research. The purpose of this review was to determine the characteristics of cluster randomized trials in long-term care facilities. A medical librarian conducted the literature search. Two independent reviewers reviewed each paper. Studies were included if the design was cluster randomized and participants were from long-term care facilities. For each included study, two independent data extractors captured data on study attributes, including: journal, location, year published, author discipline, funding, methodology, number of participants, and intervention target. The literature search yielded 7,679 unique studies, with 195 studies meeting the selection criteria and being included for data extraction. The included studies were published between 1976 and 2017, with 53% of studies published after 2009. The term cluster randomized was in the title of only 45% of the studies. The studies were conducted worldwide; the United States had the largest number of studies (23%), followed by the United Kingdom (18%). Ten percent of studies were published in journals with an impact factor &gt;10. The most frequent discipline of the first and last authors was medicine (34%), followed by nursing (17%). Forty-nine percent of the studies had government funding, while only 20% had medical industry funding. In studies with &lt;1000 residents, 85% of the studies obtained consent from the resident and/or their proxy, while in studies with ? 1000 residents, it was 31%. The most frequent intervention targets were infection (13%), falls/fracture (13%), and behavior/physical restraint (13%). Cluster randomized controlled trials in long-term care have a unique set of characteristics. Results of this review will provide guidance to researchers conducting studies in long-term care facilities.-A Scoping Review on the Attributes of Cluster Randomized Controlled Trials in Long-Term Care Facilities",1
"In a multiphase stepped wedge cluster randomized trial (MSW-CRT), more than one intervention will be initiated on each sequence in a fixed order. Hence, with the MSW-CRT design, the effect of the first intervention can be evaluated when compared to control, as well as the added-on effects of the subsequent interventions. Studies that use MSW-CRT have been proposed, but properties of this design have not been explicitly studied. We derive closed-form variance formulae to test the interventions' effects, which can be readily used for sample size and power calculation. Additionally, we provide relationships between variances to test the interventions' effects and design parameters. Under special conditions, some important properties include: (i) the variances to test different interventions' effects (ie, the first intervention effect and the second intervention effect) may be same; (ii) as the cluster-period mean autocorrelation increases, the variance to test an intervention effect may first increase and then decrease; (iii) as the amount of periods between the initiations of two interventions (ie, lag) increases, the variance to test an intervention effect may remain unchanged. We illustrate the relationships between power and design parameters using the variance formulae. From a few illustrative examples, we observe that the statistical test that uses data only relevant to a specific intervention has inferior power (relative power loss &lt;15%) compared to the test when using all the study data. Also, power is reduced when both the total number of periods and lag are decreased simultaneously (relative power loss &lt;20%).-Variance formulae for multiphase stepped wedge cluster randomized trial",3
"Mancl and DeRouen (2001, Biometrics57, 126-134) and Kauermann and Carroll (2001, JASA96, 1387-1398) proposed alternative bias-corrected covariance estimators for generalized estimating equations parameter estimates of regression models for marginal means. The finite sample properties of these estimators are compared to those of the uncorrected sandwich estimator that underestimates variances in small samples. Although the formula of Mancl and DeRouen generally overestimates variances, it often leads to coverage of 95% confidence intervals near the nominal level even in some situations with as few as 10 clusters. An explanation for these seemingly contradictory results is that the tendency to undercoverage resulting from the substantial variability of sandwich estimators counteracts the impact of overcorrecting the bias. However, these positive results do not generally hold; for small cluster sizes (e.g., &lt;10) their estimator often results in overcoverage, and the bias-corrected covariance estimator of Kauermann and Carroll may be preferred. The methods are illustrated using data from a nested cross-sectional cluster intervention trial on reducing underage drinking.-A comparison of two bias-corrected covariance estimators for generalized estimating equations.",1
"A random-effects ordinal regression model is proposed for analysis of clustered or longitudinal ordinal response data. This model is developed for both the probit and logistic response functions. The threshold concept is used, in which it is assumed that the observed ordered category is determined by the value of a latent unobservable continuous response that follows a linear regression model incorporating random effects. A maximum marginal likelihood (MML) solution is described using Gauss-Hermite quadrature to numerically integrate over the distribution of random effects. An analysis of a dataset where students are clustered or nested within classrooms is used to illustrate features of random-effects analysis of clustered ordinal data, while an analysis of a longitudinal dataset where psychiatric patients are repeatedly rated as to their severity is used to illustrate features of the random-effects approach for longitudinal ordinal data.-A random-effects ordinal regression model for multilevel analysis.",1
"Despite their promise for increasing treatment precision, Personalized Trials (i.e., N-of-1 trials) have not been widely adopted. We aimed to ascertain patient preferences for Personalized Trials. We recruited 501 adults with ?2 common chronic conditions from Harris Poll Online. We used Sawtooth Software to generate 45 plausible Personalized Trial designs comprising combinations of eight key attributes (treatment selection, treatment type, clinician involvement, blinding, time commitment, self-monitoring frequency, duration, and cost) at different levels. Conditional logistic regression was used to assess relative importance of different attributes using a random utility maximization model. Overall, participants preferred Personalized Trials with no costs vs. $100 cost (utility difference 1.52 [standard error 0.07], P?&lt;?0.001) and with less vs. more time commitment/day (0.16 [0.07], P?&lt;?0.015) but did not hold preferences for the other six attributes. In subgroup analyses, participants ?65?years, white, and with income ?$50,000 were more averse to costs than their counterparts (P all &lt;0.05). To optimize dissemination, Personalized Trial designers should seek to minimize out-of-pocket costs and time burden of self-monitoring. They should also consider adaptive designs that can accommodate subgroup differences in design preferences.-Patient preferences for personalized (N-of-1) trials: a conjoint analysis.",0
Ethical issues in the design and conduct of cluster randomised controlled trials.,1
"Cluster-based studies involving aggregate units such as hospitals or medical practices are increasingly being used in healthcare evaluation. An important characteristic of such studies is the presence of intracluster correlation, typically quantified by the intracluster correlation coefficient (ICC). Sample size calculations for cluster-based studies need to account for the ICC, or risk underestimating the sample size required to yield the desired levels of power and significance. In this article, we present values for ICCs that were obtained from data on 97,095 pregnancies and 98,072 births taking place in a representative sample of 120 hospitals in eight Latin American countries. We present ICCs for 86 variables measured on mothers and newborns from pregnancy to the time of hospital discharge, including 'process variables' representing actual medical care received for each mother and newborn. Process variables are of primary interest in the field of implementation research. We found that overall, ICCs ranged from a minimum of 0.0003 to a maximum of 0.563 (median 0.067). For maternal and newborn outcome variables, the median ICCs were 0.011 (interquartile range 0.007-0.037) and 0.054 (interquartile range 0.013-0.075) respectively; however, for process variables, the median was 0.161 (interquartile range 0.072-0.328). Thus, we confirm previous findings that process variables tend to have higher ICCs than outcome variables. We demonstrate that ICCs generally tend to increase with higher prevalences (close to 0.5). These results can help researchers calculate the required sample size for future research studies in maternal and perinatal health.-Intracluster correlation coefficients from the 2005 WHO Global Survey on Maternal and Perinatal Health: implications for implementation research.",1
"We describe and evaluate a regression tree algorithm for finding subgroups with differential treatments effects in randomized trials with multivariate outcomes. The data may contain missing values in the outcomes and covariates, and the treatment variable is not limited to two levels. Simulation results show that the regression tree models have unbiased variable selection and the estimates of subgroup treatment effects are approximately unbiased. A bootstrap calibration technique is proposed for constructing confidence intervals for the treatment effects. The method is illustrated with data from a longitudinal study comparing two diabetes drugs and a mammography screening trial comparing two treatments and a control. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Identification of subgroups with differential treatment effects for longitudinal and multiresponse variables.",0
"The frequency of randomized cluster trials is increasing in primary care research. These trials are differentiated by the randomization method, in which a group of individuals is randomly assigned to an intervention as a cluster rather than as individuals. Characteristically, individuals within a cluster tend to be more alike than individuals selected at random. For instance, evaluating the effect of an intervention across medical care providers at an institutional level or at a physician group practice level fits the randomized cluster model. Three examples in this article show how failure to account for the dependence introduced by unit of randomization can affect the analysis of binary data and the conclusions of randomized cluster trials. Greater consideration of the nested nature of patient, physician, and practice data would increase the quality of primary care research.-Adjusted chi-square statistics: application to clustered binary data in primary care.",1
"The Anglia Menorrhagia Education Study (AMES) is a randomized controlled trial testing the effectiveness of an education package applied to general practices. Binary data are available from two sources; general practitioner reported referrals to hospital, and referrals to hospital determined by independent audit of the general practices. The former may be regarded as a surrogate for the latter, which is regarded as the true endpoint. Data are only available for the true end point on a sub set of the practices, but there are surrogate data for almost all of the audited practices and for most of the remaining practices. The aim of this paper was to estimate the treatment effect using data from every practice in the study. Where the true endpoint was not available, it was estimated by three approaches, a regression method, multiple imputation and a full likelihood model. Including the surrogate data in the analysis yielded an estimate of the treatment effect which was more precise than an estimate gained from using the true end point data alone. The full likelihood method provides a new imputation tool at the disposal of trials with surrogate data.-Imputation of a true endpoint from a surrogate: application to a cluster randomized controlled trial with partial information on the true endpoint.",1
"While studying the association between risk of HIV-1 infection and vaccine-elicited immune responses in preventative HIV-1 vaccine recipients, we encountered a need to combine a collection of biomarkers in an unsupervised fashion with the goal of preserving signal diversity within that collection. Inspired by methods for weighting protein sequences from the biological sequence analysis literature, we propose novel methods for weighting biomarkers, which we call maximum diversity weights. These weights are defined as the weights that maximize measures of signal diversity within a collection of biomarkers. While the optimization problems do not admit analytical solutions, they are convex and hence can be solved efficiently using iterative search algorithms. Through Monte Carlo studies and a real data example from HIV-1 vaccine research, we show that using maximum diversity weights in association studies can lead to an increase in power over other commonly used weights such as uniform weights or principal component-based weights.-Maximum diversity weighting for biomarkers with application in HIV-1 vaccine studies.",0
"Household contact studies, a mainstay of tuberculosis transmission research, often assume that tuberculosis-infected household contacts of an index case were infected within the household. However, strain genotyping has provided evidence against this assumption. Understanding the household versus community infection dynamic is essential for designing interventions. The misattribution of infection sources can also bias household transmission predictor estimates. We present a household-community transmission model that estimates the probability of community infection, that is, the probability that a household contact of an index case was actually infected from a source outside the home and simultaneously estimates transmission predictors. We show through simulation that our method accurately predicts the probability of community infection in several scenarios and that not accounting for community-acquired infection in household contact studies can bias risk factor estimates. Applying the model to data from Vit?ria, Brazil, produced household risk factor estimates similar to two other standard methods for age and sex. However, our model gave different estimates for sleeping proximity to index case and disease severity score. These results show that estimating both the probability of community infection and household transmission predictors is feasible and that standard tuberculosis transmission models likely underestimate the risk for two important transmission predictors. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-Extensions to Bayesian generalized linear mixed effects models for household tuberculosis transmission.",0
Commentary: Right truncation in cluster randomized trials can attenuate the power of a marginal analysis,1
"The predictiveness curve is a graphical tool that characterizes the population distribution of Risk(Y)=P(D=1|Y), where D denotes a binary outcome such as occurrence of an event within a specified time period and Y denotes predictors. A wider distribution of Risk(Y) indicates better performance of a risk model in the sense that making treatment recommendations is easier for more subjects. Decisions are more straightforward when a subject's risk is deemed to be high or low. Methods have been developed to estimate predictiveness curves from cohort studies. However, early phase studies to evaluate novel risk prediction markers typically employ case-control designs. Here, we present semiparametric and nonparametric methods for evaluating a continuous risk prediction marker that accommodates case-control data. Small sample properties are investigated through simulation studies. The semiparametric methods are substantially more efficient than their nonparametric counterparts under a correctly specified model. We generalize them to settings where multiple prediction markers are involved. Applications to prostate cancer risk prediction markers illustrate methods for comparing the risk prediction capacities of markers and for evaluating the increment in performance gained by adding a marker to a baseline risk model. We propose a modified Hosmer-Lemeshow test for case-control study data to assess calibration of the risk model that is a natural complement to this graphical tool.-Assessing risk prediction models in case-control studies using semiparametric and nonparametric methods.",0
"Waist circumferences (WC) &gt;/=102 cm for men and &gt;/=88 cm for women have been proposed by an expert panel as cut-points for identifying increased risk for the development of obesity comorbidities for most adults. The aim of this investigation was to examine the predictive values of these WC cut-points for hypercholesterolemia, low concentration of high (HDL-C), and high concentration of low (LDL-C) density lipoprotein cholesterol, hypertriglyceridemia, type 2 diabetes, and hypertension in overweight American adults. Data from NHANES III were utilized for the analysis. Predictive abilities were determined by calculating sensitivity, specificity, positive (PV+) and negative (PV-) predictive values in overweight subjects with BMI 25-29.9 kg/m(2). Sensitivity of WC cut-point was stronger for high LDL-C compared to other risk factors with the highest values recorded in the 40-59 and 60-69 year age groups in men and women, respectively. PV+ of WC cut-points for dyslipidemia, type 2 diabetes, and hypertension were low in men compared to women. PV+ tended to increase with age, from 19-39, 40-59 to 60-90 year age groups in Whites, Blacks, and Hispanic men. In men, the highest PV+ were recorded for hypertriglyceridemia in the 60-90 years old groups, with values of 71.6%, 52.5%, and 43.3% in Whites, Blacks, and Hispanics, respectively. The CVD risk factor associated with the highest PV+ in women was diabetes with values of 97.2% in Whites and 88.9% in Blacks, and hypertriglyceridemia with a value of 93.8% in the 17-39 year age group in Hispanics. Among Black men 40-59 years of age, only 32% of a population of overweight hypertensives were detected by the WC cut-points, and among Black women, 40-59 years of age, only 54% were detected. Given the low sensitivity of these cut-points for detecting hypertension, one of the major co-morbidities of obesity, these cut-points failed to provide adequate evidence for the use of WC in determining or evaluating patients as to co-morbid states. We recommend further studies to determine a set of specific cut-points associated with increased risk of CVD in different population groups.-Predictive values of waist circumference for dyslipidemia, type 2 diabetes and hypertension in overweight White, Black, and Hispanic American adults.",0
"The human immunodeficiency virus (HIV) care cascade is a conceptual model used to outline the benchmarks that reflects effectiveness of HIV care in the whole HIV care continuum. The models can be used to identify barriers contributing to poor outcomes along each benchmark in the cascade such as disengagement from care or death. Recently, the HIV care cascade has been widely applied to monitor progress towards HIV prevention and care goals in an attempt to develop strategies to improve health outcomes along the care continuum. Yet, there are challenges in quantifying successes and gaps in HIV care using the cascade models that are partly due to the lack of analytic approaches. The availability of large cohort data presents an opportunity to develop a coherent statistical framework for analysis of the HIV care cascade. Motivated by data from the Academic Model Providing Access to Healthcare, which has provided HIV care to nearly 200,000 individuals in Western Kenya since 2001, we developed a state transition framework that can characterize patient-level movements through the multiple stages of the HIV care cascade. We describe how to transform large observational data into an analyzable format. We then illustrate the state transition framework via multistate modeling to quantify dynamics in retention aspects of care. The proposed modeling approach identifies the transition probabilities of moving through each stage in the care cascade. In addition, this approach allows regression-based estimation to characterize effects of (time-varying) predictors of within and between state transitions such as retention, disengagement, re-entry into care, transfer-out, and mortality. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-A state transition framework for patient-level modeling of engagement and retention in HIV care using longitudinal cohort data.",0
"Determination of comparative effectiveness in a randomized controlled trial requires consideration of an intervention's comparative uptake (or acceptance) among randomized participants and the intervention's comparative efficacy among participants who use their assigned intervention. If acceptance differs across interventions, then simple randomization of participants can result in post-randomization losses that introduce bias and limit statistical power. We develop a novel preference-adaptive randomization procedure in which the allocation probabilities are updated based on the inverse of the relative acceptance rates among randomized participants in each arm. In simulation studies, we determine the optimal frequency with which to update the allocation probabilities based on the number of participants randomized. We illustrate the development and application of preference-adaptive randomization using a randomized controlled trial comparing the effectiveness of different financial incentive structures on prolonged smoking cessation. Simulation studies indicated that preference-adaptive randomization performed best with frequent updating, accommodated differences in acceptance across arms, and performed well even if the initial values for the allocation probabilities were not equal to their true values. Updating the allocation probabilities after randomizing each participant minimized imbalances in the number of accepting participants across arms over time. In the smoking cessation trial, unexpectedly large differences in acceptance among arms required us to limit the allocation of participants to less acceptable interventions. Nonetheless, the procedure achieved equal numbers of accepting participants in the more acceptable arms, and balanced the characteristics of participants across assigned interventions. Preference-adaptive randomization, coupled with analysis methods based on instrumental variables, can enhance the validity and generalizability of comparative effectiveness studies. In particular, preference-adaptive randomization augments statistical power by maintaining balanced sample sizes in efficacy analyses, while retaining the ability of randomization to balance covariates across arms in effectiveness analyses. ClinicalTrials.gov, NCT01526265; 31 January 2012.-Preference-adaptive randomization in comparative effectiveness studies.",0
"In many resource-poor countries, hiv-infected patients receive a standardized antiretroviral cocktail. In these settings, population-level surveillance of drug resistance is needed to characterize the prevalence of resistance mutations and to enable antiretroviral therapy programs to select the optimal regimen for their local population. The surveillance strategy currently recommended by the World Health Organization is prohibitively expensive in some settings and may not provide a sufficiently precise rendering of the emergence of drug resistance. By using a novel assay on pooled sera samples, we decrease surveillance costs while simultaneously increasing the accuracy of drug resistance prevalence estimates for an important mutation that impacts first-line antiretroviral therapy. We present a Bayesian model for pooled-testing data that garners more information from each resistance assay conducted, compared with individual testing. We expand on previous pooling methods to account for uncertainty about the population distribution of within-subject resistance levels. In addition, our model accounts for measurement error of the resistance assay, and this added uncertainty naturally propagates through the Bayesian model to our inference on the prevalence parameter. We conduct a simulation study that informs our pool size recommendations and that shows that this model renders the prevalence parameter identifiable in instances when an existing non-model-based estimator fails.-Estimating the prevalence of transmitted HIV drug resistance using pooled samples.",0
"In a cluster randomized controlled trial (RCT), the number of randomized units is typically considerably smaller than in trials where the unit of randomization is the patient. If the number of randomized clusters is small, there is a reasonable chance of baseline imbalance between the experimental and control groups. This imbalance threatens the validity of inferences regarding post-treatment intervention effects unless an appropriate statistical adjustment is used. Here, we consider application of the propensity score adjustment for cluster RCTs. For the purpose of illustration, we apply the propensity adjustment to a cluster RCT that evaluated an intervention to reduce suicidal ideation and depression. This approach to adjusting imbalance had considerable bearing on the interpretation of results. A simulation study demonstrates that the propensity adjustment reduced well over 90% of the bias seen in unadjusted models for the specifications examined.-Subject-level matching for imbalance in cluster randomized trials with a small number of clusters.",1
"This article compares four mixed-model analyses valid for group-randomized trials (GRTs) involving a nested cohort design with a single pretest and a single posttest, the most common design used in GRTs. This study makes estimates of intraclass correlations (ICCs) available to investigators planning GRTs with alcohol, tobacco, and other drug measures as the outcomes of interest. It also provides formulae demonstrating the potential benefits to the standard error of the intervention effect of both adjustments for fixed and time-varying covariates, as well as correlations over time. These estimates will allow other researchers using these variables to plan their studies by performing a priori power analyses for any of four common analytic options.-Assessing the most powerful analysis method for school-based intervention studies with alcohol, tobacco, and other drug outcomes.",1
"Pragmatic clinical trials embedded within health care systems provide an important opportunity to evaluate new interventions and treatments. Networks have recently been developed to support practical and efficient studies. Pragmatic trials will lead to improvements in how we deliver health care and promise to more rapidly translate research findings into practice. The National Institutes of Health (NIH) Health Care Systems Collaboratory was formed to conduct pragmatic clinical trials and to cultivate collaboration across research areas and disciplines to develop best practices for future studies. Through a two-stage grant process including a pilot phase (UH2) and a main trial phase (UH3), investigators across the Collaboratory had the opportunity to work together to improve all aspects of these trials before they were launched and to address new issues that arose during implementation. Seven Cores were created to address the various considerations, including Electronic Health Records; Phenotypes, Data Standards, and Data Quality; Biostatistics and Design Core; Patient-Reported Outcomes; Health Care Systems Interactions; Regulatory/Ethics; and Stakeholder Engagement. The goal of this article is to summarize the Biostatistics and Design Core's lessons learned during the initial pilot phase with seven pragmatic clinical trials conducted between 2012 and 2014. Methodological issues arose from the five cluster-randomized trials, also called group-randomized trials, including consideration of crossover and stepped wedge designs. We outlined general themes and challenges and proposed solutions from the pilot phase including topics such as study design, unit of randomization, sample size, and statistical analysis. Our findings are applicable to other pragmatic clinical trials conducted within health care systems. Pragmatic clinical trials using the UH2/UH3 funding mechanism provide an opportunity to ensure that all relevant design issues have been fully considered in order to reliably and efficiently evaluate new interventions and treatments. The integrity and generalizability of trial results can only be ensured if rigorous designs and appropriate analysis choices are an essential part of their research protocols.-Statistical lessons learned for designing cluster randomized pragmatic clinical trials from the NIH Health Care Systems Collaboratory Biostatistics and Design Core.",1
"Hormone replacement therapy is universally associated with coronary heart disease (CHD) in observational studies, but it is unknown whether this association is mediated by the autonomic nervous system. We tested the hypothesis that postmenopausal hormone replacement therapy was associated with more favorable heart rate (HR) and heart rate variability (HRV) in a population sample of women (n=2,621). Hormone therapy use was measured at four examinations beginning in 1987. Supine HR and HRV indices were measured for 6 minutes at the final examination (1996-1998). In unadjusted linear regression models, hormone therapy was associated with lower HR (hormone use=64.7 vs. never=65.7 beats/min, P=.01) and higher HRV. However, following adjustment for age and CHD risk factors, both associations were eliminated. Results from this observational study suggest that hormone therapy is not associated with HR or HRV. These analyses should be replicated in a randomized trial.-Prospective association between hormone replacement therapy, heart rate, and heart rate variability. The Atherosclerosis risk in communities study.",0
"Sample size calculations for a group-randomized trial (GRT) require an estimate of the expected intraclass correlation coefficient (ICC). However, few ICC estimates from GRTs in HIV/AIDS research have been published, leaving investigators with little data on which to base expectations. We used data from a multi-country study to estimate ICCs for variables related to physical and mental health and HIV risk behaviors. ICCs for perceptions of physical and mental health tended to be higher than those for HIV risk behavior variables, which were higher than ICCs for CD4 count. Covariate adjustment for country and socio-demographic variables reduced most ICC estimates. For risk behavior variables, adjustment for country and socio-demographic variables reduced ICC estimates by as much as 84 %. Variability in ICC estimates has important implications for study design, as a larger ICC reduces power. ICC estimates presented in this analysis will allow more precise sample size estimates for future GRTs.-Parameters for sample size estimation from a group-randomized HIV prevention trial in HIV clinics in sub-Saharan Africa.",1
"Audits are often performed to assess the quality of clinical trial data, but beyond detecting fraud or sloppiness, the audit data are generally ignored. In an earlier study, using data from a nonrandomized study, Shepherd and Yu developed statistical methods to incorporate audit results into study estimates and demonstrated that audit data could be used to eliminate bias. In this article, we examine the usefulness of audit-based error-correction methods in clinical trial settings where a continuous outcome is of primary interest. We demonstrate the bias of multiple linear regression estimates in general settings with an outcome that may have errors and a set of covariates for which some may have errors and others, including treatment assignment, are recorded correctly for all subjects. We study this bias under different assumptions, including independence between treatment assignment, covariates, and data errors (conceivable in a double-blinded randomized trial) and independence between treatment assignment and covariates but not data errors (possible in an unblinded randomized trial). We review moment-based estimators to incorporate the audit data and propose new multiple imputation estimators. The performance of estimators is studied in simulations. When treatment is randomized and unrelated to data errors, estimates of the treatment effect using the original error-prone data (i.e., ignoring the audit results) are unbiased. In this setting, both moment and multiple imputation estimators incorporating audit data are more variable than standard analyses using the original data. In contrast, in settings where treatment is randomized but correlated with data errors and in settings where treatment is not randomized, standard treatment-effect estimates will be biased. And in all settings, parameter estimates for the original, error-prone covariates will be biased. The treatment and covariate effect estimates can be corrected by incorporating audit data using either the multiple imputation or moment-based approaches. Bias, precision, and coverage of confidence intervals improve as the audit size increases. The extent of bias and the performance of methods depend on the extent and nature of the error as well as the size of the audit. This study only considers methods for the linear model. Settings much different than those considered here need further study. In randomized trials with continuous outcomes and treatment assignment independent of data errors, standard analyses of treatment effects will be unbiased and are recommended. However, if treatment assignment is correlated with data errors or other covariates, naive analyses may be biased. In these settings, and when covariate effects are of interest, approaches for incorporating audit results should be considered.-Using audit information to adjust parameter estimates for data errors in clinical trials.",0
"A traditional meta-analysis examines the overall effectiveness of an intervention by producing a pooled estimate of treatment efficacy. In contrast to this, a meta-regression model seeks to determine whether a study-level covariate (X) is a plausible source of heterogeneity in a set of treatment effects. Upon performing such an analysis, the results may suggest the presence of a meaningful amount of variation in the treatment effects because of the covariate; however, the current set of trials may not provide sufficient statistical power for such a conclusion. The proposed approach provides quantitative insight into the amount of support that a new trial may provide to the hypothesis that X is a meaningful source of variation in an updated meta-regression model, which includes both the previously completed and the proposed trial. This empirical algorithm allows examination of the potential feasibility of a planned study of various sizes to further support or refute the hypothesis that X is a statistically significant source of variation. A detailed example illustrates the sample size estimation algorithm for both a planned individually or cluster randomized trial to investigate the now commonly accepted impact of geographical latitude on the observed effectiveness of the Bacillus Calmette-Gu?rin vaccine in the prevention of tuberculosis. Copyright ? 2012 John Wiley &amp; Sons, Ltd.-Evidence-based sample size estimation based upon an updated meta-regression analysis.",1
"Often in many biomedical and epidemiologic studies, estimating hazards function is of interest. The Breslow's estimator is commonly used for estimating the integrated baseline hazard, but this estimator requires the functional form of covariate effects to be correctly specified. It is generally difficult to identify the true functional form of covariate effects in the presence of time-dependent covariates. To provide a complementary method to the traditional proportional hazard model, we propose a tree-type method which enables simultaneously estimating both baseline hazards function and the effects of time-dependent covariates. Our interest will be focused on exploring the potential data structures rather than formal hypothesis testing. The proposed method approximates the baseline hazards and covariate effects with step-functions. The jump points in time and in covariate space are searched via an algorithm based on the improvement of the full log-likelihood function. In contrast to most other estimating methods, the proposed method estimates the hazards function rather than integrated hazards. The method is applied to model the risk of withdrawal in a clinical trial that evaluates the anti-depression treatment in preventing the development of clinical depression. Finally, the performance of the method is evaluated by several simulation studies.-Non-parametric estimation for baseline hazards function and covariate effects with time-dependent covariates.",0
"We describe a Bayesian approach to incorporate between-individual heterogeneity associated with parameters of complicated biological models. We emphasize the use of the Markov chain Monte Carlo (MCMC) method in this context and demonstrate the implementation and use of MCMC by analysis of simulated overdispersed Poisson counts and by analysis of an experimental data set on preneoplastic liver lesions (their number and sizes) in the presence of heterogeneity. These examples show that MCMC-based estimates, derived from the posterior distribution with uniform priors, may agree well with maximum likelihood estimates (if available). However, with heterogeneous parameters, maximum likelihood estimates can be difficult to obtain, involving many integrations. In this case, the MCMC method offers substantial computational advantages.-Exploring heterogeneity in tumour data using Markov chain Monte Carlo.",0
"Understanding the dynamics of disease spread is key to developing effective interventions to control or prevent an epidemic. The structure of the network of contacts over which the disease spreads has been shown to have a strong influence on the outcome of the epidemic, but an open question remains as to whether it is possible to estimate contact network features from data collected in an epidemic. The approach taken in this paper is to examine the distributions of epidemic outcomes arising from epidemics on networks with particular structural features to assess whether that structure could be measured from epidemic data and what other constraints might be needed to make the problem identifiable. To this end, we vary the network size, mean degree, and transmissibility of the pathogen, as well as the network feature of interest: clustering, degree assortativity, or attribute-based preferential mixing. We record several standard measures of the size and spread of the epidemic, as well as measures that describe the shape of the transmission tree in order to ascertain whether there are detectable signals in the final data from the outbreak. The results suggest that there is potential to estimate contact network features from transmission trees or pure epidemic data, particularly for diseases with high transmissibility or for which the relevant contact network is of low mean degree. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-Effects of contact network structure on epidemic transmission trees: implications for data required to estimate network structure.",0
"This article examines whether an in-store unobtrusive survey of grocery store product displays can be used to track community-level dietary behavior. The survey was conducted in 12 western communities two different times to measure two aspects of the grocery store environment: (a) the relative availability of low-fat and high-fiber products and (b) the amount of store-provided health-education information. Self-reported dietary intake of residents was obtained in the same 12 communities using a telephone survey. We compared the individual and store-level measures both cross-sectionally and over time. We found positive and statistically significant correlations between the availability of healthful products in stores and the reported healthfulness of individual diets in cross-sectional analyses, but correlations between changes over time in the two measures were weaker and not statistically significant. The variance of the grocery store measures was nonetheless sufficiently small that a grocery store survey of 15 stores in each of 8 communities (n = 120 surveys) had power comparable to that of a telephone survey of 200 individuals/community (n = 1,600) surveys, at a fraction of the cost. Although the results provide further validation of cross-sectional measures of the grocery store environment, additional efforts are required to establish the validity of the grocery store survey as a method of measuring dietary change.-Can measures of the grocery store environment be used to track community-level dietary changes?",1
"Expression quantitative trait loci (eQTL) are loci or markers on the genomes that are associated with gene expression. It is well known to biologists that some (cis) genetic influences on expression occur over short distances on the genome while some (trans) influences can operate remotely. We use a log-linear model to place structure on the prior probability for genetic control of a transcript by a marker locus so that the loci that are closest to a transcript are given a higher prior probability of controlling that transcript to reflect the important role that genomic proximity can play in the regulation of expression. This proximity model is an extension of the mixture over marker (MOM) model for the simultaneous detection of cis and trans eQTL of Kendziorski (Kendziorski et al., 2006, Biometrics62(1), 19-27). The genomic locations of the transcripts are used to improve the accuracy of the posterior distribution for the location of the eQTL. We compare the MOM method to our extension with both simulated data and data sets of recombinant inbred mouse lines. We also discuss an extension of the MOM method to model multiple eQTLs, and find that many transcripts are likely associated with more than one eQTL.-Proximity model for expression quantitative trait loci (eQTL) detection.",0
"When practices are randomized in a trial and observations are made on the patients to assess the relative effectiveness of the different interventions, sample size calculations need to estimate the number of practices required, not just the total number of patients. Our aims were to introduce the methodology for appropriate sample size calculation and discuss the implications for power. A worked example from general practice is used. Designs which randomize practices are less powerful than designs which randomize patients to intervention groups, particularly where a large number of patients is recruited from each practice. Studies which randomize few practices should be avoided if possible, as the loss of power is considerable and simple randomization may not ensure comparability of intervention groups.-Trials which randomize practices II: sample size.",1
"For mixed models generally, it is well known that modeling data with few clusters will result in biased estimates, particularly of the variance components and fixed effect standard errors. In linear mixed models, small sample bias is typically addressed through restricted maximum likelihood estimation (REML) and a Kenward-Roger correction. Yet with binary outcomes, there is no direct analog of either procedure. With a larger number of clusters, estimation methods for binary outcomes that approximate the likelihood to circumvent the lack of a closed form solution such as adaptive Gaussian quadrature and the Laplace approximation have been shown to yield less-biased estimates than linearization estimation methods that instead linearly approximate the model. However, adaptive Gaussian quadrature and the Laplace approximation are approximating the full likelihood rather than the restricted likelihood; the full likelihood is known to yield biased estimates with few clusters. On the other hand, linearization methods linearly approximate the model, which allows for restricted maximum likelihood and the Kenward-Roger correction to be applied. Thus, the following question arises: Which is preferable, a better approximation of a biased function or a worse approximation of an unbiased function? We address this question with a simulation and an illustrative empirical analysis.-Estimation Methods for Mixed Logistic Models with Few Clusters.",1
"The power to detect a treatment effect in cluster randomized trials can be increased by increasing the number of clusters. An alternative is to include covariates into the regression model that relates treatment condition to outcome. In this paper, formulae are derived in order to evaluate both strategies on basis of their costs. It is shown that the strategy that uses covariates is more cost-efficient in detecting a treatment effect when the costs to measure these covariates are small and the correlation between the covariates and outcome is sufficiently large. The minimum required correlation depends on the cluster size, and the costs to recruit a cluster and to measure the covariate, relative to the costs to recruit a person. Measuring a covariate that varies at the person level only is recommended when cluster sizes are small and the costs to recruit and measure a cluster are large. Measuring a cluster level covariate is recommended when cluster sizes are large and the costs to recruit and measure a cluster are small. An illustrative example shows the use of the formulae in a practical setting.-Power and money in cluster randomized trials: when is it worth measuring a covariate?",1
"In this study, we compared two research consent techniques: a standardized video plus usual consent and usual consent alone. Individuals who completed 24-month outcomes (completers) in the Operations and Pelvic Muscle Training in the Management of Apical Support Loss study were invited to participate in an extended, longitudinal follow-up study (extended Operations and Pelvic Muscle Training in the Management of Apical Support Loss). Potential participants who were (1) able to provide consent and (2) not in long-term care facilities were randomized 1:1 to a standardized video detailing the importance of long-term follow-up studies of pelvic floor disorders followed by the usual institutional consent process versus the usual consent process alone. Randomization, stratified by site, used randomly permuted blocks. The primary outcome was the proportion of participants who enrolled in the extended study and completed data collection events 5 years after surgery. Secondary outcomes included the proportion enrolled in the extended study, completion of follow-up at each study year, completion of data collection points, completion of in-person visits, and completion of quality of life calls. Motivation and barriers to enrollment (study-level and personal-level) and satisfaction with the study consent process were measured by questionnaire prior to recruitment into extended Operations and Pelvic Muscle Training in the Management of Apical Support Loss. Groups were compared using an intention-to-treat principle, using unadjusted Student's t-test (continuous) and chi-square or Fisher's exact (categorical) test. A sample size of 340 (170/group) was estimated to detect a 15% difference in enrollment and study completion between groups with p &lt; 0.05. Of the 327 Operations and Pelvic Muscle Training in the Management of Apical Support Loss completers, 305 were randomized to the consent process study (153 video vs 152 no video). Groups were similar in demographics, surgical treatment, and outcomes. The overall rate of extended study enrollment was high, without significant differences between groups (video 92.8% vs no video 94.1%, p = 0.65). There were no significant differences in the primary outcome (video 79.1% vs no video 75.7%, p = 0.47) or in any secondary outcomes. Being ""very satisfied"" overall with study information (97.7% vs 88.5%, p = 0.01); ""strong agreement"" for feeling informed about the study (81.3% vs 70.8%, p = 0.06), understanding the study purpose (83.6% vs 71.0%, p = 0.02), nature and extent (82.8% vs 70.2%, p = 0.02), and potential societal benefits (82.8% vs 67.9%, p = 0.01); and research coordinator/study nurse relationship being ""very important"" (72.7% vs 63.4%, p = 0.03) were better in the video compared to the no video consent group. The extended study had high enrollment; most participants completed most study tasks during the 3-year observational extension, regardless of the use of video to augment research consent. The video was associated with a higher proportion of participants reporting improved study understanding and relationship with study personnel.-Recruitment and retention: A randomized controlled trial of video-enhanced versus standard consent processes within the E-OPTIMAL study.",0
"Health care quality improvement interventions are often evaluated in randomized trials in which individual physicians serve as the unit of randomization. These cluster randomized trials present a unique data structure that consists of many clusters of highly variable size. The appropriate method of analysis for these trials is unknown. We conducted a simulation study to compare several methods for analyzing data which were generated to replicate the structure of our motivating example. We varied the treatment effect size and the distributional assumptions about the random effect. Simulation was used to estimate power, coverage, bias, and mean squared error of full maximum likelihood estimation (MLE), approximate MLE using penalized quasi-likelihood (PQL), generalized estimating equations (GEE), group-bootstrapped logistic regression, and a clustered permutation test. Across all conditions tested, GEE and full MLE performed comparably. Bootstrapped methods were less powerful and had higher mean squared error under conditions of variable cluster size. PQL yielded biased results. The permutation test preserved Type I error rates, but had less power than the other methods considered.-An evaluation of statistical approaches for analyzing physician-randomized quality improvement interventions.",1
"Although transportation safety has greatly improved over the past 2 decades, motor vehicle crash injuries remain a leading cause of morbidity and mortality, particularly among young drivers. Driver errors and behaviors such as speeding and distraction contribute disproportionately to crashes among inexperienced novices, who develop safe driving judgment only with substantial driving experience, commonly described as the ""young driver problem."" Research on young drivers has applied a range of research methods, including analyses of national archival data (mainly from police reports), crash analyses, observation of driver behavior, surveys of driver behavior and dispositions, and experimental research on driver behavior and vehicle crash worthiness. Prominent research questions regarding young driver safety include what and how do novices learn to drive safely, what are the predictors of young driver crashes, what is the variability and overtime trajectories of young driver performance and outcomes, and to what extent is the young driver problem due mainly to average population risk or high-risk groups? Current research on young drivers is complicated by small sample sizes, relatively rare events, high within and between group variability, missing data, the need to estimate exposure, and the lack of longitudinal and experimental designs, problems that require complex analytic methods. In this paper, we provide an overview of driving research methods, examples of research addressing the young driver problem, and examples of statistical collaboration on young driver research, focusing particularly on estimating prediction of crash risk and estimating variability in young driver performance and outcomes.-Driving in search of analyses.",0
"In randomized treatment studies where the primary outcome requires long follow-up of patients and/or expensive or invasive obtainment procedures, the availability of a surrogate marker that could be used to estimate the treatment effect and could potentially be observed earlier than the primary outcome would allow researchers to make conclusions regarding the treatment effect with less required follow-up time and resources. The Prentice criterion for a valid surrogate marker requires that a test for treatment effect on the surrogate marker also be a valid test for treatment effect on the primary outcome of interest. Based on this criterion, methods have been developed to define and estimate the proportion of treatment effect on the primary outcome that is explained by the treatment effect on the surrogate marker. These methods aim to identify useful statistical surrogates that capture a large proportion of the treatment effect. However, current methods to estimate this proportion usually require restrictive model assumptions that may not hold in practice and thus may lead to biased estimates of this quantity. In this paper, we propose a nonparametric procedure to estimate the proportion of treatment effect on the primary outcome that is explained by the treatment effect on a potential surrogate marker and extend this procedure to a setting with multiple surrogate markers. We compare our approach with previously proposed model-based approaches and propose a variance estimation procedure based on a perturbation-resampling method. Simulation studies demonstrate that the procedure performs well in finite samples and outperforms model-based procedures when the specified models are not correct. We illustrate our proposed procedure using a data set from a randomized study investigating a group-mediated cognitive behavioral intervention for peripheral artery disease participants.-Robust estimation of the proportion of treatment effect explained by surrogate marker information.",0
A practical look at cluster-randomized trials,1
"Analysis of data from twin and family studies provides the foundation for studies of disease inheritance. The development of advanced theory and computational software for general linear models has generated considerable interest for using mixed-effect models to analyze twin and family data, as a computationally more convenient and theoretically more sound alternative to the classical structure equation modeling. Despite the long history of twin and family data analysis, some fundamental questions remain unanswered. We addressed two important issues. One is to determine the necessary and sufficient conditions for the identifiability in the mixed-effects models for twin and family data. The other is to derive the asymptotic distribution of the likelihood ratio test, which is novel due to the fact that the standard regularity conditions are not satisfied. We considered a series of specific yet important examples in which we demonstrated how to formulate mixed-effect models to appropriately reflect the data, and our key idea is the use of the Cholesky decomposition. Finally, we applied our method and theory to provide a more precise estimate of the heritability of two data sets than the previously reported estimate.-Statistical inference in mixed models and analysis of twin and family data.",0
"This study is a cluster-randomised, community intervention trial to measure the impact of female condom introduction on STD prevalence among Kenyan agricultural workers. The intracluster correlation coefficient of baseline STD prevalences at the 12 sites was 0.0011.-Intracluster correlation of STD prevalence in a community intervention trial in Kenya.",1
"This paper outlines an approach for the design and analysis of randomized controlled trials investigating community-based interventions for behavioral change aimed at health promotion. The approach is illustrated using the Community Intervention Trial for Smoking Cessation (COMMIT), conducted from 1988 to 1993, involving 11 pairs of communities in North America, matched on geographic location, size, and sociodemographic factors. The situation discussed is when assignment to intervention is done at the community level; for COMMIT, the very nature of the intervention required this. The number of communities as a key determinant of the statistical power of the trial. The use of matched pairs of communities can achieve a gain in statistical efficiency. Randomization is used to obtain an unbiased assessment of the intervention effect; randomization also provides the basis for statistical analysis. Permutation tests (and corresponding test-based confidence intervals), using community as the unit of analysis, follow directly from the randomization distribution. Within this framework, individual-level covariates can be used for imputation of missing values and for adjusting analyses of intervention effect.-Interplay between design and analysis for behavioral intervention trials with community as the unit of randomization.",1
"Cluster randomized trials are increasingly common in general practice (family medicine). This paper will consider the design and analysis of such trials and emphasize the similarities and differences with trials in education, heath promotion and public health. Issues discussed are the estimation and range of values of the intra-cluster correlation coefficient found in general practice, and the associated sample size problems. There are problems with widely varying numbers of subjects per cluster, which leads to planning and analysis difficulties. Ethical issues in these trials, and considerations such as the principle of intention to treat are also considered. An example of the type of analysis available for a continuous outcome variable is given, and the available software is summarized briefly.-Cluster randomized trials in general (family) practice research.",1
"A simulation study is conducted in a community intervention setting. Several methods of stratified analysis of clustered binary data are compared in terms of empirical significance and empirical power levels. They are the Mantel-Haenszel test statistic (chi(2) (MH)), the adjusted Mantel-Haenszel test statistic of Donald-Donner (chi(2) (DD)), Rao-Scott (chi(2) (RSN) and chi(2) (RSP)), and Zhang-Boos (chi(2) (ZBN) and chi(2) (ZBP)), Wald (chi(2) (W)), robust Wald (chi(2) (RW)), score (chi(2) (S)), robust score (chi(2) (RS)), and the test statistic based on generalized linear mixed model (GLMM) (chi(2) (GLMM)). When rho not equal 0, chi(2) (MH) has inflated type I error, and it should not be used when observations are correlated. The results also warn of the use of chi(2) (RSN) and chi(2) (RW) due to their poor performance in terms of empirical significance level. chi(2) (ZBP) and chi(2) (GLMM) have better empirical significance levels as compared to other statistics; however, chi(2) (ZBP) tends to have lower empirical powers than other statistics when the number of clusters (N) is less than 24. chi(2) (RSP) provides the highest empirical powers when rho &gt; or = 0.1 and N &lt; or = 12. When rho &lt; or = 0.01, we recommend the use of chi(2) (RS) and chi(2) (GLMM) since they have better overall performance in terms of empirical significance levels and empirical power levels.-An evaluation of methods for the stratified analysis of clustered binary data in community intervention trials.",1
"We consider teratologic studies in which the aim is to compare the survival rate of animals in a treatment group to the corresponding rate in a control group. The design of such studies often involves the allocation of intact litters of animals to treatment, invalidating the application of standard statistical methods. We review the strengths and weaknesses of several approaches for dealing with this problem including methodology recently developed for the analysis of clustered binary data. A simulation study is conducted in which litter sizes are generated from a distribution having specified mean and degree of imbalance. It is recommended on the basis of this study and on theoretical considerations that the choice of method should depend on whether the comparison of interest is experimental or observational. For experimental comparisons, involving the random assignment of litters to different treatment groups, methods based on the adjustment of standard chi-square statistics are recommended unless the number of litters in each group is very large.-A comparison of methods for testing homogeneity of proportions in teratologic studies.",1
"Multilevel models have become a standard data analysis approach in intervention research. Although the vast majority of intervention studies involve multiple outcome measures, few studies use multivariate analysis methods. The authors discuss multivariate extensions to the multilevel model that can be used by psychotherapy researchers. Using simulated longitudinal treatment data, the authors show how multivariate models extend common univariate growth models and how the multivariate model can be used to examine multivariate hypotheses involving fixed effects (e.g., does the size of the treatment effect differ across outcomes?) and random effects (e.g., is change in one outcome related to change in the other?). An online supplemental appendix provides annotated computer code and simulated example data for implementing a multivariate model. Multivariate multilevel models are flexible, powerful models that can enhance clinical research.-Analyzing multiple outcomes in clinical research using multivariate multilevel models.",1
"MIXOR provides maximum marginal likelihood estimates for mixed-effects ordinal probit, logistic, and complementary log-log regression models. These models can be used for analysis of dichotomous and ordinal outcomes from either a clustered or longitudinal design. For clustered data, the mixed-effects model assumes that data within clusters are dependent. The degree of dependency is jointly estimated with the usual model parameters, thus adjusting for dependence resulting from clustering of the data. Similarly, for longitudinal data, the mixed-effects approach can allow for individual-varying intercepts and slopes across time, and can estimate the degree to which these time-related effects vary in the population of individuals. MIXOR uses marginal maximum likelihood estimation, utilizing a Fisher-scoring solution. For the scoring solution, the Cholesky factor of the random-effects variance-covariance matrix is estimated, along with the effects of model covariates. Examples illustrating usage and features of MIXOR are provided.-MIXOR: a computer program for mixed-effects ordinal regression analysis.",1
"The accuracy of a diagnostic test, which is often quantified by a pair of measures such as sensitivity and specificity, is critical for medical decision making. Separate studies of an investigational diagnostic test can be combined through meta-analysis; however, such an analysis can be threatened by publication bias. To the best of our knowledge, there is no existing method that accounts for publication bias in the meta-analysis of diagnostic tests involving bivariate outcomes. In this paper, we extend the Copas selection model from univariate outcomes to bivariate outcomes for the correction of publication bias when the probability of a study being published can depend on its sensitivity, specificity, and the associated standard errors. We develop an expectation-maximization algorithm for the maximum likelihood estimation under the proposed selection model. We investigate the finite sample performance of the proposed method through simulation studies and illustrate the method by assessing a meta-analysis of 17 published studies of a rapid diagnostic test for influenza.-Copas-like selection model to correct publication bias in systematic review of diagnostic test studies.",0
"Data monitoring committees are responsible for safeguarding the interests of study participants and assuring the integrity and credibility of clinical trials. The independence of data monitoring committees from sponsors and investigators is essential in achieving this mission. Creative approaches are needed to address ongoing and emerging challenges that potentially threaten data monitoring committees' independence and effectiveness. An expert panel of representatives from academia, industry and government sponsors, and regulatory agencies discussed these challenges and proposed best practices and operating principles for effective functioning of contemporary data monitoring committees. Prospective data monitoring committee members need better training. Options could include didactic instruction as well as apprenticeships to provide real-world experience. Data monitoring committee members should be protected against legal liability arising from their service. While avoiding breaches in confidentiality of interim data remains a high priority, data monitoring committees should have access to unblinded efficacy and safety data throughout the trial to enable informed judgments about risks and benefits. Because overly rigid procedures can compromise their independence, data monitoring committees should have the flexibility necessary to best fulfill their responsibilities. Data monitoring committee charters should articulate principles that guide the data monitoring committee process rather than list a rigid set of requirements. Data monitoring committees should develop their recommendations by consensus rather than through voting processes. The format for the meetings of the data monitoring committee should maintain the committee's independence and clearly establish the leadership of the data monitoring committee chair. The independent statistical group at the Statistical Data Analysis Center should have sufficient depth of knowledge about the study at hand and experience with trials in general to ensure that the data monitoring committee has access to timely, reliable, and readily interpretable insights about emerging evidence in the clinical trial. Contracts engaging data monitoring committee members for industry-sponsored trials should have language customized to the unique responsibilities of data monitoring committee members rather than use language appropriate to consultants for product development. Regulatory scientists would benefit from experiencing data monitoring committee service that does not conflict with their regulatory responsibilities.-Data monitoring committees: Promoting best practices to address emerging challenges.",0
"The published literature on cluster randomized trials focuses on outcomes that are either continuous or binary. In many trials, the outcome is an incidence rate, such as mortality, based on person-years data. In this paper we review methods for the analysis of such data in cluster randomized trials and present some simple approaches. We discuss the choice of the measure of intervention effect and present methods for confidence interval estimation and hypothesis testing which are conceptually simple and easy to perform using standard statistical software. The method proposed for hypothesis testing applies a t-test to cluster observations. To control confounding, a Poisson regression model is fitted to the data incorporating all covariates except intervention status, and the analysis is carried out on the residuals from this model. The methods are presented for unpaired data, and extensions to paired or stratified clusters are outlined. The methods are evaluated by simulation and illustrated by application to data from a trial of the effect of insecticide-impregnated bednets on child mortality. The techniques provide a straightforward approach to the analysis of incidence rates in cluster randomized trials. Both the unadjusted analysis and the analysis adjusting for confounders are shown to be robust, even for very small numbers of clusters, in situations that are likely to arise in randomized trials.-Methods for the analysis of incidence rates in cluster randomized trials.",1
"Cluster randomized trials are often used in primary care settings. In the U.K., general practices are usually the unit of allocation. The effect of variability in practice list size on sample size calculations is demonstrated using the General Medical Services Statistics for England and Wales, 1997. Summary statistics and tables are given to help design such trials assuming that a fixed proportion of patients are to be recruited from each cluster. Three different weightings of the cluster means are compared: uniform, cluster size and minimum variance weights. Minimum variance weights are shown to be superior to uniform, particularly when clusters are small, and to cluster size weights, particularly when clusters are large. Where there are large numbers of participants per cluster and cluster size weights are used, the power actually falls as more patients are recruited to large clusters. When minimum variance weights are used the increase in the design effect due to variation in list size is small, regardless of the size of intracluster correlation coefficient or the number of participants per cluster, provided there is no loss of randomized units. When the expected number of participants per practice is low a greater loss in power comes from practices which fail to recruit patients. A method to estimate the likely effect and allow for it is presented.-Unequal cluster sizes for trials in English and Welsh general practice: implications for sample size calculations.",1
"Although the consequences of ignoring a nested factor on decisions to reject the null hypothesis of no treatment effects have been discussed in the literature, typically researchers in applied psychology and education ignore treatment providers (often a nested factor) when comparing the efficacy of treatments. The incorrect analysis, however, not only invalidates tests of hypotheses, but it also overestimates the treatment effect. Formulas were derived and a Monte Carlo study was conducted to estimate the degree to which the F statistic and treatment effect size measures are inflated by ignoring the effects due to providers of treatments. These untoward effects are illustrated with examples from psychotherapeutic treatments.-The consequence of ignoring a nested factor on measures of effect size in analysis of variance",2
"Randomized control trials (RCTs) often use clustered designs, where intact clusters (such as classroom, schools, or treatment centers) are randomly assigned to treatment and control conditions. Hierarchical linear models (HLMs) are used almost universally to estimate the effects in such experiments. While study designs that utilize intact clusters have many potential advantages, there is little guidance in the literature on how to respond when cluster switching induces noncompliance with the randomization protocol. In the presence of noncompliance the intent-to-treat (ITT) effect becomes the estimand of interest. When fitting the HLM, these individuals who switch clusters can be assigned to either their as-assigned cluster (the cluster they belonged to at the time of randomization) or their as-treated cluster (the cluster they belonged to at the time the outcome was collected). We show analytically and via simulation, that using the as-treated cluster in HLM will bias the estimate of the ITT effect and using the as-assigned cluster will bias the standard error estimates when heterogeneity among clusters is because of heterogeneity in treatment effects. We show that using linear regression with two-way cluster adjusted standard errors can yield unbiased ITT estimates and consistent standard errors regardless of the source of the random effects. We recommend this method replace HLM as the method of choice for testing intervention effects with cluster-randomized trials with noncompliance and cluster switching. (PsycInfo Database Record (c) 2020 APA, all rights reserved).-Switching cluster membership in cluster randomized control trials: Implications for design and analysis",1
Contamination in trials: is cluster randomisation the answer?,1
"The Community Intervention Trial for Smoking Cessation (COMMIT) was a randomized trial to evaluate the effects of a community-wide smoking cessation intervention on smoking behavior. The statistical design involved 22 pair-matched communities and the randomization of one community in each of the 11 pairs to the intervention, with the other community in the pair acting as a comparison. Communities were matched on the basis of their geographical proximity and similarity of demographic composition. In this paper, we use the data on the rates of quitting smoking among cohorts of heavy and light/moderate smokers in each community to estimate the gains in efficiency achieved by the matched-pairs design compared to an unmatched randomized trial. We find evidence of some gain in efficiency, although the data are not extensive enough to give estimates of efficiency gain that have good precision.-The efficiency of the matched-pairs design of the Community Intervention Trial for Smoking Cessation (COMMIT).",1
"For community interventions to be effective in real-world conditions, participants need to have sufficient exposure to the intervention. It is unclear how the dose and intensity of the intervention differ among study participants in low-income areas. We aimed to understand patterns of exposure to different components of a multi-level multi-component obesity prevention program to inform our future?impact analyses. B'more Healthy Communities for Kids (BHCK) was a community-randomized controlled trial implemented in 28 low-income zones in Baltimore in two rounds (waves). Exposure to three different intervention components (corner store/carryout?restaurants, social media/text messaging, and youth-led nutrition education) was assessed via post-intervention interviews with 385 low-income urban youths and their caregivers. Exposure scores were generated based on self-reported viewing of BHCK materials (posters, handouts, educational displays, and social media posts) and participating in activities, including taste tests during the intervention. For each intervention component, points were assigned for exposure to study materials and activities, then scaled (0-1 range), yielding an overall BHCK exposure score [youths: mean 1.1 (range 0-7.6 points); caregivers: 1.1 (0-6.7), possible highest score: 13]. Ordered logit regression analyses were used to investigate correlates of youths' and caregivers' exposure level (quartile of exposure). Mean intervention exposure scores were significantly higher for intervention than comparison youths (mean 1.6 vs 0.5, p &lt; 0.001) and caregivers (mean 1.6 vs 0.6, p &lt; 0.001). However, exposure scores were low in both groups and 10% of the comparison group was moderately exposed to the intervention. For each 1-year increase in age, there was a 33% lower odds of being highly exposed to the intervention (odds ratio 0.77, 95% confidence interval 0.69; 0.88) in the unadjusted and adjusted model controlling for youths' sex and household income. Treatment effects may be attenuated in community-based trials, as participants may be differentially exposed to intervention components and the comparison group may also be exposed. Exposure should be measured to provide context to impact evaluations in multi-level trials. Future analyses linking exposure scores to the outcome should control for potential confounders in the treatment-on-the-treated approach, while recognizing that confounding and selection bias may exist affecting causal inference. ClinicalTrials.gov, NCT02181010 . Retrospectively registered on 2 July 2014.-Exposure to a multi-level multi-component childhood obesity prevention?community-randomized controlled trial: patterns, determinants, and implications.",0
"Sustained mass media campaigns have been recommended to stem the tobacco epidemic in the United States. Propensity score matching (PSM) was used to estimate the effect of awareness of a national smoking cessation media campaign (EX) on quit attempts and cessation-related cognition. Participants were 4,067 smokers and recent quitters aged 18-49 in targeted U.S. media markets. Controlling for potential confounders through PSM and regression analysis, confirmed awareness of EX was not significantly associated with either outcome at 6-month follow-up. Matched analyses excluding 217 quitters resulted in a significant effect of EX on both outcomes.-Use of propensity score matching to evaluate a national smoking cessation media campaign.",0
"Under-age drinking is an enormous public health issue in the USA. Evidence that community level structures may impact on under-age drinking has led to a proliferation of efforts to change the environment surrounding the use of alcohol. Although the focus of these efforts is to reduce drinking by individual youths, environmental interventions are typically implemented at the community level with entire communities randomized to the same intervention condition. A distinct feature of these trials is the tendency of the behaviours of individuals residing in the same community to be more alike than that of others residing in different communities, which is herein called 'clustering'. Statistical analyses and sample size calculations must account for this clustering to avoid type I errors and to ensure an appropriately powered trial. Clustering itself may also be of scientific interest. We consider the alternating logistic regressions procedure within the population-averaged modelling framework to estimate the effect of a law enforcement intervention on the prevalence of under-age drinking behaviours while modelling the clustering at multiple levels, e.g. within communities and within neighbourhoods nested within communities, by using pairwise odds ratios. We then derive sample size formulae for estimating intervention effects when planning a post-test-only or repeated cross-sectional community-randomized trial using the alternating logistic regressions procedure.-Sample size estimation for alternating logistic regressions analysis of multilevel randomized community trials of under-age drinking.",1
"Heavy episodic (binge) drinking is common among young adults and can lead to injury and illness. Young adults who seek care in the Emergency Department (ED) may be disproportionately affected with binge drinking behavior, therefore provide an opportunity to reduce future risk through screening, brief intervention and referral to treatment (SBIRT). Mobile phone text messaging (SMS) is a common form of communication among young adults and has been shown to be effective at providing behavioral support to young adult drinkers after ED discharge. Efficacy of SMS programs to reduce binge drinking remains unknown. We will conduct a three parallel arm, randomized trial. A convenience sample of adults aged 18 to 25 years attending three EDs in Pittsburgh, PA and willing to participate in the study will be screened for hazardous alcohol consumption. Participants identified as hazardous drinkers will then be allocated to either 12 weeks of weekly SMS drinking assessments with feedback (SA+F), SMS drinking assessments without feedback (SA), or a control group. Randomization will be via an independent and remote computerized randomization and will be stratified by study site. The SA+F group will be asked to provide pre-weekend drinking intention as well as post-weekend consumption via SMS and will receive feedback messages focused on health consequences of alcohol consumption, personalized normative feedback, protective drinking strategies and goal setting. Follow-up data on alcohol use and injury related to alcohol will be collected through a password-protected website three, six and nine months later. The primary outcome for the study is binge drinking days (?4 drinks for women; ?5 drinks for men) during the previous month, and the main secondary outcome is the proportion of participants who report any injury related to alcohol in the prior three months. This study will test the hypothesis that a mobile phone text-messaging program will result in immediate and durable reductions in binge drinking among at-risk young adults. By testing an intervention group to an assessment-only and control group, we will be able to separate the effect of assessment reactivity. By collecting pre-weekend drinking intentions and post-weekend consumption data in the SA+F group, we will be able to better understand mechanism of change. Clinicaltrials.gov NCT01688245.-Mobile phone text message intervention to reduce binge drinking among young adults: study protocol for a randomized controlled trial.",0
"Within epidemiology, a stepped wedge trial design (i.e., a one-way crossover trial in which several arms start the intervention at different time points) is increasingly popular as an alternative to a classical cluster randomized controlled trial. Despite this increasing popularity, there is a huge variation in the methods used to analyze data from a stepped wedge trial design. Four linear mixed models were used to analyze data from a stepped wedge trial design on two example data sets. The four methods were chosen because they have been (frequently) used in practice. Method 1 compares all the intervention measurements with the control measurements. Method 2 treats the intervention variable as a time-independent categorical variable comparing the different arms with each other. In method 3, the intervention variable is a time-dependent categorical variable comparing groups with different number of intervention measurements, whereas in method 4, the changes in the outcome variable between subsequent measurements are analyzed. Regarding the results in the first example data set, methods 1 and 3 showed a strong positive intervention effect, which disappeared after adjusting for time. Method 2 showed an inverse intervention effect, whereas method 4 did not show a significant effect at all. In the second example data set, the results were the opposite. Both methods 2 and 4 showed significant intervention effects, whereas the other two methods did not. For method 4, the intervention effect attenuated after adjustment for time. Different methods to analyze data from a stepped wedge trial design reveal different aspects of a possible intervention effect. The choice of a method partly depends on the type of the intervention and the possible time-dependent effect of the intervention. Furthermore, it is advised to combine the results of the different methods to obtain an interpretable overall result.-Different methods to analyze stepped wedge trial designs revealed different aspects of intervention effects.",3
"Cluster randomization trials have become a very attractive research strategy, particularly for the evaluation of health service interventions. The need to conduct meta-analyses of such trials is also becoming more common. However, as with cluster randomization trials in general, such analyses raise special methodologic challenges. In this paper, we discuss and illustrate several statistical approaches that might be applied to a meta-analysis of cluster randomization trials, each of which has a binary endpoint. Statistical methods for constructing inferences for a summary intervention odds ratio include those based on Mantel-Haenszel procedures, the ratio estimator approach, Woolf procedures and generalized estimating equations using robust variance estimation. The advantages and disadvantages of each method are discussed in the context of an example.-Statistical methods for the meta-analysis of cluster randomization trials.",1
"Split-cluster designs are frequently used in the health sciences when naturally occurring clusters such as multiple sites or organs in the same subject are assigned to different treatments. However, statistical methods for the analysis of binary data arising from such designs are not well developed. The purpose of this article is to propose and evaluate a new procedure for testing the equality of event rates in a design dividing each of k clusters into two segments having multiple sites (e.g., teeth, lesions). The test statistic proposed is a generalization of a previously published procedure based on adjusting the standard Pearson chi-square statistic, but can also be derived as a score test using the approach of generalized estimating equations.-Methods for the statistical analysis of binary data in split-cluster designs.",1
"To investigate the extent to which authors of cluster randomised trials adhered to two basic requirements of the World Medical Association's Declaration of Helsinki and the International Committee of Medical Journal Editors' uniform requirements for manuscripts (namely, reporting of research ethics review and informed consent), to determine whether the adequacy of reporting has improved over time, and to identify characteristics of cluster randomised trials associated with reporting of ethics practices. Review of a random sample of published cluster randomised trials from an electronic search in Medline. Cluster randomised trials in health research published in English language journals from 2000 to 2008. Study sample 300 cluster randomised trials published in 150 journals. 77 (26%, 95% confidence interval 21% to 31%) trials failed to report ethics review. The proportion reporting ethics review increased significantly over time (P&lt;0.001). Trials with data collection interventions at the individual level were more likely to report ethics review than were trials that used routine data sources only (79% (n=151) v 55% (23); P=0.008). Trials that accounted for clustering in the design and analysis were more likely to report ethics review. The median impact factor of the journal of publication was higher for trials that reported ethics review (3.4 v 2.3; P&lt;0.001). 93 (31%, 26% to 36%) trials failed to report consent. Reporting of consent increased significantly over time (P&lt;0.001). Trials with interventions targeting participants at the individual level were more likely to report consent than were trials with interventions targeting the cluster level (87% (90) v 48% (41); P&lt;0.001). Trials with data collection interventions at the individual level were more likely to report consent than were those that used routine data sources only (78% (146) v 29% (11); P&lt;0.001). Reporting of research ethics protections in cluster randomised trials is inadequate. In addition to research ethics approval, authors should report whether informed consent was sought, from whom consent was sought, and what consent was for.-Inadequate reporting of research ethics review and informed consent in cluster randomised trials: review of random sample of published trials.",1
"Different types of outcomes (e.g. binary, count, continuous) can be simultaneously modeled with multivariate generalized linear mixed models by assuming: (1) same or different link functions, (2) same or different conditional distributions, and (3) conditional independence given random subject effects. Others have used this approach for determining simple associations between subject-specific parameters (e.g. correlations between slopes). We demonstrate how more complex associations (e.g. partial regression coefficients between slopes adjusting for intercepts, time lags of maximum correlation) can be estimated. Reparameterizing the model to directly estimate coefficients allows us to compare standard errors based on the inverse of the Hessian matrix with more usual standard errors approximated by the delta method; a mathematical proof demonstrates their equivalence when the gradient vector approaches zero. Reparameterization also allows us to evaluate significance of coefficients with likelihood ratio tests and to compare this approach with more usual Wald-type t-tests and Fisher's z transformations. Simulations indicate that the delta method and inverse Hessian standard errors are nearly equivalent and consistently overestimate the true standard error. Only the likelihood ratio test based on the reparameterized model has an acceptable type I error rate and is therefore recommended for testing associations between stochastic parameters. Online supplementary materials include our medical data example, annotated code, and simulation details.-On estimating and testing associations between random coefficients from multivariate generalized linear mixed models of longitudinal outcomes.",0
Rerandomization to improve covariate balance in experiments,1
"This paper considers the regression analysis of CD4 cell counts, a commonly used indicator and prognostic factor of AIDS progression. For this purpose, a number of methods have been proposed and most of them are based on random effects models. We present an alternative that is based on a mean function regression model. The new method is more natural and particularly appropriate if population parameters such as treatment comparison are mainly of interest. To estimate regression parameters, we present an estimating equation-based approach, which does not require estimation of the baseline mean function. This makes the implementation of the approach and the study of the properties of the estimated regression parameters much easier than those based on random effects models. The method is illustrated with a set of CD4 cell count data arising from an AIDS clinical trial and simulated data.-A semi-parametric regression analysis of CD4 cell counts.",0
Cluster randomised trials: time for improvement. The implications of adopting a cluster design are still largely being ignored.,1
"To examine how disparities in adult disability by educational attainment vary across US states. We used the nationally representative data of more than 6 million adults aged 45 to 89 years in the 2010-2014 American Community Survey. We defined disability as difficulty with activities of daily living. We categorized education as low (less than high school), mid (high school or some college), or high (bachelor's or higher). We estimated age-standardized disability prevalence by educational attainment and state. We assessed whether the variation in disability across states occurs primarily among low-educated adults and whether it reflects the socioeconomic resources of low-educated adults and their surrounding contexts. Disparities in disability by education vary markedly across states-from a 20 percentage point disparity in Massachusetts to a 12-point disparity in Wyoming. Disparities vary across states mainly because the prevalence of disability among low-educated adults varies across states. Personal and contextual socioeconomic resources of low-educated adults account for 29% of the variation. Efforts to reduce disparities in disability by education should consider state and local strategies that reduce poverty among low-educated adults and their surrounding contexts.-Disparities in Disability by Educational Attainment Across US States.",0
"In this study, we evaluated the estimation of three important parameters for data collected in a multisite cluster-randomized trial (MS-CRT): the treatment effect, and the treatment by covariate interactions at Levels 1 and 2. The Level 1 and Level 2 interaction parameters are the coefficients for the products of the treatment indicator, with the covariate centered on its Level 2 expected value and with the Level 2 expected value centered on its Level 3 expected value, respectively. A comparison of a model-based approach to design-based approaches was performed using simulation studies. The results showed that both approaches produced similar treatment effect estimates and interaction estimates at Level 1, as well as similar Type I error rates and statistical power. However, the estimate of the Level 2 interaction coefficient for the product of the treatment indicator and an arithmetic mean of the Level 1 covariate was severely biased in most conditions. Therefore, applied researchers should be cautious when using arithmetic means to form a treatment by covariate interaction at Level 2 in MS-CRT data.-Comparison of model- and design-based approaches to detect the treatment effect and covariate by treatment interactions in three-level models for multisite cluster-randomized trials.",1
"A common class of models for longitudinal data are random effects (mixed) models. In these models, the random effects covariance matrix is typically assumed constant across subject. However, in many situations this matrix may differ by measured covariates. In this paper, we propose an approach to model the random effects covariance matrix by using a special Cholesky decomposition of the matrix. In particular, we will allow the parameters that result from this decomposition to depend on subject-specific covariates and also explore ways to parsimoniously model these parameters. An advantage of this parameterization is that there is no concern about the positive definiteness of the resulting estimator of the covariance matrix. In addition, the parameters resulting from this decomposition have a sensible interpretation. We propose fully Bayesian modelling for which a simple Gibbs sampler can be implemented to sample from the posterior distribution of the parameters. We illustrate these models on data from depression studies and examine the impact of heterogeneity in the covariance matrix on estimation of both fixed and random effects.-Modelling the random effects covariance matrix in longitudinal data.",0
"Strategies to avoid the penalties of extra variation and reduced degrees of freedom in community trials were compared in Monte Carlo simulations. Three conditions were found necessary to ensure nominal Type I and II error rates: (a) Condition variation must be assessed against assignment unit variation, (b) the critical value for the test statistic must be based on the assignment unit degrees of freedom, and (c) estimation of negative intraclass correlations must be allowed in the analysis. Using other test statistics and other degrees of freedom, and fixing negative intraclass correlations at zero often gave Type I and II error rates far from their nominal levels.-A Monte Carlo study of alternative responses to intraclass correlation in community trials. Is it ever possible to avoid Cornfield's penalties?",1
"Split-cluster experiments are being used by investigators in health sciences when naturally occurring aggregate of individuals with nested sub-groups may be assigned to different treatments. Cited examples include the split-mouth trials, in which a subject's mouth is divided into two segments that are randomly assigned to different treatment groups. In other situation, randomization to treatment conditions may be possible at the person level within the cluster. In this case, when the treatment conditions are available within each cluster, the design is referred to as a multisite or split-cluster design (SCD). The major attractiveness of this design is that it removes a large portion of the inter-subject variation from the estimates of the treatment effect; hence, it has the potential to require lesser number of measurements than a parallel arm design with the same power. When the response variable of interest is binary, statistical methods developed to evaluate the effect of interventions depended on nonparametric methods. Though these methods are simple to apply, they are known to be less efficient. Taking the relative risk (RR) as an effect measure, we construct a bivariate-correlated model under which a score test is applied to test H(0): RR = 1.0. Moreover, we construct Wald- and Fieller-based confidence intervals on RR. Since the efficiency of SCD increases when the interclass correlation coefficient (???) is high, we present a goodness-of-fit procedure for testing H(0):??? = 0, which may be helpful in choosing a design for a future study. For illustrating the proposed methodology, we consider two application data from the published literature; the first from a split-mouth trial on 23 patients evaluating the effect of chlorhexidine in the treatment of gingivitis, and second from study of mental health (depression and anxiety) as outcome measure obtained on 173 patients evaluated by two screening instruments. Moreover, we discussed the efficiency gained using our approach in these design settings. The likelihood approach makes more assumptions as compared to previous approaches that have been described. We have developed a bivariate beta-binomial model, from which we can conduct a full likelihood statistical inference. Based on this model, we may construct Wald's confidence intervals and score tests, which are known to possess optimal statistical properties. For the purpose of comparison with nonparametric methods, we constructed the Fieller's confidence interval.-Likelihood inference on the relative risk in split-cluster designs.",1
"We investigate by simulation the properties of four different estimation procedures under a linear model for correlated data with Gaussian error: maximum likelihood based on the normal mixed linear model; generalized estimating equations; a four-stage method, and a bootstrap method that resamples clusters rather than individuals. We pay special attention to the group randomized trials where the number of independent clusters is small, cluster sizes are big, and the correlation within the cluster is weak. We show that for balanced and near balanced data when the number of independent clusters is small (&lt; or = 10), the bootstrap is superior if analysts do not want to impose strong distribution and covariance structure assumptions. Otherwise, ML and four-stage methods are slightly better. All four methods perform well when the number of independent clusters reaches 50.-A comparison of statistical methods for clustered data analysis with Gaussian error.",1
"To test whether frequent bullying victimisation in childhood increases the likelihood of self harming in early adolescence, and to identify which bullied children are at highest risk of self harm. The Environmental Risk (E-Risk) longitudinal study of a nationally representative UK cohort of 1116 twin pairs born in 1994-95 (2232 children). England and Wales, United Kingdom. Children assessed at 5, 7, 10, and 12 years of age. Relative risks of children's self harming behaviour in the six months before their 12th birthday. Self harm data were available for 2141 children. Among children aged 12 who had self harmed (2.9%; n=62), more than half were victims of frequent bullying (56%; n=35). Exposure to frequent bullying predicted higher rates of self harm even after children's pre-morbid emotional and behavioural problems, low IQ, and family environmental risks were taken into account (bullying victimisation reported by mother: adjusted relative risk 1.92, 95% confidence interval 1.18 to 3.12; bullying victimisation reported by child: 2.44, 1.36 to 4.40). Victimised twins were more likely to self harm than were their non-victimised twin sibling (bullying victimisation reported by mother: 13/162 v 3/162, ratio=4.3, 95% confidence interval 1.3 to 14.0; bullying victimisation reported by child: 12/144 v 7/144, ratio=1.7, 0.71 to 4.1). Compared with bullied children who did not self harm, bullied children who self harmed were distinguished by a family history of attempted/completed suicide, concurrent mental health problems, and a history of physical maltreatment by an adult. Prevention of non-suicidal self injury in young adolescents should focus on helping bullied children to cope more appropriately with their distress. Programmes should target children who have additional mental health problems, have a family history of attempted/completed suicide, or have been maltreated by an adult.-Bullying victimisation and risk of self harm in early adolescence: longitudinal cohort study.",0
"Mammographic breast density is one of the strongest risk factors for breast cancer after age and family history. Mandatory breast density disclosure policies are increasing nationally without clear guidance on how to communicate density status to women. Coupling density disclosure with personalized risk counseling and decision support through a web-based tool may be an effective way to allow women to make informed, values-consistent risk management decisions without increasing distress. This paper describes the design and methods of Engaged, a prospective, randomized controlled trial examining the effect of online personalized risk counseling and decision support on risk management decisions in women with dense breasts and increased breast cancer risk. The trial is embedded in a large integrated health care system in the Pacific Northwest. A total of 1250 female health plan members aged 40-69 with a recent negative screening mammogram who are at increased risk for interval cancer based on their 5-year breast cancer risk and BI-RADS? breast density will be randomly assigned to access either a personalized web-based counseling and decision support tool or standard educational content. Primary outcomes will be assessed using electronic health record data (i.e., chemoprevention and breast MRI utilization) and telephone surveys (i.e., distress) at baseline, six weeks, and twelve months. Engaged will provide evidence about whether a web-based personalized risk counseling and decision support tool is an effective method for communicating with women about breast density and risk management. An effective intervention could be disseminated with minimal clinical burden to align with density disclosure mandates. Clinical Trials Registration Number:NCT03029286.-A web-based personalized risk communication and decision-making tool for women with dense breasts: Design and methods of a randomized controlled trial within an integrated health care system.",0
"It is essential that outcome research permit clear conclusions to be drawn about the efficacy of interventions. The common practice of nesting therapists within conditions can pose important methodological challenges that affect interpretation, particularly if the study is not powered to account for the nested design. An obstacle to the optimal design of these studies is the lack of data about the intraclass correlation coefficient (ICC), which measures the statistical dependencies introduced by nesting. To begin the development of a public database of ICC estimates, the authors investigated ICCs for a variety outcomes reported in 20 psychotherapy outcome studies. The magnitude of the 495 ICC estimates varied widely across measures and studies. The authors provide recommendations regarding how to select and aggregate ICC estimates for power calculations and show how researchers can use ICC estimates to choose the number of patients and therapists that will optimize power. Attention to these recommendations will strengthen the validity of inferences drawn from psychotherapy studies that nest therapists within conditions.-Intraclass correlation associated with therapists: estimates and applications in planning psychotherapy research.",2
"This study was conducted to examine a patient's age and condition at the time of diagnosis as one potential factor contributing to the ""gender gap"" in cystic fibrosis. The study population consisted of 11,275 US patients diagnosed during 1986-1998 and reported to the Cystic Fibrosis Foundation Registry in the same or the following calendar year. Parallel analyses were performed for Wisconsin patients identified prospectively during 1985-1994 to obtain more detailed information on their condition at diagnosis. Analyses of the registry data showed that females identified because of symptoms other than meconium ileus were diagnosed at significantly older ages (median, 12.7 months) than were males (median, 8.7 months) (p &lt; 0.001). The delay in diagnosis for females was most evident among patients presenting with respiratory symptoms only (median, 40.7 vs. 22.3 months; p &lt; 0.001). Analyses of Wisconsin patients demonstrated no significant gender differences in cough and wheezing experiences or in chest radiographic severity scores between males and females during their first 10 years of life, although a disproportionately high number of males were referred for diagnostic sweat testing. A delay in diagnosis of females with cystic fibrosis was discovered, suggesting either differential recognition of respiratory symptoms or a gender bias.-Delayed diagnosis of US females with cystic fibrosis.",0
"To evaluate the antiretroviral activity of antiretroviral agents and to compare the effects of two different antiretroviral agents, we propose a non-parametric mixed-effects model to investigate change of CD4+ counts. The proposed model and methods are applied to analyse the data from PACTG345 study. Population and individual patterns of change of CD4+ counts and a reference band are obtained. Our results indicate that treatment with high-dose ritonavir is significantly superior compared with low-dose ritonavir.-Evaluation of change in CD4+ cell counts in AIDS clinical trials.",0
"Service disengagement is a pervasive challenge the mental health care system faces. Mental health services are of little value should persons with mental illnesses continue to opt out of receiving them. Consumers attribute disengagement from care to an absence of choice in their treatment. In response, the mental health system is adopting a person-centered model, based upon recovery principles, to engage consumers more actively in their care. Person-centered care planning is a promising practice involving collaboration to develop and implement an actionable plan to assist the person in achieving personal recovery goals. This study design combines a parallel-group randomized controlled trial of community mental health organizations with qualitative methods to assess the effectiveness of person-centered care planning. Participants at 14 sites in Delaware and Connecticut will be randomized to treatment as usual or the person-centered care planning intervention. Participants will be in leadership (n = 70) or supervisory or direct care (n = 210) roles. The person-centered care planning intervention involves intensive staff training and 12 months of ongoing technical assistance. Quantitative survey data will be collected at baseline, 6 months and 12 months measuring person-centered care planning competency and organizational factors. Consumer outcomes (engagement, medication adherence, functioning and consumer satisfaction) will be assessed by Medicaid and state-level data. Qualitative data focused on process factors will include staff and consumer interviews and focus groups. In this intent-to-treat analysis, we will use mixed-effects multivariate regression models to evaluate the differential impact of the person-centered care planning intervention on each consumer and implementation outcome as well as the extent to which clinician assessments of organizational factors are associated with the implementation outcome. Mixed methods will triangulate and strengthen the interpretation of outcomes. The aim of this study is to generate valuable guidance for state systems engaged in scale-up and transformation efforts. Targeted staff selection for training to support sustainability will serve to provide further insight into important intervention implementation strategies. Person-centered care planning has the potential to enhance the impact of all evidence-based and recovery-oriented practices and bring practice into line with the emerging national guidelines in health care reform. This trial was registered with ClinicalTrials.gov (Identifier: NCT02299492) on 21 November 2014 as New York University Protocol Record PCCP-13-9762, Person-Centered Care Planning and Service Engagement.-Person-centered care planning and service engagement: a study protocol for a randomized controlled trial.",0
"Individual-level baseline covariate imbalance could happen more frequently in cluster randomized trials, and may influence the observed treatment effect. Using computer and real-data simulations, this paper quantifies the extent and impact of covariate imbalance on the estimated treatment effect for both continuous and binary outcomes, and relates it to the degree of imbalance for different numbers of clusters, cluster sizes, and covariate intraclass correlation coefficients. We focused on the impact of race as a covariate, given the emphasis of regulatory and funding bodies on understanding the influence of demographic characteristics on treatment effectiveness. We found that bias in the treatment effect is proportional to both the degree of baseline covariate imbalance and the covariate effect size. Larger numbers of clusters result in lower covariate imbalance, and increasing cluster size is less effective in reducing imbalance compared to increasing the number of clusters. Models adjusted for important baseline confounders are superior to unadjusted models for minimizing bias in both model-based simulations and an innovative simulation based on real clinical trial data. Higher outcome intraclass correlation coefficients did not affect bias but resulted in greater variance in treatment estimates.-Impact of baseline covariate imbalance on bias in treatment effect estimation in cluster randomized trials: Race as an example",1
"Previous reviews have focussed on the rationale for employing the stepped wedge design (SWD), the areas of research to which the design has been applied and the general characteristics of the design. However these did not focus on the statistical methods nor addressed the appropriateness of sample size methods used.This was a review of the literature of the statistical methodology used in stepped wedge cluster randomised trials. Literature Review. The Medline, Embase, PsycINFO, CINAHL and Cochrane databases were searched for methodological guides and RCTs which employed the stepped wedge design. This review identified 102 trials which employed the stepped wedge design compared to 37 from the most recent review by Beard et al. 2015. Forty six trials were cohort designs and 45?% (n = 46) had fewer than 10 clusters. Of the 42 articles discussing the design methodology 10 covered analysis and seven covered sample size. For cohort stepped wedge designs there was only one paper considering analysis and one considering sample size methods. Most trials employed either a GEE or mixed model approach to analysis (n = 77) but only 22 trials (22?%) estimated sample size in a way which accounted for the stepped wedge design that was subsequently used. Many studies which employ the stepped wedge design have few clusters but use methods of analysis which may require more clusters for unbiased and efficient intervention effect estimates. There is the need for research on the minimum number of clusters required for both types of stepped wedge design. Researchers should distinguish in the sample size calculation between cohort and cross sectional stepped wedge designs. Further research is needed on the effect of adjusting for the potential confounding of time on the study power.-Stepped wedge cluster randomised trials: a review of the statistical methodology used and available.",3
"To summarize and evaluate all publications including cluster-randomized trials used for maternal and child health research in developing countries during the last 10 years. All cluster-randomized trials published between 1998 and 2008 were reviewed, and those that met our criteria for inclusion were evaluated further. The criteria for inclusion were that the trial should have been conducted in maternal and child health care in a developing country and that the conclusions should have been made on an individual level. Methods of accounting for clustering in design and analysis were evaluated in the eligible trials. Thirty-five eligible trials were identified. The majority of them were conducted in Asia, used community as randomization unit, and had less than 10,000 participants. To minimize confounding, 23 of the 35 trials had stratified, blocked, or paired the clusters before they were randomized, while 17 had adjusted for confounding in the analysis. Ten of the 35 trials did not account for clustering in sample size calculations, and seven did not account for the cluster-randomized design in the analysis. The number of cluster-randomized trials increased over time, and the trials generally improved in quality. Shortcomings exist in the sample-size calculations and in the analysis of cluster-randomized trials conducted during maternal and child health research in developing countries. Even though there has been improvement over time, further progress in the way that researchers utilize and analyse cluster-randomized trials in this field is needed.-Evaluation of cluster-randomized trials on maternal and child health research in developing countries.",1
"The sample size required for a cluster randomized trial depends on the magnitude of the intracluster correlation coefficient (ICC). The usual sample size calculation makes no allowance for the fact that the ICC is not known precisely in advance. We develop methods which allow for the uncertainty in a previously observed ICC, using a variety of distributional assumptions. Distributions for the power are derived, reflecting this uncertainty. Further, the observed ICC in a future study will not equal its true value, and we consider the impact of this on power. We implement calculations within a Bayesian simulation approach, and provide one simplification that can be performed using simple simulation within spreadsheet software. In our examples, recognizing the uncertainty in a previous ICC estimate decreases expected power, especially when the power calculated naively from the ICC estimate is high. To protect against the possibility of low power, sample sizes may need to be very substantially increased. Recognizing the variability in the future observed ICC has little effect if prior uncertainty has already been taken into account. We show how our method can be extended to the case in which multiple prior ICC estimates are available. The methods presented in this paper can be used by applied researchers to protect against loss of power, or to choose a design which reduces the impact of uncertainty in the ICC.-Allowing for imprecision of the intracluster correlation coefficient in the design of cluster randomized trials.",1
"To compare three ad hoc methods to estimate the marginal hazard of incident cancer acquired immune deficiency syndrome (AIDS) in a highly active antiretroviral therapy (1996-2006) relative to a monotherapy/combination therapy (1990-1996) calendar period, accounting for other AIDS events and deaths as competing risks. Among 1,911 human immunodeficiency virus (HIV)-positive men from the Multicenter AIDS Cohort Study, 228 developed cancer AIDS and 745 developed competing risks in 14,202 person-years from 1990 to 2006. Method 1 censored competing risks at the time they occurred, method 2 excluded competing risks, and method 3 censored competing risks at the date of analysis. The age, race, and infection duration adjusted hazard ratios (HRs) for cancer AIDS were similar for all methods (HR approximately 0.15). We estimated bias and confidence interval coverage of each method with Monte Carlo simulation. On average, across 24 scenarios, method 1 produced less-biased estimates than methods 2 or 3. When competing risks are independent of the event of interest, only method 1 produced unbiased estimates of the marginal HR, although independence cannot be verified from the data. When competing risks are dependent, method 1 generally produced the least-biased estimates of the marginal HR for the scenarios explored; however, alternative methods may be preferred.-A comparison of ad hoc methods to account for non-cancer AIDS and deaths as competing risks when estimating the effect of HAART on incident cancer AIDS among HIV-infected men.",0
Intraclass correlation coefficient (ICC) estimates must be provided when reporting the results of a cluster randomized trial. This study demonstrates that estimating this parameter with one-way ANOVA and an underlying mixed-effects statistical model leads to biased estimates. The bias depends on the effect size of the studied treatment.-Model mis-specification and overestimation of the intraclass correlation coefficient in cluster randomized trials.,1
"Evaluation of diagnostic performance is a necessary component of new developments in many fields including medical diagnostics and decision making. The methodology for statistical analysis of diagnostic performance continues to develop, offering new analytical tools for conventional inferences and solutions for novel and increasingly more practically relevant questions. In this paper, we focus on the partial area under the Receiver Operating Characteristic (ROC) curve or pAUC. This summary index is considered to be more practically relevant than the area under the entire ROC curve (AUC), but because of several perceived limitations, it is not used as often. To improve interpretation, results for pAUC analysis are frequently reported using a rescaled index such as the standardized partial AUC proposed by McClish (1989). We derive two important properties of the relationship between the 'standardized' pAUC and the defined range of interest, which could facilitate a wider and more appropriate use of this important summary index. First, we mathematically prove that the 'standardized' pAUC increases with increasing range of interest for practically common ROC curves. Second, using comprehensive numerical investigations, we demonstrate that, contrary to common belief, the uncertainty about the estimated standardized pAUC can either decrease or increase with an increasing range of interest. Our results indicate that the partial AUC could frequently offer advantages in terms of statistical uncertainty of the estimation. In addition, selection of a wider range of interest will likely lead to an increased estimate even for standardized pAUC.-On use of partial area under the ROC curve for evaluation of diagnostic performance.",0
"The choice of study design for guideline implementation studies will determine the confidence with which the observed effects can be attributed to the interventions under study. In general, cluster randomized trials, of which there are different types, provide the most robust design. However, the use of these designs has implications for the power, conduct and analysis of studies. Wherever possible, designs allowing head-to-head comparisons, which incorporate baseline measures of performance, should be used.-Experimental and quasi-experimental designs for evaluating guideline implementation strategies.",1
"Prediabetes and diabetes disproportionately impact Latino youth, yet few diabetes prevention programs have prioritized inclusion of this underrepresented population. This report describes the recruitment process, yields, associated costs, and phenotypic characteristics of Latino youth with obesity and prediabetes enrolled in a randomized controlled diabetes prevention study in the USA. Recruitment efforts included referrals from clinics, community outlets, local media, and word of mouth with the goal of enrolling 120 Latino adolescents aged 12-16 with obesity (BMI &gt; 95th percentile) and prediabetes. Prediabetes eligibility was determined by any of the following: HbA1c between 5.7 and 6.5%, fasting glucose between 100 and 125 mg/dL, or a 2-h glucose between 120 and 199 mg/dL following a 75-g oral glucose tolerance test (OGTT), but not meeting any of the diagnostic criteria for diabetes. Eligible participants were randomized 2:1 to either a 6-month community-based lifestyle intervention that included group nutrition and health education classes (1 day/week) and group exercise classes (2 days/week) or usual care control arm. Recruitment yields were determined by review of referral source in the study screening database. Recruitment costs were determined by an after-the-fact financial review of actual and in-kind costs. Participant phenotypic characteristics (i.e., demographics, anthropometrics, and biochemical data) were compared by recruitment strategy using a one-way ANOVA. Recruitment efforts covered 160?mile2 (414 km2) across 26 ZIP codes (postcode) in the Phoenix Metropolitan Area and yielded 655 referrals from clinics (n?= 344), community (n?= 143), media (n?= 137), and word-of-mouth (n?= 31). From this pool, 26% (n?= 167) did not meet general, pre-screening eligibility criteria; 29% (n?= 187) declined participation; and 10% (n?= 64) were unable to be contacted. A total of 237 youth were invited to the clinical research unit to determine final eligibility. Following the OGTT, 52% (n?= 122) met prediabetes criteria and 117 were subsequently randomized. Clinical recruitment yielded the highest number of referrals (53%; n?= 344) while word-of-mouth yielded the highest proportion (35%; n?= 11) of randomized participants per referred youth. There were no significant differences in anthropometric or biochemical measures among youth by recruitment strategy. Based upon final enrollment numbers, community recruitment was the costliest approach ($486/randomized participant) followed by clinical ($248/randomized participant) and media ($236/randomized participant). The ability to meet enrollment goals for a clinical trial of an underrepresented population required multiple recruitment strategies. Although strategies vary in yields and costs, it appears they produce similar phenotypical risk profiles of eligible youth. ClinicalTrials.gov NCT02615353 . Registered on 26 November 2015.-Yields and costs of recruitment methods with participant phenotypic characteristics for a diabetes prevention research study in an underrepresented pediatric population.",0
"We consider design issues for cluster randomized trials (CRTs) with a binary outcome where both unit costs and intraclass correlation coefficients (ICCs) in the two arms may be unequal. We first propose a design that maximizes cost efficiency (CE), defined as the ratio of the precision of the efficacy measure to the study cost. Because such designs can be highly sensitive to the unknown ICCs and the anticipated success rates in the two arms, a local strategy based on a single set of best guesses for the ICCs and success rates can be risky. To mitigate this issue, we propose a maximin optimal design that permits ranges of values to be specified for the success rate and the ICC in each arm. We derive maximin optimal designs for three common measures of the efficacy of the intervention, risk difference, relative risk and odds ratio, and study their properties. Using a real cancer control and prevention trial example, we ascertain the efficiency of the widely used balanced design relative to the maximin optimal design and show that the former can be quite inefficient and less robust to mis-specifications of the ICCs and the success rates in the two arms.-Maximin optimal designs for cluster randomized trials.",1
"In community-intervention trials, communities, rather than individuals, are randomized to experimental arms. Generalized linear mixed models offer a flexible parametric framework for the evaluation of community-intervention trials, incorporating both systematic and random variations at the community and individual levels. We propose here a simple two-stage inference method for generalized linear mixed models, specifically tailored to the analysis of community-intervention trials. In the first stage, community-specific random effects are estimated from individual-level data, adjusting for the effects of individual-level covariates. This reduces the model approximately to a linear mixed model with the unit of analysis being community. Because the number of communities is typically small in community-intervention studies, we apply the small-sample inference method of Kenward and Roger (1997, Biometrics53, 983-997) to the linear mixed model of second stage. We show by simulation that, under typical settings of community-intervention studies, the proposed approach improves the inference on the intervention-effect parameter uniformly over both the linearized mixed-effect approach and the adaptive Gaussian quadrature approach for generalized linear mixed models. This work is motivated by a series of large randomized trials that test community interventions for promoting cancer preventive lifestyles and behaviors.-Evaluation of community-intervention trials via generalized linear mixed models.",1
"While the United States has the largest number of children with type 1 diabetes mellitus, less is known regarding adult-onset disease. The present study utilizes nationwide data to compare the incidence of type 1 diabetes in youth (0-19 years) to that of adults (20-64 years). In this longitudinal study, the Clinformatics? Data Mart Database was used, which contains information from 61 million commercially insured Americans (years 2001-2015). Incidence rates and exact Poisson 95% confidence intervals were calculated by age group, sex, census division, and year of diagnosis. Changes in rates over time were assessed by negative binomial regression. Overall, there were 32,476 individuals who developed type 1 diabetes in the cohort. The incidence rate was greatest in youth aged 10-14 years (45.5 cases/100,000 person-years); however, because adulthood spans over a longer period than childhood, there was a greater number of new cases in adults than in youth (n = 19,174 adults; n = 13,302 youth). Predominance in males was evident by age 10 and persisted throughout adulthood. The male to female incidence rate ratio was 1.32 (95% CI 1.30-1.35). The incidence rate of type 1 diabetes in youth increased by 1.9% annually from 2001 to 2015 (95% CI 1.1-2.7%; P &lt; 0.001), but there was variation across regions. The greatest increases were in the East South Central (3.8%/year; 95% CI 2.0-5.6%; P &lt; 0.001) and Mountain divisions (3.1%/year; 95% CI 1.6-4.6%; P &lt; 0.001). There were also increases in the East North Central (2.7%/year; P = 0.010), South Atlantic (2.4%/year; P &lt; 0.001), and West North Central divisions (2.4%/year; P &lt; 0.001). In adults, however, the incidence decreased from 2001 to 2015 (-1.3%/year; 95% CI -2.3% to -0.4%; P = 0.007). Greater percentages of cases were diagnosed in January, July, and August for both youth and adults. The number of new cases of type 1 diabetes (ages 0-64 years) in the United States is estimated at 64,000 annually (27,000 cases in youth and 37,000 cases in adults). There are more new cases of type 1 diabetes occurring annually in the United States than previously recognized. The increase in incidence rates in youth, but not adults, suggests that the precipitating factors of youth-onset disease may differ from those of adult-onset disease.-Fluctuations in the incidence of type 1 diabetes in the United States from 2001 to 2015: a longitudinal study.",0
"Full-information item bifactor analysis is an important statistical method in psychological and educational measurement. Current methods are limited to single-group analysis and inflexible in the types of item response models supported. We propose a flexible multiple-group item bifactor analysis framework that supports a variety of multidimensional item response theory models for an arbitrary mixing of dichotomous, ordinal, and nominal items. The extended item bifactor model also enables the estimation of latent variable means and variances when data from more than 1 group are present. Generalized user-defined parameter restrictions are permitted within or across groups. We derive an efficient full-information maximum marginal likelihood estimator. Our estimation method achieves substantial computational savings by extending Gibbons and Hedeker's (1992) bifactor dimension reduction method so that the optimization of the marginal log-likelihood requires only 2-dimensional integration regardless of the dimensionality of the latent variables. We use simulation studies to demonstrate the flexibility and accuracy of the proposed methods. We apply the model to study cross-country differences, including differential item functioning, using data from a large international education survey on mathematics literacy.-Generalized full-information item bifactor analysis.",0
Cluster effects to the design and analysis of a randomised trial.,1
The design of controlled experiments in the evaluation of non-therapeutic interventions.,1
"The ""credibility ceiling"" method was proposed to conduct sensitivity analysis for unmeasured confounding and other forms of bias in meta-analyses and has been used in umbrella reviews to grade evidence strength. However, we explain that the method has fundamental statistical flaws. We use statistical reasoning to assess the method's validity, providing intuition for our findings by presenting simple applied examples in which the method yields clearly incorrect conclusions. The credibility ceiling is not a valid bias correction, as we show mathematically and illustrate using examples in which, for example, the method incorrectly ""adjusts"" the meta-analytic point estimate in the wrong direction. Although the originators describe the method as limiting the credibility of any given observational study to a fixed ceiling, we show why this interpretation in fact bears little relation to what the method actually does. Given the fundamental problems with the credibility ceiling method and its demonstrated potential for misleading conclusions, we recommend against its use.-Controversy and debate on credibility ceilings. Paper 1: Fundamental problems with the ""credibility ceiling"" method for meta-analyses.",0
"The positivity assumption, or the experimental treatment assignment (ETA) assumption, is important for identifiability in causal inference. Even if the positivity assumption holds, practical violations of this assumption may jeopardize the finite sample performance of the causal estimator. One of the consequences of practical violations of the positivity assumption is extreme values in the estimated propensity score (PS). A common practice to address this issue is truncating the PS estimate when constructing PS-based estimators. In this study, we propose a novel adaptive truncation method, Positivity-C-TMLE, based on the collaborative targeted maximum likelihood estimation (C-TMLE) methodology. We demonstrate the outstanding performance of our novel approach in a variety of simulations by comparing it with other commonly studied estimators. Results show that by adaptively truncating the estimated PS with a more targeted objective function, the Positivity-C-TMLE estimator achieves the best performance for both point estimation and confidence interval coverage among all estimators considered.-On adaptive propensity score truncation in causal inference.",0
"The best data for cost-effectiveness analyses (CEAs) of group-level interventions often come from cluster randomized trials (CRTs), where randomization is by cluster (e.g., the hospital attended), not by individual. for these CEAs need to recognize both the correlation between costs and outcomes and that these data may be dependent on the cluster. General checklists and methodological guidance for critically appraising CEA ignore these issues. This article develops a new checklist and applies it in a systematic review of CEAs that use CRTs. The authors developed a checklist for CEAs that use CRTs, informed by a conceptual review of statistical methods. This checklist included criteria such as whether the analysis allowed for both clustering and the correlation between individuals' costs and outcomes. The authors undertook a systematic literature review of full economic evaluations that used CRTs. The quality of studies was assessed with the new checklist and by the ""Drummond checklist."" The authors identified 62 papers that met the inclusion criteria. On average, studies satisfied 9 of the 10 criteria for the checklist but only 20% of criteria for the new checklist. More than 40% of studies adopted statistical methods that completely ignored clustering, and 75% disregarded any correlation between costs and outcomes. Only 4 studies employed appropriate statistical methods that allowed for both clustering and correlation. Most economic evaluations that use data from CRTs ignored clustering or correlation. Statistical methods that address these issues are available, and their use should be encouraged. The new checklist can supplement generic CEA guidelines and highlight where research practice can be improved.-Statistical methods for cost-effectiveness analyses that use data from cluster randomized trials: a systematic review and checklist for critical appraisal.",1
"Including twins in randomised trials leads to non-independence or clustering in the data. Clustering has important implications for sample size calculations, yet few trials take this into account. Estimates of the intracluster correlation coefficient (ICC), or the correlation between outcomes of twins, are needed to assist with sample size planning. Our aims were to provide ICC estimates for infant outcomes, describe the information that must be specified in order to account for clustering due to twins in sample size calculations, and develop a simple tool for performing sample size calculations for trials including twins. ICCs were estimated for infant outcomes collected in four randomised trials that included twins. The information required to account for clustering due to twins in sample size calculations is described. A tool that calculates the sample size based on this information was developed in Microsoft Excel and in R as a Shiny web app. ICC estimates ranged between -0.12, indicating a weak negative relationship, and 0.98, indicating a strong positive relationship between outcomes of twins. Example calculations illustrate how the ICC estimates and sample size calculator can be used to determine the target sample size for trials including twins. Clustering among outcomes measured on twins should be taken into account in sample size calculations to obtain the desired power. Our ICC estimates and sample size calculator will be useful for designing future trials that include twins. Publication of additional ICCs is needed to further assist with sample size planning for future trials.-Accounting for twin births in sample size calculations for randomised trials.",1
"The linear mixed effects model based on a full likelihood is one of the few methods available to model longitudinal data subject to left censoring. However, a full likelihood approach is complicated algebraically because of the large dimension of the numeric computations, and maximum likelihood estimation can be computationally prohibitive when the data are heavily censored. Moreover, for mixed models, the complexity of the computation increases as the dimension of the random effects in the model increases. We propose a method based on pseudo likelihood that simplifies the computational complexities, allows a wide class of multivariate models, and that can be used for many different data structures including settings where the level of censoring is high. The motivation for this work comes from the need for a joint model to assess the joint effect of pro-inflammatory and anti-inflammatory biomarker data on 30-day mortality status while simultaneously accounting for longitudinal left censoring and correlation between markers in the analysis of Genetic and Inflammatory Markers for Sepsis study conducted at the University of Pittsburgh. Two markers, interleukin-6 and interleukin-10, which naturally are correlated because of a shared similar biological pathways and are left-censored because of the limited sensitivity of the assays, are considered to determine if higher levels of these markers is associated with an increased risk of death after accounting for the left censoring and their assumed correlation. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Pseudo maximum likelihood approach for the analysis of multivariate left-censored longitudinal data.",0
"Differences in resources, knowledge, and infrastructure between countries initiating and countries hosting HIV prevention research trials frequently yield ethical dilemmas. Community Advisory Boards (CABs) have emerged as one strategy for establishing partnerships between researchers and host communities to promote community consultation in socially sensitive research. To understand the evolution of CABs and community partnerships at international research sites conducting HIV prevention trials. Three research sites of the HIV Prevention Trials Network (HPTN) were selected to include geographical representation and diverse populations at risk for HIV/AIDS - in Lima, Peru; Chitungwiza, Zimbabwe; and Chiang Mai, Thailand. Data collection included review of secondary data, including academic publications and site-specific progress reports; observations at the research sites; face-to-face interviews with CAB members, research staff, and other key informants; and focus groups with study participants. Rapid assessment techniques were used for data analysis. Two of the three CABs developed new strategies for community representation in response to new studies. All three CABs expanded their original function and became advocates for broader community interests beyond HIV prevention. The participation and input of community representatives, in response to critical incidents that occurred at the sites over the past five years, helped to solidify partnerships between researchers and communities. Rapid Assessment is an exploratory methodology designed to provide an understanding of a situation based on the integration of multiple data sources, collected within a short period of time, without a formal examination of transcribed and coded data. Case studies, as a method, are meant to draw out what can be learned from a single case but are not, in the scientific sense, generalizable. In developing countries, CABs can be dynamic entities that enhance the HIV research process, assist in responding to issues involving research ethics, and prepare communities for HIV research.-Building community partnerships: case studies of Community Advisory Boards at research sites in Peru, Zimbabwe, and Thailand.",0
Randomization by group: a formal analysis.,1
"More than 1.2 million people in the United States are living with human immunodeficiency virus (HIV), and 3.2 million are living with hepatitis C virus (HCV). An estimated 25?% of persons living with HIV also have HCV. It is therefore of great public health importance to ensure the prompt diagnosis of both HIV and HCV in populations that have the highest prevalence of both infections, including individuals with substance use disorders (SUD). In this theory-driven, efficacy-effectiveness-implementation hybrid study, we will develop and test an on-site bundled rapid HIV/HCV testing intervention for SUD treatment programs. Its aim is to increase the receipt of HIV and HCV test results among SUD treatment patients. Using a rigorous process involving patients, providers, and program managers, we will incorporate rapid HCV testing into evidence-based HIV testing and linkage to care interventions. We will then test, in a randomized controlled trial, the extent to which this bundled rapid HIV/HCV testing approach increases receipt of HIV and HCV test results. Lastly, we will conduct formative research to understand the barriers to, and facilitators of, the adoption, implementation, and sustainability of the bundled rapid testing strategy in SUD treatment programs. Novel approaches that effectively integrate on-site rapid HIV and rapid HCV testing are needed to address both the HIV and HCV epidemics. If feasible and efficacious, bundled rapid HIV/HCV testing may offer a scalable, potentially cost-effective approach to testing high-risk populations, such as patients of SUD treatment programs. It may ultimately lead to improved linkage to care and progress through the HIV and HCV care and treatment cascades. ClinicalTrials.gov: NCT02355080 . (30 January 2015).-On-site bundled rapid HIV/HCV testing in substance use disorder treatment programs: study protocol for a hybrid design randomized controlled trial.",0
"We propose a prediction model for the cumulative incidence functions of competing risks, based on a logit link. Because of a concern about censoring potentially depending on time-varying covariates in our motivating human immunodeficiency virus (HIV) application, we describe an approach for estimating the parameters in the prediction models using inverse probability of censoring weighting under a missingness at random assumption. We then illustrate the application of this methodology to identify predictors of the competing outcomes of virologic failure, an efficacy outcome, and treatment limiting adverse event, a safety outcome, among human immunodeficiency virus-infected patients first starting antiretroviral treatment. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Evaluating predictors of competing risk outcomes when censoring depends on time-dependent covariates, with application to safety and efficacy of HIV treatment.",0
"Reference curves are commonly used to identify individuals with extreme values of clinically relevant variables or stages of progression which depend naturally on age or maturation. Estimation of reference curves can be complicated by a technical limit of detection (LOD) that censors the measurement from the left, as is the case in our study of reproductive hormone levels in boys around the time of the onset of puberty. We discuss issues with common approaches to the LOD problem in the context of our pubertal hormone study, and propose a two-part model that addresses these issues. One part of the proposed model specifies the probability of a measurement exceeding the LOD as a function of age. The other part of the model specifies the conditional distribution of a measurement given that it exceeds the LOD, again as a function of age. Information from the two parts can be combined to estimate the identifiable portion (i.e. above the LOD) of a reference curve and to calculate the relative standing of a given measurement above the LOD. Unlike some common approaches to LOD problems, the two-part model is free of untestable assumptions involving unobservable quantities, flexible for modeling the observable data, and easy to implement with existing software. The method is illustrated with hormone data from the Third National Health and Nutrition Examination Survey.-A two-part model for reference curve estimation subject to a limit of detection.",0
"The NIH project 'Inflammatory and Host Response to Injury' (Glue) is being conducted to study the changes in the body over time in response to trauma and burn. Patients are monitored for changes in their clinical status, such as the onset of and recovery from organ failure. Blood samples are drawn over the first days and weeks after the injury to obtain gene expression levels over time. Our goal was to develop a method of selecting genes that differentially expressed in patients who either improved or experienced organ failure. For this, we needed a test for the association between longitudinal gene expressions and the time to the occurrence of ordered categorical outcomes indicating recovery, stable disease, and organ failure. We propose a test for which the relationship between the gene expression and the events is modeled using the cumulative proportional odds model that is a generalization of the pooling repeated observation method. Given the high-dimensionality of the microarray data, it was necessary to control for the multiplicity of the testing. To control for the false discovery rate (FDR), we applied both a permutational approach as well as Efron's empirical estimation method. We explore our method through simulations and provide the analysis of the multi-center, longitudinal study of immune response to inflammation and trauma (http://www.gluegrant.org).-Analysis of the relationship between longitudinal gene expressions and ordered categorical event data.",0
"Concern about potential imbalance on risk factors in community intervention trials often prompts researchers to adopt a pair-matched design in which similar clusters of individuals are paired and one member of each matched pair is then randomly assigned to the intervention group. It is known that if there are few clusters in trial, it becomes increasingly difficult to obtain close matches on all potential risk factors. One may thus offset any gain in precision with loss in degrees of freedom due to matching. We shown in this paper that there are also several analytic limitations with pair-matched designs. These include: the restriction of prediction models to cluster-level baseline risk factors (for example, cluster size), the inability to test for homogeneity of odds ratios, and difficulties in estimating the intracluster correlation coefficient. These limitations lead us to present arguments that favour stratified designs in which there are more than two clusters in each stratum.-The merits of matching in community intervention trials: a cautionary tale.",1
"Differential brain response to sensory stimuli is very small (a few microvolts) compared to the overall magnitude of spontaneous electroencephalogram (EEG), yielding a low signal-to-noise ratio (SNR) in studies of event-related potentials (ERP). To cope with this phenomenon, stimuli are applied repeatedly and the ERP signals arising from the individual trials are averaged at the subject level. This results in loss of information about potentially important changes in the magnitude and form of ERP signals over the course of the experiment. In this article, we develop a meta-preprocessing step utilizing a moving average of ERP across sliding trial windows, to capture such longitudinal trends. We embed this procedure in a weighted linear mixed effects model to describe longitudinal trends in features such as ERP peak amplitude and latency across trials while adjusting for the inherent heteroskedasticity created at the meta-preprocessing step. The proposed unified framework, including the meta-processing and the weighted linear mixed effects modeling steps, is referred to as MAP-ERP (moving-averaged-processed ERP). We perform simulation studies to assess the performance of MAP-ERP in reconstructing existing longitudinal trends and apply MAP-ERP to data from young children with autism spectrum disorder (ASD) and their typically developing counterparts to examine differences in patterns of implicit learning, providing novel insights about the mechanisms underlying social and/or cognitive deficits in this disorder.-Identifying longitudinal trends within EEG experiments.",0
"The aim of the study was to extend a previously published checklist of study design features to include study designs often used by health systems researchers and economists. Our intention is to help review authors in any field to set eligibility criteria for studies to include in a systematic review that relate directly to the intrinsic strength of the studies in inferring causality. We also seek to clarify key equivalences and differences in terminology used by different research communities. Expert consensus meeting. The checklist comprises seven questions, each with a list of response items, addressing: clustering of an intervention as an aspect of allocation or due to the intrinsic nature of the delivery of the intervention; for whom, and when, outcome data are available; how the intervention effect was estimated; the principle underlying control for confounding; how groups were formed; the features of a study carried out after it was designed; and the variables measured before intervention. The checklist clarifies the basis of credible quasi-experimental studies, reconciling different terminology used in different fields of investigation and facilitating communications across research communities. By applying the checklist, review authors' attention is also directed to the assumptions underpinning the methods for inferring causality.-Quasi-experimental study designs series-paper 5: a checklist for?classifying studies evaluating the effects on health interventions-a taxonomy without labels.",1
"To examine change in health functioning as women progress through the menopausal transition. Prospective study of 2,489 women followed through four phases of the Whitehall II study. Health functioning was assessed with the eight subscales of the SF-36. Compared with peri- and postmenopausal women who did not experience menopausal symptoms, women who reported vasomotor symptoms or menopausal depression experienced large and significant declines on most scales of the SF-36. Women who reported the greatest symptom severity experienced the largest declines in functioning. For example, decline in physical functioning for perimenopausal women experiencing severe vasomotor symptoms was 3.3 (standard error SE=1.1) greater than those who experienced no vasomotor symptoms. Decline in role limitation-emotional for perimenopausal women experiencing severe menopause-associated depression was 18.4 (SE=2.3), compared with those who did not experience these symptoms. Vasomotor symptom reporting was predicted by low socioeconomic position, high body mass index, and limiting long-term illness at baseline. Menopause-associated depression was additionally predicted by smoking and depression. The menopausal transition is associated with decreased health functioning in women who report menopausal symptoms. Menopausal symptoms are strongly related to all aspects of health functioning assessed by the SF-36. Socioeconomic and behavioral risk factors for menopausal symptoms and associated declines in health functioning have been identified.-The menopausal transition was associated in a prospective study with decreased health functioning in women who report menopausal symptoms.",0
"In a cross-sectional stepped-wedge trial with unequal cluster sizes, attained power in the trial depends on the realized allocation of the clusters. This attained power may differ from the expected power calculated using standard formulae by averaging the attained powers over all allocations the randomization algorithm can generate. We investigated the effect of design factors and allocation characteristics on attained power and developed models to predict attained power based on allocation characteristics. Based on data simulated and analyzed using linear mixed-effects models, we evaluated the distribution of attained powers under different scenarios with varying intraclass correlation coefficient (ICC) of the responses, coefficient of variation (CV) of the cluster sizes, number of cluster-size groups, distributions of group sizes, and number of clusters. We explored the relationship between attained power and two allocation characteristics: the individual-level correlation between treatment status and time period, and the absolute treatment group imbalance. When computational time was excessive due to a scenario having a large number of possible allocations, we developed regression models to predict attained power using the treatment-vs-time period correlation and absolute treatment group imbalance as predictors. The risk of attained power falling more than 5% below the expected or nominal power decreased as the ICC or number of clusters increased and as the CV decreased. Attained power was strongly affected by the treatment-vs-time period correlation. The absolute treatment group imbalance had much less impact on attained power. The attained power for any allocation was predicted accurately using a logistic regression model with the treatment-vs-time period correlation and the absolute treatment group imbalance as predictors. In a stepped-wedge trial with unequal cluster sizes, the risk that randomization yields an allocation with inadequate attained power depends on the ICC, the CV of the cluster sizes, and number of clusters. To reduce the computational burden of simulating attained power for allocations, the attained power can be predicted via regression modeling. Trial designers can reduce the risk of low attained power by restricting the randomization algorithm to avoid allocations with large treatment-vs-time period correlations.-Explaining the variation in the attained power of a stepped-wedge trial with unequal cluster sizes",3
"Trial planning requires making efficient yet practical design choices. In a cluster randomized crossover trial, clusters of subjects cross back and forth between implementing the control and intervention conditions over the course of the trial, with each crossover marking the start of a new period. If it is possible to set up such a trial with more crossovers, a pertinent question is whether there are efficiency gains from clusters crossing over more frequently, and if these gains are substantial enough to justify the added complexity and cost of implementing more crossovers. We seek to determine the optimal number of crossovers for a fixed trial duration, and then identify other highly efficient designs by allowing the total number of clusters to vary and imposing thresholds on maximum cost and minimum statistical power. Our results pertain to trials with continuous recruitment and a continuous primary outcome, with the treatment effect estimated using a linear mixed model. To account for the similarity between subjects' outcomes within a cluster, we assume a correlation structure in which the correlation decays gradually in a continuous manner as the time between subjects' measurements increases. The optimal design is characterized by crossovers between the control and intervention conditions with each successive subject. However, this design is neither practical nor cost-efficient to implement, nor is it necessary: the gains in efficiency increase sharply in moving from a two-period to a four-period trial design, but approach an asymptote for the scenarios considered as the number of crossovers continues to increase.-How many times should a cluster randomized crossover trial cross over?",1
"In this paper, we describe the implementation and evaluation of a cluster-based enrichment strategy to call hits from a high-throughput screen using a typical cell-based assay of 160,000 chemical compounds. Our focus is on statistical properties of the prospective design choices throughout the analysis, including how to choose the number of clusters for optimal power, the choice of test statistic, the significance thresholds for clusters and the activity threshold for candidate hits, how to rank selected hits for carry-forward to the confirmation screen, and how to identify confirmed hits in a data-driven manner. Whereas previously the literature has focused on choice of test statistic or chemical descriptors, our studies suggest that cluster size is the more important design choice. We recommend clusters to be ranked by enrichment odds ratio, not by p-value. Our conceptually simple test statistic is seen to identify the same set of hits as more complex scoring methods proposed in the literature do. We prospectively confirm that such a cluster-based approach can outperform the naive top X approach and estimate that we improved confirmation rates by about 31.5% from 813 using the top X approach to 1187 using our cluster-based method.-Analysis of high-throughput screening assays using cluster enrichment.",0
"In this paper we consider longitudinal studies in which the outcome to be measured over time is binary, and the covariates of interest are categorical. In longitudinal studies it is common for the outcomes and any time-varying covariates to be missing due to missed study visits, resulting in non-monotone patterns of missingness. Moreover, the reasons for missed visits may be related to the specific values of the response and/or covariates that should have been obtained, i.e. missingness is non-ignorable. With non-monotone non-ignorable missing response and covariate data, a full likelihood approach is quite complicated, and maximum likelihood estimation can be computationally prohibitive when there are many occasions of follow-up. Furthermore, the full likelihood must be correctly specified to obtain consistent parameter estimates. We propose a pseudo-likelihood method for jointly estimating the covariate effects on the marginal probabilities of the outcomes and the parameters of the missing data mechanism. The pseudo-likelihood requires specification of the marginal distributions of the missingness indicator, outcome, and possibly missing covariates at each occasions, but avoids making assumptions about the joint distribution of the data at two or more occasions. Thus, the proposed method can be considered semi-parametric. The proposed method is an extension of the pseudo-likelihood approach in Troxel et al. to handle binary responses and possibly missing time-varying covariates. The method is illustrated using data from the Six Cities study, a longitudinal study of the health effects of air pollution.-Pseudo-likelihood methods for longitudinal binary data with non-ignorable missing responses and covariates.",0
"The introduction of pre-exposure prophylaxis (PrEP) for human immunodeficiency virus-1 (HIV-1) prevention in Africa presents new challenges for health systems that are already overburdened because PrEP delivery requires frequent clinic visits (generally every 3 months) for HIV-1 testing and PrEP refills. HIV-1 self-testing (HIVST) has the potential to improve the efficiency of PrEP delivery by decreasing the number of clinic visits. Here, we describe the rationale and design of a randomized, noninferiority trial designed to test the effectiveness and safety of using HIVST to support PrEP delivery in Kenya. The JiPime-JiPrEP (Kiswahili for 'Test Yourself, PrEP Yourself') study is a three-arm randomized trial taking place in Thika, Kenya. Participants (n?= 495) are eligible for enrollment if they are at least 18 years old, HIV-1 seronegative, and have been taking PrEP for 1 month. Three distinct participant types will be enrolled: men (n?= 165) and women (n?= 165) who are in mutually disclosed HIV-1 serodiscordant relationships, and women (n?= 165) who are?at HIV-1 risk and not in a known serodiscordant relationship. Participants in each of these subpopulations will be 1:1:1 randomized to: 1) the standard of care, with quarterly clinic visits; 2) oral HIVST, with biannual clinic visits plus oral HIVSTs to use at the quarters between those visits; or 3) blood-based HIVST, with biannual clinic visits plus blood-based HIVSTs. All participants will complete quantitative surveys and provide blood samples for the?objective measurement of PrEP adherence at baseline, 6 months, and 12 months. The primary outcomes are PrEP adherence, PrEP continuation, and HIV-1 testing, measured at 6 months and secondarily at 12 months. The findings from this trial can help to understand how HIVST-a new HIV-1 testing?technology-can support health systems in sub-Saharan Africa. Additionally, the findings can inform policy aimed at improving the efficiency of PrEP implementation and scale-up in Kenya. ClinicalTrials.gov, NCT03593629 . Retrospectively registered on 20 July 2018.-HIV-1 self-testing to improve the efficiency of pre-exposure prophylaxis delivery: a randomized trial in Kenya.",0
"The Health Evaluation and Referral Assistant (HERA) is a web-based program designed to facilitate screening, brief intervention, and referral to treatment (SBIRT) for tobacco, alcohol, and drug abuse. After the patient completes a computerized substance abuse assessment, the HERA produces a summary report with evidence-based recommended clinical actions for the healthcare provider (the Healthcare Provider Report) and a report for the patient (the Patient Feedback Report) that provides education regarding the consequences of use, personally tailored motivational messages, and a tailored substance abuse treatment referral list. For those who provide authorization, the HERA faxes the individual's contact information to a substance abuse treatment provider matched to the individual's substance use severity and personal characteristics, like insurance and location of residence (dynamic referral). This paper summarizes the methods used for a randomized controlled trial to evaluate the HERA's efficacy in leading to increased treatment initiation and reduced substance use. The study was performed in four emergency departments. Individual patients were randomized into one of two conditions: the HERA or assessment only. A total of 4269 patients were screened and 1006 participants enrolled. The sample was comprised of 427 tobacco users, 212 risky alcohol users, and 367 illicit drug users. Forty-two percent used more than one substance class. The enrolled sample was similar to the eligible patient population. The study should enhance understanding of whether computer-facilitated SBIRT can impact process of care variables, such as promoting substance abuse treatment initiation, as well as its effect on subsequent substance abuse and related outcomes.-A randomized clinical trial of the health evaluation and referral assistant (HERA): research methods.",0
"Cluster randomization trials are increasingly popular among healthcare researchers. Intact groups (called 'clusters') of subjects are randomized to receive different interventions and all subjects within a cluster receive the same intervention. In cluster randomized trials, a cluster is the unit of randomization and a subject is the unit of analysis. Variation in cluster sizes can affect the sample size estimate or the power of the study. Guittet et al. (2006) investigated the impact of an imbalance in cluster size on the power of trials with continuous outcomes through simulations. In this paper, we examine the impact of cluster size variation and intracluster correlation on the power of the study for binary outcomes through simulations. Because the sample size formula for cluster randomization trials is based on a large sample approximation, we evaluate the performance of the sample size formula with small sample sizes through simulation. Simulation study findings show that the sample size formula (m(p)) accounting for unequal cluster sizes yields empirical powers closer to the nominal power than the sample size formula (m(a)) for the average cluster size method. The differences in sample size estimates and empirical powers between m(a) and m(p) get smaller as the imbalance in cluster sizes gets smaller.-Effect of Imbalance and Intracluster Correlation Coefficient in Cluster Randomized Trials with Binary Outcomes.",1
"Hepatitis C virus infection is a major cause of hepatocellular carcinoma worldwide. Interferon has been the major antiviral treatment, yielding viral clearance in approximately half of patients. New direct-acting antivirals substantially improved the cure rate to above 90%. However, access to therapies remains limited due to the high costs and under-diagnosis of infection in specific subpopulations, e.g., baby boomers, inmates, and injection drug users, and therefore, hepatocellular carcinoma incidence is predicted to increase in the next decades even in high-resource countries. Moreover, cancer risk persists even after 10?years of viral cure, and thus a clinical strategy for its monitoring is urgently needed. Several risk-predictive host factors, e.g., advanced liver fibrosis, older age, accompanying metabolic diseases such as diabetes, persisting hepatic inflammation, and elevated alpha-fetoprotein, as well as viral factors, e.g., core protein variants and genotype 3, have been reported. Indeed, a molecular signature in the liver has been associated with cancer risk even after viral cure. Direct-acting antivirals may affect cancer development and recurrence, which needs to be determined in further investigation.-Hepatitis C-related hepatocellular carcinoma in the era of new generation antivirals.",0
"The concordance correlation coefficient (CCC), a measure of concordance in ratings from multiple raters, was used to study inter-rater agreement in measurements of time to event, generally not observed with perfect consistency among raters. As a function of the first two moments of rating measures, the CCC can be estimated with data subject to censoring, using a likelihood-based estimation method applied under the assumptions of random censoring and parametric distribution models for the ratings of time to event. A simulation study was conducted for small sample performance under various censoring proportions. The use of the CCC with censored data is illustrated with an example taken from a data set containing data on time to an event with two raters per subject.-Concordance correlation in the measurements of time to event.",0
"A common objective in health care quality studies involves measuring and comparing the quality of care delivered to cohorts of patients by different health care providers. The data used for inference involve observations on units grouped within clusters, such as patients treated within hospitals. Unlike cluster randomization trials where often clusters are randomized to interventions to learn about individuals, the target of inference in health quality studies is the cluster. Furthermore, randomization is often not performed and the resulting biases may invalidate standard tests. In this paper, we discuss approaches to sample size determination in the design of observational health quality studies when the outcome is binary. Methods for calculating sample size using marginal models are briefly reviewed, but the focus is on hierarchical binomial models. Sample size in unbalanced clusters and stratified designs are characterized. We draw upon the experiences that have arisen from a study funded by the Agency for Healthcare Research and Quality involving assessment of quality of care for patients with cardiovascular disease. If researchers are interested in comparing clusters, hierarchical models are preferred.-Sample size considerations in observational health care quality studies.",0
"Between-community variance or community-by-time variance is one of the key factors driving the cost of conducting group randomized trials, which are often very expensive. We investigated empirically whether between-community variance could be reduced by controlling individual- and/or community-level covariates and identified these covariates from four large community-based group randomized trials or surveys: the Working Well Trial; Kaiser Adolescent Survey; Kaiser Adults Survey; and the Eating Patterns Study. We found that adjusting for covariates will often substantially reduce the between-community variance component. Therefore investigators could block the communities according to these covariates, or adjust for these covariates to improve the power of community trials. We found that the community-by-time variance components are always near zero in these data sets, especially for the surveys where a cohort was followed over time. The covariate adjustment had less impact on reducing the community-by-time variance for the cohort samples than for the cross-sectional samples. This suggests that blocking may not be necessary for the design of the group randomized trials where the change from baseline is of primary interest. The Working Well Trial data were used to illustrate this point.-Explaining community-level variance in group randomized trials.",1
"We extend the pattern-mixture approach to handle missing continuous outcome data in longitudinal cluster randomized trials, which randomize groups of individuals to treatment arms, rather than the individuals themselves. Individuals who drop out at the same time point are grouped into the same dropout pattern. We approach extrapolation of the pattern-mixture model by applying multilevel multiple imputation, which imputes missing values while appropriately accounting for the hierarchical data structure found in cluster randomized trials. To assess parameters of interest under various missing data assumptions, imputed values are multiplied by a sensitivity parameter, k, which increases or decreases imputed values. Using simulated data, we show that estimates of parameters of interest can vary widely under differing missing data assumptions. We conduct a sensitivity analysis using real data from a cluster randomized trial by increasing k until the treatment effect inference changes. By performing a sensitivity analysis for missing data, researchers can assess whether certain missing data assumptions are reasonable for their cluster randomized trial.-A pattern-mixture model approach for handling missing continuous outcome data in longitudinal cluster randomized trials.",1
"This paper describes an approach for the analysis of barrier contraceptive efficacy trials that accounts for timing frequency of intercourse and compliance. We allow exposure variables to vary for each act of intercourse and we control for timing of each act through a specific parametric function of the day of the act relative to the last day of the follicular phase of the cycle. The model can be used to examine the level of protection provided by a barrier versus no contraceptive method even when no control group of non-users is studied, as long as there are acts with no barrier use during the fertile window. We present results of a simulation study which examines performance of estimators and power under a variety of scenarios, including situations where an accurate benchmark for ovulation day is not available. As compared to the survival analysis approach commonly used in this setting, simulation results show that the new approach yields considerable gains in power to detect differences between the efficacy of contraceptive methods. An application to data from the FemCap versus diaphragm trial show results consistent with previous findings suggesting superiority of the diaphragm but also provides new evidence of the per act protection provided by both methods.-A statistical model for the evaluation of barrier contraceptive efficacy.",0
"The COVID-19 pandemic has highlighted the challenges of evidence-based health policymaking, as critical precautionary decisions, such as school closures, had to be made urgently on the basis of little evidence. As primary and secondary schools once again close in the face of surging infections, there is an opportunity to rigorously study their reopening. School-aged children appear to be less affected by COVID-19 than adults, yet schools may drive community transmission of the virus. Given the impact of school closures on both education and the economy, schools cannot remain closed indefinitely. But when and how can they be reopened safely? We argue that a cluster randomized trial is a rigorous and ethical way to resolve these uncertainties. We discuss key scientific, ethical, and resource considerations both to inform trial design of school reopenings and to prompt discussion of the merits and feasibility of conducting such a trial.-Reopening schools safely in the face of COVID-19: Can cluster randomized trials help?",1
"Methods of confidence interval construction are provided for summary measures of treatment effect arising from designs randomizing clusters to one of two treatment groups. Three basic designs are considered for the case of continuous and dichotomous variables: completely randomized, pair-matched and stratified.-Confidence interval construction for effect measures arising from cluster randomization trials.",1
Optimality of equal vs. unequal cluster sizes in multilevel intervention studies: A Monte Carlo study for small sample sizes,1
"Current ongoing genome-wide association (GWA) studies represent a powerful approach to uncover common unknown genetic variants causing common complex diseases. The discovery of these genetic variants offers an important opportunity for early disease prediction, prevention, and individualized treatment. We describe here a method of combining multiple genetic variants for early disease prediction, based on the optimality theory of the likelihood ratio (LR). Such theory simply shows that the receiver operating characteristic (ROC) curve based on the LR has maximum performance at each cutoff point and that the area under the ROC curve so obtained is highest among that of all approaches. Through simulations and a real data application, we compared it with the commonly used logistic regression and classification tree approaches. The three approaches show similar performance if we know the underlying disease model. However, for most common diseases we have little prior knowledge of the disease model and in this situation the new method has an advantage over logistic regression and classification tree approaches. We applied the new method to the type 1 diabetes GWA data from the Wellcome Trust Case Control Consortium. Based on five single nucleotide polymorphisms, the test reaches medium level classification accuracy. With more genetic findings to be discovered in the future, we believe a predictive genetic test for type 1 diabetes can be successfully constructed and eventually implemented for clinical use.-Using the optimal robust receiver operating characteristic (ROC) curve for predictive genetic tests.",0
"The Wilcoxon rank sum test is frequently used in statistical practice for the comparison of measures of location when the underlying distributions are far from normal or not known in advance. An assumption of the ordinary rank sum test is that individual sampling units are independent. In many ophthalmologic clinical trials, the Early Treatment for Diabetic Retinopathy Scale (ETDRS) is a principal endpoint used for measuring the level of diabetic retinopathy. This is an ordinal scale, and it is natural to consider the Wilcoxon rank sum test for the comparison of the level of diabetic retinopathy between treatment groups. However, under this design, unlike the usual Wilcoxon rank sum test, the subject is the unit of randomization, but the eye is the unit of analysis. Furthermore, a person will tend to have different, but correlated, ETDRS scores for fellow eyes. Thus, we propose a correction to the variance of the Wilcoxon rank sum statistic that accounts for clustering effects and that can be used for both balanced (same number of subunits per cluster) or unbalanced (different number of subunits per cluster) data, both in the presence or absence of ties, with p-value adjusted accordingly. In this article, we present large-sample theory and simulation results for this test procedure and apply it to diabetic retinopathy data from type I diabetics in the Sorbinil Retinopathy Trial.-Incorporation of clustering effects for the Wilcoxon rank sum test: a large-sample approach.",1
Analysis did not account for cluster randomisation.,1
"Here, we review the research we have conducted on social contagion. We describe the methods we have employed (and the assumptions they have entailed) to examine several datasets with complementary strengths and weaknesses, including the Framingham Heart Study, the National Longitudinal Study of Adolescent Health, and other observational and experimental datasets that we and others have collected. We describe the regularities that led us to propose that human social networks may exhibit a 'three degrees of influence' property, and we review statistical approaches we have used to characterize interpersonal influence with respect to phenomena as diverse as obesity, smoking, cooperation, and happiness. We do not claim that this work is the final word, but we do believe that it provides some novel, informative, and stimulating evidence regarding social contagion in longitudinally followed networks. Along with other scholars, we are working to develop new methods for identifying causal effects using social network data, and we believe that this area is ripe for statistical development as current methods have known and often unavoidable limitations.-Social contagion theory: examining dynamic social networks and human behavior.",0
"The aim of this review was to assess the methodological quality of cluster randomised controlled trials (CRCT) for the management of tropical parasitic disease published between 1998 and 2007. A literature survey was conducted using Medline for CRCTs of interventions aimed at managing any one of the six major tropical parasitic diseases: malaria, leishmaniasis, lymphatic filariasis, onchocerciasis, schistosomiasis and trypanosomiasis (Chagas disease). Information was extracted from the published articles in order that, for each trial, categorical responses could be made to a pre-specified list of 12 questions concerning issues relating to the methodological quality of the trial, including choice of design, generalisability, baseline assessment, blinding, use or non-use of a matched design, and accounting for the intraclass correlation in both design and analysis. The literature survey found 38 CRCTs. Of the 35 CRCTs that reported at least one human outcome, 27 were for interventions in the management of malaria whilst the rest were for managing leishmaniasis (4 trials), lymphatic filariasis (2 trials) and schistosomiasis (2 trials). For every one of the pre-specified questions that concerned an issue associated with methodological quality, the responses were consistent with the practice of trialists in relation to the given issue being generally poor.-The methodological quality of cluster randomised controlled trials for managing tropical parasitic disease: a review of trials published from 1998 to 2007.",1
"Age-related macular degeneration (AMD) is a debilitating, common cause of visual impairment. While the last decade has seen great progress in understanding the pathophysiology of AMD, the molecular changes that occur in eyes with AMD are still poorly understood. In the current issue of Genome Medicine, Newman and colleagues present the first systematic transcriptional profile analysis of AMD-affected tissues, providing a comprehensive set of expression data for different regions (macula versus periphery), tissues (retina versus retinal pigment epithelium (RPE)/choroid), and disease state (control versus early or advanced AMD). Their findings will serve as a foundation for additional systems-level research into the pathogenesis of this blinding disease.-Transcriptome changes in age-related macular degeneration.",0
"Likelihood-based approaches, which naturally incorporate left censoring due to limit of detection, are commonly utilized to analyze censored multivariate normal data. However, the maximum likelihood estimator (MLE) typically underestimates variance parameters. The restricted maximum likelihood estimator (REML), which corrects the underestimation of variance parameters, cannot be easily extended to analyze censored multivariate normal data. In the light of the connection between the REML and a Bayesian approach discovered in 1974 by Dr Harville, this paper describes a Bayesian approach to censored multivariate normal data. This Bayesian approach is justified through its link to the REML via Laplace's approximation and its performance is evaluated through a simulation study. We consider the Bayesian approach as a valuable alternative because it yields less biased variance parameter estimates than the MLE, and because a solid REML is technically difficult when data are left censored.-Estimating variance parameters from multivariate normal variables subject to limit of detection: MLE, REML, or Bayesian approaches?",0
"Smoking is associated with low serum carotenoid concentrations. Prospective studies have found lower diabetes risk among persons with high-carotenoid diets. Whether diabetes risk is low in the rare smoker who has high serum carotenoid levels is unknown. The authors investigated the interaction of serum carotenoid concentrations and smoking with diabetes mellitus in 4,493 Black and White men and women aged 18-30 years in the Coronary Artery Risk Development in Young Adults (CARDIA) Study. The authors assessed 15-year (1985-2001) incident diabetes (148 cases), insulin concentration, and insulin resistance (homeostasis model assessment) in smokers and nonsmokers according to baseline levels of serum alpha-carotene, beta-carotene, zeaxanthin, beta-cryptoxanthin, and lycopene. Diabetes incidence was inversely associated with the sum of carotenoid concentrations in nonsmokers (per standard deviation (SD) increase, relative hazard = 0.74, 95% confidence interval: 0.55, 0.99) but not in current smokers (relative hazard = 1.13, 95% confidence interval: 0.83, 1.53) (p for interaction = 0.02). Similarly, year 15 insulin and insulin resistance values, adjusted for baseline levels, were inversely related to sum of carotenoids only in nonsmokers (per SD increase in insulin level, slope = -0.46 (p = 0.03); per SD increase in insulin resistance, slope = -0.14 (p = 0.01)). In CARDIA, higher serum carotenoid concentrations are associated with lower risk of diabetes and insulin resistance in nonsmokers but not in smokers.-Associations of serum carotenoid concentrations with the development of diabetes and with insulin concentration: interaction with smoking: the Coronary Artery Risk Development in Young Adults (CARDIA) Study.",0
"High postpartum weight retention is a strong independent risk factor for lifetime obesity, cardiovascular disease, and type 2 diabetes in women. Interventions to promote postpartum weight loss have met with some success but have been limited by high attrition. Internet-based treatment has the potential to overcome this barrier and reduce postpartum weight retention, but no study has evaluated the effects of an internet-based program to prevent high postpartum weight retention in women. Fit Moms/Mam?s Activas targets recruitment of 12 Women, Infants and Children (WIC) Supplemental Nutrition Program clinics with a total of 408 adult (&gt;18?years), postpartum (&lt;1?year) women with 14.5?kg or more weight retention or a body mass index of 25.0?kg/m(2) or higher. Clinics are matched on size and randomly assigned within county to either a 12-month standard WIC intervention or to a 12-month WIC enhanced plus internet-based weight loss intervention. The intervention includes: monthly face-to-face group sessions; access to a website with weekly lessons, a web diary, instructional videos, and computer-tailored feedback; four weekly text messages; and brief reinforcement from WIC counselors. Participants are assessed at baseline, six months, and 12?months. The primary outcome is weight loss over six and 12?months; secondary outcomes include diet and physical activity behaviors, and psychosocial measures. Fit Moms/Mam?s Activas is the first study to empirically examine the effects of an internet-based treatment program, coupled with monthly group contact at the WIC program, designed to prevent sustained postpartum weight retention in low-income women at high risk for weight gain, obesity, and related comorbidities. This trial was registered with Clinicaltrials.gov (identifier: NCT01408147 ) on 29 July 2011.-'Fit Moms/Mam?s Activas' internet-based weight control program with group support to reduce postpartum weight retention in low-income women: study protocol for a randomized controlled trial.",0
"The Selenium and Vitamin E Cancer Prevention Trial (SELECT) was a randomized, double-blind, placebo-controlled, prostate cancer prevention study funded by the National Cancer Institute and conducted by SWOG (Southwest Oncology Group). A total of 35,533 men were assigned randomly to one of four treatment groups (vitamin E + placebo, selenium + placebo, vitamin E + selenium, placebo + placebo). At the time of the trial's development, NIH had invested substantial resources in evaluating the potential benefits of these antioxidants. To capitalize on the knowledge gained from following a large cohort of healthy, aging males on the effects of selenium and/or vitamin E, ancillary studies with other disease endpoints were solicited. Four ancillary studies were added. Each drew from the same population but had independent objectives and an endpoint other than prostate cancer. These studies fell into two categories: those prospectively enrolling and following participants (studies of Alzheimer's disease and respiratory function) and those requiring a retrospective medical record review after a reported event (cataracts/age-related macular degeneration and colorectal screening). An examination of the challenges and opportunities of adding ancillary studies is provided. The impact of the ancillary studies on adherence to SELECT was evaluated using a Cox proportional hazards model. While the addition of ancillary studies appears to have improved participant adherence to the primary trial, this did not come without added complexity. Activation of the ancillary studies happened after the SELECT randomizations had begun resulting in accrual problems to some of the studies. Study site participation in the ancillary trials varied greatly and depended on the interest of the study site principal investigator. Procedures for each were integrated into the primary trial and all monitoring was done by the SELECT Data and Safety Monitoring Committee. The impact of the early closure of the primary trial was different for each of the ancillary trials. The ancillary studies allowed study sites to broaden the research opportunities for their participants. Their implementation was efficient because of the established infrastructure of the primary trial. Implementation of these ancillary trials took substantial planning and coordination but enriched the overall primary trial. NCT00006392-S0000 : Selenium and Vitamin E in Preventing Prostate Cancer (SELECT) (4 October 2000). NCT00780689-S0000A :? Prevention of Alzheimer's Disease by Vitamin E and Selenium (PREADVISE) (25 June 2002). NCT00784225-S0000B : Vitamin E and/or Selenium in Preventing Cataract and Age-Related Macular Degeneration in Men on SELECT SWOG-S0000 (SEE) (31 October 2008). NCT00706121-S0000D : Effect of Vitamin E and/or Selenium on Colorectal Polyps in Men Enrolled on SELECT Trial SWOG-S0000 (ACP) (26 June 2008). NCT00063453-S0000C : Vitamin E and/or Selenium in Preventing Loss of Lung Function in Older Men Enrolled on SELECT Clinical Trial SWOG-S0000 (26 June 2003).-Opportunities and challenges in incorporating ancillary studies into a cancer prevention randomized clinical trial.",0
"The difference in restricted mean survival times between two groups is a clinically relevant summary measure. With observational data, there may be imbalances in confounding variables between the two groups. One approach to account for such imbalances is estimating a covariate-adjusted restricted mean difference by modeling the covariate-adjusted survival distribution and then marginalizing over the covariate distribution. Because the estimator for the restricted mean difference is defined by the estimator for the covariate-adjusted survival distribution, it is natural to expect that a better estimator of the covariate-adjusted survival distribution is associated with a better estimator of the restricted mean difference. We therefore propose estimating restricted mean differences with stacked survival models. Stacked survival models estimate a weighted average of several survival models by minimizing predicted error. By including a range of parametric, semi-parametric, and non-parametric models, stacked survival models can robustly estimate a covariate-adjusted survival distribution and, therefore, the restricted mean treatment effect in a wide range of scenarios. We demonstrate through a simulation study that better performance of the covariate-adjusted survival distribution often leads to better mean squared error of the restricted mean difference although there are notable exceptions. In addition, we demonstrate that the proposed estimator can perform nearly as well as Cox regression when the proportional hazards assumption is satisfied and significantly better when proportional hazards is violated. Finally, the proposed estimator is illustrated with data from the United Network for Organ Sharing to evaluate post-lung transplant survival between large-volume and small-volume centers. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Estimating restricted mean treatment effects with stacked survival models.",0
"A random-effects regression model is proposed for the analysis of data arising from multicenter clinical trials. Advantages of the random regression model (RRM) in this context include that it allows for varying numbers of subjects within the different centers, it can assess the influence of variables measured both at the level of the subject and at the level of the center on the subject's clinical outcome, and it controls for and estimates the amount of intracenter variation that is present in the data. An example utilizing data collected in the National Institute of Mental Health schizophrenia collaborative study, where subjects were clustered within nine centers, illustrates the usefulness of the statistical model. Other applications and extensions of RRM within a psychiatric framework are discussed.-Random regression models for multicenter clinical trials data.",1
"Public health interventions are increasingly evaluated using cluster-randomised trials in which groups rather than individuals are allocated randomly to treatment and control arms. Outcomes for individuals within the same cluster are often more correlated than outcomes for individuals in different clusters. This needs to be taken into account in sample size estimations for planned trials, but most estimates of intracluster correlation for perinatal health outcomes come from hospital-based studies and may therefore not reflect outcomes in the community. In this study we report estimates for perinatal health outcomes from community-based trials to help researchers plan future evaluations. We estimated the intracluster correlation and the coefficient of variation for a range of outcomes using data from five community-based cluster randomised controlled trials in three low-income countries: India, Bangladesh and Malawi. We also performed a simulation exercise to investigate the impact of cluster size and number of clusters on the reliability of estimates of the coefficient of variation for rare outcomes. Estimates of intracluster correlation for mortality outcomes were lower than those for process outcomes, with narrower confidence intervals throughout for trials with larger numbers of clusters. Estimates of intracluster correlation for maternal mortality were particularly variable with large confidence intervals. Stratified randomisation had the effect of reducing estimates of intracluster correlation. The simulation exercise showed that estimates of intracluster correlation are much less reliable for rare outcomes such as maternal mortality. The size of the cluster had a greater impact than the number of clusters on the reliability of estimates for rare outcomes. The breadth of intracluster correlation estimates reported here in terms of outcomes and contexts will help researchers plan future community-based public health interventions around maternal and newborn health. Our study confirms previous work finding that estimates of intracluster correlation are associated with the prevalence of the outcome of interest, the nature of the outcome of interest (mortality or behavioural) and the size and number of clusters. Estimates of intracluster correlation for maternal mortality need to be treated with caution and a range of estimates should be used in planning future trials.-Intracluster correlation coefficients and coefficients of variation for perinatal outcomes from five cluster-randomised controlled trials in low and middle-income countries: results and methodological implications.",1
"The Intracluster Correlation Coefficient (ICC) is a major parameter of interest in cluster randomized trials that measures the degree to which responses within the same cluster are correlated. There are several types of ICC estimators and its confidence intervals (CI) suggested in the literature for binary data. Studies have compared relative weaknesses and advantages of ICC estimators as well as its CI for binary data and suggested situations where one is advantageous in practical research. The commonly used statistical computing systems currently facilitate estimation of only a very few variants of ICC and its CI. To address the limitations of current statistical packages, we developed an R package, ICCbin, to facilitate estimating ICC and its CI for binary responses using different methods. The ICCbin package is designed to provide estimates of ICC in 16 different ways including analysis of variance methods, moments based estimation, direct probabilistic methods, correlation based estimation, and resampling method. CI of ICC is estimated using 5 different methods. It also generates cluster binary data using exchangeable correlation structure. ICCbin package provides two functions for users. The function rcbin() generates cluster binary data and the function iccbin() estimates ICC and it's CI. The users can choose appropriate ICC and its CI estimate from the wide selection of estimates from the outputs. The R package ICCbin presents very flexible and easy to use ways to generate cluster binary data and to estimate ICC and it's CI for binary response using different methods. The package ICCbin is freely available for use with R from the CRAN repository (https://cran.r-project.org/package=ICCbin). We believe that this package can be a very useful tool for researchers to design cluster randomized trials with binary outcome.-R package to estimate intracluster correlation coefficient with confidence interval for binary data.",1
"Clustered failure time data are commonly encountered in biomedical research where the study subjects from the same cluster (e.g., family) share the common genetic and/or environmental factors such that the failure times within the same cluster are correlated. Two approaches that are commonly used to account for the intra-cluster association are frailty models and marginal models. In this paper, we study the marginal proportional hazards model, where the structure of dependence between individuals within a cluster is unspecified. An estimation procedure is developed based on a pseudo-likelihood approach, and a risk set sampling method is proposed for the formulation of the pseudo-likelihood. The asymptotic properties of the proposed estimators are studied, and the related issues regarding the statistical efficiencies are discussed. The performances of the proposed estimator are demonstrated by the simulation studies. A data example from a child vitamin A supplementation trial in Nepal (Nepal Nutrition Intervention Project-Sarlahi, or NNIPS) is used to illustrate this methodology.-Marginal analysis for clustered failure time data.",1
Cluster Randomised Trials,1
"Complex intervention trials should be able to answer both pragmatic and explanatory questions in order to test the theories motivating the intervention and help understand the underlying nature of the clinical problem being tested. Key to this is the estimation of direct effects of treatment and indirect effects acting through intermediate variables which are measured post-randomisation. Using psychological treatment trials as an example of complex interventions, we review statistical methods which crucially evaluate both direct and indirect effects in the presence of hidden confounding between mediator and outcome. We review the historical literature on mediation and moderation of treatment effects. We introduce two methods from within the existing causal inference literature, principal stratification and structural mean models, and demonstrate how these can be applied in a mediation context before discussing approaches and assumptions necessary for attaining identifiability of key parameters of the basic causal model. Assuming that there is modification by baseline covariates of the effect of treatment (i.e. randomisation) on the mediator (i.e. covariate by treatment interactions), but no direct effect on the outcome of these treatment by covariate interactions leads to the use of instrumental variable methods. We describe how moderation can occur through post-randomisation variables, and extend the principal stratification approach to multiple group methods with explanatory models nested within the principal strata. We illustrate the new methodology with motivating examples of randomised trials from the mental health literature.-Mediation and moderation of treatment effects in randomised controlled trials of complex interventions",1
"When treatments are administered in groups, clients interact in ways that lead to violations of a key assumption of most statistical analyses-the assumption of independence of observations. The resulting dependencies, when not properly accounted for, can increase Type I errors dramatically. Of the 33 studies of group-administered treatment on the empirically supported treatments list, none appropriately analyzed their data. The current authors provide corrections that can be applied to improper analyses. After the corrections, only 12.4% to 68.2% of tests that were originally reported as significant remained significant, depending on what assumptions were made about how large the dependencies among observations really are. Of the 33 studies, 6-19 studies no longer had any significant results after correction. The authors end by providing recommendations for researchers planning group-administered treatment research.-Empirically supported treatments or type I errors? Problems with the analysis of data from group-administered treatments.",2
"Researchers commonly collect repeated measures on individuals nested within groups such as students within schools, patients within treatment groups, or siblings within families. Often, it is most appropriate to conceptualize such groups as dynamic entities, potentially undergoing stochastic structural and/or functional changes over time. For instance, as a student progresses through school, more senior students matriculate while more junior students enroll, administrators and teachers may turn over, and curricular changes may be introduced. What it means to be a student within that school may thus differ from 1 year to the next. This article demonstrates how to use multilevel linear models to recover time-varying group effects when analyzing repeated measures data on individuals nested within groups that evolve over time. Two examples are provided. The 1st example examines school effects on the science achievement trajectories of students, allowing for changes in school effects over time. The 2nd example concerns dynamic family effects on individual trajectories of externalizing behavior and depression.-Analyzing repeated measures data on individuals nested within groups: accounting for dynamic group effects.",2
"Overweight and obesity during pregnancy represents a considerable health burden. While research has focused on interventions to limit gestational weight gain, there is little information describing their impact on neonatal health. Our aim was to investigate the effect on a range of pre-specified secondary neonatal outcomes of providing antenatal dietary and lifestyle advice to women who are overweight or obese. We report a range of pre-specified secondary neonatal outcomes from a large randomised trial in which antenatal dietary and lifestyle advice was provided to women who were overweight or obese. Pregnant women were eligible for participation with a body mass index of 25 kg/m(2) or over, and singleton gestation between 10(+0) and 20(+0) weeks. Outcome measures included gestational age at birth; Apgar score below 7 at 5 minutes of age; need for resuscitation at birth; birth weight above 4.5 kg or below 2.5 kg; birth weight, length and head circumference (and Z-scores); admission to the nursery; respiratory distress syndrome; and postnatal length of stay. Data relating to the primary outcome (large for gestational age infants defined as birth weight above the 90th centile) and birth weight above 4 kg have been reported previously. Analyses used intention-to-treat principles. In total, 2,142 infants were included in the analyses. Infants born to women following lifestyle advice were significantly less likely to have birth weight above 4.5 kg (2.15% versus 3.69%; adjusted risk ratio (aRR)=0.59; 95% confidence interval (CI) 0.36 to 0.98; P=0.04), or respiratory distress syndrome (1.22% versus 2.57%; aRR=0.47; 95% CI 0.24 to 0.90; P=0.02), particularly moderate or severe disease, and had a shorter length of postnatal hospital stay (3.94?7.26 days versus 4.41?9.87 days; adjusted ratio of means 0.89; 95% CI 0.82 to 0.97; P=0.006) compared with infants born to women who received Standard Care. For women who are overweight or obese, antenatal dietary and lifestyle advice has health benefits for infants, without an increase in the risk of harm. Continued follow-up into childhood will be important to assess the longer-term effects of a reduction in high infant birth weight on risk of child obesity. Please see related articles: http://www.biomedcentral.com/1741-7015/12/161 and http://www.biomedcentral.com/1741-7015/12/201 . Australian and New Zealand Clinical Trials Registry ( ACTRN12607000161426 ).-The effects of antenatal dietary and lifestyle advice for women who are overweight or obese on neonatal health outcomes: the LIMIT randomised trial.",0
"We examined the effect of a large reduction in the price of alcohol that occurred in Finland in 2004 on alcohol-related and all-cause mortality, and mortality due to cardiovascular diseases (CVDs) from which alcohol-attributable cases were excluded. Time series intervention analysis modelling was applied to the monthly aggregations of deaths in Finland for the period 1996-2006 to assess the impact of the reduction in alcohol prices. Alcohol-related mortality was defined using information on both underlying and contributory causes of death. Analyses were carried out for men and women aged 15-39, 40-49, 50-69 and &gt;69 years. Alcohol-related deaths increased in men aged 40-49 years, and in men and women aged 50-69 years, after the price reduction when trends and seasonal variation were taken into account: the mean rate of alcohol-related mortality increased by 17% [95% confidence interval (CI) 1.5, 33.7], 14% (95% CI 1.1, 28.0) and 40% (95% CI) 7.1, 81.7), respectively, which implies 2.5, 2.9 and 1.6 additional monthly deaths per 100,000 person-years following the price reduction. In contrast to alcohol-related mortality, CVD and all-cause mortality decreased: among men and women aged &gt;69 years a decrease of 7 and 10%, respectively, in CVD mortality implied 19 and 25 fewer monthly deaths per 100,000 person-years, and a decrease of 7 and 14%, respectively, in all-cause mortality similarly implied 42 and 69 fewer monthly deaths. These results obtained from the time series analyses suggest that the reduction in alcohol prices led to an increase in alcohol-related mortality, except in persons &lt;40 years of age. However, it appears that beneficial effects in older age, when CVD deaths are prevalent, counter-balance these adverse effects, at least to some extent.-An evaluation of the impact of a large reduction in alcohol prices on alcohol-related and all-cause mortality: time series analysis of a population-based natural experiment.",0
"Multiple baseline designs (MBDs) have been suggested as alternatives to group-randomized trials (GRT). We reviewed structural features of MBDs and considered their potential effectiveness in public health research. We also reviewed the effect of staggered starts on statistical power. We reviewed the MBD literature to identify key structural features, recent suggestions that MBDs be adopted in public health research, and the literature on power in GRTs with staggered starts. We also computed power for MBDs and GRTs. The features that have contributed to the success of small MBDs in some fields are not likely to translate well to public health research. MBDs can be more powerful than GRTs under some conditions, but those conditions involve assumptions that require careful evaluation in practice. MBDs will often serve better as a complement of rather than as an alternative to GRTs. GRTs may employ staggered starts for logistical or ethical reasons, but this will always increase their duration and will often increase their cost.-Studies with staggered starts: multiple baseline designs and group-randomized trials.",1
"Some patients will experience more or less benefit from treatment than the averages reported from clinical trials; such variation in therapeutic outcome is termed heterogeneity of treatment effects (HTE). Identifying HTE is necessary to individualize treatment. The degree to which heterogeneity is sought and analyzed correctly in the general medical literature is unknown. We undertook this literature sample to track the use of HTE analyses over time, examine the appropriateness of the statistical methods used, and explore the predictors of such analyses. Articles were selected through a probability sample of randomized controlled trials (RCTs) published in Annals of Internal Medicine, BMJ, JAMA, The Lancet, and NEJM during odd numbered months of 1994, 1999, and 2004. RCTs were independently reviewed and coded by two abstractors, with adjudication by a third. Studies were classified as reporting: (1) HTE analysis, utilizing a formal test for heterogeneity or treatment-by-covariate interaction, (2) subgroup analysis only, involving no formal test for heterogeneity or interaction; or (3) neither. Chi-square tests and multiple logistic regression were used to identify variables associated with HTE reporting. 319 studies were included. Ninety-two (29%) reported HTE analysis; another 88 (28%) reported subgroup analysis only, without examining HTE formally. Major covariates examined included individual risk factors associated with prognosis, responsiveness to treatment, or vulnerability to adverse effects of treatment (56%); gender (30%); age (29%); study site or center (29%); and race/ethnicity (7%). Journal of publication and sample size were significant independent predictors of HTE analysis (p &lt; 0.05 and p &lt; 0.001, respectively). HTE is frequently ignored or incorrectly analyzed. An iterative process of exploratory analysis followed by confirmatory HTE analysis will generate the data needed to facilitate an individualized approach to evidence-based medicine.-Dealing with heterogeneity of treatment effects: is the literature up to the challenge?",0
"Re: ""Type 2 diabetes and the risk of colorectal adenomas: Black Women's Health Study"". Four authors reply.",0
"The Bayesian model averaging continual reassessment method (CRM) is a Bayesian dose-finding design. It improves the robustness and overall performance of the continual reassessment method (CRM) by specifying multiple skeletons (or models) and then using Bayesian model averaging to automatically favor the best-fitting model for better decision making. Specifying multiple skeletons, however, can be challenging for practitioners. In this paper, we propose a default way to specify skeletons for the Bayesian model averaging CRM. We show that skeletons that appear rather different may actually lead to equivalent models. Motivated by this, we define a nonequivalence measure to index the difference among skeletons. Using this measure, we extend the model calibration method of Lee and Cheung (2009) to choose the optimal skeletons that maximize the average percentage of correct selection of the maximum tolerated dose and ensure sufficient nonequivalence among the skeletons. Our simulation study shows that the proposed method has desirable operating characteristics. We provide software to implement the proposed method. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-A default method to specify skeletons for Bayesian model averaging continual reassessment method for phase I clinical trials.",0
Stepped wedge cluster randomized trials are efficient and provide a method of evaluation without which some interventions would not be evaluated.,3
"Experiments with multiple nested levels where randomization can take place at any level bring challenges to the computation of sample sizes. Formulas derived under simple single-level experiments must be adjusted using multiplicative factors or design effects. In this work, we take a unified approach to finding the design effects in terms of intracluster correlations and present formulas to compute sample sizes of different levels. Equal cluster sample sizes and homogeneous within cluster variances are assumed.-Design effects for sample size computation in three-level designs.",1
"There is a widely recognized need for more pragmatic trials that evaluate interventions in real-world settings to inform decision-making by patients, providers, and health system leaders. Increasing availability of electronic health records, centralized research ethics review, and novel trial designs, combined with support and resources from governments worldwide for patient-centered research, have created an unprecedented opportunity to advance the conduct of pragmatic trials, which can ultimately improve patient health and health system outcomes. Such trials raise ethical issues that have not yet been fully addressed, with existing literature concentrating on regulations in specific jurisdictions rather than arguments grounded in ethical principles. Proposed solutions (e.g. using different regulations in ""learning healthcare systems"") are speculative with no guarantee of improvement over existing oversight procedures. Most importantly, the literature does not reflect a broad vision of protecting the core liberty and welfare interests of research participants. Novel ethical guidance is required. We have assembled a team of ethicists, trialists, methodologists, social scientists, knowledge users, and community members with the goal of developing guidance for the ethical design and conduct of pragmatic trials. Our project will combine empirical and conceptual work and a consensus development process. Empirical work will: (1) identify a comprehensive list of ethical issues through interviews with a small group of key informants (e.g. trialists, ethicists, chairs of research ethics committees); (2) document current practices by reviewing a random sample of pragmatic trials and surveying authors; (3) elicit views of chairs of research ethics committees through surveys in Canada, UK, USA, France, and Australia; and (4) elicit views and experiences of community members and health system leaders through focus groups and surveys. Conceptual work will consist of an ethical analysis of identified issues and the development of new ethical solutions, outlining principles, policy options, and rationales. The consensus development process will involve an independent expert panel to develop a final guidance document. Planned output includes manuscripts, educational materials, and tailored guidance documents to inform and support researchers, research ethics committees, journal editors, regulators, and funders in the ethical design and conduct of pragmatic trials.-Developing a framework for the ethical design and conduct of pragmatic trials in healthcare: a mixed methods research protocol.",1
"Preterm birth is a common adverse birth outcome known to be associated with increased infant mortality, and it often results in a higher burden of offspring morbidity in both the short and long terms. The potential for environmental factors, particularly air pollution and meteorological parameters, to increase preterm birth risk has received significant attention worldwide, but the findings are generally inconsistent, with variations in study designs and methods across populations and geographic locations. In the current issue of the Journal, Giorgis-Allemand et al. (Am J Epidemiol. 2017;185(4):247-258) take the field a step further than most prior investigations of the ambient environment. They examined the associations of ambient air pollution and meteorological factors with preterm risk among 13 cohorts across 11 European countries. No association with air pollution was observed, but associations with increased preterm birth risk were found for both increased atmospheric pressure and ambient temperature exposures during the first trimester. The study is notable in attempting to address several important issues that challenge the field, including exposure misclassification and defining critical windows of exposure. Their comprehensive evaluation of ambient exposures is to be commended.-Invited Commentary: Ambient Environment and the Risk of Preterm Birth.",0
Analysis Issues in the Evaluation of Community Trials - Progress toward Solutions in Sas/Stat Mixed,1
"We often seek to estimate the impact of an exposure naturally occurring or randomly assigned at the cluster-level. For example, the literature on neighborhood determinants of health continues to grow. Likewise, community randomized trials are applied to learn about real-world implementation, sustainability, and population effects of interventions with proven individual-level efficacy. In these settings, individual-level outcomes are correlated due to shared cluster-level factors, including the exposure, as well as social or biological interactions between individuals. To flexibly and efficiently estimate the effect of a cluster-level exposure, we present two targeted maximum likelihood estimators (TMLEs). The first TMLE is developed under a non-parametric causal model, which allows for arbitrary interactions between individuals within a cluster. These interactions include direct transmission of the outcome (i.e. contagion) and influence of one individual's covariates on another's outcome (i.e. covariate interference). The second TMLE is developed under a causal sub-model assuming the cluster-level and individual-specific covariates are sufficient to control for confounding. Simulations compare the alternative estimators and illustrate the potential gains from pairing individual-level risk factors and outcomes during estimation, while avoiding unwarranted assumptions. Our results suggest that estimation under the sub-model can result in bias and misleading inference in an observational setting. Incorporating working assumptions during estimation is more robust than assuming they hold in the underlying causal model. We illustrate our approach with an application to HIV prevention and treatment.-A new approach to hierarchical data analysis: Targeted maximum likelihood estimation for the causal effect of a cluster-level exposure.",1
"Multilevel models provide a flexible modelling framework for cost-effectiveness analyses that use cluster randomised trial data. However, there is a lack of guidance on how to choose the most appropriate multilevel models. This paper illustrates an approach for deciding what level of model complexity is warranted; in particular how best to accommodate complex variance-covariance structures, right-skewed costs and missing data. Our proposed models differ according to whether or not they allow individual-level variances and correlations to differ across treatment arms or clusters and by the assumed cost distribution (Normal, Gamma, Inverse Gaussian). The models are fitted by Markov chain Monte Carlo methods. Our approach to model choice is based on four main criteria: the characteristics of the data, model pre-specification informed by the previous literature, diagnostic plots and assessment of model appropriateness. This is illustrated by re-analysing a previous cost-effectiveness analysis that uses data from a cluster randomised trial. We find that the most useful criterion for model choice was the deviance information criterion, which distinguishes amongst models with alternative variance-covariance structures, as well as between those with different cost distributions. This strategy for model choice can help cost-effectiveness analyses provide reliable inferences for policy-making when using cluster trials, including those with missing data.-Multilevel models for cost-effectiveness analyses that use cluster randomised trial data: An approach to model choice.",1
"The time is right for the use of Bayesian Adaptive Designs (BAD) in comparative effectiveness trials. For example, Patient Centered Outcomes Research Institute has joined the Food and Drug Administration and National Intitutes of Health in adopting policies/guidelines encouraging their use. There are multiple aspects to BAD that need to be considered when designing a comparative effectiveness design. First, the adaptation rules can determine the expected size of the trial. Second, a utility function can be used to combine extremely important co-endpoints (e.g., efficacy and tolerability) and is a valuable tool for incorporating clinical expertise and potentially patient preference. Third, accrual rate is also very, very important. Specifically, there is a juxtaposition related to accrual and BAD. If accrual rate is too fast we never gain efficient information for adapting. If accrual rate is too slow we never finish the clinical trial. We propose methodology for finding the 'sweet spot' for BAD that addresses these as design parameters. We demonstrate the methodology on a comparative effectiveness BAD of pharmaceutical agents in cryptogenic sensory polyneuropathy. The study has five arms with two endpoints that are combined with a utility function. The accrual rate is assumed to stem from multiple sites. We perform simulations from which the composite accrual rates across sites result in various piecewise Poisson distributions as parameter inputs. We balance both average number of patients needed and average length of time to finish the study.-Building efficient comparative effectiveness trials through adaptive designs, utility functions, and accrual rate optimization: finding the sweet spot.",0
"To describe the sample size calculation, analysis and reporting of split-plot (S-P) randomized controlled trials in health care (trials that use two units of randomization: one at a cluster-level and one at a level lower than the cluster). We carried out a comprehensive search in the EMBASE database from 1946 to 2016. Health care trials with a S-P design in human subjects were included. Three authors screened and assessed the studies, and the data were extracted on methodology and reporting standards based on CONSORT. Eighteen S-P studies were included, with authors using nine different designations to describe them. Units of randomization were unclear in nine abstracts. Explicit rationale for choosing the design was not given. Ten studies presented a sample size calculation accounting for clustering; the analyses were coherent with that. Flow of participant diagrams was presented but was incomplete in 14 articles. S-P designs can be useful complex designs but challenging to report. Researchers need to clearly describe the rationale, sample size calculation, and participant flow. We provide a suggested CONSORT style participant flow diagram to aid reporting. There is need for more research regarding sample size calculation for S-P.-The split-plot design was useful for evaluating complex, multilevel interventions, but there is need for improvement in its design and report.",1
"The neurotoxic effects of chemical agents are often investigated in controlled studies on rodents, with multiple binary and continuous endpoints routinely collected. One goal is to conduct quantitative risk assessment to determine safe dose levels. Such studies face two major challenges for continuous outcomes. First, characterizing risk and defining a benchmark dose are difficult. Usually associated with an adverse binary event, risk is clearly definable in quantal settings as presence or absence of an event; finding a similar probability scale for continuous outcomes is less clear. Often, an adverse event is defined for continuous outcomes as any value below a specified cutoff level in a distribution assumed normal or log normal. Second, while continuous outcomes are traditionally analyzed separately for such studies, recent literature advocates also using multiple outcomes to assess risk. We propose a method for modeling and quantitative risk assessment for bivariate continuous outcomes that address both difficulties by extending existing percentile regression methods. The model is likelihood based; it allows separate dose-response models for each outcome while accounting for the bivariate correlation and overall characterization of risk. The approach to estimation of a benchmark dose is analogous to that for quantal data without the need to specify arbitrary cutoff values. We illustrate our methods with data from a neurotoxicity study of triethyl tin exposure in rats.-Quantitative risk assessment for multivariate continuous outcomes with application to neurotoxicology: the bivariate case.",0
"This investigation studies racial and socioeconomic differences in mortality from colorectal cancer, and how they vary by stage and age at diagnosis. Cox proportional hazards models were used to estimate the hazard ratio of dying from colorectal cancer, controlling for tumor characteristics and sociodemographic factors. Black adults had a greater risk of death from colorectal cancer, especially in early stages. The gender gap in mortality is wider among blacks than whites. Differences in tumor characteristics and socioeconomic factors each accounted for approximately one third of the excess risk of death among blacks. Effects of socioeconomic factors and race varied significantly by age. Higher stage-specific mortality rates and more advanced stage at diagnosis both contribute to the higher case-fatality rates from colorectal cancer among black adults, only some of which is due to socioeconomic differences. Socioeconomic and racial factors have their most significant effects in different age groups.-Racial differences in colorectal cancer mortality. The importance of stage and socioeconomic status.",0
"Combining multiple markers can improve classification accuracy compared with using a single marker. In practice, covariates associated with markers or disease outcome can affect the performance of a biomarker or biomarker combination in the population. The covariate-adjusted receiver operating characteristic (ROC) curve has been proposed as a tool to tease out the covariate effect in the evaluation of a single marker; this curve characterizes the classification accuracy solely because of the marker of interest. However, research on the effect of covariates on the performance of marker combinations and on how to adjust for the covariate effect when combining markers is still lacking. In this article, we examine the effect of covariates on classification performance of linear marker combinations and propose to adjust for covariates in combining markers by maximizing the nonparametric estimate of the area under the covariate-adjusted ROC curve. The proposed method provides a way to estimate the best linear biomarker combination that is robust to risk model assumptions underlying alternative regression-model-based methods. The proposed estimator is shown to be consistent and asymptotically normally distributed. We conduct simulations to evaluate the performance of our estimator in cohort and case/control designs and compare several different weighting strategies during estimation with respect to efficiency. Our estimator is also compared with alternative regression-model-based estimators or estimators that maximize the empirical area under the ROC curve, with respect to bias and efficiency. We apply the proposed method to a biomarker study from an human immunodeficiency virus vaccine trial. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-Combining biomarkers for classification with covariate adjustment.",0
"Stepped-wedge cluster randomized trials, which randomize clusters of subjects to treatment sequences in which clusters switch from control to intervention conditions, are being conducted with increasing frequency. Due to the real-world nature of this design, methodological and implementation challenges are ubiquitous. To account for such challenges, more complex statistical models to plan studies and analyze data are required. In this paper, we consider stepped-wedge trials that accommodate treatment effect heterogeneity across clusters, implementation periods during which no data are collected, or both treatment effect heterogeneity and implementation periods. Previous work has shown that the sequence-period cells of a stepped-wedge design contribute unequal amounts of information to the estimation of the treatment effect. In this paper, we extend that work by considering the amount of information available for the estimation of the treatment effect in each sequence-period cell, sequence, and period of stepped-wedge trials with more complex designs and outcome models. When either treatment effect heterogeneity and/or implementation periods are present, the pattern of information content of sequence-period cells tends to be clustered around the times of the switch from control to intervention condition, similarly to when these complexities are absent. However, the presence and degree of treatment effect heterogeneity and the number of implementation periods can influence the information content of periods and sequences markedly.-Information content of stepped-wedge designs when treatment effect heterogeneity and/or implementation periods are present",3
"In designing a longitudinal cluster randomized clinical trial (cluster-RCT), the interventions are randomly assigned to clusters such as clinics. Subjects within the same clinic will receive the identical intervention. Each will be assessed repeatedly over the course of the study. A mixed-effects linear regression model can be applied in a cluster-RCT with three-level data to test the hypothesis that the intervention groups differ in the course of outcome over time. Using a test statistic based on maximum likelihood estimates, we derived closed-form formulae for statistical power to detect the intervention by time interaction and the sample size requirements for each level. Importantly, the sample size does not depend on correlations among second-level data units and the statistical power function depends on the number of second- and third-level data units through their product. A simulation study confirmed that theoretical power estimates based on the derived formulae are nearly identical to empirical estimates.-Sample size requirements to detect an intervention by time interaction in longitudinal cluster randomized clinical trials.",1
"The cluster randomised crossover (CRXO) design is gaining popularity in trial settings where individual randomisation or parallel group cluster randomisation is not feasible or practical. Our aim is to stimulate discussion on the content of a reporting guideline for CRXO trials and to assess the reporting quality of published CRXO trials. We undertook a systematic review of CRXO trials. Searches of MEDLINE, EMBASE, and CINAHL Plus as well as citation searches of CRXO methodological articles were conducted to December 2014. Reporting quality was assessed against both modified items from 2010 CONSORT and 2012 cluster trials extension and other proposed quality measures. Of the 3425 records identified through database searching, 83 trials met the inclusion criteria. Trials were infrequently identified as ""cluster randomis(z)ed crossover"" in title (n = 7, 8%) or abstract (n = 21, 25%), and a rationale for the design was infrequently provided (n = 20, 24%). Design parameters such as the number of clusters and number of periods were well reported. Discussion of carryover took place in only 17 trials (20%). Sample size methods were only reported in 58% (n = 48) of trials. A range of approaches were used to report baseline characteristics. The analysis method was not adequately reported in 23% (n = 19) of trials. The observed within-cluster within-period intracluster correlation and within-cluster between-period intracluster correlation for the primary outcome data were not reported in any trial. The potential for selection, performance, and detection bias could be evaluated in 30%, 81%, and 70% of trials, respectively. There is a clear need to improve the quality of reporting in CRXO trials. Given the unique features of a CRXO trial, it is important to develop a CONSORT extension. Consensus amongst trialists on the content of such a guideline is essential.-The quality of reporting in cluster randomised crossover trials: proposal for reporting items and an assessment of reporting quality.",1
"Estimates of life expectancy are useful in assessing whether different prevention strategies are appropriate in different populations. We developed sex-specific Cox proportional-hazard models that use Medicare claims data to predict life expectancy and risk of death at up to 10 years for older adults. We identified a cohort of Medicare beneficiaries 66-90 years of age from the 5% Medicare claims data in 2000 (n = 1,137,311) and tracked each subject's vital status until December 31, 2009. Subjects were split randomly into training and validation samples. Models were developed from the training sample and validated by comparison of predicted to actual survival in the validation sample. The C statistics for the models including predictors of age and Elixhauser comorbidities were 0.76-0.79 for men and women for prediction of death at the 1-, 5-, 7-, and 10-year follow-up periods. More than 80% of subjects with &lt;25% risk of death at 5, 7, and 10 years survived longer than the chosen cutoff years. More than 80% of subjects with ?75% risk of death at 5, 7, and 10 years died by those cutoff years. The models overestimated the risk of death at 1 year for the high-risk groups. Sex-specific models that use age and Elixhauser comorbidities can accurately predict patient life expectancy and risk of death at 5-10 years.-Predicting life expectancy for community-dwelling older adults from Medicare claims data.",0
"In the Dutch EASYcare Study, pseudo cluster randomization (PCR) randomized clinicians in two groups (H and L) with a high or a low proportion of the patients of the clinician randomized to intervention or to control arm accordingly. We used PCR because cluster randomization risked selection bias and individual randomization risked contamination. We evaluated the performance of PCR. Clinicians were asked about treatment arm preferences, recruitment behavior, possible contaminating behavior, and what they thought the allocation ratio was. We compared patients' baseline characteristics and clinicians' recruitment rates. The groups were comparable at baseline. Clinicians favored the intervention arm (Visual Analogue Scale 14.5 [SD 15.6]; 0-100; 0=strongly favoring intervention arm, 100=strongly favoring usual care arm) and 58% said they would have recruited fewer patients had every participant been allocated to the control group. Sixty five percent of clinicians used intervention elements in control patients. Sixty seven percent of clinicians estimated that a 50:50 allocation ratio was used. The assumptions underlying PCR largely applied in this study. PCR performed satisfactorily without signs of unblinding or selection bias.-Pseudo cluster randomization performed well when used in practice.",1
"Stepped-wedge cluster randomised trials (SW-CRTs) are being used with increasing frequency in health service evaluation. Conventionally, these studies are cross-sectional in design with equally spaced steps, with an equal number of clusters randomised at each step and data collected at each and every step. Here we introduce several variations on this design and consider implications for power. One modification we consider is the incomplete cross-sectional SW-CRT, where the number of clusters varies at each step or where at some steps, for example, implementation or transition periods, data are not collected. We show that the parallel CRT with staggered but balanced randomisation can be considered a special case of the incomplete SW-CRT. As too can the parallel CRT with baseline measures. And we extend these designs to allow for multiple layers of clustering, for example, wards within a hospital. Building on results for complete designs, power and detectable difference are derived using a Wald test and obtaining the variance-covariance matrix of the treatment effect assuming a generalised linear mixed model. These variations are illustrated by several real examples. We recommend that whilst the impact of transition periods on power is likely to be small, where they are a feature of the design they should be incorporated. We also show examples in which the power of a SW-CRT increases as the intra-cluster correlation (ICC) increases and demonstrate that the impact of the ICC is likely to be smaller in a SW-CRT compared with a parallel CRT, especially where there are multiple levels of clustering. Finally, through this unified framework, the efficiency of the SW-CRT and the parallel CRT can be compared.-Stepped-wedge cluster randomised controlled trials: a generic framework including parallel and multiple-level designs.",3
"Cluster randomized trials (CRTs) are increasingly used to study the efficacy of interventions targeted at the population level. Formulae exist to calculate sample sizes for CRTs, but they assume that the domain of the outcomes being considered covers the full range of values of the considered distribution. This assumption is frequently incorrect in epidemiological trials in which counts of infection episodes are right-truncated due to practical constraints on the number of times a person can be tested. Motivated by a malaria vector control trial with right-truncated Poisson-distributed outcomes, we investigated the effect of right-truncation on power using Monte Carlo simulations. The results demonstrate that the adverse impact of right-truncation is directly proportional to the magnitude of the event rate, ?, with calculations of power being overestimated in instances where right-truncation was not accounted for. The severity of the adverse impact of right-truncation on power was more pronounced when the number of clusters was ?30 but decreased the further the right-truncation point was from zero. Potential right-truncation should always be accounted for in the calculation of sample size requirements at the study design stage.-Power calculations for cluster randomized trials (CRTs) with right-truncated Poisson-distributed outcomes: a motivating example from a malaria vector control trial",1
"The objective of the study was to evaluate the impact of #Tamojunto, a Brazilian adaptation of the Unplugged prevention program, on patterns of drug use among adolescents and to characterize their trajectories of drug use over time. An in-cluster randomized controlled trial was conducted in 2014-2015 with 2 parallel arms (intervention and control). The intervention group attended 12 weekly classes of the #Tamojunto intervention. The control schools did not offer a prevention program. The target population was students attending seventh and eighth grades. The primary dichotomous outcome measures were use of drugs (any alcohol use, binge drinking, tobacco, marijuana, inhalants, and cocaine) in the past year assessed using a questionnaire before intervention and in 2 waves of follow-up (9 and 21 months). A latent transition analysis in 6,391 students from 72 public schools in 6 Brazilian cities revealed 3 distinct patterns of drug use behavior: abstainers/low users (81.54% at baseline, 70.61% after 21 months), alcohol users/binge drinkers (16.65% at baseline, 21.45% after 21 months), and polydrug users (1.80% at baseline, 7.92% after 21 months). No differences in the probabilities of transitions between these drug use patterns were found between the intervention and control groups. The most likely trajectory was no transition between patterns, regardless of the intervention and baseline pattern. The intervention was not successful in changing adolescent drug use patterns over time, showing that the components of the Brazilian adaptation of the Unplugged prevention program should be reevaluated. (PsycINFO Database Record-A latent transition analysis of a cluster randomized controlled trial for drug use prevention.",1
"A stepped wedge cluster randomized trial is a type of longitudinal cluster design that sequentially switches clusters to intervention over time until all clusters are treated. While the traditional posttest-only parallel design requires adjustment for a single intraclass correlation coefficient, the stepped wedge design allows multiple outcome measurements from the same cluster and so additional correlation parameters are necessary to characterize the within-cluster correlation structure. Although a number of studies have differentiated between the concepts of within-period and between-period correlations, only a few studies have allowed the between-period correlation to decay over time. In this article, we consider the proportional decay correlation structure for a cohort stepped wedge design, and provide a matrix-adjusted quasi-least squares approach to accurately estimate the correlation parameters along with the marginal intervention effect. We further develop the sample size and power procedures accounting for the correlation decay, and investigate the accuracy of the power procedure with continuous outcomes in a simulation study. We show that the empirical power agrees well with the prediction even with as few as nine clusters, when data are analyzed with matrix-adjusted quasi-least squares concurrently with a suitable bias-corrected sandwich variance. Two trial examples are provided to illustrate the new sample size procedure.-Design and analysis considerations for cohort stepped wedge cluster randomized trials with a decay correlation structure",3
"The stepped wedge design (SWD) and the interrupted time-series design (ITSD) are two alternative research designs that maximize efficiency and statistical power with small samples when contrasted to the operating characteristics of conventional randomized controlled trials (RCT). This paper provides an overview and introduction to previous work with these designs and compares and contrasts them with the dynamic wait-list design (DWLD) and the regression point displacement design (RPDD), which were presented in a previous article (Wyman, Henry, Knoblauch, and Brown, Prevention Science. 2015) in this special section. The SWD and the DWLD are similar in that both are intervention implementation roll-out designs. We discuss similarities and differences between the SWD and DWLD in their historical origin and application, along with differences in the statistical modeling of each design. Next, we describe the main design characteristics of the ITSD, along with some of its strengths and limitations. We provide a critical comparative review of strengths and weaknesses in application of the ITSD, SWD, DWLD, and RPDD as small sample alternatives to application of the RCT, concluding with a discussion of the types of contextual factors that influence selection of an optimal research design by prevention researchers working with small samples.-Research Designs for Intervention Research with Small Samples II: Stepped Wedge and Interrupted Time-Series Designs.",3
"For the past few decades, randomized clinical trials have provided evidence for effective treatments by comparing several competing therapies. Their successes have led to numerous new therapies to combat many diseases. However, since their conclusions are based on the entire cohort in the trial, the treatment recommendation is for everyone, and may not be the best option for an individual. Medical research is now focusing more on providing personalized care for patients, which requires investigating how patient characteristics, including novel biomarkers, modify the effect of current treatment modalities. This is known as heterogeneity of treatment effects. A better understanding of the interaction between treatment and patient-specific prognostic factors will enable practitioners to expand the availability of tailored therapies, with the ultimate goal of improving patient outcomes. The Subpopulation Treatment Effect Pattern Plot (STEPP) approach was developed to allow researchers to investigate the heterogeneity of treatment effects on survival outcomes across values of a (continuously measured) covariate, such as a biomarker measurement. Here, we extend the Subpopulation Treatment Effect Pattern Plot approach to continuous, binary, and count outcomes, which can be easily modeled using generalized linear models. With this extension of Subpopulation Treatment Effect Pattern Plot, these additional types of treatment effects within subpopulations defined with respect to a covariate of interest can be estimated, and the statistical significance of any observed heterogeneity of treatment effect can be assessed using permutation tests. The desirable feature that commonly used models are applied to well-defined patient subgroups to estimate treatment effects is retained in this extension. We describe a simulation study to confirm that the proper Type I error rate is maintained when there is no treatment heterogeneity, and a power study to show that the statistics have power to detect treatment heterogeneity under alternative scenarios. As an illustration, we apply the methods to data from the Aspirin/Folate Polyp Prevention Study, a clinical trial evaluating the effect of oral aspirin, folic acid, or both as a chemoprevention agent against colorectal adenomas. The pre-existing R software package stepp has been extended to handle continuous, binary, and count data using Gaussian, Bernoulli, and Poisson models, and it is available on the Comprehensive R Archive Network. The extension of the method and the availability of new software now permit STEPP to be applied to the full range of clinical trial end points.-Subpopulation Treatment Effect Pattern Plot (STEPP) analysis for continuous, binary, and count outcomes.",0
"Quasi-experimental studies are increasingly used to establish causal relationships in epidemiology and health systems research. Quasi-experimental studies offer important opportunities to increase and improve evidence on causal effects: (1) they can generate causal evidence when randomized controlled trials are impossible; (2) they typically generate causal evidence with a high degree of external validity; (3) they avoid the threats to internal validity that arise when participants in nonblinded experiments change their behavior in response to the experimental assignment to either intervention or control arm (such as compensatory rivalry or resentful demoralization); (4) they are often well suited to generate causal evidence on long-term health outcomes of an intervention, as well as nonhealth outcomes such as economic and social consequences; and (5) they can often generate evidence faster and at lower cost than experiments and other intervention studies.-Quasi-experimental study designs series-paper 4: uses and value.",0
"To determine the relation between concentrations of prostate specific antigen at age 60 and subsequent diagnosis of clinically relevant prostate cancer in an unscreened population to evaluate whether screening for prostate cancer and chemoprevention could be stratified by risk. Case-control study with 1:3 matching nested within a highly representative population based cohort study. General population of Sweden taking part in the Malmo Preventive Project. Cancer registry at the National Board of Health and Welfare. 1167 men aged 60 who provided blood samples in 1981 and were followed up to age 85. Metastasis or death from prostate cancer. The rate of screening during the course of the study was low. There were 43 cases of metastasis and 35 deaths from prostate cancer. Concentration of prostate specific antigen at age 60 was associated with prostate cancer metastasis (area under the curve 0.86, 95% confidence interval 0.79 to 0.92; P&lt;0.001) and death from prostate cancer (0.90, 0.84 to 0.96; P&lt;0.001). The greater the number for the area under the curve (values from 0 to 1) the better the test. Although only a minority of the men with concentrations in the top quarter (&gt;2 ng/ml) develop fatal prostate cancer, 90% (78% to 100%) of deaths from prostate cancer occurred in these men. Conversely, men aged 60 with concentrations at the median or lower (?1 ng/ml) were unlikely to have clinically relevant prostate cancer (0.5% risk of metastasis by age 85 and 0.2% risk of death from prostate cancer). The concentration of prostate specific antigen at age 60 predicts lifetime risk of metastasis and death from prostate cancer. Though men aged 60 with concentrations below the median (?1 ng/ml) might harbour prostate cancer, it is unlikely to become life threatening. Such men could be exempted from further screening, which should instead focus on men with higher concentrations.-Prostate specific antigen concentration at age 60 and death or metastasis from prostate cancer: case-control study.",0
"Behavioral intervention research has lagged behind biomedical research in developing principles for defining, categorizing, identifying, reporting, and monitoring adverse events and unanticipated problems. In this article we present a set of principles for defining adverse events and how they were applied in a large national multi-site family therapy study for substance-using adolescents, The Brief Strategic Family Therapy (BSFT) Effectiveness Study. The BSFT Effectiveness study tested how BSFT compares to Treatment as Usual (TAU) for the treatment of drug-abusing adolescents. During protocol development, experts in the BSFT intervention, medical safety officers, ethicists and senior investigators defined the procedures for identifying, tracking and reporting adverse events for drug using adolescents as well as their family members. During this process the team identified five key guiding principles. The five guiding principles that were used for defining adverse events in this behavioral trial were that that the adverse events should be validated and plausible, and that monitoring systems should assess relatedness, be systematic, and are a shared responsibility. The following non-serious adverse events were identified: arrest, school suspension and drop out, runaway, kicked out of home and violence. The serious adverse events in this study for the identified adolescent participant and all other consented family members were physical or sexual abuse, suicidal behavior, homicidal behavior, hospitalization (drug related or psychiatric related only) and death. The methods used in categorizing, identifying and reporting adverse events in the BSFT trial are outlined. More than 50% of the adolescent population (277/481 = 57.5 %) experienced an adverse event during the trial. Family members experienced less adverse events, (61/1338 = 4.5%). The most common event for the adolescent group was arrest (164/277= 59.2%), followed by school suspension/dropout (143/277 = 51.6%), and runaway (79/277= 28.5 %). For the family member group, the most common event was violence (25/ 61 = 40.9%) followed by arrest (13/61 = 21.3%). There was a significant difference in the presence of adverse events in family members that were randomized to BSFT 44/721 (6.1%) when compared to Treatment as Usual 17/617 (2.8%) (p = 0.004). A probable explanation for this is that there were more opportunities to identify adverse events for family members assigned to BSFT because family members attended therapy sessions. This difference may also represent the risk for family members that participate in an evidence-based family intervention like BSFT. The utility of the principles outside of the BSFT trial is unknown. Based on the events reported in this trial, the efforts for monitoring and categorizing adverse events appeared justified and appropriate. The strategies and principles described in this paper may be useful for those developing safety plans for behavioral intervention research, and to family therapy researchers for assessing the safety of behavioral family interventions.-Principles for defining adverse events in behavioral intervention research: lessons from a family-focused adolescent drug abuse trial.",0
"Prediabetes is an asymptomatic condition in which patients' blood glucose levels are higher than normal but do not meet diagnostic criteria for type 2 diabetes mellitus (T2DM). A key window of opportunity to increase engagement of patients with prediabetes in strategies to prevent T2DM is when they are screened for T2DM and found to have prediabetes, yet the effects of this screening and brief counseling are unknown. In this parallel-design randomized controlled trial we will recruit 315 non-diabetic patients from the Ann Arbor VA Medical Center (AAVA) who have one or major risk factors for T2DM and an upcoming primary care appointment at the AAVA, but have not had a hemoglobin A1c (HbA1c) test to screen for T2DM in the previous 12?months. After informed consent, participants will complete a baseline survey and be randomly assigned to, at the time of their next primary care appointment, one of two arms: (1) to have a hemoglobin A1c (HbA1c) test to screen for T2DM and receive brief, standardized counseling about these results or (2) to review a brochure about clinical preventive services. Participants will complete surveys 2?weeks, 3?months, and 12?months after their primary care appointment, and a weight measurement 12?months after their primary care appointment. The primary outcome is weight change after 12?months. The secondary outcomes are changes in perception of risk for T2DM; knowledge of T2DM prevention; self-efficacy and motivation to prevent T2DM; use of pharmacotherapy for T2DM prevention; physical activity; participation in weight management programs; and mental health. Quantitative analyses will compare outcomes among participants in the HbA1c test arm found to have prediabetes with participants in the brochure arm. Among participants in the HbA1c test arm found to have prediabetes we will conduct semi-structured interviews about their understanding of and reactions to receiving a prediabetes diagnosis. This trial will generate foundational data on the effects of a prediabetes diagnosis and brief counseling on patients' preventive behaviors and mediators of these behaviors that will enable the development of novel strategies to improve patient engagement in T2DM prevention. ClinicalTrials.gov, NCT02747108 . Registered on 18 April 2016.-ForgIng New paths in DIabetes PrevenTion (FINDIT): Study Protocol for a Randomized Controlled Trial.",0
"The relation between consumption of different types of dairy and risk of type 2 diabetes (T2D) remains uncertain. Therefore, we aimed to evaluate the association between total dairy and individual types of dairy consumptions and incident T2D in US adults. We followed 41,436 men in the Health Professionals Follow-Up Study (1986 to 2010), 67,138 women in the Nurses' Health Study (1980 to 2010), and 85,884 women in the Nurses' Health Study II (1991 to 2009). Diet was assessed by validated food-frequency questionnaires, and data were updated every four years. Incident T2D was confirmed by a validated supplementary questionnaire. During 3,984,203 person-years of follow-up, we documented 15,156 incident T2D cases. After adjustment for age, body mass index (BMI) and other lifestyle and dietary risk factors, total dairy consumption was not associated with T2D risk and the pooled hazard ratio (HR) (95% confidence interval (CI)) of T2D for one serving/day increase in total dairy was 0.99 (0.98, 1.01). Among different types of dairy products, neither low-fat nor high-fat dairy intake was appreciably associated with risk of T2D. However, yogurt intake was consistently and inversely associated with T2D risk across the three cohorts with the pooled HR of 0.83 (0.75, 0.92) for one serving/day increment (P for trend &lt;0.001). We conducted a meta-analysis of 14 prospective cohorts with 459,790 participants and 35,863 incident T2D cases; the pooled relative risks (RRs) (95% CIs) were 0.98 (0.96, 1.01) and 0.82 (0.70, 0.96) for one serving total dairy/day and one serving yogurt/day, respectively. Higher intake of yogurt is associated with a reduced risk of T2D, whereas other dairy foods and consumption of total dairy are not appreciably associated with incidence of T2D.-Dairy consumption and risk of type 2 diabetes: 3 cohorts of US adults and an updated meta-analysis.",0
"HIV testing for marginalized populations is critical to controlling the HIV epidemic. However, the HIV testing rate among men who have sex with men (MSM) in China remains low. Crowdsourcing, the process of shifting individual tasks to a group, has been increasingly adopted in public health programs and may be a useful tool for spurring innovation in HIV testing campaigns. We designed a multi-site study to develop a crowdsourced HIV test promotion campaign and evaluate its effectiveness against conventional campaigns among MSM in China. This study will use an adaptation of the stepped wedge, randomized controlled trial design. A total of eight major metropolitan cities in China will be randomized to sequentially initiate interventions at 3-month intervals. The intervention uses crowdsourcing at multiple steps to sustain crowd contribution. Approximately 1280 MSM, who are 16?years of age or over, live in the intervention city, have not been tested for HIV in the past 3?months, and are not living with HIV, will be recruited. Recruitment will take place through banner advertisements on a large gay dating app along with other social media platforms. Participants will complete one follow-up survey every 3?months for 12?months to evaluate their HIV testing uptake in the past 3?months and secondary outcomes including syphilis testing, sex without condoms, community engagement, testing stigma, and other related outcomes. MSM HIV testing rates remain poor in China. Innovative methods to promote HIV testing are urgently needed. With a large-scale, stepped wedge, randomized controlled trial our study can improve understanding of crowdsourcing's long-term effectiveness in public health campaigns, expand HIV testing coverage among a key population, and inform intervention design in related public health fields. ClinicalTrials.gov, NCT02796963 . Registered on 23 May 2016.-Crowdsourcing to promote HIV testing among MSM in China: study protocol for a stepped wedge randomized controlled trial.",0
"Sensitivity and specificity are two customary performance measures associated with medical diagnostic tests. Typically, they are modeled independently as a function of risk factors using logistic regression, which provides estimated functions for these probabilities. Change in these probabilities across levels of risk factors is of primary interest and the indirect relationship is often displayed using a receiver operating characteristic curve. We refer to this as analysis of 'first-order' behavior. Here, we consider what we refer to as 'second-order' behavior where we examine the stochastic dependence between the (random) estimates of sensitivity and specificity. To do so, we argue that a model for the four cell probabilities that determine the joint distribution of screening test result and outcome result is needed. Such a modeling induces sensitivity and specificity as functions of these cell probabilities. In turn, this raises the issue of a coherent specification for these cell probabilities, given risk factors, i.e. a specification that ensures that all probabilities calculated under it fall between 0 and 1. This leads to the question of how to provide models that are coherent and mechanistically appropriate as well as computationally feasible to fit, particularly with large data sets. The goal of this article is to illuminate these issues both algebraically and through analysis of a real data set.-Joint modeling of sensitivity and specificity.",0
"Heart failure research suggests that multiple biomarkers could be combined with relevant clinical information to more accurately quantify individual risk and guide patient-specific treatment strategies. Therefore, statistical methodology is required to determine multi-marker risk scores that yield improved prognostic performance. Development of a prognostic score that combines biomarkers with clinical variables requires specification of an appropriate statistical model and is most frequently achieved using standard regression methods such as Cox regression. We demonstrate that care is needed in model specification and that maximal use of marker information requires consideration of potential non-linear effects and interactions. The derived multi-marker score can be evaluated using time-dependent receiver operating characteristic methods, or risk reclassification methods adapted for survival outcomes. We compare the performance of alternative model accuracy methods using simulations, both to evaluate power and to quantify the potential loss in accuracy associated with use of a sub-optimal regression model to develop the multi-marker score. We illustrate development and evaluation strategies using data from the Penn Heart Failure Study. Based on our results, we recommend that analysts carefully examine the functional form for component markers and consider plausible forms for effect modification to maximize the prognostic potential of a model-derived multi-marker score.-Development and evaluation of multi-marker risk scores for clinical prognosis.",0
"Vaccination and naturally acquired immunity against microbial pathogens may have complex interactions that influence disease outcomes. To date, only vaccine-specific immune responses have routinely been investigated in malaria vaccine trials conducted in endemic areas. We hypothesized that RTS,S/A01E immunization affects acquisition of antibodies to Plasmodium falciparum antigens not included in the vaccine and that such responses have an impact on overall malaria protective immunity. We evaluated IgM and IgG responses to 38 P. falciparum proteins putatively involved in naturally acquired immunity to malaria in 195 young children participating in a case-control study nested within the African phase 3 clinical trial of RTS,S/AS01E (MAL055 NCT00866619) in two sites of different transmission intensity (Kintampo high and Manhi?a moderate/low). We measured antibody levels by quantitative suspension array technology and applied regression models, multimarker analysis, and machine learning techniques to analyze factors affecting their levels and correlates of protection. RTS,S/AS01E immunization decreased antibody responses to parasite antigens considered as markers of exposure (MSP142, AMA1) and levels correlated with risk of clinical malaria over 1-year follow-up. In addition, we show for the first time that RTS,S vaccination increased IgG levels to a specific group of pre-erythrocytic and blood-stage antigens (MSP5, MSP1 block 2, RH4.2, EBA140, and SSP2/TRAP) which levels correlated with protection against clinical malaria (odds ratio [95% confidence interval] 0.53 [0.3-0.93], p = 0.03, for MSP1; 0.52 [0.26-0.98], p = 0.05, for SSP2) in multivariable logistic regression analyses. Increased antibody responses to specific P. falciparum antigens in subjects immunized with this partially efficacious vaccine upon natural infection may contribute to overall protective immunity against malaria. Inclusion of such antigens in multivalent constructs could result in more efficacious second-generation multistage vaccines.-RTS,S/AS01E immunization increases antibody responses to vaccine-unrelated Plasmodium falciparum antigens associated with protection against clinical malaria in African children: a case-control study.",0
"We consider a situation where there is rich historical data available for the coefficients and their standard errors in a linear regression model describing the association between a continuous outcome variable Y and a set of predicting factors X, from a large study. We would like to use this summary information for improving inference in an expanded model of interest, Y given X,B. The additional variable B is a new biomarker, measured on a small number of subjects in a new dataset. We formulate the problem in an inferential framework where the historical information is translated in terms of nonlinear constraints on the parameter space and propose both frequentist and Bayes solutions to this problem. We show that a Bayesian transformation approach proposed by Gunn and Dunson is a simple and effective computational method to conduct approximate Bayesian inference for this constrained parameter problem. The simulation results comparing these methods indicate that historical information on E(Y|X) can improve the efficiency of estimation and enhance the predictive power in the regression model of interest E(Y|X,B). We illustrate our methodology by enhancing a published prediction model for bone lead levels in terms of blood lead and other covariates, with a new biomarker defined through a genetic risk score.-Improving estimation and prediction in linear regression incorporating external information from an established reduced model.",0
"Comment: The Essential Role of Pair Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation",1
"In planning large longitudinal field trials, one is often faced with a choice between a cohort design and a cross-sectional design, with attendant issues of precision, sample size, and bias. To provide a practical method for assessing these trade-offs quantitatively, we present a unifying statistical model that embraces both designs as special cases. The model takes account of continuous and discrete endpoints, site differences, and random cluster and subject effects of both a time-invariant and a time-varying nature. We provide a comprehensive design equation, relating sample size to precision for cohort and cross-sectional designs, and show that the follow-up cost and selection bias attending a cohort design may outweigh any theoretical advantage in precision. We provide formulae for the minimum number of clusters and subjects. We relate this model to the recently published prevalence model for COMMIT, a multi-site trial of smoking cessation programmes. Finally, we tabulate parameter estimates for some physiological endpoints from recent community-based heart-disease prevention trials, work an example, and discuss the need for compiling such estimates as a basis for informed design of future field trials.-Cohort versus cross-sectional design in large field trials: precision, sample size, and a unifying model.",1
"Gene-gene (G?G) interactions have been shown to be critical for the fundamental mechanisms and development of complex diseases beyond main genetic effects. The commonly adopted marginal analysis is limited by considering only a small number of G factors at a time. With the ""main effects, interactions"" hierarchical constraint, many of the existing joint analysis methods suffer from prohibitively high computational cost. In this study, we propose a new method for identifying important G?G interactions under joint modeling. The proposed method adopts tensor regression to accommodate high data dimensionality and the penalization technique for selection. It naturally accommodates the strong hierarchical structure without imposing additional constraints, making optimization much simpler and faster than in the existing studies. It outperforms multiple alternatives in simulation. The analysis of The Cancer Genome Atlas (TCGA) data on lung cancer and melanoma demonstrates that it can identify markers with important implications and better prediction performance.-Identifying gene-gene interactions using penalized tensor regression.",0
"Cluster randomized trials are designed to evaluate interventions at the cluster or group level. When clusters are randomized but some clusters report no or non-analyzable data, intent-to-treat analysis, the gold standard for the analysis of randomized controlled trials, can be compromised. This article presents a very flexible statistical methodology for cluster randomized trials whose outcome is a cluster-level proportion (e.g. proportion from a cluster reporting an event) in the setting where clusters report non-analyzable data (which in general could be due to nonadherence, dropout, missingness, etc.). The approach is motivated by a previously published stratified randomized controlled trial called, ""The Randomized Recruitment Intervention Trial (RECRUIT),"" designed to examine the effectiveness of a trust-based continuous quality improvement intervention on increasing minority recruitment into clinical trials (ClinicalTrials.gov Identifier: NCT01911208). The novel approach exploits the use of generalized estimating equations for cluster-level reports, such that all clusters randomized at baseline are able to be analyzed, and intervention effects are presented as risk ratios. Simulation studies under different outcome missingness scenarios and a variety of intra-cluster correlations are conducted. A comparative analysis of the method with imputation and per protocol approaches for RECRUIT is presented. Simulation results show the novel approach produces unbiased and efficient estimates of the intervention effect that maintain the nominal type I error rate. Application to RECRUIT shows similar effect sizes when compared to the imputation and per protocol approach. The article demonstrates that an innovative bivariate generalized estimating equations framework allows one to implement an intent-to-treat analysis to obtain risk ratios or odds ratios, for a variety of cluster randomized designs.-Intent-to-treat analysis of cluster randomized trials when clusters report unidentifiable outcome proportions",1
"In oncology, dose escalation is often carried out to search for the maximum tolerated dose (MTD) in phase I clinical trials. We propose a Bayesian hybrid dose-finding method that inherits the robustness of model-free methods and the efficiency of model-based methods. In the Bayesian hypothesis testing framework, we compute the Bayes factor and adaptively assign a dose to each cohort of patients based on the adequacy of the dose-toxicity information that has been collected thus far. If the data observed at the current treatment dose are adequately informative about the toxicity probability of this dose (e.g. whether this dose is below or above the MTD), we make the decision of dose assignment (e.g. either to escalate or to de-escalate the dose) directly without assuming a parametric dose-toxicity curve. If the observed data at the current dose are not sufficient to deliver such a definitive decision, we resort to a parametric dose-toxicity curve, such as that of the continual reassessment method (CRM), in order to borrow strength across all the doses under study to guide dose assignment. We examine the properties of the hybrid design through extensive simulation studies, and also compare the new method with the CRM and the '3 + 3' design. The simulation results show that our design is more robust than parametric model-based methods and more efficient than nonparametric model-free methods.-Bayesian hybrid dose-finding design in phase I oncology clinical trials.",0
"Cluster randomized trials (CRTs) are studies in which clusters of subjects are randomized to different trial arms. Due to the nature of outcomes within the same cluster to be correlated, generalized estimating equations (GEE) are growing as a popular choice for the analysis of data arising from CRTs. In the past, research has shown that analyses using GEE could result in liberal inference due to the use of the empirical sandwich covariance matrix estimator, which can yield negatively biased standard error estimates when the number of clusters is not large. Many techniques have been presented to correct this negative bias; however, use of these corrections can still result in biased standard error estimates and thus test sizes that are not consistently at their nominal level. Therefore, there is a need for an improved correction such that nominal type I error rates will consistently result. In this manuscript, we study the use of recently developed corrections for empirical standard error estimation and the use of a combination of two popular corrections. In an extensive simulation study, we found that nominal type I error rates can be consistently attained when using an average of two popular corrections developed by Mancl and DeRouen (, Biometrics 57, 126-134) and Kauermann and Carroll (, Journal of the American Statistical Association 96, 1387-1396). Therefore, use of this new correction was found to notably outperform the use of previously recommended corrections.-Improved standard error estimator for maintaining the validity of inference in cluster randomized trials with a small number of clusters.",1
"Clinical trial investigators and sponsors invest vast amounts of resources and energy into conducting trials and often face daily challenges with data management, project management, and data quality control. Rather than waiting months for study progress reports, investigators need the ability to use real-time data for the coordination and management of study activities across all study team members including site investigators, oversight committees, data and safety monitoring boards, and medical safety monitors. Web-based data management systems are beginning to meet this need but what distinguishes one system from the other are user needs/requirements and cost. To illustrate the development and implementation of a web-based data and project management system for a multicenter clinical trial designed to test the superiority of repeated transcranial magnetic stimulation versus sham for the treatment of patients with major depression. The authors discuss the reasons for not using a commercially available system for this study and describe the approach to developing their own web-based system for the OPT-TMS study. Timelines, effort, system architecture, and lessons learned are shared with the hope that this information will direct clinical trial researchers and software developers towards more efficient, user-friendly systems. The developers use a combination of generic and custom application code to allow for the flexibility to adapt the system to the needs of the study. Features of the system include: central participant registration and randomization; secure data entry at the site; participant progress/study calendar; safety data reporting; device accounting; monitor verification; and user-configurable generic reports and built-in customized reports. Hard coding was more time-efficient to address project-specific issues compared with the effort of creating a generic code application. As a consequence of this strategy, the required maintenance of the system is increased and the value of using this system for other trials is reduced. Web-based central computerized systems offer time-saving, secure options for managing clinical trial data. The choice of a commercially available system or an internally developed system is determined by the requirements of the study and users. Pros and cons to both approaches were discussed. If the intention is to use the system for various trials (single and multi-center, phases I-III) across various therapeutic areas, then the overall design should be a generic structure that simplifies the general application with minimal loss of functionality.-A web-based clinical trial management system for a sham-controlled multicenter clinical trial in depression.",0
"A random-effects probit model is developed for the case in which the outcome of interest is a series of correlated binary responses. These responses can be obtained as the product of a longitudinal response process where an individual is repeatedly classified on a binary outcome variable (e.g., sick or well on occasion t), or in ""multilevel"" or ""clustered"" problems in which individuals within groups (e.g., firms, classes, families, or clinics) are considered to share characteristics that produce similar responses. Both examples produce potentially correlated binary responses and modeling these person- or cluster-specific effects is required. The general model permits analysis at both the level of the individual and cluster and at the level at which experimental manipulations are applied (e.g., treatment group). The model provides maximum likelihood estimates for time-varying and time-invariant covariates in the longitudinal case and covariates which vary at the level of the individual and at the cluster level for multilevel problems. A similar number of individuals within clusters or number of measurement occasions within individuals is not required. Empirical Bayesian estimates of person-specific trends or cluster-specific effects are provided. Models are illustrated with data from mental health research.-Application of random-effects probit regression models.",1
"Pathways is a multi-centre school-based trial sponsored by the National Heart, Lung, and Blood Institute testing the efficacy of an obesity prevention intervention in American Indian children. During the study's protocol development, we prepared an analysis plan that accounted for missing data. In this paper, we present a case study of the process we used to decide upon the final analysis plan. The primary endpoint of the Pathways study is a comparison of per cent body fat between treatment and usual care groups at the end of a three-year intervention. Other studies on children and Native Americans have had moderate to large amounts of missing data. As a result we were concerned that missing data in Pathways would affect the type I error rate and power of the test of our primary endpoint. We present results from our evaluation of three alternative procedures in this paper. The first is a multiple imputation procedure in which we replace missing values with resampled values from the observed data. The second is based on the Wilcoxon rank sum test; missing data in the intervention group receive the worst ranks. In the third, we use a multiple imputation procedure and replace missing values with predicted values from a regression equation with the coefficients estimated from observed follow-up data and baseline values. We found that the multiple imputation procedure that replaces missing values with predicted values had the best properties of the procedures we considered. The results from our simulation study showed that, for missing data patterns that are relevant to the Pathways study, this procedure has high power and maintains the type I error rate. Published in 2001 by John Wiley &amp; Sons, Ltd.-Imputation strategies for missing data in a school-based multi-centre study: the Pathways study.",1
"Bernoulli (or binomial) regression using a generalized linear model with a log link function, where the exponentiated regression parameters have interpretation as relative risks, is often more appropriate than logistic regression for prospective studies with common outcomes. In particular, many researchers regard relative risks to be more intuitively interpretable than odds ratios. However, for the log link, when the outcome is very prevalent, the likelihood may not have a unique maximum. To circumvent this problem, a 'COPY method' has been proposed, which is equivalent to creating for each subject an additional observation with the same covariates except the response variable has the outcome values interchanged (1's changed to 0's and 0's changed to 1's). The original response is given weight close to 1, while the new observation is given a positive weight close to 0; this approach always leads to convergence of the maximum likelihood algorithm, except for problems with convergence due to multicollinearity among covariates. Even though this method produces a unique maximum, when the outcome is very prevalent, and/or the sample size is relatively small, the COPY method can yield biased estimates. Here, we propose using the jackknife as a bias-reduction approach for the COPY method. The proposed method is motivated by a study of patients undergoing colorectal cancer surgery.-Using the jackknife for estimation in log link Bernoulli regression models.",0
This article proposes an approach to modelling partially cross-classified multilevel data where some of the level-1 observations are nested in one random factor and some are cross-classified by two random factors. Comparisons between a proposed approach to two other commonly used approaches which treat the partially cross-classified data as either fully nested or fully cross-classified are completed with a simulation study. Results show that the proposed approach demonstrates desirable performance in terms of parameter estimates and statistical inferences. Both the fully nested model and the fully cross-classified model suffer from biased estimates of some variance components and statistical inferences of some fixed effects. Results also indicate that the proposed model is robust against cluster size imbalance.-Modelling partially cross-classified multilevel data,2
"Random effects two-part models have been applied to longitudinal studies for zero-inflated (or semi-continuous) data, characterized by a large portion of zero values and continuous non-zero (positive) values. Examples include monthly medical costs, daily alcohol drinks, relative abundance of microbiome, etc. With the advance of information technology for data collection and storage, the number of variables available to researchers can be rather large in such studies. To avoid curse of dimensionality and facilitate decision making, it is critically important to select covariates that are truly related to the outcome. However, owing to its intricate nature, there is not yet a satisfactory variable selection method available for such sophisticated models. In this paper, we seek a feasible way of conducting variable selection for random effects two-part models on the basis of the recently proposed ""minimum information criterion"" (MIC) method. We demonstrate that the MIC formulation leads to a reasonable formulation of sparse estimation, which can be conveniently solved with SAS Proc NLMIXED. The performance of our approach is evaluated through simulation, and an application to a longitudinal alcohol dependence study is provided.-Variable selection for random effects two-part models.",0
"This is a short tutorial on two key questions that pertain to cluster randomized trials (CRTs): 1) Should I perform a CRT? and 2) If so, how do I derive the sample size? In summary, a CRT is the best option when you ""must"" (e.g., the intervention can only be administered to a group) or you ""should"" (e.g., because of issues such as feasibility and contamination). CRTs are less statistically efficient and usually more logistically complex than individually randomized trials, and so reviewing the rationale for their use is critical. The most straightforward approach to the sample size calculation is to first perform the calculation as if the design were randomized at the level of the patient and then to inflate this sample size by multiplying by the ""design effect"", which quantifies the degree to which responses within a cluster are similar to one another. Although trials with large numbers of small clusters are more statistically efficient than those with a few large clusters, trials with large clusters can be more feasible. Also, if results are to be compared across individual sites, then sufficient sample size will be required to attain adequate precision within each site. Sample size calculations should include sensitivity analyses, as inputs from the literature can lack precision. Collaborating with a statistician is essential. To illustrate these points, we describe an ongoing CRT testing a mobile-based app to systematically engage families of intensive care unit patients and help intensive care unit clinicians deliver needs-targeted palliative care.-Two Questions About the Design of Cluster Randomized Trials: A Tutorial",1
"Sarcopenia is a geriatric syndrome characterized by significant loss of muscle mass. Based on a commonly used definition of the condition that involves three measurements, different subclinical and clinical states of sarcopenia are formed. These states constitute a partially ordered set (poset). This article focuses on the analysis of longitudinal poset in the context of sarcopenia. We propose an extension of the generalized linear mixed model and a recoding scheme for poset analysis such that two submodels-one for ordered categories and one for nominal categories-that include common random effects can be jointly estimated. The new poset model postulates random effects conceptualized as latent variables that represent an underlying construct of interest, that is, susceptibility to sarcopenia over time. We demonstrate how information can be gleaned from nominal sarcopenic states for strengthening statistical inference on a person's susceptibility to sarcopenia.-Longitudinal partially ordered data analysis for preclinical sarcopenia.",0
The Ottawa Statement on the Ethical Design and Conduct of Cluster Randomized Trials.,1
"Atrial fibrillation increases the risk of stroke, which is a leading cause of death and disability worldwide. The use of oral anticoagulation in patients with atrial fibrillation at moderate or high risk of stroke, estimated by established criteria, improves outcomes. However, to ensure that the benefits exceed the risks of bleeding, appropriate patient selection is essential. Vitamin K antagonism has been the mainstay of treatment; however, newer drugs with novel mechanisms are also available. These novel oral anticoagulants (direct thrombin inhibitors and factor Xa inhibitors) obviate many of warfarin's shortcomings, and they have demonstrated safety and efficacy in large randomized trials of patients with non-valvular atrial fibrillation. However, the management of patients taking warfarin or novel agents remains a clinical challenge. There are several important considerations when selecting anticoagulant therapy for patients with atrial fibrillation. This review will discuss the rationale for anticoagulation in patients with atrial fibrillation; risk stratification for treatment; available agents; the appropriate implementation of these agents; and additional, specific clinical considerations for treatment.-Anticoagulation in atrial fibrillation.",0
"Randomized field trials provide unique opportunities to examine the effectiveness of an intervention in real world settings and to test and extend both theory of etiology and theory of intervention. These trials are designed not only to test for overall intervention impact but also to examine how impact varies as a function of individual level characteristics, context, and across time. Examination of such variation in impact requires analytical methods that take into account the trial's multiple nested structure and the evolving changes in outcomes over time. The models that we describe here merge multilevel modeling with growth modeling, allowing for variation in impact to be represented through discrete mixtures--growth mixture models--and nonparametric smooth functions--generalized additive mixed models. These methods are part of an emerging class of multilevel growth mixture models, and we illustrate these with models that examine overall impact and variation in impact. In this paper, we define intent-to-treat analyses in group-randomized multilevel field trials and discuss appropriate ways to identify, examine, and test for variation in impact without inflating the Type I error rate. We describe how to make causal inferences more robust to misspecification of covariates in such analyses and how to summarize and present these interactive intervention effects clearly. Practical strategies for reducing model complexity, checking model fit, and handling missing data are discussed using six randomized field trials to show how these methods may be used across trials randomized at different levels.-Methods for testing theory and evaluating impact in randomized field trials: intent-to-treat analyses for integrating the perspectives of person, place, and time.",1
Preventing bias in cluster randomised trials.,1
"Power and sample size calculation formulas for stepped-wedge trials with two levels (subjects within clusters) are available. However, stepped-wedge trials with more than two levels are possible. An example is the CHANGE trial which randomizes nursing homes (level 4) consisting of nursing home wards (level 3) in which nurses (level 2) are observed with respect to their hand hygiene compliance during hand hygiene opportunities (level 1) in the care of patients. We provide power and sample size methods for such trials and illustrate these in the setting of the CHANGE trial. We extend the original sample size methodology derived for stepped-wedge trials based on a random intercepts model, to accommodate more than two levels of clustering. We derive expressions that can be used to determine power and sample size for p levels of clustering in terms of the variances at each level or, alternatively, in terms of intracluster correlation coefficients. We consider different scenarios, depending on whether the same units in a particular level are repeatedly measured as a cohort sample or whether different units are measured cross-sectionally. A simple variance inflation factor is obtained that can be used to calculate power and sample size for continuous and by approximation for binary and rate outcomes. It is the product of (1) variance inflation due to the multilevel structure and (2) variance inflation due to the stepped-wedge manner of assigning interventions over time. Standard and non-standard designs (i.e. so-called ""hybrid designs"" and designs with more, less, or no data collection when the clusters are all in the control or are all in the intervention condition) are covered. The formulas derived enable power and sample size calculations for multilevel stepped-wedge trials. For the two-, three-, and four-level case of the standard stepped wedge, we provide programs to facilitate these calculations.-Sample size calculation for stepped-wedge cluster-randomized trials with more than two levels of clustering",3
"Cluster randomized crossover trials: Aspects of power, variance, and bias in the stepped wedge design",3
"To estimate the prevalence of time-to-event (TTE) outcomes in cluster randomized trials (CRTs) and to examine their statistical management. We searched PubMed to identify primary reports of CRTs published in six major general medical journals (2013-2018). Nature of outcomes and, for TTE outcomes, statistical methods for sample size, analysis, and measures of intracluster correlation were extracted. A TTE analysis was used in 17% of the CRTs (32/184) either as a primary or secondary outcome analysis, or in a sensitivity analysis. Among the five CRTs with a TTE primary outcome, two accounted for both intracluster correlation and the TTE nature of the outcome in sample size calculation; one reported a measure of intracluster correlation in the analysis. Among the 32 CRTs with a least one TTE analysis, 44% (14/32) accounted for clustering in all TTE analyses. We identified 12 additional CRTs in which there was at least one outcome not analyzed as TTE for which a TTE analysis might have been preferred. TTE outcomes are not uncommon in CRTs but appropriate statistical methods are infrequently used. Our results suggest that further methodological development and explicit recommendations for TTE outcomes in CRTs are needed.-Methodological review showed that time-to-event outcomes are often inadequately handled in cluster randomized trials.",1
"Examination of families represents an important priority in health research. In this paper we report on individual and family-level factors associated with enrollment in a cancer prevention research project. We approached families affected by melanoma for possible participation in a randomized controlled trial of a web-based communication and support intervention. We recruited three family members per family for assessment - the melanoma case, a first-degree relative (FDR), and a relative who is a parent of a child age 18 or younger. Recruitment involved three steps: requesting the physician's consent to approach the melanoma case, approaching the case to request their participation and family contact information, and they approaching the FDRs and parents. Of the 1380 families approached, 313 were enrolled, 263 were excluded because we could not find or contact a family member (FDR or parent), 331 did not have eligible family members, and 473 refused. The most frequently noted reason for refusal was being too busy or having no time. The primary predictors of participation for cases (OR=1.6; CI=1.01-2.51) and FDRs (OR=2.15; CI=1.11-4.13) included higher educational attainment. FDRs were more likely to enroll if they were female (OR=1.77; CI=1.1-.85) and parents were more likely to enroll if the case had been diagnosed more recently (OR=3.3; CI=1.9-5.93), if the parent was partnered (OR=4.37; CI=1.86-10.26), and if the parent lived in the same city as the case (OR=2.88; CI=1.08-7.68). The results can provide information on potential directions for future family recruitment.-Predictors of recruited melanoma families into a behavioral intervention project.",0
"To assess the quality of methods and reporting of recently published cluster randomized trials (CRTs) in oral health. We searched PubMed for CRTs that included at least one oral health-related outcome and were published from 2005 to 2009 inclusive. We developed a list of criteria for assessing trial quality and reporting. This was influenced largely by the extended CONSORT statement for CRTs but also included criteria suggested by other authors. We examined the extent to which trials were consistent with these criteria. Twenty-three trials were included in the review. In 15 (65%) trials, clustering had been accounted for in sample size calculations, and in 18 (78%) authors had accounted for clustering in analysis. Intraclass correlation coefficients (ICCs) were reported for eight (35%) trials; the outcome assessor was reported as having been blinded to allocation in 12 (52%) trials; 17 (74%) described eligibility criteria at individual level, but only nine (39%) described such criteria at cluster level. Sixteen of 20 trials (80%), in which individuals were recruited, reported that individual informed consent was obtained. These results suggest that the quality of recent CRTs in oral health is relatively high and appears to compare favourably with other fields. However, there remains room for improvement. Authors of future trials should endeavour to ensure sample size calculations and analyses properly account for clustering (and are reported as such), consider the potential for recruitment/identification bias at the design stage, describe the steps taken to avoid this in the final report and report observed ICCs and cluster-level eligibility criteria.-Quality of cluster randomized controlled trials in oral health: a systematic review of reports published between 2005 and 2009.",1
"The goal of this study was to determine the validity and utility of a pharmacy-based time-to-refill measure of antiretroviral therapy adherence. We performed an observational cohort study of 110 HIV-infected subjects on a stable, highly active antiretroviral regimen for at least 3 months at a Veterans' Affairs Medical Center in Philadelphia, Pennsylvania. The viral load decreased by 0.12 log c/mL (95% confidence interval [CI] 0.01-0.23 log c/mL) for each 10% increase in pharmacy-based time-to-refill defined adherence as compared with 0.05 log c/mL (95% CI -0.14-0.25 log c/mL) for the self-reported adherence measure. Thus, only the refill-defined measure was statistically significantly associated with viral load change. When adherence was classified as good (&gt; or = 85%) versus poor (&lt;85%), both measures demonstrated a significant difference in outcome between groups. Yet, in individuals self-reporting 100% adherence, those classified as good adherers using the pharmacy-based measure had greater viral load reductions than poor adherers (2.4 log c/mL [interquartile range 1.4-3.4] vs. 1.5 log c/mL [interquartile range 0.8-2.4, P=.03). The pharmacy-based technique is a valid measure of antiretroviral therapy adherence. Because it provides clinically relevant information in subjects who self-report 100% adherence, it should be incorporated into clinical practice and adherence research.-A time-to-prescription-refill measure of antiretroviral adherence predicted changes in viral load in HIV.",0
"Prescriber preference has been used as an instrumental variable (IV) in a prior study of nonselective nonsteroidal anti-inflammatory drugs (NSAIDs) vs. selective cyclooxygenase-2 (COX-2) inhibitors, with preference expressed as the drug constituting the immediately preceding prescription by the same prescriber (instantaneous preference). We sought to compare the correlations between different IV measures with exposure. In an ambulatory electronic medical record database of university-based physicians, we compared correlations with exposure among three measures of prescriber preference: instantaneous preference, and the proportion of that prescriber's prescriptions in the past 3 and 6 months that were for an NSAID. We identified 37,934 initial NSAID/COX-2 prescriptions. The correlation with exposure was 0.283 (95% confidence interval 0.274-0.292) for instantaneous preference, 0.197 (0.187-0.206) for 3-month preference, and 0.170 (0.160-0.180) for 6-month preference. Instantaneous NSAID/COX-2 prescribing preference was most strongly correlated, and therefore the strongest IV. Future research should focus on the robustness of IV methods to violations of underlying assumptions, extension of IV methods to more than two groups, ratio measures of association, second and subsequent prescriptions per person, and time-varying exposures.-Instantaneous preference was a stronger instrumental variable than 3- and 6-month prescribing preference for NSAIDs.",0
"This study was designed to examine the societal cost-effectiveness and the impact on government payers of earlier initiation of antiretroviral therapy for uninsured HIV-infected adults. A state-transition simulation model of HIV disease was used. Data were derived from the Multicenter AIDS Cohort Study, published randomized trials, and medical care cost estimates for all government payers and for Massachusetts, NewYork, and Florida. Quality-adjusted life expectancy increased from 7.64 years with therapy initiated at 200 CD4 cells/microL to 8.21 years with therapy initiated at 500 CD4 cells/microL. Initiating therapy at 500 CD4/microL was a more efficient use of resources than initiating therapy at 200 CD4/microL and had an incremental cost-effectiveness ratio of $17,300 per quality-adjusted life-year gained, compared with no therapy. Costs to state payers in the first 5 years ranged from $5,500 to $24,900 because of differences among the states in the availability of federal funds forAIDS drug assistance programs. Antiretroviral therapy initiated at 500 CD4 cells/microL is cost-effective from a societal: perspective compared with therapy initiated later. States should consider Medicaid waivers to expand access to early therapy.-Cost-effectiveness of earlier initiation of antiretroviral therapy for uninsured HIV-infected adults.",0
Timeline cluster: a graphical tool to identify risk of bias in cluster randomised trials,1
"When non-compliance occurs in a clinical trial, it may be of interest to supplement the intent-to-treat analysis with an analysis of the efficacy (or biological effect) of therapy. Sommer and Zeger (1991) developed a method for estimating efficacy applicable to the case of a binary response variable and all-or-none compliance that assumes independent subject responses. We extend this approach to accommodate within-cluster correlations as may be expected in a cluster-randomized design. The method is illustrated using data from a controlled village-randomized clinical trial conducted in Indonesia to investigate the effect of vitamin A supplementation on mortality in children. We find that within-cluster correlations for these data are very small and that taking into account the clustering does not substantially affect inferences in this case. Additional calculations show that small within-cluster correlations (though larger than those found in the vitamin A data) may have a large impact on efficacy inferences. We also present the results of a simulation study that demonstrates the validness of the proposed approach for finite sample sizes.-Estimating efficacy in clinical trials with clustered binary responses.",1
"Meta-analyses can be powerful tools to combine the results of randomized clinical trials and observational studies to make consensus inferences about a medical issue. It will be demonstrated that a common practice of testing for homogeneity of effect size, and acting upon the inference to decide between fixed vs random effects, can lead to potentially misleading results. A by-product of this paper is a new ratio estimator approach to random effects meta-analysis of a large set of studies with low event rates. As a case study, we shall use the recent Rosiglitazone example, where diagnostic testing failed to reject homogeneity, leading the investigators to use fixed effects. The results for the fixed and random effects analyses are discordant. In the fixed (random) effects analysis, the p-values for myocardial infarction were 0.03 (0.11) while those for cardiac death were 0.06 (0.0017). Had the fixed effects analysis controlled the study error for multiple testing via a Bonferonni correction, the joint 95+ per cent confidence rectangle for the two outcomes would have included odds ratios of (1.0, 1.0). For the Rosiglitazone example, random effects analysis, where all studies receive the same weight, is the superior choice over fixed effects, where two large studies dominate.-Fixed vs random effects meta-analysis in rare event studies: the rosiglitazone link with myocardial infarction and cardiac death.",0
"There is considerable interest in community interventions for health promotion, where the community is the experimental unit. Because such interventions are expensive, the number of experimental units (communities) is usually small. Because of the small number of communities involved, investigators often match treatment and control communities on demographic variables before randomization to minimize the possibility of a bad split. Unfortunately, matching has been shown to decrease the power of the design when the number of pairs is small, unless the matching variable is very highly correlated with the outcome variable (in this case, with change in the health behaviour). We used computer simulation to examine the performance of an approach in which we matched communities but performed an unmatched analysis. If the appropriate matching variables are unknown, and there are fewer than ten pairs, an unmatched design and analysis has the most power. If, however, one prefers a matched design, then for N &lt; 10, power can be increased by performing an unmatched analysis of the matched data. We also discuss a variant of this procedure, in which an unmatched analysis is performed only if the matching 'did not work'.-Breaking the matches in a paired t-test for community interventions when the number of pairs is small.",1
"To assess markers of selection bias risk in a sample of recently published cluster randomized controlled trials compared with individually randomized trials. We used OVID Medline and the online archives of the Journal of the American Medical Association to search for cluster randomized trials published between January 2015 and June 2017 from four high-impact journals and compared them to a matched sample of individually randomized trials. We identified 23 cluster trials: 57% (n?=?13) described a robust allocation method and 17% (n?=?4) recruited all participants before randomization. Four (17%), eight (35%), and 11 (48%) were classified as at low, medium, and high bias risk, respectively. Meta-analysis showed significant age imbalance (-0.05, 95% CI = -0.057 to -0.043, I2?=?93.2%) in cluster trials, while the matched individually randomized trials showed no imbalance (0.005, 95% CI = -0.026 to 0.035, I2?=?0%). Cluster trials finding a statistically significant outcome in their primary measure showed a larger age imbalance (0.082, 95% CI = -0.091 to -0.073, I2?=?87%) than trials finding a nonstatistically significant outcome (0.022, 95% CI = 0.008 to 0.035, I2?=?83%). There is strong evidence in this sample of an effect of selection bias seen in an imbalance in baseline participant age, something not seen in a comparable sample of individually randomized trials.-A review of cluster randomized trials found statistical evidence of selection bias",1
Sample size in cluster randomisation.,1
"The 2014 US Surgeon General's report noted research gaps necessary to determine a causal relationship between active cigarette smoking and invasive breast cancer risk, including the role of alcohol consumption, timing of exposure, modification by menopausal status and heterogeneity by oestrogen receptor (ER) status. To address these issues, we pooled data from 14 cohort studies contributing 934 681 participants (36 060 invasive breast cancer cases). Cox proportional hazard regression models were used to calculate multivariable-adjusted hazard ratios (HRs) and 95% confidence intervals (CIs). Smoking duration before first birth was positively associated with risk ( P -value for trend = 2 ? 10 -7 ) with the highest HR for initiation &gt;10 years before first birth (HR = 1.18, CI 1.12-1.24). Effect modification by current alcohol consumption was evident for the association with smoking duration before first birth ( P -value=2?10 -4 ); compared with never-smoking non-drinkers, initiation &gt;10 years before first birth was associated with risk in every category of alcohol intake, including non-drinkers (HR = 1.15, CI 1.04-1.28) and those who consumed at least three drinks per day (1.85, 1.55-2.21). Associations with smoking before first birth were limited to risk of ER+ breast cancer ( P -value for homogeneity=3?10 -3 ). Other smoking timing and duration characteristics were associated with risk even after controlling for alcohol, but were not associated with risk in non-drinkers. Effect modification by menopause was not evident. Smoking, particularly if initiated before first birth, was modestly associated with ER+ breast cancer risk that was not confounded by amount of adult alcohol intake. Possible links with breast cancer provide additional motivation for young women to not initiate smoking.-Pooled analysis of active cigarette smoking and invasive breast cancer risk in 14 cohort studies.",0
"Random effects are often used in generalized linear models to explain the serial dependence for longitudinal categorical data. Marginalized random effects models (MREMs) for the analysis of longitudinal binary data have been proposed to permit likelihood-based estimation of marginal regression parameters. In this paper, we propose a model to extend the MREM to accommodate longitudinal ordinal data. Maximum marginal likelihood estimation is proposed utilizing quasi-Newton algorithms with Monte Carlo integration of the random effects. Our approach is applied to analyze the quality of life data from a recent colorectal cancer clinical trial. Dropout occurs at a high rate and is often due to tumor progression or death. To deal with events due to progression/death, we used a mixture model for the joint distribution of longitudinal measures and progression/death times and use principal stratification to draw causal inferences about survivors.-Marginalized models for longitudinal ordinal data with application to quality of life studies.",0
Efficacy and effectiveness trials in health promotion and disease prevention:  Design and analysis of group-randomized trials,1
"Conventionally, evaluation of a new drug, A, is done in three phases. Phase I is based on toxicity to determine a ""maximum tolerable dose"" (MTD) of A, phase II is conducted to decide whether A at the MTD is promising in terms of response probability, and if so a large randomized phase III trial is conducted to compare A to a control treatment, C , usually based on survival time or progression free survival time. It is widely recognized that this paradigm has many flaws. A recent approach combines the first two phases by conducting a phase I-II trial, which chooses an optimal dose based on both efficacy and toxicity, and evaluation of A at the selected optimal phase I-II dose then is done in a phase III trial. This paper proposes a new design paradigm, motivated by the possibility that the optimal phase I-II dose may not maximize mean survival time with A. We propose a hybridized design, which we call phase I-II/III, that combines phase I-II and phase III by allowing the chosen optimal phase I-II dose of A to be re-optimized based on survival time data from phase I-II patients and the first portion of phase III. The phase I-II/III design uses adaptive randomization in phase I-II, and relies on a mixture model for the survival time distribution as a function of efficacy, toxicity, and dose. A simulation study is presented to evaluate the phase I-II/III design and compare it to the usual approach that does not re-optimize the dose of A in phase III.-A hybrid phase I-II/III clinical trial design allowing dose re-optimization in phase III.",0
"The OsteoArthritis and Therapy for Sleep (OATS) study is a population-based randomized controlled trial of cognitive behavioral therapy for insomnia (CBTI) with four innovative methodological aims. These are to: (1) Enroll representative participants across Washington state, including those from medically underserved communities; (2) Enroll persons with persistent insomnia and chronic osteoarthritis (OA) pain; (3) Test a scalable CBT-I intervention; and (4) Evaluate patient-reported outcomes (insomnia, pain severity, fatigue, depression) and cost-effectiveness over one year. This paper describes progress towards achieving these aims. The target population was persons age 60+ who had received OA care within the Kaiser Permanente Washington (KPW) health care system. We employed a two-phase screening via mail survey and telephone follow-up, with a 3-week interval between screens to exclude persons with spontaneous improvement in sleep or pain symptoms. Participants were randomized to a 6-session telephone-delivered CBT-I intervention or a 6-session telephone education only control condition (EOC). Blinded outcome assessments (completed online or on mailed paper forms) included primary and secondary sleep and pain outcome measures and quality of life measures. We obtained healthcare utilization from administrative claims data. Intent to treat analyses, including all participants randomized when they scheduled the first telephone session, will be conducted to compare CBT-I and EOC outcomes. The trial will be the largest experimental evaluation of telephone CBT-I to date, and the first to evaluate its cost-effectiveness. Trial registration: ClinicalTrials.gov identifier: NCT02946957.-Telephone interventions for co-morbid insomnia and osteoarthritis pain: The OsteoArthritis and Therapy for Sleep (OATS) randomized trial design.",0
"Three papers in this issue focus on the role of calibration in model fit statistics, including the net reclassification improvement (NRI) and integrated discrimination improvement (IDI). This commentary reviews the development of such reclassification statistics along with more recent advances in our understanding of these measures. We show how the two-category NRI and the IDI are affected by changes in the event rate in theory and in an applied example. We also describe the role of calibration and how it may be assessed. Finally, we discuss the relevance of the event rate NRI for clinical use. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-Clinical risk reclassification at 10?years.",0
"There is no clear classification rule to rapidly identify trauma patients who are severely hemorrhaging and may need substantial blood transfusions. Massive transfusion (MT), defined as the transfusion of at least 10 units of red blood cells within 24?h of hospital admission, has served as a conventional surrogate that has been used to develop early predictive algorithms and establish criteria for ordering an MT protocol from the blood bank. However, the conventional MT rule is a poor proxy, because it is likely to misclassify many severely hemorrhaging trauma patients as they could die before receiving the 10th red blood cells transfusion. In this article, we propose to use a latent class model to obtain a more accurate and complete metric in the presence of early death. Our new approach incorporates baseline patient information from the time of hospital admission, by combining respective models for survival time and usage of blood products transfused within the framework of latent class analysis. To account for statistical challenges, caused by induced dependent censoring inherent in 24-h sums of transfusions, we propose to estimate an improved standard via a pseudo-likelihood function using an expectation-maximization algorithm with the inverse weighting principle. We evaluated the performance of our new standard in simulation studies and compared with the conventional MT definition using actual patient data from the Prospective Observational Multicenter Major Trauma Transfusion study. Copyright ? 2015 John Wiley &amp; Sons, Ltd.-A joint latent class analysis for adjusting survival bias with application to a trauma transfusion study.",0
"The Youden Index is often used as a summary measure of the receiver operating characteristic curve. It measures the effectiveness of a diagnostic marker and permits the selection of an optimal threshold value or cutoff point for the biomarker of interest. Some markers, while basically continuous and positive, have a spike or positive mass of probability at the value zero. We provide a flexible modeling approach for estimating the Youden Index and its associated cutoff point for such spiked data and compare it with the standard empirical approach. We show how this modeling approach can be adjusted to take covariate information into account. This approach is applied to data on the Coronary Calcium Score, a marker for atherosclerosis.-Youden Index and the optimal threshold for markers with mass at zero.",0
"Some failure time data come from a population that consists of some subjects who are susceptible to and others who are nonsusceptible to the event of interest. The data typically have heavy censoring at the end of the follow-up period, and a standard survival analysis would not always be appropriate. In such situations where there is good scientific or empirical evidence of a nonsusceptible population, the mixture or cure model can be used (Farewell, 1982, Biometrics 38, 1041-1046). It assumes a binary distribution to model the incidence probability and a parametric failure time distribution to model the latency. Kuk and Chen (1992, Biometrika 79, 531-541) extended the model by using Cox's proportional hazards regression for the latency. We develop maximum likelihood techniques for the joint estimation of the incidence and latency regression parameters in this model using the nonparametric form of the likelihood and an EM algorithm. A zero-tail constraint is used to reduce the near nonidentifiability of the problem. The inverse of the observed information matrix is used to compute the standard errors. A simulation study shows that the methods are competitive to the parametric methods under ideal conditions and are generally better when censoring from loss to follow-up is heavy. The methods are applied to a data set of tonsil cancer patients treated with radiation therapy.-Estimation in a Cox proportional hazards cure model.",0
"The N-acetyltransferase 2 (NAT2) enzyme detoxifies aromatic amines, an important class of carcinogens in tobacco smoke. Slow acetylation phenotype individuals have reduced detoxification capacity compared with those with a rapid/intermediate phenotype. Analysis of the Spanish Bladder Cancer Study found an odds ratio (OR) for slow acetylators relative to rapid/intermediate acetylators of 0.9 in never-smokers and 1.6 in ever-smokers, a 1.8-fold enhancement in smokers. Evidence indicates that acetylation is an exposure-dependent process, and thus the magnitude of the interaction may also depend on exposure level. We extend a comprehensive three-parameter linear-exponential model for the excess odds ratio (EOR) for smoking to include effects of NAT2 status, and reanalyse smoking and NAT2 status for the bladder cancer data. We show that variations in smoking risk with NAT2 status result from interactions with smoking intensity (cigarettes per day) and not total pack-years of exposure. In addition, the relative increase in smoking risk in NAT2 slo acetylators increases with smoking intensity. Analyses reveal an enhanced effect for smoking intensity and bladder cancer in NAT2 slow acetylators which increases with intensity.-Evidence for an intensity-dependent interaction of NAT2 acetylation genotype and cigarette smoking in the Spanish Bladder Cancer Study.",0
"Cluster randomized trials (CRTs) use as the unit of randomization clusters, which are usually defined as a collection of individuals sharing some common characteristics. Common examples of clusters include entire dental practices, hospitals, schools, school classes, villages, and towns. Additionally, several measurements (repeated measurements) taken on the same individual at different time points are also considered to be clusters. In dentistry, CRTs are applicable as patients may be treated as clusters containing several individual teeth. CRTs require certain methodological procedures during sample calculation, randomization, data analysis, and reporting, which are often ignored in dental research publications. In general, due to similarity of the observations within clusters, each individual within a cluster provides less information compared with an individual in a non-clustered trial. Therefore, clustered designs require larger sample sizes compared with non-clustered randomized designs, and special statistical analyses that account for the fact that observations within clusters are correlated. It is the purpose of this article to highlight with relevant examples the important methodological characteristics of cluster randomized designs as they may be applied in orthodontics and to explain the problems that may arise if clustered observations are erroneously treated and analysed as independent (non-clustered).-Cluster randomized clinical trials in orthodontics: design, analysis and reporting issues.",1
"We propose a sample size calculation method for rank tests comparing two survival distributions under cluster randomization with possibly variable cluster sizes. Here, sample size refers to number of clusters. Our method is based on simulation procedure generating clustered exponential survival variables whose distribution is specified by the marginal hazard rate and the intracluster correlation coefficient. Sample size is calculated given significance level, power, marginal hazard rates (or median survival times) under the alternative hypothesis, intracluster correlation coefficient, accrual rate, follow-up period, and cluster size distribution.-Sample size calculation for weighted rank tests comparing survival distributions under cluster randomization: a simulation method.",1
"Mediation analyses can help us to understand the biological mechanism in which an exposure or treatment affects an outcome. Single mediator analyses have been used in various applications, but may not be appropriate for analyzing intricate mechanisms involving multiple mediators that affect each other. Thus, in this article, we studied multiple sequentially ordered mediators for a dichotomous outcome and presented the identifiability assumptions for the path-specific effects on the outcome, that is, the effect of an exposure on the outcome mediated by a specific set of mediators. We proposed a closed-form estimator for the path-specific effects by modeling the dichotomous outcome using a probit model. Asymptotic variance of the proposed estimator is derived and can be approximated via delta method or bootstrapping. Simulations under a finite sample showed the validity of our method in capturing the path-specific effects when the probability of each potential counterfactual outcome is not small and demonstrated the utility of a computationally efficient alternative to bootstrapping for calculating variance. The method is applied to investigate the effects of polycystic ovarian syndrome on live birth rates mediated by estradiol levels and the number of oocytes retrieved in a large electronic in vitro fertilization database. We implemented the method into an R package SOMM, which is available at https://github.com/roqe/SOMM.-A mediation analysis for a nonrare dichotomous outcome with sequentially ordered multiple mediators.",0
"Karelia project of the Minnesota Heart Health Program. International Journal of Epidemiology 1986, 15: 176-182. Community-based cardiovascular disease control studies represent an effort to change cardiovascular disease rates in entire communities. Communities, rather than individuals, are the primary units of analysis. The cross-community multiple time series model to estimate and test the effects is based on multiple communities that are evaluated at several points over time. Issues that influence the power of the analysis include: the number of communities to be studied, community size and composition, sample sizes of surveys, the decision to use cohorts or cross-sectional surveys, the number of surveys conducted in each community, and assumptions of latencies in the effects. These points are illustrated using the experiences of the North Karelia Project and the Minnesota Heart Health Program. The North Karelia Project was a community-based cardiovascular disease (CVD) prevention programme consisting of a five-year intervention period in 1972-7. It took place in two provinces in Finland. The Minnesota Heart Health Program is similar, taking place between 1980 and 1990 in six communities in the American Midwest.-Analysis of community-based cardiovascular disease prevention studies--evaluation issues in the North Karelia Project and the Minnesota Heart Health Program.",1
"Interest in targeted disease prevention has stimulated development of models that assign risks to individuals, using their personal covariates. We need to evaluate these models and quantify the gains achieved by expanding a model to include additional covariates. This paper reviews several performance measures and shows how they are related. Examples are used to show that appropriate performance criteria for a risk model depend upon how the model is used. Application of the performance measures to risk models for hypothetical populations and for US women at risk of breast cancer illustrate two additional points. First, model performance is constrained by the distribution of risk-determining covariates in the population. This complicates the comparison of two models when applied to populations with different covariate distributions. Second, all summary performance measures obscure model features of relevance to its utility for the application at hand, such as performance in specific subgroups of the population. In particular, the precision gained by adding covariates to a model can be small overall, but large in certain subgroups. We propose new ways to identify these subgroups and to quantify how much they gain by measuring the additional covariates. Those with largest gains could be targeted for cost-efficient covariate assessment.-Evaluating health risk models.",0
"Approximately 13-19% of women experience postpartum depression and approximately one-third of women who have a history of depression develop depression during the postpartum phase. Exercise is an efficacious intervention for depression among adults; however, few studies have examined the effect of exercise on postpartum depression. The purpose of this study was to conduct a randomized controlled trial examining the effect of exercise and wellness interventions on preventing postpartum depression among women at risk. Specifically, women (n?=?450) who were on average 4.35?weeks postpartum and had a history of depression were randomly assigned to one of the following three conditions: (1) Telephone-based exercise intervention; (2) telephone-based wellness/support intervention (covered topics such as sleep, stress, and healthy eating); or (3) usual care. Both interventions lasted six months. The exercise intervention was based on social cognitive theory and the Transtheoretical model and was specifically designed to motivate postpartum women to exercise. The primary dependent variable was depression based on the Structured Clinical Diagnostic Interview (SCID). Secondary dependent variables included the Edinburgh Postnatal Depression Scale, PHQ-9, and Perceived Stress Scale. Potential mediator variables included quality of sleep, postpartum social support, fatigue, and exercise attitudes. Questionnaires were administered at baseline, six, and nine months. The purpose of this paper is to summarize the methodology, study design, and baseline data for this study. This trial will provide important information regarding the efficacy of exercise and wellness interventions for preventing postpartum depression.-Rationale, design, and baseline data for the Healthy Mom II Trial: A randomized trial examining the efficacy of exercise and wellness interventions for the prevention of postpartum depression.",0
"We conducted a case-control study of the association between subsets of colorectal polyps, including adenomas and serrated polyps, and single-nucleotide polymorphisms (SNPs) related to colorectal cancer through prior genome-wide association studies (GWAS). Participants were enrollees in the Group Health Cooperative (Seattle, Washington) aged 24-79 years who received a colonoscopy from 1998 to 2007, donated a buccal or blood sample, and completed a structured questionnaire. We performed genotyping of 13 colorectal cancer susceptibility SNPs. Polytomous logistic regression models were used to estimate odds ratios and 95% confidence intervals for associations between polyps and the colorectal cancer risk allele for each SNP under a log-additive model. Analyses included 781 controls, 489 cases with adenoma, 401 cases with serrated polyps, and 188 cases with both polyp types. The following SNPs were associated with advanced adenomas: rs10936599, rs10795668, rs16892766, and rs9929218 (P &lt; 0.05). For nonadvanced adenomas and for serrated polyps overall, only rs961253 was statistically significant (P &lt; 0.05). These associations were in the same directions as those in prior colorectal cancer GWAS. No SNP was significantly associated with hyperplastic polyps, and only rs6983267 was significantly associated with sessile serrated polyps, but this association was opposite of that found in colorectal cancer GWAS. Our results suggest that the association between colorectal cancer susceptibility SNPs and colorectal polyps varies by polyp type.-Variation in the association between colorectal cancer susceptibility loci and colorectal polyps by polyp type.",0
"In randomised controlled trials, the assumption of independence of individual observations is fundamental to the design, analysis and interpretation of studies. However, in individually randomised trials in primary care, this assumption may be violated because patients are naturally clustered within primary care practices. Ignoring clustering may lead to a loss of power or, in some cases, type I error. Clustering can be quantified by intra-cluster correlation (ICC), a measure of the similarity between individuals within a cluster with respect to a particular outcome. We reviewed 17 trials undertaken by the Department of Primary Care at the University of Southampton over the last ten years. We calculated the ICC for the primary and secondary outcomes in each trial at the practice level and determined whether ignoring practice-level clustering still gave valid inferences. Where multiple studies collected the same outcome measure, the median ICC was calculated for that outcome. The median intra-cluster correlation (ICC) for all outcomes was 0.016, with interquartile range 0.00-0.03. The median ICC for symptom severity was 0.02 (interquartile range (IQR) 0.01 to 0.07) and for reconsultation with new or worsening symptoms was 0.01 (IQR 0.00, 0.07). For HADS anxiety the ICC was 0.04 (IQR 0.02, 0.05) and for HADS depression was 0.02 (IQR 0.00, 0.05). The median ICC for EQ. 5D-3 L was 0.01 (IQR 0.01, 0.04). There is evidence of clustering in individually randomised trials primary care. The non-zero ICC suggests that, depending on study design, clustering may not be ignorable. It is important that this is fully considered at the study design phase.-Clustering of continuous and binary outcomes at the general practice level in individually randomised studies in primary care - a review of 10 years of primary care trials",2
"Randomized trials in which the unit of randomization is a community, worksite, school or family are becoming widely used in the evaluation of life-style interventions for the prevention of disease. The increasing interest in adopting a cluster randomization design is being matched by rapid methodological developments. In this paper we describe several of these developments. Brief mention is also made of issues related to economic analysis and to the planning and conduct of meta-analyses for cluster randomization trials. Recommendations for reporting are also discussed.-Current and future challenges in the design and analysis of cluster randomization trials.",1
"Early identification of participants at risk of run-in failure (RIF) may present opportunities to improve trial efficiency and generalizability. We conducted a partial factorial-design, randomized, controlled trial of calcium and vitamin D to prevent colorectal adenoma recurrence at 11 centers in the United States. At baseline, participants completed two self-administered questionnaires (SAQs) and a questionnaire administered by staff. Participants in the full factorial randomization (calcium, vitamin D, both, or neither) received a placebo during a 3-month single-blinded run-in; women electing to take calcium enrolled in a two-group randomization (calcium with vitamin D, or calcium alone) and received calcium during the run-in. Using logistic regression models, we examined baseline factors associated with RIF in three subgroups: men (N = 1606) and women (N = 301) in the full factorial randomization and women in the two-group randomization (N = 666). Overall, 314/2573 (12?%) participants failed run-in; 211 (67?%) took fewer than 80?% of their tablets (poor adherence), and 103 (33?%) withdrew or were uncooperative. In multivariable models, 8- to 13-fold variation was seen by study center in odds of RIF risk in the two largest groups. In men, RIF decreased with age (adjusted odds ratio [OR] per 5?years 0.85 [95?% confidence interval, CI; 0.76-0.96]) and was associated with being single (OR 1.65 [95?% CI; 1.10-2.47]), not graduating from high school (OR 2.77 [95?% CI; 1.58-4.85]), and missing SAQ data (OR 1.97 [1.40-2.76]). Among women, RIF was associated primarily with health-related factors; RIF risk was lower with higher physical health score (OR 0.73 [95?% CI; 0.62-0.86]) and baseline multivitamin use (OR 0.44 [95?% CI; 0.26-0.75]). Women in the 5-year colonoscopy surveillance interval were at greater risk of RIF than those with 3-year follow-up (OR 1.91 [95?% CI; 1.08-3.37]), and the number of prescription medicines taken was also positively correlated with RIF (p = 0.03). Perceived toxicities during run-in were associated with 12- to 29-fold significantly increased odds of RIF. There were few common baseline predictors of run-in failure in the three randomization groups. However, heterogeneity in run-in failure associated with study center, and missing SAQ data reflect potential opportunities for intervention to improve trial efficiency and retention. ClinicalTrials.gov: NCT00153816 . Registered September 2005.-Randomized controlled trials: who fails run-in?",0
"Recent improvements in our understanding of drug metabolism have led to the development of anticancer therapies that accommodate patient differences in drug tolerance. Such methods adjust the dose level according to measurable patient characteristics in order to obtain a target drug exposure. This paper describes the utilization of a patient specific dosing scheme in the statistical design of a phase I clinical trial involving patients with advanced adenocarcinomas of gastrointestinal origin. During the trial, dose levels were adjusted according to each patient's pretreatment concentration of an antibody that was shown in preclinical testing to moderate the effect of the agent under investigation. The design of the trial permitted a continual adjustment of the model used to tailor the dose to each patient's individual needs.-Patient specific dosing in a cancer phase I clinical trial.",0
"Several types of common model misspecifications can be re-formulated as problems of omitted covariates. These include situations with unmeasured confounders, measurement errors in observed covariates and informative censoring. Longitudinal data present special opportunities for detecting omitted covariates that are related to the observed ones differently across time than across individuals. This situation arises with period and cohort effects, as well as with usual formulations of classical measurement error in observed covariates. In this article we focus on testing for the existence of omitted covariates in longitudinal data analysis when models are fit by generalized estimation equations. When omitted covariates are present, specification of the correct link function conditionally on only observed covariates under the alternative usually involves complicated numerical integration. We propose a quasi-score test statistic that avoids the need to fit such alternative models. The statistic is asymptotically chi-square distributed under the null hypothesis of no omitted covariates with degrees of freedom determined by the assumed alternative structure. We study the significance level and the power of the quasi-score test in linear and logistic regression models. The test is then applied to an analysis of excessive daytime sleepiness.-Testing model fit in longitudinal data analysis against alternatives with omitted covariates.",0
"Prospective trial registration is a powerful tool to prevent reporting bias. We aimed to determine the extent to which published randomized controlled trials (RCTs) were registered and registered prospectively. We searched MEDLINE and EMBASE from January 2005 to October 2017; we also screened all articles cited by or citing included and excluded studies, and the reference lists of related reviews. We included studies that examined published RCTs and evaluated their registration status, regardless of medical specialty or language. We excluded studies that assessed RCT registration status only through mention of registration in the published RCT, without searching registries or contacting the trial investigators. Two independent reviewers blinded to the other's work performed the selection. Following PRISMA guidelines, two investigators independently extracted data, with discrepancies resolved by consensus. We calculated pooled proportions and 95% confidence intervals using random-effects models. We analyzed 40 studies examining 8773 RCTs across a wide range of clinical specialties. The pooled proportion of registered RCTs was 53% (95% confidence interval 44% to 58%), with considerable between-study heterogeneity. A subset of 24 studies reported data on prospective registration across 5529 RCTs. The pooled proportion of prospectively registered RCTs was 20% (95% confidence interval 15% to 25%). Subgroup analyses showed that registration was higher for industry-supported and larger RCTs. A meta-regression analysis across 19 studies (5144 RCTs) showed that the proportion of registered trials significantly increased over time, with a mean proportion increase of 27%, from 25 to 52%, between 2005 and 2015. The prevalence of trial registration has increased over time, but only one in five published RCTs is prospectively registered, undermining the validity and integrity of biomedical research.-Registration of published randomized trials: a systematic review and meta-analysis.",0
"The authors explored the group as a source of change in mindfulness-based stress reduction (MBSR). Participants consisted of 606 adults in 59 groups who completed an 8-week MBSR program. The authors examined change in the General Symptom Index (GSI) of the Symptom Checklist-90-Revised and the Medical Symptom Checklist (MSC) from pre- to postintervention. Multilevel models were used to examine the extent to which groups differed in the amount of change reported by the participants. After controlling for pretreatment severity, group accounted for 7% of the variability in the GSI and 0% in the MSC. The authors discuss the implications of these findings for the practice of MBSR as well as for research investigating the effects of MBSR and other programs or psychotherapies.-Beyond the individual: group effects in mindfulness-based stress reduction.",2
"The rigorous evaluation of the impact of combination HIV prevention packages at the population level will be critical for the future of HIV prevention. In this review, we discuss important considerations for the design and interpretation of cluster randomized controlled trials (C-RCTs) of combination prevention interventions. We focus on three large C-RCTs that will start soon and are designed to test the hypothesis that combination prevention packages, including expanded access to antiretroviral therapy, can substantially reduce HIV incidence. Using a general framework to integrate mathematical modelling analysis into the design, conduct, and analysis of C-RCTs will complement traditional statistical analyses and strengthen the evaluation of the interventions. Importantly, even with combination interventions, it may be challenging to substantially reduce HIV incidence over the 2- to 3-y duration of a C-RCT, unless interventions are scaled up rapidly and key populations are reached. Thus, we propose the innovative use of mathematical modelling to conduct interim analyses, when interim HIV incidence data are not available, to allow the ongoing trials to be modified or adapted to reduce the likelihood of inconclusive outcomes. The preplanned, interactive use of mathematical models during C-RCTs will also provide a valuable opportunity to validate and refine model projections.-HIV treatment as prevention: considerations in the design, conduct, and analysis of cluster randomized controlled trials of combination HIV prevention.",1
"To describe the application of the stepped wedge cluster randomized controlled trial (CRCT) design. Systematic review. We searched Medline, Embase, PsycINFO, HMIC, CINAHL, Cochrane Library, Web of Knowledge, and Current Controlled Trials Register for articles published up to January 2010. Stepped wedge CRCTs from all fields of research were included. Two authors independently reviewed and extracted data from the studies. Twenty-five studies were included in the review. Motivations for using the design included ethical, logistical, financial, social, and political acceptability and methodological reasons. Most studies were evaluating an intervention during routine implementation. For most of the included studies, there was also a belief or empirical evidence suggesting that the intervention would do more good than harm. There was variation in data analysis methods and insufficient quality of reporting. The stepped wedge CRCT design has been mainly used for evaluating interventions during routine implementation, particularly for interventions that have been shown to be effective in more controlled research settings, or where there is lack of evidence of effectiveness but there is a strong belief that they will do more good than harm. There is need for consistent data analysis and reporting.-Systematic review of stepped wedge cluster randomized trials shows that design is particularly used to evaluate interventions during routine implementation.",3
"Two common statistical problems in pooling survival data from several studies are addressed. The first problem is that the data are doubly censored in that the origin is interval censored and the endpoint event may be right censored. Two approaches to incorporate the uncertainty of interval-censored origins are developed, and then compared with more usual analyses using imputation of a single fixed value for each origin. The second problem is that the data are collected from multiple studies and it is likely that heterogeneity exists among the study populations. A random-effects hierarchical Cox proportional hazards model is therefore used. The scientific problem motivating this work is a pooled survival analysis of data sets from three studies to examine the effect of GB virus type C (GBV-C) coinfection on survival of HIV-infected individuals. The time of HIV infection is the origin and for each subject this time is unknown, but is known to lie later than the last time at which the subject was known to be HIV negative, and earlier than the first time the subject was known to be HIV positive. The use of an approximate Bayesian approach using the partial likelihood as the likelihood is recommended because it more appropriately incorporates the uncertainty of interval-censored HIV infection times.-A Bayesian analysis of doubly censored data using a hierarchical Cox model.",0
"Ethics has been identified as a central reason for choosing the stepped wedge trial over other kinds of trial designs. The potential advantage of the stepped wedge design is that it provides all arms of the trial with the active intervention over the course of the study. Some groups receive it later than others, but the study intervention is not withheld from any group. This feature of the stepped wedge design seems particularly ethically advantageous in two instances: (1) when the study intervention appears especially likely to be effective and (2) when the consequences of not receiving the intervention may be dire. But despite an increase in the use of the stepped wedge design and appeals to its ethical superiority as the motivation for its selection, there has been limited attention to the stepped wedge trial in the ethics literature. In the following, I examine whether there are persuasive ethical reasons to prefer or to require a stepped wedge trial. I argue that while the stepped wedge design is ethically permissible, it is not morally superior to other kinds of trials. To this end, I examine the ethical justification for providing, withholding, and delaying interventions in research.-Delaying and withholding interventions: ethics and the stepped wedge trial",3
"Reviews of the handling of covariates in trials have explicitly excluded cluster randomized trials (CRTs). In this study, we review the use of covariates in randomization, the reporting of covariates, and adjusted analyses in CRTs. We reviewed a random sample of 300 CRTs published between 2000 and 2008 across 150 English language journals. Fifty-eight percent of trials used covariates in randomization. Only 69 (23%) included tables of cluster- and individual-level covariates. Fifty-eight percent reported significance tests of baseline balance. Of 207 trials that reported baseline measures of the primary outcome, 155 (75%) subsequently adjusted for these in analyses. Of 174 trials that used covariates in randomization, 30 (17%) included an analysis adjusting for all those covariates. Of 219 trial reports that included an adjusted analysis of the primary outcome, only 71 (32%) reported that covariates were chosen a priori. There are some marked discrepancies between practice and guidance on the use of covariates in the design, analysis, and reporting of CRTs. It is essential that researchers follow guidelines on the use and reporting of covariates in CRTs, promoting the validity of trial conclusions and quality of trial reports.-A review of the use of covariates in cluster randomized trials uncovers marked discrepancies between guidance and practice.",1
"A logistic regression with random effects model is commonly applied to analyze clustered binary data, and every cluster is assumed to have a different proportion of success. However, it could be of interest to obtain the proportion of success over clusters (i.e. the marginal proportion of success). Furthermore, the degree of correlation among data of the same cluster (intraclass correlation) is also a relevant concept to assess, but when using logistic regression with random effects it is not possible to get an analytical expression of the estimators for marginal proportion and intraclass correlation. In our paper, we assess and compare approaches using different kinds of approximations: based on the logistic-normal mixed effects model (LN), linear mixed model (LMM), and generalized estimating equations (GEE). The comparisons are completed by using two real data examples and a simulation study. The results show the performance of the approaches strongly depends on the magnitude of the marginal proportion, the intraclass correlation, and the sample size. In general, the reliability of the approaches get worsen with low marginal proportion and large intraclass correlation. LMM and GEE approaches arises as reliable approaches when the sample size is large.-Estimating marginal proportions and intraclass correlations with clustered binary data",1
"Clinical risk prediction models are increasingly being developed and validated on multicenter datasets. In this article, we present a comprehensive framework for the evaluation of the predictive performance of prediction models at the center level and the population level, considering population-averaged predictions, center-specific predictions, and predictions assuming an average random center effect. We demonstrated in a simulation study that calibration slopes do not only deviate from one because of over- or underfitting of patterns in the development dataset, but also as a result of the choice of the model (standard versus mixed effects logistic regression), the type of predictions (marginal versus conditional versus assuming an average random effect), and the level of model validation (center versus population). In particular, when data is heavily clustered (ICC 20%), center-specific predictions offer the best predictive performance at the population level and the center level. We recommend that models should reflect the data structure, while the level of model validation should reflect the research question.-Does ignoring clustering in multicenter data influence the performance of prediction models? A simulation study.",1
"Within the framework of a clinical trial, one is sometimes brought to randomize groups of subjects rather than the subjects themselves. This type of experimental design has methodological consequences of importance (in particular biostatistical consequences) and can limit the extrapolation of the results. A cluster randomized trial is thus justified only if the context or the question of interest imposes it and biostatistical issues have then to be handled in a rigorous way.-[The cluster-randomized trial].",1
"Multilevel analysis which was primarily introduced to deal with hierarchical data was later applied extensively for research in other fields of science and not only for nested data, but also for repeated measurements or clustered trials. This method of statistical analysis was applied in dental studies in the 1991 for the first time but despite its value for data analysis in dental studies, its application for dental studies remains limited until now. This manuscript reviews the applications of this method in dental studies.-Application of Multilevel Models in Dentistry.",1
"Experiments allow researchers to randomly vary the key manipulation, the instruments of measurement, and the sequences of the measurements and manipulations across participants. To date, however, the advantages of randomized experiments to manipulate both the aspects of interest and the aspects that threaten internal validity have been primarily used to make inferences about the average causal effect of the experimental manipulation. This article introduces a general framework for analyzing experimental data to make inferences about individual differences in causal effects. Approaches to analyzing the data produced by a number of classical designs and 2 more novel designs are discussed. Simulations highlight the strengths and weaknesses of the data produced by each design with respect to internal validity. Results indicate that, although the data produced by standard designs can be used to produce accurate estimates of average causal effects of experimental manipulations, more elaborate designs are often necessary for accurate inferences with respect to individual differences in causal effects. The methods described here can be diversely applied by researchers interested in determining the extent to which individuals respond differentially to an experimental manipulation or treatment and how differential responsiveness relates to individual participant characteristics.-Individual differences methods for randomized experiments.",0
"To evaluate the cumulative incidence of cervical intraepithelial neoplasia II or worse (grade II+) or cervical intraepithelial neoplasia grade III+ after short term persistence of prevalently detected carcinogenic human papillomavirus (HPV). Population based cohort study. Guanacaste, Costa Rica. 2282 sexually active women actively followed after enrolment. Primary end points: three year and five year cumulative incidence of histologically confirmed cervical intraepithelial neoplasia grade II+ (n=70). Cervical specimens collected at each visit tested for more than 40 HPV genotypes. HPV 16, 18, 26, 31, 33, 35, 39, 45, 51, 52, 56, 58, 59, 66, 68, 73, and 82 were considered the primary carcinogenic genotypes. Women who tested positive for a carcinogenic HPV at enrolment and after about one year (9-21 months) (positive/positive) had a three year cumulative incidence of cervical intraepithelial neoplasia grade II+ of 17.0% (95% confidence interval 12.1% to 22.0%). Those who tested negative/positive (3.4%, 0.1% to 6.8%), positive/negative (1.2%, -0.2% to 2.5%), and negative/negative (0.5%, 0.1% to 0.9%) were at a significantly lower risk. There was little difference in the cumulative incidence of cervical intraepithelial neoplasia grade II+ between testing positive twice for any carcinogenic HPV genotype (same genotype or different genotypes) v testing positive twice for the same carcinogenic genotype (17.0% v 21.3%, respectively). Short term persistence of HPV 16 strongly predicted cervical intraepithelial neoplasia grade II+, with a three year cumulative incidence of 40.8% (26.4% to 55.1%). Similar patterns were observed for the five year cumulative incidence of grade II+ and for three year and five year cumulative incidence of grade III+. Short term persistence of a prevalently detected carcinogenic HPV infection, especially HPV 16, strongly predicts a subsequent diagnosis of cervical intraepithelial neoplasia II+ over the next few years.-Short term persistence of human papillomavirus and risk of cervical precancer and cancer: population based cohort study.",0
"In the early phase development of molecularly targeted agents (MTAs), a commonly encountered situation is that the MTA is expected to be more effective for a certain biomarker subgroup, say marker-positive patients, but there is no adequate evidence to show that the MTA does not work for the other subgroup, that is, marker-negative patients. After establishing that marker-positive patients benefit from the treatment, it is often of great clinical interest to determine whether the treatment benefit extends to marker-negative patients. The authors propose optimal sequential enrichment (OSE) designs to address this practical issue in the context of phase II clinical trials. The OSE designs evaluate the treatment effect first in marker-positive patients and then in marker-negative patients if needed. The designs are optimal in the sense that they minimize the expected sample size or the maximum sample size under the null hypothesis that the MTA is futile. An efficient, accurate optimization algorithm is proposed to find the optimal design parameters. One important advantage of the OSE design is that the go/no-go interim decision rules are specified prior to the trial conduct, which makes the design particularly easy to use in practice. A simulation study shows that the OSE designs perform well and are ethically more desirable than the commonly used marker-stratified design. The OSE design is applied to an endometrial carcinoma trial. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Optimal sequential enrichment designs for phase II clinical trials.",0
"The most common and conceptually sound ethical concerns with financial incentives for research participation are that they may (1) represent undue inducements by blunting peoples' perceptions of research risks, thereby preventing fully informed consent; or (2) represent unjust inducements by encouraging enrollment preferentially among the poor. Neither of these concerns has been shown to manifest in studies testing the effects of incentives on decisions to participate in hypothetical randomized clinical trials (RCTs), but neither has been assessed in real RCTs. We are conducting randomized trials of real incentives embedded within two parent RCTs. In each of two trials conducted in parallel, we are randomizing 576 participants to one of three incentive groups. Following preliminary determination of patients' eligibility in the parent RCT, we assess patients' research attitudes, demographic characteristics, perceived research risks, time spent reviewing consent documents, ability to distinguish research from patient care, and comprehension of key trial features. These quantitative assessments will be supplemented by semi-structured interviews for a selected group of participants that more deeply explore patients' motivations for trial participation. The trials are each designed to have adequate power to rule out undue and unjust inducement. We are also exploring potential benefits of incentives, including possible increased attention to research risks and cost-effectiveness.-Randomized evaluation of trial acceptability by INcentive (RETAIN): Study protocol for two embedded randomized controlled trials.",0
"Increasingly used in health-related applications, latent variable models provide an appealing framework for handling high-dimensional exposure and response data. Item response theory (IRT) models, which have gained widespread popularity, were originally developed for use in the context of educational testing, where extremely large sample sizes permitted the estimation of a moderate-to-large number of parameters. In the context of public health applications, smaller sample sizes preclude large parameter spaces. Therefore, we propose a penalized likelihood approach to reduce mean square error and improve numerical stability. We present a continuous family of models, indexed by a tuning parameter, that range between the Rasch model and the IRT model. The tuning parameter is selected by cross validation or approximations such as Akaike Information Criterion. While our approach can be placed easily in a Bayesian context, we find that our frequentist approach is more computationally efficient. We demonstrate our methodology on a study of methylation silencing of gene expression in bladder tumors. We obtain similar results using both frequentist and Bayesian approaches, although the frequentist approach is less computationally demanding. In particular, we find high correlation of methylation silencing among 16 loci in bladder tumors, that methylation is associated with smoking and also with patient survival.-Penalized item response theory models: application to epigenetic alterations in bladder cancer.",0
"Critically ill infants with congenital heart disease (CHD) are often prescribed stress ulcer prophylaxis (SUP) to prevent upper gastrointestinal bleeding, despite the low incidence of stress ulcers and limited data on the safety and efficacy of SUP in infants. Recently, SUP has been associated with an increased incidence of hospital-acquired infections, community-acquired pneumonia, and necrotizing enterocolitis. The objective of this pilot study is to investigate the feasibility of performing a randomized controlled trial to assess the safety and efficacy of withholding SUP in infants with congenital heart disease admitted to the cardiac intensive care unit. A single center, prospective, double-blinded, randomized placebo-controlled pilot feasibility trial will be performed in infants with CHD admitted to the cardiac intensive care unit and anticipated to require respiratory support for &gt; 24 h. Patients will be randomized to receive a histamine-2 receptor antagonist (H2RA) or placebo until they are discontinued from respiratory support. Randomization will be performed within 2 strata defined by admission type (medical or surgical) and age (neonate, age &lt; 30 days, or infant, 1 month to 1 year). Allocation will be a 1:1 ratio using permuted blocks to ensure balanced allocations across the two treatment groups within each stratum. The primary outcomes include feasibility of screening, consent, timely allocation of study drug, and protocol adherence. The primary safety outcome is the rate of clinically significant upper gastrointestinal bleeding. The secondary outcomes are the difference in the relative and absolute abundance of the gut microbiota and functional microbial profiles between the two study groups. We plan to enroll 100 patients in this pilot study. Routine use of SUP to prevent upper gastrointestinal bleeding in infants is controversial due to a low incidence of bleeding events and concern for adverse effects. The role of SUP in infants with CHD has not been examined, and there is equipoise on the risks and benefits of withholding this therapy. In addition, this therapy has been discontinued in other neonatal populations due to the concern for hospital-acquired infections and necrotizing enterocolitis. Furthermore, exploring changes to the microbiome after exposure to SUP may highlight the mechanisms by which SUP impacts potential microbial dysbiosis of the gut and its association with hospital-acquired infections. Assessment of the feasibility of a trial of withholding SUP in critically ill infants with CHD will facilitate planning of a larger multicenter trial of safety and efficacy of SUP in this vulnerable population. ClinicalTrials.gov , NCT03667703. Registered 12 September 2018, https://clinicaltrials.gov/ct2/show/NCT03667703?term=SUPPRESS+CHD&amp;draw=2&amp;rank=1 . All WHO Trial Registration Data Set Criteria are met in this manuscript.-Stress ulcer prophylaxis versus placebo-a blinded randomized control trial to evaluate the safety of two strategies in critically ill infants with congenital heart disease (SUPPRESS-CHD).",0
"The effects of clustering in randomized controlled trials (RCTs) and the resulting potential violation of assumptions of independence are now well recognized. When patients in a single study are treated by several therapists, there is good reason to suspect that the variation in outcome will be smaller for patients treated in the same group than for patients treated in different groups. This potential correlation of outcomes results in a loss of independence of observations. The purpose of this study is to examine the current use of clustering analysis in RCTs published in the top five journals of orthopaedic surgery. RCTs published from 2006 to 2010 in the top five journals of orthopaedic surgery, as determined by 5-year impact factor, that included multiple therapists and/or centers were included. Identified articles were assessed for accounting for the effects of clustering of therapists and/or centers in randomization or analysis. Logistic regression used both univariate and multivariate models, with use of clustering analysis as the outcome. Multivariate models were constructed using stepwise deletion. An alpha level of 0.10 was considered significant. A total of 271 articles classified as RCTs were identified from the five journals included in the study. Thirty-two articles were excluded due to inclusion of nonhuman subjects. Of the remaining 239 articles, 186 were found to include multiple centers and/or therapists. The prevalence of use of clustering analysis was 21.5%. Fewer than half of the studies reported inclusion of a statistician, epidemiologist or clinical trials methodologist on the team. In multivariate modeling, adjusting for clustering was associated with a 6.7 times higher odds of inclusion of any type of specialist on the team (P = 0.08). Likewise, trials that accounted for clustering had 3.3 times the odds of including an epidemiologist/clinical trials methodologist than those that did not account for clustering (P = 0.04). Including specialists on a study team, especially an epidemiologist or clinical trials methodologist, appears to be important in the decision to account for clustering in RCT reporting. The use of clustering analysis remains an important piece of unbiased reporting, and accounting for clustering in RCTs should be a standard reporting practice.-Use of clustering analysis in randomized controlled trials in orthopaedic surgery.",2
"A goal of many health studies is to determine the causal effect of a treatment or intervention on health outcomes. Often, it is not ethically or practically possible to conduct a perfectly randomized experiment, and instead, an observational study must be used. A major challenge to the validity of observational studies is the possibility of unmeasured confounding (i.e., unmeasured ways in which the treatment and control groups differ before treatment administration, which also affect the outcome). Instrumental variables analysis is a method for controlling for unmeasured confounding. This type of analysis requires the measurement of a valid instrumental variable, which is a variable that (i) is independent of the unmeasured confounding; (ii) affects the treatment; and (iii) affects the outcome only indirectly through its effect on the treatment. This tutorial discusses the types of causal effects that can be estimated by instrumental variables analysis; the assumptions needed for instrumental variables analysis to provide valid estimates of causal effects and sensitivity analysis for those assumptions; methods of estimation of causal effects using instrumental variables; and sources of instrumental variables in health studies.-Instrumental variable methods for causal inference.",0
"Random-coefficient pattern-mixture models (RCPMMs) have been proposed for longitudinal data when drop-out is thought to be non-ignorable. An RCPMM is a random-effects model with summaries of drop-out time included among the regressors. The basis of every RCPMM is extrapolation. We review RCPMMs, describe various extrapolation strategies, and show how analyses may be simplified through multiple imputation. Using simulated and real data, we show that alternative RCPMMs that fit equally well may lead to very different estimates for parameters of interest. We also show that minor model misspecification can introduce biases that are quite large relative to standard errors, even in fairly small samples. For many scientific applications, where the form of the population model and nature of the drop-out are unknown, interval estimates from any single RCPMM may suffer from undercoverage because uncertainty about model specification is not taken into account.-On the performance of random-coefficient pattern-mixture models for non-ignorable drop-out.",0
"A problem often encountered in epidemiology is the evaluation of the validity of a short questionnaire for diet or physical activity administered to large numbers of subjects, where the gold standard is a diet record or physical activity diary. It is well known that random measurement error can attenuate the interclass correlation coefficient (validity coefficient) between these two variables. Several authors have proposed a de-attenuated (or corrected) correlation coefficient which is an estimate of the true correlation between the two variables after removing the effect of random measurement error. By true correlation we mean the correlation between the questionnaire and the mean of a large or 'infinite' number of diaries (representing the truth). In this paper the authors propose three methods (two ad hoc methods, the pairwise and weighted sib-mean estimators, and the maximum likelihood with confidence limits computed using the Wald statistic and profile likelihood approaches) to estimate the true correlation between a single questionnaire and the mean of an infinite number of follow-up diaries, in the general case where an unequal number of diaries are available for each individual. A simulation study is done under the assumption that the measured variables are normally distributed. Under the null hypothesis of no correlation between the questionnaires and the diaries, all methods had negligible biases. In cases closer to what is usually seen in practice (true correlation between 0.4 and 0.6), the degree of bias and coverage probability depends heavily on the reliability (intraclass correlation) of the diaries. The maximum likelihood estimator with confidence intervals computed by the profile likelihood approach, while not systematically outperforming the other methods, is shown to be the best of the three proposed approaches.-Comparisons of measures of interclass correlations: the general case of unequal group size.",1
"Although human leukocyte antigen (HLA) DQ and DR loci appear to confer the strongest genetic risk for type 1 diabetes, more detailed information is required for other loci within the HLA region to understand causality and stratify additional risk factors. The Type 1 Diabetes Genetics Consortium (T1DGC) study design included high-resolution genotyping of HLA-A, B, C, DRB1, DQ, and DP loci in all affected sibling pair and trio families, and cases and controls, recruited from four networks worldwide, for analysis with clinical phenotypes and immunological markers. In this article, we present the operational strategy of training, classification, reporting, and quality control of HLA genotyping in four laboratories on three continents over nearly 5 years. Methods to standardize HLA genotyping at eight loci included: central training and initial certification testing; the use of uniform reagents, protocols, instrumentation, and software versions; an automated data transfer; and the use of standardized nomenclature and allele databases. We implemented a rigorous and consistent quality control process, reinforced by repeated workshops, yearly meetings, and telephone conferences. A total of 15,246 samples have been HLA genotyped at eight loci to four-digit resolution; an additional 6797 samples have been HLA genotyped at two loci. The genotyping repeat rate decreased significantly over time, with an estimated unresolved Mendelian inconsistency rate of 0.21%. Annual quality control exercises tested 2192 genotypes (4384 alleles) and achieved 99.82% intra-laboratory and 99.68% inter-laboratory concordances. The chosen genotyping platform was unable to distinguish many allele combinations, which would require further multiple stepwise testing to resolve. For these combinations, a standard allele assignment was agreed upon, allowing further analysis if required. High-resolution HLA genotyping can be performed in multiple laboratories using standard equipment, reagents, protocols, software, and communication to produce consistent and reproducible data with minimal systematic error. Many of the strategies used in this study are generally applicable to other large multi-center studies.-HLA genotyping in the international Type 1 Diabetes Genetics Consortium.",0
"To describe the health care settings, purposes, and study reporting quality of the 2 ? 2 cluster randomized controlled factorial trial design. This study is a systematic review. We searched Medline, Embase, Cochrane Library, and Web of Knowledge for articles published up to May 2012. Cluster randomized controlled 2 ? 2 factorial trials in health, evaluating at least one complex intervention, were included. Two authors independently reviewed and extracted data from the studies. Twenty-nine studies covering a wide range of clinical areas and health care settings were included. The cluster design was mostly used to minimize contamination. The factorial design was mostly used to assess the effects of two interventions in the same study and to explore interactions between interventions. However, although most studies explored the presence or absence of intervention interactions, they were often either not sufficiently powered to detect any interactions or did not provide information on whether the study was sufficiently powered to detect any interactions. There was a considerable variability in the reporting of a number of study characteristics and methodological aspects. Study quality was also variable within and across studies. The design has been used in a wide range of health care settings and clinical areas to minimize contamination, assess the effects of two interventions in the same study, and explore intervention interactions. There is need for improvement on and guidelines for the reporting of factorial trials.-The 2 ? 2 cluster randomized controlled factorial trial design is mainly used for efficiency and to explore intervention interactions: a systematic review.",1
"In failure-time settings, a competing event is any event that makes it impossible for the event of interest to occur. For example, cardiovascular disease death is a competing event for prostate cancer death because an individual cannot die of prostate cancer once he has died of cardiovascular disease. Various statistical estimands have been defined as possible targets of inference in the classical competing risks literature. Many reviews have described these statistical estimands and their estimating procedures with recommendations about their use. However, this previous work has not used a formal framework for characterizing causal effects and their identifying conditions, which makes it difficult to interpret effect estimates and assess recommendations regarding analytic choices. Here we use a counterfactual framework to explicitly define each of these classical estimands. We clarify that, depending on whether competing events are defined as censoring events, contrasts of risks can define a total effect of the treatment on the event of interest or a direct effect of the treatment on the event of interest not mediated by the competing event. In contrast, regardless of whether competing events are defined as censoring events, counterfactual hazard contrasts cannot generally be interpreted as causal effects. We illustrate how identifying assumptions for all of these counterfactual estimands can be represented in causal diagrams, in which competing events are depicted as time-varying covariates. We present an application of these ideas to data from a randomized trial designed to estimate the effect of estrogen therapy on prostate cancer mortality.-A causal framework for classical statistical estimands in failure-time settings with competing events.",0
"A number of antibody biomarkers have been developed to distinguish between recent and established Human Immunodeficiency Virus (HIV) infection and used for HIV incidence estimation from cross-sectional specimens. In general, a cut-off value is specified, and estimates of the following parameters are needed: (i) the mean time interval .w/ between seroconversion and reaching that cut-off; (ii) the probability of correctly identifying individuals who became infected in the last w years (sensitivity); and (iii) the probability of correctly identifying individuals who have been infected for more than w years (specificity). We develop two statistical methods to study the distribution of a biomarker and derive a formula for estimating HIV incidence from a cross-sectional survey. Both methods allow handling interval censored data and basically consist of using a generalized mixture model to model the growth of the biomarker as a function of time since infection. The first uses data from all followed-up individuals and allows incidence estimation in the cohort, whereas the second only uses data from seroconverters. We illustrate our methods using repeated measures of the IgG capture BED enzyme immunoassay. Estimates of calibration parameters, that is, mean window period, mean recency period, sensitivity, and specificities obtained from both models are comparable. The formula derived for incidence estimation gives the maximum likelihood estimate of incidence which, for a given window period, depends only on sensitivity and specificity. The optimal choice of the window period is discussed. Numerical simulations suggest that data from seroconverters can provide reasonable estimates of the calibration parameters.-Mixture models for calibrating the BED   for HIV incidence testing.",0
Modeling Cluster Design Effects When Cluster Sizes Vary,1
"Analyses of trials of group administered treatments require an identifier for therapy group to account for clustering by group. All patients randomized to receive the group administered treatment could be assigned an intended group identifier following randomization. Alternatively, an actual group could be based on those patients that comply with group therapy. We investigate the implications for intention-to-treat (ITT) analyses of using either the intended or actual group to adjust for the clustering effect. We also consider causal models using the actual group. A simulation study showed that ITT estimates based on random effects models or GEE with an exchangeable correlation matrix performed much better when using the intended group than the actual group. OLS with robust standard errors performed well with both. Most compliance average causal effect (CACE) models performed well. While practical constraints of the clinical setting may determine the choice between an intended or actual group analyses, it is desirable to record both. An ITT analysis using mixed models can then be fitted using the intended group with data generation assumptions checked by a causal model using the actual group. Where an ITT analysis is based on the actual group, worse outcome for never-takers than compliers may allow one to infer that some estimators are biased toward no treatment effect. The work here is motivated and illustrated by a trial of a group therapy, but also has relevance to trials with treatment related clustering due to therapist examples of which include physical and talking therapies or surgery.-The implications of noncompliance for randomized trials with partial nesting due to group treatment",2
"Randomized controlled trials are crucial for the evaluation of interventions such as vaccinations, but the design and analysis of these studies during infectious disease outbreaks is complicated by statistical, ethical, and logistical factors. Attempts to resolve these complexities have led to the proposal of a variety of trial designs, including individual randomization and several types of cluster randomization designs: parallel-arm, ring vaccination, and stepped wedge designs. Because of the strong time trends present in infectious disease incidence, however, methods generally used to analyze stepped wedge trials might not perform well in these settings. Using simulated outbreaks, we evaluated various designs and analysis methods, including recently proposed methods for analyzing stepped wedge trials, to determine the statistical properties of these methods. While new methods for analyzing stepped wedge trials can provide some improvement over previous methods, we find that they still lag behind parallel-arm cluster-randomized trials and individually randomized trials in achieving adequate power to detect intervention effects. We also find that these methods are highly sensitive to the weighting of effect estimates across time periods. Despite the value of new methods, stepped wedge trials still have statistical disadvantages compared with other trial designs in epidemic settings.-Statistical Properties of Stepped Wedge Cluster-Randomized Trials in Infectious Disease Outbreaks",3
"Five counties (Kern, Riverside, Sacramento, San Diego, San Francisco) that demonstrate both variations and similarities in their implementation of Proposition 36 (e.g., treatment approaches, urine testing) and patient mix have been selected to participate in a study assessing how California's Proposition 36 is affecting the drug treatment system and patient outcomes. Except for San Francisco, treatment admissions increased during the first year of Proposition 36 implementation over the prior year (27% in Kern, 21% in Riverside, 17% in Sacramento, and 16% in San Diego), mostly in outpatient drug-free programs. Compared to non-Proposition 36 patients, Proposition 36 patients were more likely to be men, first-time admissions, treated in outpatient drug-free programs, employed full-time, and users of methamphetamine or marijuana. They were less likely to be treated in residential programs or methadone maintenance programs and fewer reported heroin use or injection drug use. Guided by the multilevel open systems framework, the study examines key issues of Proposition 36 that influence treatment systems and outcomes and empirically identifies ""best practice"" approaches in treating drug-abusing offenders.-Treating drug-abusing offenders. Initial findings from a five-county study on the impact of California's Proposition 36 on the treatment system and patient outcomes.",0
"An economic evaluation of five outpatient adolescent treatment approaches (12 total site-by-conditions) was conducted. The economic cost of each of the 12 site-specific treatment conditions was determined by the Drug Abuse Treatment Cost Analysis Program (DATCAP). Economic benefits of treatment were estimated by first monetizing a series of treatment outcomes and then analyzing the magnitude of these monetized outcomes from baseline through the 12-month follow-up. The average economic costs ranged from $90 to $313 per week and from $839 to $3,279 per episode. Relative to the quarter before intake, the average quarterly cost to society for the next 12 months (including treatment costs) significantly declined in 4 of the 12 site-by-treatment conditions, remained unchanged in 6 conditions, and increased in 2 treatment conditions (both in the same site). These results suggest that some types of substance-abuse intervention for adolescents can reduce social costs immediately after treatment.-Outpatient marijuana treatment for adolescents. Economic evaluation of a multisite field experiment.",0
"We conducted a descriptive study to assess the relationship between increasing age and the reporting of melanoma signs/symptoms in 634 hospital-based and 624 population-based incident cases of melanoma. Multivariate logistic regression was used to evaluate the relationship between older age (&gt; or = 50 years) and the reporting of melanoma signs/symptoms. Older patients were less likely to report itching and change in elevation of their lesions (P &lt; 0.05). Change in color was also less likely to be reported by older patients, although not statistically significant. Ulceration of the lesion was reported significantly more by older patients (P &lt; 0.05). Older individuals may be less likely to report itching and change in elevation/color of their lesions, but more likely to report ulceration, a symptom associated with advanced disease and poor prognosis. Further research is necessary to provide a better understanding of the development of melanoma in older populations so that new strategies can be explored to improve early detection in this age group.-Signs and symptoms of melanoma in older populations.",0
Rejoinder: Matched Pairs and the Future of Cluster-Randomized Experiments,1
"To investigate the association between stress at work and the metabolic syndrome. [table: see text]. Prospective cohort study investigating the association between work stress and the metabolic syndrome. 10 308 men and women, aged 35-55, employed in 20 London civil service departments at baseline (the Whitehall II study); follow-up was an average of 14 years. Work stress based on the iso-strain model, measured on four occasions (1985-99). Biological measures of the metabolic syndrome, based on the National Cholesterol Education Program definition, measured in 1997-9. A dose-response relation was found between exposure to work stressors over 14 years and risk of the metabolic syndrome, independent of other relevant risk factors. Employees with chronic work stress (three or more exposures) were more than twice as likely to have the syndrome than those without work stress (odds ratio adjusted for age and employment grade 2.25, 95% confidence interval 1.31 to 3.85). Stress at work is an important risk factor for the metabolic syndrome. The study provides evidence for the biological plausibility of the link between psychosocial stressors from everyday life and heart disease.-Chronic stress at work and the metabolic syndrome: prospective study.",0
Correcting a significance test for clustering,1
"Multilevel mediation analyses play an essential role in helping researchers develop, probe, and refine theories of action underlying interventions and document how interventions impact outcomes. However, little is known about how to plan studies with sufficient power to detect such multilevel mediation effects. In this study, we describe how to prospectively estimate power and identify sufficient sample sizes for experiments intended to detect multilevel mediation effects. We outline a simple approach to estimate the power to detect mediation effects with individual- or cluster-level mediators using summary statistics easily obtained from empirical literature and the anticipated magnitude of the mediation effect. We draw on a running example to illustrate several different types of mediation and provide an accessible introduction to the design of multilevel mediation studies. The power formulas are implemented in the R package PowerUpR and the PowerUp software ( causalevaluation.org ).-Sample Size Planning for Cluster-Randomized Interventions Probing Multilevel Mediation.",1
When and how should we cluster and cross over: methodological and ethical issues.,1
"This article is part of a series of papers examining ethical issues in cluster randomized trials (CRTs) in health research. In the introductory paper in this series, we set out six areas of inquiry that must be addressed if the CRT is to be set on a firm ethical foundation. This paper addresses the first of the questions posed, namely, who is the research subject in a CRT in health research? The identification of human research subjects is logically prior to the application of protections as set out in research ethics and regulation. Aspects of CRT design, including the fact that in a single study the units of randomization, experimentation, and observation may differ, complicate the identification of human research subjects. But the proper identification of human research subjects is important if they are to be protected from harm and exploitation, and if research ethics committees are to review CRTs efficiently.We examine the research ethics literature and international regulations to identify the core features of human research subjects, and then unify these features under a single, comprehensive definition of human research subject. We define a human research subject as any person whose interests may be compromised as a result of interventions in a research study. Individuals are only human research subjects in CRTs if: (1) they are directly intervened upon by investigators; (2) they interact with investigators; (3) they are deliberately intervened upon via a manipulation of their environment that may compromise their interests; or (4) their identifiable private information is used to generate data. Individuals who are indirectly affected by CRT study interventions, including patients of healthcare providers participating in knowledge translation CRTs, are not human research subjects unless at least one of these conditions is met.-Who is the research subject in cluster randomized trials in health research?",1
"When a clustered randomized controlled trial is considered at a design stage of a clinical trial, it is useful to consider the consequences of unequal cluster size (i.e., sample size per cluster). Furthermore, the assumption of independence of observations within cluster does not hold, of course, because the subjects share the same cluster. Moreover, when the clustered outcomes are binary, a mixed effect logistic regression model is applicable. This article compares the performance of a maximum likelihood estimation of the mixed effects logistic regression model with equal and unequal cluster sizes. This was evaluated in terms of type I error rate, power, bias, and standard error through computer simulations that varied treatment effect, number of clusters, and intracluster correlation coefficients. The results show that the performance of the mixed effects logistic regression model is very similar, regardless of inequality in cluster size. This is illustrated using data from the Prevention Of Suicide in Primary care Elderly: Collaborative Trial (PROSPECT) study.-Performance of a mixed effects logistic regression model for binary outcomes with unequal cluster size.",1
"Experimental studies of prevention programs often randomize clusters of individuals rather than individuals to treatment conditions. When the correlation among individuals within clusters is not accounted for in statistical analysis, the standard errors are biased, potentially resulting in misleading conclusions about the significance of treatment effects. This study demonstrates the generalized estimating equations (GEE) method, focusing specifically on the GEE-independent method, to control for within-cluster correlation in regression models with either continuous or binary outcomes. The GEE-independent method yields consistent and robust variance estimates. Data from project DARE, a youth substance abuse prevention program, are used for illustration.-Analysis of prevention program effectiveness with clustered data using generalized estimating equations.",1
"In many public health and medical research settings, information on key covariates may not be readily available or too expensive to gather for all individuals in the study. In such settings, the two-phase design provides a way forward by first stratifying an initial (large) phase I sample on the basis of covariates readily available (including, possibly, the outcome), and sub-sampling participants at phase II to collect the expensive measure(s). When the outcome of interest is binary, several methods have been proposed for estimation and inference for the parameters of a logistic regression model, including weighted likelihood, pseudo-likelihood and maximum likelihood. Although these methods yield consistent estimation and valid inference, they do so solely on the basis of the phase I stratification and the detailed covariate information obtained at phase II. Moreover, they ignore any additional information that is readily available at phase I but was not used as part of the stratified sampling design. Motivated by the potential for efficiency gains, especially concerning parameters corresponding to the additional phase I covariates, we propose a novel augmented pseudo-likelihood estimator for two-phase studies that makes use of all available information. In contrast to recently-proposed weighted likelihood-based methods that calibrate to the influence function of the model of interest, the methods we propose do not require the development of additional models and, therefore, enjoy a degree of robustness. In addition, we expand the broader framework for pseudo-likelihood based estimation and inference to permit link functions for binary regression other than the logit link. Comprehensive simulations, based on a one-time cross sectional survey of 82,887 patients undergoing anti-retroviral therapy in Malawi between 2005 and 2007, illustrate finite sample properties of the proposed methods and compare their performance competing approaches. The proposed method yields the lowest standard errors when the model is correctly specified. Finally, the methods are applied to a large implementation science project examining the effect of an enhanced community health worker program to improve adherence to WHO guidelines for at least four antenatal visits, in Dar es Salaam, Tanzania.-Augmented pseudo-likelihood estimation for two-phase studies.",0
"In experimental research, it is not uncommon to assign clusters to conditions. When analysing the data of such cluster-randomized trials, a multilevel analysis should be applied in order to take into account the dependency of first-level units (i.e., subjects) within a second-level unit (i.e., a cluster). Moreover, the multilevel analysis can handle covariates on both levels. If a first-level covariate is involved, usually the within-cluster effect of this covariate will be estimated, implicitly assuming the contextual effect to be equal. However, this assumption may be violated. The focus of the present simulation study is the effects of ignoring the inequality of the within-cluster and contextual covariate effects on parameter and standard error estimates of the treatment effect, which is the parameter of main interest in experimental research. We found that ignoring the inequality of the within-cluster and contextual effects does not affect the estimation of the treatment effect or its standard errors. However, estimates of the variance components, as well as standard errors of the constant, were found to be biased.-Robustness of parameter and standard error estimates against ignoring a contextual effect of a subject-level covariate in cluster-randomized trials.",1
"In primary care research interventional studies often address organisational changes or educational interventions, for example, in the context of guideline implementation. For pragmatic reasons randomisation is often conducted at practice level instead of at the individual patient level. Patients from one practice form a cluster, thus violating the basic assumption of independent patient samples. Hence an increased number of participants and a more complex analysis are required. Using the example of two cluster randomised trials the present article provides insights into the advantages and disadvantages of cluster randomisation as well as its practical significance for the planning and analysis of cluster randomised trials.-[Cluster randomised trials: an important method in primary care research].",1
"Cluster-randomized trials represent an important experimental design, supplementing ordinary randomized clinical trials. They are particularly relevant when evaluating interventions at the level of clinic, hospital, district or region. They are necessary when it is not feasible to randomize individual patients, and desirable when there may be contamination between clusters. But they also carry serious design and analysis implications, and the use of clusters as the unit of randomization must be justified. Sample sizes will usually need to be greatly increased, an adequate number of clusters is essential, and the statistical analysis must allow for the cluster design. And one should rigorously guard against selection bias.-Cluster-randomized trials.",1
"When calculating sample size or power for stepped wedge or other types of longitudinal cluster randomized trials, it is critical that the planned sampling structure be accurately specified. One common assumption is that participants will provide measurements in each trial period, that is, a closed cohort, and another is that each participant provides only one measurement during the course of the trial. However some studies have an ""open cohort"" sampling structure, where participants may provide measurements in variable numbers of periods. To date, sample size calculations for longitudinal cluster randomized trials have not accommodated open cohorts. Feldman and McKinlay (1994) provided some guidance, stating that the participant-level autocorrelation could be varied to account for the degree of overlap in different periods of the study, but did not indicate precisely how to do so. We present sample size and power formulas that allow for open cohorts and discuss the impact of the degree of ""openness"" on sample size and power. We consider designs where the number of participants in each cluster will be maintained throughout the trial, but individual participants may provide differing numbers of measurements. Our results are a unification of closed cohort and repeated cross-sectional sample results of Hooper et al (2016), and indicate precisely how participant autocorrelation of Feldman and McKinlay should be varied to account for an open cohort sampling structure. We discuss different types of open cohort sampling schemes and how open cohort sampling structure impacts on power in the presence of decaying within-cluster correlations and autoregressive participant-level errors.-Sample size and power calculations for open cohort longitudinal cluster randomized trials",3
"Mathematical and computational models for infectious diseases are increasingly used to support public-health decisions; however, their reliability is currently under debate. Real-time forecasts of epidemic spread using data-driven models have been hindered by the technical challenges posed by parameter estimation and validation. Data gathered for the 2009 H1N1 influenza crisis represent an unprecedented opportunity to validate real-time model predictions and define the main success criteria for different approaches. We used the Global Epidemic and Mobility Model to generate stochastic simulations of epidemic spread worldwide, yielding (among other measures) the incidence and seeding events at a daily resolution for 3,362 subpopulations in 220 countries. Using a Monte Carlo Maximum Likelihood analysis, the model provided an estimate of the seasonal transmission potential during the early phase of the H1N1 pandemic and generated ensemble forecasts for the activity peaks in the northern hemisphere in the fall/winter wave. These results were validated against the real-life surveillance data collected in 48 countries, and their robustness assessed by focusing on 1) the peak timing of the pandemic; 2) the level of spatial resolution allowed by the model; and 3) the clinical attack rate and the effectiveness of the vaccine. In addition, we studied the effect of data incompleteness on the prediction reliability. Real-time predictions of the peak timing are found to be in good agreement with the empirical data, showing strong robustness to data that may not be accessible in real time (such as pre-exposure immunity and adherence to vaccination campaigns), but that affect the predictions for the attack rates. The timing and spatial unfolding of the pandemic are critically sensitive to the level of mobility data integrated into the model. Our results show that large-scale models can be used to provide valuable real-time forecasts of influenza spreading, but they require high-performance computing. The quality of the forecast depends on the level of data integration, thus stressing the need for high-quality data in population-based models, and of progressive updates of validated available empirical knowledge to inform these models.-Real-time numerical forecast of global epidemic spreading: case study of 2009 A/H1N1pdm.",0
"ROC curves are a popular method for displaying sensitivity and specificity of a continuous diagnostic marker, X, for a binary disease variable, D. However, many disease outcomes are time dependent, D(t), and ROC curves that vary as a function of time may be more appropriate. A common example of a time-dependent variable is vital status, where D(t) = 1 if a patient has died prior to time t and zero otherwise. We propose summarizing the discrimination potential of a marker X, measured at baseline (t = 0), by calculating ROC curves for cumulative disease or death incidence by time t, which we denote as ROC(t). A typical complexity with survival data is that observations may be censored. Two ROC curve estimators are proposed that can accommodate censored data. A simple estimator is based on using the Kaplan-Meier estimator for each possible subset X &gt; c. However, this estimator does not guarantee the necessary condition that sensitivity and specificity are monotone in X. An alternative estimator that does guarantee monotonicity is based on a nearest neighbor estimator for the bivariate distribution function of (X, T), where T represents survival time (Akritas, M. J., 1994, Annals of Statistics 22, 1299-1327). We present an example where ROC(t) is used to compare a standard and a modified flow cytometry measurement for predicting survival after detection of breast cancer and an example where the ROC(t) curve displays the impact of modifying eligibility criteria for sample size and power in HIV prevention trials.-Time-dependent ROC curves for censored survival data and a diagnostic marker.",0
"Disclosing financial interests to potential research participants during the informed consent process is one strategy for managing conflicts of interest. Given that clinical research coordinators are typically charged with administering the informed consent process, it is critical to understand their experiences, attitudes and beliefs regarding the disclosure of financial interests in research. To understand the role of clinical research coordinators in disclosing financial interests in research, and potential barriers to such disclosures. We developed a survey designed to measure clinical research coordinators' awareness of financial interests in clinical research, previous experience with disclosing financial interests, comfort with answering questions about financial interests and barriers to disclosing financial interests to potential research participants. Next we conducted cognitive interviews with 10 clinical research coordinators to assess understandability and content validity and to further refine the survey. We then administered the survey to clinical research coordinators attending the 2006 Global Conference of the Association of Clinical Research Professionals. Among 300 clinical research coordinators who completed the survey, there was a general awareness of financial interests in research. Forty-one percent reported disclosing such financial interests to potential research participants, and 28% reported being asked about them. Greater comfort in responding to questions about financial interests was associated with previous experience with disclosure, previous experience answering questions about financial interests, and greater length of time obtaining informed consent. Respondents indicated that there were barriers to disclosure, including lack of information (76%) and that participants would not understand disclosures (26%). Possible sample bias due to using a convenience sample. Making information about financial interests in research readily available to clinical research coordinators, as well as providing education and training, should facilitate the disclosure of financial interests in research to potential research participants during the informed consent process.-Perspectives of clinical research coordinators on disclosing financial conflicts of interest to potential research participants.",0
"Length-biased time-to-event data are commonly encountered in applications ranging from epidemiological cohort studies or cancer prevention trials to studies of labor economy. A longstanding statistical problem is how to assess the association of risk factors with survival in the target population given the observed length-biased data. In this article, we demonstrate how to estimate these effects under the semiparametric Cox proportional hazards model. The structure of the Cox model is changed under length-biased sampling in general. Although the existing partial likelihood approach for left-truncated data can be used to estimate covariate effects, it may not be efficient for analyzing length-biased data. We propose two estimating equation approaches for estimating the covariate coefficients under the Cox model. We use the modern stochastic process and martingale theory to develop the asymptotic properties of the estimators. We evaluate the empirical performance and efficiency of the two methods through extensive simulation studies. We use data from a dementia study to illustrate the proposed methodology, and demonstrate the computational algorithms for point estimates, which can be directly linked to the existing functions in S-PLUS or R.-Statistical methods for analyzing right-censored length-biased data under cox model.",0
"Small number of clusters and large variation of cluster sizes commonly exist in cluster-randomized trials (CRTs) and are often the critical factors affecting the validity and efficiency of statistical analyses. F tests are commonly used in the generalized linear mixed model (GLMM) to test intervention effects in CRTs. The most challenging issue for the approximate Wald F test is the estimation of the denominator degrees of freedom (DDF). Some DDF approximation methods have been proposed, but their small sample performances in analysing binary outcomes in CRTs with few heterogeneous clusters are not well studied. The small sample performances of five DDF approximations for the F test are compared and contrasted under CRT frameworks with simulations. Specifically, we illustrate how the intraclass correlation (ICC), sample size, and the variation of cluster sizes affect the type I error and statistical power when different DDF approximation methods in GLMM are used to test intervention effect in CRTs with binary outcomes. The results are also illustrated using a real CRT dataset. Our simulation results suggest that the Between-Within method maintains the nominal type I error rates even when the total number of clusters is as low as 10 and is robust to the variation of the cluster sizes. The Residual and Containment methods have inflated type I error rates when the cluster number is small (&lt;30) and the inflation becomes more severe with increased variation in cluster sizes. In contrast, the Satterthwaite and Kenward-Roger methods can provide tests with very conservative type I error rates when the total cluster number is small (&lt;30) and the conservativeness becomes more severe as variation in cluster sizes increases. Our simulations also suggest that the Between-Within method is statistically more powerful than the Satterthwaite or Kenward-Roger method in analysing CRTs with heterogeneous cluster sizes, especially when the cluster number is small. We conclude that the Between-Within denominator degrees of freedom approximation method for F tests should be recommended when the GLMM is used in analysing CRTs with binary outcomes and few heterogeneous clusters, due to its type I error properties and relatively higher power.-Comparing denominator degrees of freedom approximations for the generalized linear mixed model in analyzing binary outcome in small sample cluster-randomized trials.",1
"Communities differ in the prevalence of various health behaviors, but it is not known to what extent these differences are due to ""different types"" of people living in them. We used data from the evaluation of the Henry J Kaiser Family Foundation Community Health Promotion Grant Program to study individual-level and community-level variation in health behaviors for 15 communities. Our results show (1) there was significant variation among these communities in prevalences of smoking, consumption of alcohol and dietary fat, and use of seatbelts; (2) these differences persisted after control for demographic, health status, and other health behavioral characteristics of the people in the communities; (3) the community effect on a particular person's behavior, as represented by R2, was very small (less than 1%); and (4) the adjusted differences in prevalences among communities were potentially large (for example, a 7 percentage point difference in the probability of smoking). Unique features of communities may influence health behaviors. These findings affirm the potential importance of contextual effects on individual health behavior and thus support the theory that changing the community environment may offer effective ways to change individual health behavior.-Do communities differ in health behaviors?",1
"In this paper, we evaluate the usefulness of local Bayes factors as a tool for spatial cluster detection. In particular, we consider whether local Bayes factors from models with a fixed, but overly large number of clusters can consistently identify the evidence for clustering for a variety of prior specifications for the cluster locations. We also investigate the robustness of the local Bayes factor to the number of clusters included in the model. We explore the impacts of prior choice for cluster location and the number of clusters on posterior inference for disease rates. We conduct the comparison by analysing data on 1990 breast cancer incidence in Wisconsin.-Impact of prior choice on local Bayes factors for cluster detection.",0
Design and analysis of group-randomized trials: A review of recent developments,1
This paper deals with the competing risks model as a special case of a multi-state model. The properties of the model are reviewed and contrasted to the so-called latent failure time approach. The relation between the competing risks model and right-censoring is discussed and regression analysis of the cumulative incidence function briefly reviewed. Two real data examples are presented and a guide to the practitioner is given.-Competing risks as a multi-state model.,0
"The VITamin D and OmegA-3 TriaL (VITAL) is a completed randomized, placebo-controlled trial of vitamin D3 (2000?IU/day) and marine omega-3 (1?g/day) supplements in the primary prevention of cancer and cardiovascular disease. Here we examine baseline and change in 25-hydroxyvitamin D (25(OH)D) and related biomarkers with randomized treatment and by clinical factors. Baseline 25(OH)D was measured in 15,804 participants (mean age 68?years.; 50.8% women; 15.7% African Americans) and in 1660 1-year follow-up samples using liquid chromatography-tandem mass spectrometry and chemiluminescence. Calcium and parathyroid hormone (iPTH) were measured by chemiluminescence and spectrophotometry respectively. Mean baseline total 25(OH)D (ng/mL???SD) was 30.8???10.0?ng/mL, and correlated inversely with iPTH (r?=?-0.28), p?&lt;?.001. After adjusting for clinical factors, 25(OH)D (ng/mL???SE) was lower in men vs women (29.7???0.30 vs 31.4???0.30, p?&lt;?.0001) and in African Americans vs whites (27.9???0.29 vs 32.5???0.22, p?&lt;?.0001). It was also lower with increasing BMI, smoking, and latitude, and varied by season. Mean 1-year 25(OH)D increased by 11.9?ng/mL in the active group and decreased by 0.7?ng/mL in placebo. The largest increases were noted among individuals with low baseline and African Americans. Results were similar for chemiluminescent immunoassay. Mean calcium was unchanged, and iPTH decreased with treatment. In VITAL, baseline 25(OH)D varied by clinical subgroups, was lower in men and African Americans. Concentrations increased with vitamin D supplementation, with the greatest increases in those with lower baseline 25(OH)D. The seasonal trends in 25(OH)D, iPTH, and calcium may be relevant when interpreting 25(OH)D levels for clinical treatment decisions. VITAL ClinicalTrials.gov number NCT01169259.-Serum 25-hydroxyvitamin D in the VITamin D and OmegA-3 TriaL (VITAL): Clinical and demographic characteristics associated with baseline and change with randomized vitamin D treatment.",0
"Three-level data occur frequently in behaviour and medical sciences. For example, in a multi-centre trial, subjects within a given site are randomly assigned to treatments and then studied over time. In this example, the repeated observations (level-1) are nested within subjects (level-2) who are nested within sites (level-3). Similarly, in twin studies, repeated measurements (level-1) are taken on each twin (level-2) within each twin pair (level-3). A three-level mixed-effects regression model is described here. Random effects at the second and third level are included in the model. Additionally, both proportional odds and non-proportional odds models are developed. The latter allows the effects of explanatory variables to vary across the cumulative logits of the model. A maximum marginal likelihood (MML) solution is described and Gauss-Hermite numerical quadrature is used to integrate over the distribution of random effects. The random effects are normally distributed in this instance. Features of this model are illustrated using data from a school-based smoking prevention trial and an Alzheimer's disease clinical trial.-A mixed-effects regression model for three-level ordinal response data.",1
"Meta-analysis of trans-ethnic genome-wide association studies (GWAS) has proven to be a practical and profitable approach for identifying loci that contribute to the risk of complex diseases. However, the expected genetic effect heterogeneity cannot easily be accommodated through existing fixed-effects and random-effects methods. In response, we propose a novel random effect model for trans-ethnic meta-analysis with flexible modeling of the expected genetic effect heterogeneity across diverse populations. Specifically, we adopt a modified random effect model from the kernel regression framework, in which genetic effect coefficients are random variables whose correlation structure reflects the genetic distances across ancestry groups. In addition, we use the adaptive variance component test to achieve robust power regardless of the degree of genetic effect heterogeneity. Simulation studies show that our proposed method has well-calibrated type I error rates at very stringent significance levels and can improve power over the traditional meta-analysis methods. We reanalyzed the published type 2 diabetes GWAS meta-analysis (Consortium et al., 2014) and successfully identified one additional SNP that clearly exhibits genetic effect heterogeneity across different ancestry groups. Furthermore, our proposed method provides scalable computing time for genome-wide datasets, in which an analysis of one million SNPs would require less than 3 hours.-A novel random effect model for GWAS meta-analysis and its application to trans-ethnic meta-analysis.",0
"Methods have recently been developed for the estimation and testing of mother-child correlations. In this report, these methods are extended to the general case of assessing interclass correlations where multiple replicates are allowed for each of the two classes of individuals under consideration. An algorithm is presented for obtaining the maximal likelihood estimator and an asymptotic test of significance is provided. In addition, a computationally convenient significance test is derived based on the pairwise estimator whereby one estimates the effective number of degrees of freedom in a family as a function of the number of replicates and the estimated intraclass correlation for each of the two types of individuals and sums up the effective degrees of freedom over all families in the sample. These methods are shown to be applicable to more general situations than the analysis of familial data, including the assessment of correlations between two variables measured at one point in time or the same variable measured at two points in time.-On the estimation and testing of interclass correlations: the general case of multiple replicates for each variable.",1
"Clinical trials aimed at shortening the time-to-discharge need to have rational and easily understood effect size estimates for health care management organizations. A natural choice is a scale model, where the distribution of time to discharge on one treatment is assumed to be the same as rho times that on the other. If, for example rho=0.6, then one treatment is associated with a 40% reduction in discharge time. In designing and analyzing these studies, we need to have the capability to accommodate right censored data, as it is plausible that some patients may never meet the discharge criteria, even though they do get discharged. Utilizing the ideas of Hodges and Lehmann, to provide methods for analysis of trials aimed at shortening hospital discharge times, using point and interval estimates of scale parameters based on the Gehan generalization of the Mann-Whitney-Wilcoxon test, which accommodates right censoring for situations where patients never meet discharge criteria (+infinity). For every value of rho &gt; 0, we shall test the null hypothesis that the distribution of discharge times on one treatment is the same rho times that on the other. The values of rho that we fail to reject form the confidence interval for the true rho. The methods were developed and applied to a real clinical trial for times to meet the three objective discharge criteria in knee replacement surgery for two post-operative pain control strategies (usual care plus a perineural infusion of either placebo or 0.2% ropivocaine, until the morning following surgery). Based on 48 randomized patients, the point estimate (95% confidence limits) for rho was 0.47 (0.32-0.67), favoring ropivocaine. The methods cannot as yet be applied to group sequential designs or studies with more than two treatments. This methodology is an effective way to analyze two-arm trials involving continuous hospital discharge time data.-Applying Hodges-Lehmann scale parameter estimates to hospital discharge times.",0
"Cluster randomized controlled trials (CRCTs) often require a large number of clusters in order to detect small effects with high probability. However, there are contexts where it may be possible to design a CRCT with a much smaller number of clusters (10 or fewer) and still detect meaningful effects. The objective is to offer recommendations for best practices in design and analysis for small CRCTs. I use simulations to examine alternative design and analysis approaches. Specifically, I examine (1) which analytic approaches control Type I errors at the desired rate, (2) which design and analytic approaches yield the most power, (3) what is the design effect of spurious correlations, and (4) examples of specific scenarios under which impacts of different sizes can be detected with high probability. I find that (1) mixed effects modeling and using Ordinary Least Squares (OLS) on data aggregated to the cluster level both control the Type I error rate, (2) randomization within blocks is always recommended, but how best to account for blocking through covariate adjustment depends on whether the precision gains offset the degrees of freedom loss, (3) power calculations can be accurate when design effects from small sample, spurious correlations are taken into account, and (4) it is very difficult to detect small effects with just four clusters, but with six or more clusters, there are realistic circumstances under which small effects can be detected with high probability.-Design and Analysis Considerations for Cluster Randomized Controlled Trials That Have a Small Number of Clusters.",1
"The objective of this study was to assess the value of research of the RxPONDER study, an ongoing comparative effectiveness RCT designed to evaluate a 21-gene profile in early stage, node-positive breast cancer. We developed a disease-based decision-analytic model to compare use of the 21-gene profile versus standard care. Key clinical data were derived from SWOG-8814, an RCT of chemotherapy in lymph node-positive breast cancer. Other model parameters were obtained from published sources. Probabilistic simulations and value of information calculations were used to assess the expected value of sample information (EVSI) and the expected value of sample parameter information (EVSPI). The cost of the RxPONDER trial is expected to be at least $27 million. The expected value of research of the RxPONDER trial ranged from $450 million to $1 billion, representing a return of 17 to 39 times the projected cost of the trial. The primary objective of RxPONDER, to assess survival, had the largest estimated value relative to other model inputs. The value of RxPONDER increased by $50 million to $100 million after stakeholder input on additional data collection. The RxPONDER study appears to represent a good investment of public research funds. Stakeholder engagement and assessment of the return on investment should be considered to optimize and quantify the value of comparative effectiveness studies.-The value of comparative effectiveness research: projected return on investment of the RxPONDER trial (SWOG S1007).",0
"To determine the validity of self-reported hepatitis B virus (HBV) and hepatitis C virus (HCV) in HIV-infected injection drug users (IDUs) vs. nonIDUs. A cross-sectional study was performed among HIV-infected IDUs and nonIDUs in the Penn Center for AIDS Research (CFAR) Database. Self-reported past HBV, HCV, and serostatus were obtained from the CFAR Database. Among 970 subjects (798 nonIDUs; 172 IDUs), there was no difference in sensitivity of self-reported HBV between nonIDUs (27% [95/346]; 95% CI, 23%-32%) and IDUs (26% [31/117]; 95% CI, 19%-35%; P&gt;0.5), but specificity was greater among nonIDUs (96% [360/374; 95% CI, 94%-98%] vs. 78% [28/36; 95% CI, 61%-90%]; P&lt;0.001). Sensitivity of self-reported HCV was greater among IDUs (78% [101/130; 95% CI, 70%-85%] vs. 62% [47/76; 95% CI, 50%-73%]; P=0.02), but there was no difference in specificity (97% [626/643]; 95% CI, 96%-98% for nonIDUs vs. 93% [26/28]; 95% CI, 76%-99%] for IDUs; P=0.2). The sensitivity of self-reported HBV and HCV compared to actual serostatus are not sufficiently high enough to warrant their use to estimate the prevalence and incidence of these infections.-Self-reported hepatitis B and C virus infections had low sensitivity among HIV-infected patients.",0
"Researchers should consider five questions before starting a stepped wedge trial. Why are you planning one? Researchers sometimes think that stepped wedge trials are useful when there is little doubt about the benefit of the intervention being tested. However, if the primary reason for an intervention is to measure its effect, without equipoise there is no ethical justification for delaying implementation in some clusters. By contrast, if you are undertaking pragmatic research, where the primary reason for rolling out the intervention is for it to exert its benefits, and if phased implementation is inevitable, a stepped wedge trial is a valid option and provides better evidence than most non-randomized evaluations. What design will you use? Two common stepped wedge designs are based on the recruitment of a closed or open cohort. In both, individuals may experience both control and intervention conditions and you should be concerned about carry-over effects. In a third, continuous-recruitment, short-exposure design, individuals are recruited as they become eligible and experience either control or intervention condition, but not both. How will you conduct the primary analysis? In stepped wedge trials, control of confounding factors through secular variation is essential. 'Vertical' approaches preserve randomization and compare outcomes between randomized groups within periods. 'Horizontal' approaches compare outcomes before and after crossover to the intervention condition. Most analysis models used in practice combine both types of comparison. The appropriate analytic strategy should be considered on a case-by-case basis. How large will your trial be? Standard sample size calculations for cluster randomized trials do not accommodate the specific features of stepped wedge trials. Methods exist for many stepped wedge designs, but simulation-based calculations provide the greatest flexibility. In some scenarios, such as when the intracluster correlation coefficient is moderate or high, or the cluster size is large, a stepped wedge trial may require fewer clusters than a parallel cluster trial. How will you report your trial? Stepped wedge trials are currently challenging to report using CONSORT principles. Researchers should consider how to demonstrate balance achieved by randomization and how to describe trends for outcomes in both intervention and control clusters.-Five questions to consider before conducting a stepped wedge trial.",3
Proceedings of the University of Pennsylvania 6th annual conference on statistical issues in clinical trials: Dynamic treatment regimes.,0
"Group randomized trial design is common in cancer prevention and health promotion research with intervention development. Several methods have been developed to handle the design and analytical issues for group randomized trial including the intraclass correlation coefficient. The widely used methods for the sample size calculation for the group randomized trial assume equal sizes across groups. In practice this assumption often fails and group randomized trial studies suffer from considerably lower statistical power than as planned. A few studies have developed sample size calculation methods for unequal group sizes, but most of them are limited to continuous outcomes. In this study, we develop a method for sample size calculation for group randomized trial studies with unequal group sizes based on Monte Carlo simulation in the mixed effect model framework. This approach incorporates the variation of group sizes and can be applied to group randomized trials with different types of outcomes. Further, it is easy to implement and can be applied to most commonly used group randomized trial designs such as pre-and-post cross-sectional and cohort study designs. We demonstrate the application of the proposed approach to two-arm group randomized trial studies with continuous and binary outcomes through simulations and analysis of a real group randomized trial dataset.-Sample size calculations for group randomized trials with unequal group sizes through Monte Carlo simulations.",1
"Two features commonly exhibited by randomized trials of health promotion interventions are cluster randomization and stratification. Ignoring correlations between individuals within clusters can lead to an inflated type I error rate and hence a P-value which overstates the significance of the result. This paper compares several methods for analysing categorical data from a stratified cluster randomized trial. We propose an extension of a method from survey sampling that uses the design effect to reduce the effective sample size. We compare this with three methods from Zhang and Boos that extend the standard Cochran-Mantel-Haenszel (CMH) statistic by using appropriate covariance matrices, and with a bootstrap method. The comparison is based on empirical type I error rates from a simulation study, in which the number of clusters randomized is small, as in most public health intervention studies. The method that performs consistently well is one of the Zhang and Boos extensions of the standard CMH statistic.-Comparison of tests for categorical data from a stratified cluster randomized trial.",1
"Studies evaluating the efficacy of HIV/AIDS interventions often involve the random assignment of groups of participants or the treatment of participants in groups. These studies require analytic methods that take within-group correlation into account. We reviewed published studies to determine the extent to which within-group correlation was dealt with properly. We reviewed group-randomized trials (GRTs) and individually randomized group treatment (IRGT) trials published in HIV/AIDS and general public health journals 2005-2009. At least two of the authors reviewed each article, recording descriptive characteristics, sample size estimation methods, analytic methods, and judgments about whether the methods took intraclass correlation into account properly. Of those articles including sufficient information to judge whether analytic methods were correct, only 24% used only appropriate methods for dealing with the intraclass correlation. The percentages differed substantially for GRTs (41.7%) and IRGT trials (8.0%). Most of the articles (69.2%) also made no mention of a priori sample size estimation. A majority of the articles in our review reported analyses ignoring the intraclass correlation. This practice may result in underestimated variance, inappropriately small P values, and incorrect conclusions about the effectiveness of interventions. Previous trials that were analyzed incorrectly need to be re-analyzed, and future trials should be designed and analyzed with appropriate methods. Also, journal reviewers and editors need to be aware of the special requirements for design and analysis of GRTs and IRGT trials and judge the quality of articles reporting on such trials according to appropriate standards.-Ignoring the group in group-level HIV/AIDS intervention trials: a review of reported design and analytic methods.",2
"Although randomized controlled trials are often a gold standard for determining intervention effects, in the area of practice-based research (PBR), there are many situations in which individual randomization is not possible. Alternative approaches to evaluating interventions have received increased attention, particularly those that can retain elements of randomization such that they can be considered ""controlled"" trials. Methodological design elements and practical implementation considerations for two quasi-experimental design approaches that have considerable promise in PBR settings--the stepped-wedge design, and a variant of this design, a wait-list cross-over design, are presented along with a case study from a recent PBR intervention for patients with diabetes. PBR-relevant design features include: creation of a cohort over time that collects control data but allows all participants (clusters or patients) to receive the intervention; staggered introduction of clusters; multiple data collection points; and one-way cross-over into the intervention arm. Practical considerations include: randomization versus stratification, training run in phases; and extended time period for overall study completion. Several design features of practice based research studies can be adapted to local circumstances yet retain elements to improve methodological rigor. Studies that utilize these methods, such as the stepped-wedge design and the wait-list cross-over design, can increase the evidence base for controlled studies conducted within the complex environment of PBR.-Quasi-experimental designs in practice-based research settings: design and implementation considerations.",1
"Clustered data are often encountered in biomedical studies, and to date, a number of approaches have been proposed to analyze such data. However, the phenomenon of informative cluster size (ICS) is a challenging problem, and its presence has an impact on the choice of a correct analysis methodology. For example, Dutta and Datta (2015, Biometrics) presented a number of marginal distributions that could be tested. Depending on the nature and degree of informativeness of the cluster size, these marginal distributions may differ, as do the choices of the appropriate test. In particular, they applied their new test to a periodontal data set where the plausibility of the informativeness was mentioned, but no formal test for the same was conducted. We propose bootstrap tests for testing the presence of ICS. A balanced bootstrap method is developed to successfully estimate the null distribution by merging the re-sampled observations with closely matching counterparts. Relying on the assumption of exchangeability within clusters, the proposed procedure performs well in simulations even with a small number of clusters, at different distributions and against different alternative hypotheses, thus making it an omnibus test. We also explain how to extend the ICS test to a regression setting and thereby enhancing its practical utility. The methodologies are illustrated using the periodontal data set mentioned earlier. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-Tests for informative cluster size using a novel balanced bootstrap scheme.",1
"In educational psychology, observational units are oftentimes nested within superordinate groups. Researchers need to account for hierarchy in the data by means of multilevel modeling, but especially in three-level longitudinal models, it is often unclear which sample size is necessary for reliable parameter estimation. To address this question, we generated a population dataset based on a study in the field of educational psychology, consisting of 3000 classrooms (level-3) with 55000 students (level-2) measured at 5 occasions (level-1), including predictors on each level and interaction effects. Drawing from this data, we realized 1000 random samples each for various sample and missing value conditions and compared analysis results with the true population parameters. We found that sampling at least 15 level-2 units each in 35 level-3 units results in unbiased fixed effects estimates, whereas higher-level random effects variance estimates require larger samples. Overall, increasing the level-2 sample size most strongly improves estimation soundness. We further discuss how data characteristics influence parameter estimation and provide specific sample size recommendations.-The Influence of Sample Size on Parameter Estimates in Three-Level Random-Effects Models.",1
"Early detection and effective treatments have dramatically improved breast cancer survivorship, yet the risk of relapse persists even 15 years after the initial diagnosis. It is important to identify prognostic factors for late breast cancer events. The authors investigated time-varying effects of tumor characteristics on breast-cancer-free survival using data on 3,088 breast cancer survivors from 4 US states who participated in a randomized dietary intervention trial in 1995-2006, with maximum follow-up through 15 years (median, 9 years). A piecewise constant penalized spline approach incorporating time-varying coefficients was adopted, allowing for deviations from the proportional hazards assumption. This method is more flexible than standard approaches, provides direct estimates of hazard ratios across time intervals, and is computationally tractable. Having a stage II or III tumor was associated with a 3-fold higher hazard of breast cancer than having a stage I tumor during the first 2.5 years after diagnosis; this hazard ratio decreased to 2.1 after 7.7 years, but higher tumor stage remained a significant risk factor. Similar diminishing effects were found for poorly differentiated tumors. Interestingly, having a positive estrogen receptor status was protective up to 4 years after diagnosis but detrimental after 7.7 years (hazard ratio = 1.5). These results emphasize the importance of careful statistical modeling allowing for possibly time-dependent effects in long-term survivorship studies.-Time-varying effects of prognostic factors associated with disease-free survival in breast cancer.",0
"The large-scale multiple testing problems resulting from the measurement of thousands of genes in microarray experiments have received increasing interest during the past several years. This article describes some commonly used criteria for controlling false positive errors, including familywise error rates, false discovery rates and false discovery proportion rates. Various statistical methods controlling these error rates are described. The advantages and disadvantages of these methods are discussed. These methods are applied to gene expression data from two microarray studies and the properties of these multiple testing procedures are compared.-Multiple testing and its applications to microarrays.",0
Response to: How to design and analyse cluster randomized trials with a small number of clusters? Comment on Leyrat et al.,1
"The genetic variance of a quantitative trait is often controlled by the segregation of multiple interacting loci. Linear model regression analysis is usually applied to estimating and testing effects of these quantitative trait loci (QTL). Including all the main effects and the effects of interaction (epistatic effects), the dimension of the linear model can be extremely high. Variable selection via stepwise regression or stochastic search variable selection (SSVS) is the common procedure for epistatic effect QTL analysis. These methods are computationally intensive, yet they may not be optimal. The LASSO (least absolute shrinkage and selection operator) method is computationally more efficient than the above methods. As a result, it has been widely used in regression analysis for large models. However, LASSO has never been applied to genetic mapping for epistatic QTL, where the number of model effects is typically many times larger than the sample size. In this study, we developed an empirical Bayes method (E-BAYES) to map epistatic QTL under the mixed model framework. We also tested the feasibility of using LASSO to estimate epistatic effects, examined the fully Bayesian SSVS, and reevaluated the penalized likelihood (PENAL) methods in mapping epistatic QTL. Simulation studies showed that all the above methods performed satisfactorily well. However, E-BAYES appears to outperform all other methods in terms of minimizing the mean-squared error (MSE) with relatively short computing time. Application of the new method to real data was demonstrated using a barley dataset.-An empirical Bayes method for estimating epistatic effects of quantitative trait loci.",0
"Screening has become one of our best tools for early detection and prevention of cancer. The group-randomized trial is the most rigorous experimental design for evaluating multilevel interventions. However, identifying the proper sample size for a group-randomized trial requires reliable estimates of intraclass correlation (ICC) for screening outcomes, which are not available to researchers. We present crude and adjusted ICC estimates for cancer screening outcomes for various levels of aggregation (physician, clinic, and county) and provide an example of how these ICC estimates may be used in the design of a future trial. Investigators working in the area of cancer screening were contacted and asked to provide crude and adjusted ICC estimates using the analysis of variance method estimator. Of the 29 investigators identified, estimates were obtained from 10 investigators who had relevant data. ICC estimates were calculated from 13 different studies, with more than half of the studies collecting information on colorectal screening. In the majority of cases, ICC estimates could be adjusted for age, education, and other demographic characteristics, leading to a reduction in the ICC. ICC estimates varied considerably by cancer site and level of aggregation of the groups. Previously, only two articles had published ICCs for cancer screening outcomes. We have complied more than 130 crude and adjusted ICC estimates covering breast, cervical, colon, and prostate screening and have detailed them by level of aggregation, screening measure, and study characteristics. We have also demonstrated their use in planning a future trial and the need for the evaluation of the proposed interval estimator for binary outcomes under conditions typically seen in GRTs.-Intraclass correlation estimates for cancer screening outcomes: estimates and applications in the design of group-randomized cancer screening studies.",1
"Extensions of various non-parametric regression techniques (for example, additive models, trees, MARS) have been devised for right censored survival data. These approaches directly handle the difficulties posed by censoring. However, it is possible to bypass these difficulties by utilizing standard non-parametric regression procedures applied with (say) martingale residuals as outcome. Analytic correspondences between the direct and residual-based approaches have been established for additive models while more qualitative comparisons have been provided for MARS. Here we develop such correspondences for tree-structured regression. In particular, we provide an analytic relationship between logrank and martingale residual sum-of-squares split functions that explains the widely observed similarity of the resultant trees. Further investigation is provided by simulation and an illustrative example using time to AIDS with data deriving from a Western Australian HIV cohort study.-Residual-based tree-structured survival analysis.",0
"Cluster randomization trials randomize groups (called clusters) of subjects (called subunits) between intervention arms, and observations are collected from each subject. In this case, subunits within each cluster share common frailties, so that the observations from subunits of each cluster tend to be correlated. Oftentimes, the outcome of a cluster randomization trial is a time-to-event endpoint with censoring. In this article, we propose a closed form sample size formula for weighted rank tests to compare the marginal survival distributions between intervention arms under cluster randomization with possibly variable cluster sizes. Extensive simulation studies are conducted to evaluate the performance of our sample size formula under various design settings. Real study examples are taken to demonstrate our method.-Sample size calculation for cluster randomization trials with a time-to-event endpoint",1
"This study compares an observational study of diabetes treatment effectiveness to randomized controlled trials to assess their convergent validity. Multivariate models were developed using observational data to describe change in hemoglobin A1c (HbA1c; % unit) and weight (kilograms) after addition of a second-line oral diabetes drug to metformin monotherapy. Randomized trials of these scenarios were systematically identified. The models were used to simulate each trial, and simulated and actual results were compared by linear regression and meta-analysis. Thirty-two randomized trials of second-line diabetes oral therapy were identified. For all outcomes and drugs studied, simulation and actual results correlated (P &lt; 0.001). There were no statistically significant differences between meta-analyzed randomized and simulated results for effect on HbA1c. For effect on weight, results were qualitatively comparable, but for sulfonylureas, the simulated weight gain was nominally greater than seen in the randomized controlled trials. An observational study of diabetes drug effectiveness showed convergent validity with randomized data. This supports cautious use of the observational research to draw conclusions about drug effectiveness in populations not studied in clinical trials. This approach may be useful in other situations where observational and randomized data need integration.-Observational and clinical trial findings on the comparative effectiveness of diabetes drugs showed agreement.",0
Efficient design and sample size calculation for trials with clustered data.,1
"Interventions based in the community can be evaluated by randomising clusters, such as general practices, rather than individuals, as in conventional randomised trials. This increases the sample size needed because of intracluster correlation. To estimate sample size requirements for cluster randomised trials of interventions based in general practice directed at common health problems affecting mothers and infants. Data were collected from a pilot trial of the effect of Citizen's Advice Bureau services involving six general practices. Outcome measures included the Edinburgh postnatal depression score, the Warwick child health and morbidity profile, number of visits to the general practitioner, and two questionnaires delivered at the beginning and end of the study. Intracluster correlation coefficients and inflation factors (the ratio of the sample size required for a cluster randomised trial to that required for an individually randomised trial) were calculated. Intracluster correlation coefficients ranged from 0 (sleeping problems, accidental injury, hospitalisation) to 0.09 (maternal smoking), with most being &lt; 0.04 (for example, maternal depression, breast feeding, general health, minor illness, behavioural problems, and visits to the general practitioner). Assuming 50 cases/practice, cluster randomised trials require sample sizes up to 3 times greater than individually randomised trials for most health outcomes measured. These data enable sample sizes to be estimated for cluster randomised trials into a range of maternal and child health outcomes. Using such a design, approximately 40 practices would be sufficient to evaluate the effect of an intervention on maternal depression, sleeping, and behavioural problems, and non-routine visits to the general practitioner.-Cluster randomised trials in maternal and child health: implications for power and sample size.",1
"Typical advice on the design and analysis of cluster randomized trials (C-RCTs) focuses on allowance for the clustering at the level of the unit of allocation. However often C-RCTs are also organised spatially as may occur in the fields of Public Health and Primary Care where populations may even overlap. We allowed for spatial effects on the error variance by a multiple membership model. These are a form of hierarchical model in which each lower level unit is a member of more than one higher level unit. Membership may be determined through adjacency or through Euclidean distance of centroids or in other ways such as the proportion of overlapping population. Such models may be estimated for Normal, binary and Poisson responses in Stata (v10 or above) as well as in WinBUGS or MLWin. We used this to analyse a dummy trial and two real, previously published cluster-allocated studies (one allocating general practices within one City and the other allocating general practices within one County) to investigate the extent to which ignoring spatial effects affected the estimate of treatment effect, using different methods for defining membership with Akaike's Information Criterion to determine the ""best"" model. The best fitting model included both a fixed North-South gradient and a random cluster effect for the dummy RCT. For one of the real RCTs the best fitting model included both a random practice effect plus a multiple membership spatial term, while for the other RCT the best fitting model ignored the clustering but included a fixed North-South gradient. Alternative models which fitted only slightly less well all included spatial effects in one form or another, with some variation in parameter estimates (greater when less well fitting models were included). These particular results are only illustrative. However, we believe when designing C-RCTs in a primary care setting the possibility of spatial effects should be considered in relation to the intervention and response, as well as any explanatory effect of fixed covariates, together with any implications for sample size and methods for planned analyses.-Spatial effects should be allowed for in primary care and other community-based cluster RCTS.",1
"PartyIntents examines whether portal survey methods could be used to anonymously survey gay and bisexual men about HIV-risk behaviors before and after a weekend party-oriented vacation. The study recruited 97% of eligible men and of these 489 participants 47% completed the follow-up assessment. Approximately one half of the men intended to use illegal drugs over the weekend, and almost 20% thought that they might have anal intercourse and not use a condom. The methodology can be applied and provides useful information about HIV risk at these events, though refinements may be needed to increase the follow-up rates.-PartyIntents: a portal survey to assess gay and bisexual men's risk behaviors at weekend parties.",0
"The stepped wedge cluster randomised trial: rationale, design, analysis, and reporting.",3
Erratum: Robust variance estimation in meta-regression with dependent effect size estimates.,1
Multilevel modeling was used to assess the program characteristics associated with treatment retention among 637 women in 16 residential drug treatment programs in the Drug Abuse Treatment Outcome Study. Women who were pregnant or had dependent children had higher rates of retention in programs in which there were higher percentages of other such women. Longer retention was associated with higher rates of posttreatment abstinence. Bivariate analyses showed that programs with higher proportions of pregnant and parenting women provided more services related to women's needs. The findings support the provision of specialized services and programs for women in order to improve outcomes of drug abuse treatment.-Program variation in treatment outcomes among women in residential drug treatment.,0
Analysis of a trial randomised in clusters.,1
Strategies for improving precision in group-randomized experiments,1
"In phase I clinical trials, the ""3+3"" algorithmic design has been unparalleled in its popularity. The statistical properties of the ""3+3"" design have been studied in the literature either in comparison with other methods or by deriving exact formulae of statistical quantities. However, there is still much to be known about its capabilities of describing and accounting for uncertainties in the observed data. The objective of this study is to provide a probabilistic support for analyzing the heuristic performance of the ""3+3"" design. The operating characteristics of the algorithm are computed under different hypotheses, levels of evidence, and true (or best guessed) toxicity rates. The dose-finding rules are further compared with those generated by the modified toxicity probability interval design and generalized for implementation in all ""A+B"" designs. Our likelihood method is based on the evidential paradigm. Two hypotheses are chosen to correspond to two hypothesized dose-limiting toxicity rates, for example, [Formula: see text]-unsafe versus [Formula: see text]-acceptable. Given observed toxicities per dose, the likelihood-ratio is calculated and compared to a certain k threshold (level of evidence). Under various true toxicities, the probabilities of weak evidence, favoring [Formula: see text] and [Formula: see text], were computed under four sets of hypotheses and several k thresholds. For scenarios where the midpoint of the two hypothesized dose-limiting toxicity rates is around 0.30, and for a threshold of k = 2, the ""3+3"" design has a reduced probability (?0.50) of identifying unsafe doses, but high chances of identifying acceptable doses. For more extreme scenarios targeting a relatively high or relatively low dose-limiting toxicity rate, the ""3+3"" design has no probabilistic support, and therefore, it should not be used. In our comparisons, the likelihood method is in agreement with the modified toxicity probability interval design for the majority of the hypothesized scenarios. Even so, based on the evidential paradigm, a ""3+3"" design is often incapable of providing sufficient levels of evidence of acceptability for doses under reasonable scenarios. Given the small sample size per dose, the levels of evidence are limited in their ability to provide strong evidence favoring either of the hypotheses. In many situations, the ""3+3"" design does not treat enough patients per dose to have confidence in correct dose selection and the safety of the selected/unselected doses. This likelihood method allows consistent inferences to be made at each dose level, and evidence to be quantified regardless of cohort size. The approach can be used in phase I studies for identifying acceptably safe doses, but also for defining stopping rules in other types of dose-finding designs.-A likelihood-based approach for computing the operating characteristics of the 3+3 phase I clinical trial design with extensions to other A+B designs.",0
"Many psychological processes unfold over time, necessitating longitudinal research designs. Longitudinal research poses a host of methodological challenges, foremost of which is participant attrition. Building on Dillman's work, we provide a review of how social influence and relationship research informs retention strategies in longitudinal studies. Objective: We introduce the tailored panel management (TPM) approach, which is designed to establish communal norms that increase commitment to a longitudinal study, and this commitment, in turn, increases response rates and buffers against attrition. Specifically, we discuss practices regarding compensation, communication, consistency, and credibility that increase longer term commitment to panel participation. Research design: Throughout the article, we describe how TPM is being used in a national longitudinal study of undergraduate minority science students. TheScienceStudy is a continuing panel, which has 12 waves of data collected across 6 academic years, with response rates ranging from 70% to 92%. Although more than 90% of participants have either left or graduated from their undergraduate degree program, this highly mobile group of people remains engaged in the study. TheScienceStudy has usable longitudinal data from 96% of the original panel. Conclusion: This article combines social psychological theory, current best practice, and a detailed case study to illustrate the TPM approach to longitudinal data collection. The approach provides guidance for other longitudinal researchers, and advocates for empirical research into longitudinal research methodologies.-Tailored Panel Management: A Theory-Based Approach to Building and Maintaining Participant Commitment to a Longitudinal Study.",0
JPEN Journal Club 45. Cluster Randomization,1
"Simple guidelines for calculating efficient sample sizes in cluster randomized trials with unknown intraclass correlation (ICC) and varying cluster sizes. A simple equation is given for the optimal number of clusters and sample size per cluster. Here, optimal means maximizing power for a given budget or minimizing total cost for a given power. The problems of cluster size variation and specification of the ICC of the outcome are solved in a simple yet efficient way. The optimal number of clusters goes up, and the optimal sample size per cluster goes down as the ICC goes up or as the cluster-to-person cost ratio goes down. The available budget, desired power, and effect size only affect the number of clusters and not the sample size per cluster, which is between 7 and 70 for a wide range of cost ratios and ICCs. Power loss because of cluster size variation is compensated by sampling 10% more clusters. The optimal design for the ICC halfway the range of realistic ICC values is a good choice for the first stage of a two-stage design. The second stage is needed only if the first stage shows the ICC to be higher than assumed. Efficient sample sizes for cluster randomized trials are easily computed, provided the cost per cluster and cost per person are specified.-Calculating sample sizes for cluster randomized trials: we can keep it simple and efficient!",1
"We developed a general procedure for estimating the transmission probability adjusting for covariates when susceptibles are exposed to several infectives concurrently and taking correlation within transmission units into account. The procedure is motivated by a study estimating efficacy of pertussis vaccination based on the secondary attack rate in a rural sub-Saharan community (Niakhar, Senegal) and illustrated with simulations. The procedure is also appropriate to estimate the pairwise transmission probability in transmission studies of live vaccine virus in a collection of transmission units, such as day-care centres or retirement centres. Previously, analyses either excluded transmission units with multiple infectives or ignored co-infectives. Excluding transmission units with multiple infectives is statistically less efficient and ignoring co-infectives can lead to biased estimation. Modelling is carried out by regressing the latent pairwise transmission probability from each infective to a susceptible on covariates and specifying a transmission linkage function linking the latent pairwise transmission probability to the overall transmission probability. Parameters are estimated using Markov chain Monte Carlo methods.-Estimating heterogeneous transmission with multiple infectives using MCMC methods.",0
"Important sources of variation in the spread of HIV in communities arise from overlapping sexual networks and heterogeneity in biological and behavioral risk factors in populations. These sources of variation are not routinely accounted for in the design of HIV prevention trials. In this paper, we use agent-based models to account for these sources of variation. We illustrate the approach with an agent-based model for the spread of HIV infection among men who have sex with men in South Africa. We find that traditional sample size approaches that rely on binomial (or Poisson) models are inadequate and can lead to underpowered studies. We develop sample size and power formulas for community randomized trials that incorporate estimates of variation determined from agent-based models. We conclude that agent-based models offer a useful tool in the design of HIV prevention trials.-Stochastic variation in network epidemic models: implications for the design of community level HIV prevention trials.",0
"The growing interest in community-based approaches to health promotion and disease prevention (HP/DP) has been accompanied by a growing need to evaluate the effectiveness of such programs. Special issues that arise in these evaluation studies include (1) entire communities are assigned to intervention and control groups, (2) only a small number of communities can usually be studied, (3) the time course of changes in behavior and other outcomes is often of interest, and (4) surveys to measure such changes over time can be conducted with either repeated cross-sectional samples or with longitudinal samples. This paper shows how these issues can be addressed under a mixed-model analysis of variance approach. This approach serves to unify several ideas in the literature on evaluation of community studies, including use of time-series regression and the question of whether the individual or the community should be the unit of analysis. We also describe how the method can be used to estimate sample size requirements, statistical power, or minimum detectable program effect.-Data analysis and sample size issues in evaluations of community-based health promotion and disease prevention programs: a mixed-model analysis of variance approach.",1
"The group randomized trial (GRT) is a common study design to assess the effect of an intervention program aimed at health promotion or disease prevention. In GRTs, groups rather than individuals are randomized into intervention or control arms. Then, responses are measured on individuals within those groups. A number of analytical problems beset GRT designs. The major problem emerges from the likely positive intraclass correlation among observations of individuals within a group. This paper provides an overview of the analytical method for GRT data and applies this method to a randomized cancer prevention trial, where multiple binary primary endpoints were obtained. We develop an index of extra variability to investigate group-specific effects on response. The purpose of the index is to understand the influence of individual groups on evaluating the intervention effect, especially, when a GRT study involves a small number of groups. The multiple endpoints from the GRT design are analyzed using a generalized linear mixed model and the stepdown Bonferroni method of Holm.-Analysis of group randomized trials with multiple binary endpoints and small number of groups.",1
"The objective of this research was to identify determinants of the magnitude of intracluster correlation coefficients (ICCs) in cluster randomized trials from the field of implementation research. A survey of experts was conducted to generate a priori hypotheses of factors that might affect ICC size. Hypotheses were tested on empirical estimates of ICCs calculated from 21 implementation research datasets, mainly from the UK. Effects of setting (primary or secondary care), type of variable (process or outcome), type of measurement (objective or subjective), prevalence of outcome and size of cluster were tested. In total, 220 ICCs were available (range 0 to 0.415). Significant differences in ICC magnitude were found. The ICCs were significantly higher for process than for outcome variables, and for secondary care outcomes compared with primary care outcomes. The effects of prevalence and size were less clear cut. There was no evidence to suggest that type of measurement affected ICC size. In conclusion, accurate estimates of ICCs are essential for sample size calculations for cluster randomized trials of professional behaviour change interventions. This study demonstrates that ICCs are sensitive to a number of trial factors, particularly setting and outcome type. These factors must be considered when planning such cluster randomized trials.-Determinants of the intracluster correlation coefficient in cluster randomized trials: the case of implementation research.",1
"Survival regression is commonly applied in biomedical studies or clinical trials, and evaluating their predictive performance plays an essential role for model diagnosis and selection. The presence of censored data, particularly if informative, may pose more challenges for the assessment of predictive accuracy. Existing literature mainly focuses on prediction for survival probabilities with limitation work for survival time. In this work, we focus on accuracy measures of predicted survival times adjusted for a potentially informative censoring mechanism (ie, coarsening at random (CAR); non-CAR) by adopting the technique of inverse probability of censoring weighting. Our proposed predictive metric can be adaptive to various survival regression frameworks including but not limited to accelerated failure time models and proportional hazards models. Moreover, we provide the asymptotic properties of the inverse probability of censoring weighting estimators under CAR. We consider the settings of high-dimensional data under CAR or non-CAR for extensions. The performance of the proposed method is evaluated through extensive simulation studies and analysis of real data from the Critical Assessment of Microarray Data Analysis.-Assessing predictive accuracy of survival regressions subject to nonindependent censoring.",0
"Viral genotype data aid in understanding the development of antiretroviral drug resistance and in identifying appropriate treatments. Using HIV-1 protease sequences and measures of in vitro sensitivity to the drug amprenavir, we develop a novel statistical approach that can be used to investigate combinations of mutations that alter drug susceptibility. Our method is based on the use of order statistics whose null distributions are estimated through resampling and used for formal hypothesis testing. We present a step-down testing method that preserves the overall family-wise error rate in finite samples via an application of the monotonicity condition of Romano and Wolf. Simulations demonstrate that the power of this new approach is comparable to a traditional resampling method; however, this approach can also be used as a visual diagnostic that may be informative even when specified hypotheses are not rejected, for example, in suggesting candidate regression models. Analysis of the data from the Stanford HIV database shows that while M46I/L mutations are associated with drug resistance, addition of the L88D/S mutation leads to hypersusceptible virus. Further addition of T90M/L mutations results in highly resistant virus. Use of this order statistics method allows the investigation of how mutations act in the presence of others and may suggest mechanisms by which resistance occurs or is reversed through the accumulation of mutations.-Resampling-based analyses of the effects of combinations of HIV genetic mutations on drug susceptibility.",0
"Suppose that we are interested in using new bio- or clinical markers, in addition to the conventional markers, to improve prediction or diagnosis of the patient's clinical outcome. The incremental value from the new markers is typically assessed by averaging across patients in the entire study population. However, when measuring the new markers is costly or invasive, an overall improvement does not justify measuring the new markers in all patients. A more practical strategy is to utilize the patient's conventional markers to decide whether the new markers are needed for improving prediction of his/her health outcomes. In this article, we propose inference procedures for the incremental values of new markers across various subgroups of patients classified by the conventional markers. The resulting point and interval estimates can be quite useful for medical decision makers seeking to balance the predictive or diagnostic value of new markers against their associated cost and risk. Our proposals are theoretically justified and illustrated empirically with two real examples.-Identifying subjects who benefit from additional information for better prediction of the outcome variables.",0
"An update of the chapter on Mental, Behavioral and Neurodevelopmental Disorders in the International Classification of Diseases and Related Health Problems (ICD) is of great interest around the world. The recent approval of the 11th Revision of the ICD (ICD-11) by the World Health Organization (WHO) raises broad questions about the status of nosology of mental disorders as a whole as well as more focused questions regarding changes to the diagnostic guidelines for specific conditions and the implications of these changes for practice and research. This Forum brings together a broad range of experts to reflect on key changes and controversies in the ICD-11 classification of mental disorders. Taken together, there is consensus that the WHO's focus on global applicability and clinical utility in developing the diagnostic guidelines for this chapter will maximize the likelihood that it will be adopted by mental health professionals and administrators. This focus is also expected to enhance the application of the guidelines in non-specialist settings and their usefulness for scaling up evidence-based interventions. The new mental disorders classification in ICD-11 and its accompanying diagnostic guidelines therefore represent an important, albeit iterative, advance for the field.-Mental, behavioral and neurodevelopmental disorders in the ICD-11: an international perspective on key changes and controversies.",0
"In a cluster randomised crossover (CRXO) design, a sequence of interventions is assigned to a group, or 'cluster' of individuals. Each cluster receives each intervention in a separate period of time, forming 'cluster-periods'. Sample size calculations for CRXO trials need to account for both the cluster randomisation and crossover aspects of the design. Formulae are available for the two-period, two-intervention, cross-sectional CRXO design, however implementation of these formulae is known to be suboptimal. The aims of this tutorial are to illustrate the intuition behind the design; and provide guidance on performing sample size calculations. Graphical illustrations are used to describe the effect of the cluster randomisation and crossover aspects of the design on the correlation between individual responses in a CRXO trial. Sample size calculations for binary and continuous outcomes are illustrated using parameters estimated from the Australia and New Zealand Intensive Care Society - Adult Patient Database (ANZICS-APD) for patient mortality and length(s) of stay (LOS). The similarity between individual responses in a CRXO trial can be understood in terms of three components of variation: variation in cluster mean response; variation in the cluster-period mean response; and variation between individual responses within a cluster-period; or equivalently in terms of the correlation between individual responses in the same cluster-period (within-cluster within-period correlation, WPC), and between individual responses in the same cluster, but in different periods (within-cluster between-period correlation, BPC). The BPC lies between zero and the WPC. When the WPC and BPC are equal the precision gained by crossover aspect of the CRXO design equals the precision lost by cluster randomisation. When the BPC is zero there is no advantage in a CRXO over a parallel-group cluster randomised trial. Sample size calculations illustrate that small changes in the specification of the WPC or BPC can increase the required number of clusters. By illustrating how the parameters required for sample size calculations arise from the CRXO design and by providing guidance on both how to choose values for the parameters and perform the sample size calculations, the implementation of the sample size formulae for CRXO trials may improve.-Understanding the cluster randomised crossover design: a graphical illustraton of the components of variation and?a sample size?tutorial.",1
"Most smokers struggle to overcome tobacco addiction. Neuroscientific models of addiction emphasize the importance of brain regions associated with cognitive control and reward to understand the cycle of addiction and relapse. During an attempt at abstinence, the cognitive control system appears to be underpowered to override the heightened reward system of the addicted brain. Thus, one neural target for treatment is to strengthen the cognitive control system. It may be possible to improve the functioning of the cognitive control system via deliberate practice. This study will determine the effects of practicing delaying smoking on brain and behavioral measures of cognitive control. Smoking patterns will be monitored for 1 week and then smokers (N = 80) will be randomized to either practice cognitive control by delaying their first cigarette of the day for 2 weeks (practice group) or they will continue monitoring only (no practice group). Functional magnetic resonance imaging will be performed while smokers regulate their responses to smoking images (i) at baseline and (ii) after 2 weeks of practice (or no practice). The primary aim of this study will be to identify the impact of practicing cognitive control on functional brain activation changes in response to smoking cues. If successful, this project will establish a neurobiological biomarker for increasing cognitive control and demonstrate the feasibility of neuroimaging methods to predict the efficacy of an intervention without a large clinical trial. ClinicalTrials.gov, NCT03080844 .?Registered March 15, 2017.-Functional brain activation changes associated with practice in delaying smoking among moderate to heavy smokers: study protocol and rationale of a randomized trial (COPE).",0
"Lifelong antiretroviral therapy (ART) (Option B+) is recommended for all HIV-infected pregnant/postpartum women, but high adherence is required to maximize HIV prevention potential and maintain maternal health. Mobile health (mHealth) interventions may provide treatment adherence support for women during, and beyond, the pregnancy and postpartum periods. We are conducting an unblinded, triple-arm randomized clinical trial (Mobile WACh X) of one-way short message service (SMS) vs. two-way SMS vs. control (no SMS) to improve maternal ART adherence and retention in care by 2years postpartum. We will enroll 825 women from Nairobi and Western Kenya. Women in the intervention arms receive weekly, semi-automated motivational and educational SMS and visit reminders via an interactive, human-computer hybrid communication system. Participants in the two-way SMS arm are also asked to respond to a question related to the message. SMS are based in behavioral theory, are tailored to participant characteristics through SMS tracks, and are timed along the pregnancy/postpartum continuum. After enrollment, follow-up visits are scheduled at 6weeks; 6, 12, 18, and 24months postpartum. The primary outcomes, virological failure (HIV viral load ?1000copies/mL), maternal retention in care, and infant HIV infection or death, will be compared in an intent to treat analysis. We will also measure ART adherence and drug resistance. Personalized and tailored SMS to support HIV-infected women during and after pregnancy may be an effective strategy to motivate women to adhere to ART and remain in care and improve maternal and infant outcomes.-Evaluation of mHealth strategies to optimize adherence and efficacy of Option B+ prevention of mother-to-child HIV transmission: Rationale, design and methods of a 3-armed randomized controlled trial.",0
"Group-randomized trials are randomized studies that allocate intact groups of individuals to different comparison arms. A frequent practical limitation to adopting such research designs is that only a limited number of groups may be available, and therefore, simple randomization is unable to adequately balance multiple group-level covariates between arms. Therefore, covariate-based constrained randomization was proposed as an allocation technique to achieve balance. Constrained randomization involves generating a large number of possible allocation schemes, calculating a balance score that assesses covariate imbalance, limiting the randomization space to a prespecified percentage of candidate allocations, and randomly selecting one scheme to implement. When the outcome is binary, a number of statistical issues arise regarding the potential advantages of such designs in making inference. In particular, properties found for continuous outcomes may not directly apply, and additional variations on statistical tests are available. Motivated by two recent trials, we conduct a series of Monte Carlo simulations to evaluate the statistical properties of model-based and randomization-based tests under both simple and constrained randomization designs, with varying degrees of analysis-based covariate adjustment. Our results indicate that constrained randomization improves the power of the linearization F-test, the KC-corrected GEE t-test (Kauermann and Carroll, 2001, Journal of the American Statistical Association 96, 1387-1396), and two permutation tests when the prognostic group-level variables are controlled for in the analysis and the size of randomization space is reasonably small. We also demonstrate that constrained randomization reduces power loss from redundant analysis-based adjustment for non-prognostic covariates. Design considerations such as the choice of the balance metric and the size of randomization space are discussed.-An evaluation of constrained randomization for the design and analysis of group-randomized trials with binary outcomes.",1
Effect sizes in cluster-randomized designs,1
"Design and Analysis of Cluster Randomized Trials. Workshop proceedings. University of Sheffield, 5-7 July 1999.",1
The therapist-as-fixed-effect fallacy in psychotherapy research.,2
"Children living in nonmetropolitan communities are underserved by evidence-based mental health care and are underrepresented in clinical trials. In this article, we describe lessons learned in conducting the Children's Attention-Deficit Hyperactivity Disorder (ADHD) Telemental Health (TMH) Treatment Study (CATTS), a randomized controlled trial testing the effectiveness of TMH in improving outcomes of children with ADHD living in underserved communities. Children were referred by primary care providers (PCPs). The test intervention group received six telepsychiatry sessions with each session followed by an caregiver behavior training session delivered in-person by a local therapist. A secure website was used to support decision making by the telepsychiatrists and to facilitate real-time collaboration between the telepsychiatrists and community therapists. The control group received a single telepsychiatry consultation. Questionnaires tapping ADHD symptoms and other outcomes were administered to parents and teachers online through a secure portal from personal computers. total of 88 PCPs in seven communities referred the 223 children who participated in the trial. Attrition in treatment sessions and research assessments was very low. Lessons learned TMH proved to be a viable means of providing evidence-based pharmacological services to children and training to local therapists. Recruitment was enhanced by offering the control group a telepsychiatry consultation. Site-specific strategies were needed to meet recruitment targets. The CATTS trial used methods designed to optimize inclusion of children living in multiple dispersed and underserved areas. The study will serve as a model for other research projects aiming at reducing geographic disparities in access to quality mental health care.-Methodology for conducting the children's attention-deficit hyperactivity disorder telemental health treatment study in multiple underserved communities.",0
"Dependent binary responses, such as health outcomes in twin pairs or siblings, frequently arise in perinatal epidemiologic research. This gives rise to correlated data, which must be taken into account during analysis to avoid erroneous statistical and biological inferences. An analysis of perinatal mortality (fetal deaths plus deaths within the first 28 days) in twins in relation to cluster-varying (those that are unique to each fetus within a twin pregnancy such as birthweight) and cluster-constant (those that are identical for both twins within a sibship such as maternal smoking status) risk factors is presented. Marginal (ordinary logistic regression [OLR] and logistic regression using generalized estimating equations [GEE]) and cluster-specific (conditional and random-intercept logistic regression models) regression models are fit and their results contrasted. The United States ""matched multiple data"" file of twin births (1995-1997), which includes 285,226 twins from 142,613 pregnancies, was used to examine the implications of ignoring of clustering on regression inferences. The OLR models provide variance estimates for cluster constant covariates that ranged from 7% to 71% smaller than those from GEE-based models. This underestimation is even more pronounced for some cluster-varying covariates, ranging from 21% to 198%. Ignoring the cluster dependency is likely to affect the precision of covariate effects and consequently interpretation of results. With widespread availability of appropriate software, statistical methods for taking the intracluster dependency into account are easily implemented and necessary.-Regression models for clustered binary responses: implications of ignoring the intracluster correlation in an analysis of perinatal mortality in twin gestations.",1
"We consider the problem of sample size determination for three-level mixed-effects linear regression models for the analysis of clustered longitudinal data. Three-level designs are used in many areas, but in particular, multicenter randomized longitudinal clinical trials in medical or health-related research. In this case, level 1 represents measurement occasion, level 2 represents subject, and level 3 represents center. The model we consider involves random effects of the time trends at both the subject level and the center level. In the most common case, we have two random effects (constant and a single trend), at both subject and center levels. The approach presented here is general with respect to sampling proportions, number of groups, and attrition rates over time. In addition, we also develop a cost model, as an aid in selecting the most parsimonious of several possible competing models (i.e., different combinations of centers, subjects within centers, and measurement occasions). We derive sample size requirements (i.e., power characteristics) for a test of treatment-by-time interaction(s) for designs based on either subject-level or cluster-level randomization. The general methodology is illustrated using two characteristic examples.-Sample size determination for hierarchical longitudinal designs with differential attrition rates.",1
"Nonalcoholic fatty liver disease (NAFLD) in children can lead to steatohepatitis, cirrhosis, and end-stage liver disease. The cause of NAFLD is unknown, but it is commonly associated with obesity, insulin resistance, and dyslipidemia. TONIC is conducted to test whether treatment with metformin, an insulin sensitizer, or vitamin E, a naturally available antioxidant, will lead to improvements in biochemical and histological features of nondiabetic children with biopsy-proven NAFLD. TONIC is a randomized, multicenter, double-masked, placebo-controlled trial of 96 weeks of treatment with metformin or vitamin E. The primary outcome measure chosen for the trial is improvement in serum alanine aminotransferase (ALT) levels with treatment as compared to placebo. An improvement in ALT is defined as reduction in serum ALT levels to below 50% of the baseline values or into the normal range (40 U/L or less) during the last 48 weeks of treatment. Histological improvement is defined by changes in liver histology between a baseline and end-of-treatment liver biopsy in regards to (1) steatohepatitis, (2) NAFLD Activity Score, consisting of scores for steatosis, lobular inflammation, and hepatocellular injury (ballooning), and (3) fibrosis score. Between September 2005 and September 2007, 173 children were enrolled into TONIC at 10 clinical centers in the United States. Participants were randomized to receive either metformin (500 mg b.i.d.), vitamin E (400 IU b.i.d.), or placebo for 96 weeks. This protocol was approved by all participating center Institutional Review Boards (IRBs) and an independent Data and Safety Monitoring Board (DSMB). (ClinicalTrials.gov number, NCT00063635.).-Treatment of nonalcoholic fatty liver disease in children: TONIC trial design.",0
"Stepped wedge cluster randomized trials (SW-CRTs) have become increasingly popular and are used for a variety of interventions and outcomes, often chosen for their feasibility advantages. SW-CRTs must account for time trends in the outcome because of the staggered rollout of the intervention. Robust inference procedures and nonparametric analysis methods have recently been proposed to handle such trends without requiring strong parametric modeling assumptions, but these are less powerful than model-based approaches. We propose several novel analysis methods that reduce reliance on modeling assumptions while preserving some of the increased power provided by the use of mixed effects models. In one method, we use the synthetic control approach to find the best matching clusters for a given intervention cluster. Another method makes use of within-cluster crossover information to construct an overall estimator. We also consider methods that combine these approaches to further improve power. We test these methods on simulated SW-CRTs, describing scenarios in which these methods have increased power compared with existing nonparametric methods while preserving nominal validity when mixed effects models are misspecified. We also demonstrate theoretical properties of these estimators with less restrictive assumptions than mixed effects models. Finally, we propose avenues for future research on the use of these methods; motivation for such research arises from their flexibility, which allows the identification of specific causal contrasts of interest, their robustness, and the potential for incorporating covariates to further increase power. Investigators conducting SW-CRTs might well consider such methods when common modeling assumptions may not hold.-Novel methods for the analysis of stepped wedge cluster randomized trials",3
"We consider analysis of clustered binary data from multiple observations for each subject in which any two observations from a subject are assumed to have a common correlation coefficient. In the weighted sign test on proportion in clustered binary data, three weighting schemes are considered: equal weights to observations, equal weights to clusters and the optimal weights that minimize the variance of the estimator. Since the distribution of cluster sizes may not be exactly specified before the trial starts, the sample size is usually determined using an average cluster size without taking into account any potential imbalance in cluster size even though cluster size usually varies among clusters. In this paper we investigate the relative efficiency (RE) of unequal versus equal cluster sizes for clustered binary data using the weighted sign test estimators. The REs are computed as a function of correlation among observations within each subject and the various cluster size distributions. The required sample size for unequal cluster sizes will not exceed the sample size for an equal cluster size multiplied by the maximum RE. It is concluded that the maximum RE for various cluster size distributions considered here does not exceed 1.50, 1.61 and 1.12 for equal weights to observations, equal weights to clusters and optimal weights, respectively. It suggests sampling 50%, 61% and 12% more clusters depending on the weighting schemes than the number of clusters computed using an average cluster size.-Relative Efficiency of Unequal Versus Equal Cluster Sizes for the Nonparametric Weighted Sign Test Estimators in Clustered Binary Data.",1
"In noninferiority studies, a limit of indifference is used to express a tolerance in results such that the clinician would regard such results as being acceptable or 'not worse'. We applied this concept to a measure of accuracy, the Receiver Operating Characteristic (ROC) curve, for a sequence of tests. We expressed a limit of indifference for the range of acceptable sensitivity values and examined the associated cost of testing within this range. In doing so, we generated the minimum cost maximum ROC (MCMROC) curve, which reflects the reduced sensitivity and cost of testing. We compared the MCMROC and its associated cost curve between limits of indifference set to 0.999 [a 0.1% reduction in true positive rate (TPR)], 0.95 (a 5% reduction in TPR), and 1 (no reduction in TPR). The limit of indifference tended to have less of an effect on the MCMROC curves than on the associated cost curves that were greatly affected. Cost was reduced at high false positive rates (FPRs) at higher limit of indifference (0.999) and at small FPRs as the limit of indifference decreased (0.95). These patterns were also observed as applied to sequential strategies used to diagnose diabetes in the Pima Indians.-Reducing cost in sequential testing: a limit of indifference approach.",0
"A cluster trial with unequal cluster sizes often has lower precision than one with equal clusters, with a corresponding inflation of the design effect. For parallel group trials, adjustments to the design effect are available under sampling models with a single intracluster correlation. Design effects for equal clusters under more complex scenarios have appeared recently (including stepped wedge trials under cross-sectional or longitudinal sampling). We investigate the impact of unequal cluster size in these more general settings. Assuming a linear mixed model with an exchangeable correlation structure that incorporates cluster and subject autocorrelation, we compute the relative efficiency (RE) of a trial with clusters of unequal size under a size-stratified randomization scheme, as compared to an equal cluster trial with the same total number of observations. If there are no within-cluster time effects, the RE exceeds that for a parallel trial. In general, the RE is a weighted average of the RE for a parallel trial and the RE for a crossover trial in the same clusters. Existing approximations for parallel designs are extended to the general setting. Increasing the cluster size by the factor (1?+?CV2 ), where CV is the coefficient of variation of cluster size, leads to conservative sample sizes, as in a popular method for parallel trials. Methods to assess experimental precision for single-period parallel trials with unequal cluster sizes can be extended to stepped wedge and other complete layouts under longitudinal or cross-sectional sampling. In practice, the loss of precision due to unequal cluster sizes is unlikely to exceed 12%.-Relative efficiency of unequal cluster sizes in stepped wedge and other trial designs under longitudinal or cross-sectional sampling.",3
"Cluster randomized trials are commonly used to evaluate public health, knowledge translation, and health service interventions. Cluster trials raise novel ethical issues, however, and the Ottawa Statement on the Ethical Design and Conduct of Cluster Randomized Trials (2012) provides researchers and research ethics committees with needed guidance. In this journal, van der Graaf et?al. reflect on the Ottawa Statement and propose three revisions. In this paper, we respond to each of these proposed revisions. First, van der Graaf et al. argue that patients who are merely indirectly affected by study interventions ought nonetheless to be considered research participants. We disagree. So long as the practice change is evidence based and the physician continues to make individualized judgments regarding patient care, patient liberty and welfare interests are not substantially affected. Second, although they agree that health providers who are targeted are research participants, they argue that such providers ought to be treated differently and should not be allowed to withdraw from a study too easily. In our view, this position fails to weigh adequately the potential for coercion and harms faced by employees in research. Third, they argue that the potential for bias may require blinding participants to allocation and study interventions in the consent process of a cluster trial. We agree on this point and support this approach in a limited set of cases. While we reject two of van der Graaf et al.'s proposed revisions, we agree that further guidance on informed consent and study bias is needed.-The ethics of cluster randomized trials: response to a proposal for revision of the Ottawa Statement",1
"The most severe HIV epidemics worldwide occur in Lesotho, Botswana and Swaziland. Here we focus on the Lesotho epidemic, which has received little attention. We determined the within-country heterogeneity in the severity of the epidemic, and identified the risk factors for HIV infection. We also determined whether circumcised men in Lesotho have had a decreased risk of HIV infection in comparison with uncircumcised men. We discuss the implications of our results for expanding treatment (current coverage is only 60%) and reducing transmission. We used data from the 2009 Lesotho Demographic and Health Survey, a nationally representative survey of 3,849 women and 3,075 men in 9,391 households. We performed multivariate analysis to identify factors associated with HIV infection in the sexually active population and calculated age-adjusted odds ratios (aORs). We constructed cartographic country-level prevalence maps using geo-referenced data. HIV is hyperendemic in the general population. The average prevalence is 27% in women and 18% in men, but shows substantial geographic variation. Throughout the country prevalence is higher in urban centers (31% in women; 21% in men) than in rural areas (25% in women; 17% in men), but the vast majority of HIV-infected individuals live in rural areas. Notably, prevalence is extremely high in women (18%) and men (12%) with only one lifetime sex partner. Women with more partners have a greater risk of infection: aOR 2.3 (2 to 4 partners), aOR 4.4 (?5 partners). A less substantial effect was found for men: aOR 1.4 (3 to 6 partners), aOR 1.8 (?7 partner). Medical circumcision protected against infection (aOR 0.5), traditional circumcision did not (aOR 0.9). Less than 5% of men in Lesotho have been medically circumcised; approximately 50% have been circumcised using traditional methods. There is a substantial need for treatment throughout Lesotho, particularly in rural areas where there is the greatest burden of disease. Interventions aimed at reducing the number of sex partners may only have a limited effect on reducing transmission. Substantially increasing levels of medical circumcision could be very effective in reducing transmission, but will be very difficult to achieve given the current high prevalence of traditional circumcision.-Current drivers and geographic patterns of HIV in Lesotho: implications for treatment and prevention in Sub-Saharan Africa.",0
"A pragmatic cluster-randomized trial (CRT) is a research design that may be used to efficiently test promising interventions that directly inform dialysis care. While the Ottawa Statement on the Ethical Design and Conduct of Cluster Randomized Trials provides general ethical guidance for CRTs, the dialysis setting raises additional considerations. In this article, we outline ethical issues raised by pragmatic CRTs in dialysis facilities. These issues may be divided into 7 key domains: justifying the use of cluster randomization, adopting randomly allocated individual-level interventions as a facility standard of care, conducting benefit-harm analyses, gatekeepers and their responsibilities, obtaining informed consent from research participants, patient notification, and including vulnerable participants. We describe existing guidelines relevant to each domain, illustrate how they were considered in the Time to Reduce Mortality in End-Stage Renal Disease (TiME) trial (a prototypical pragmatic hemodialysis CRT), and highlight remaining areas of uncertainty. The following is the first step in an interdisciplinary mixed-methods research project to guide the design and conduct of pragmatic CRTs in dialysis facilities. Subsequent work will expand on these concepts and when possible, argue for a preferred solution.-Ethical Issues in Pragmatic Cluster-Randomized Trials in Dialysis Facilities",1
"This paper analyses a case in censored failure time data problems where some observations are potentially censored. The traditional models for failure time data implicitly assume that the censoring status for each observation is deterministic. Therefore, they cannot be applied directly to the potentially censored data. We propose an estimator that uses resampling techniques to approximate censoring probabilities for individual observations. A Monte Carlo simulation study shows that the proposed estimator properly corrects biases that would otherwise be present had it been assumed that either all potentially censored observations are censored or that no censoring has occurred. Finally, we apply the estimator to a health insurance claims database.-Estimating episode lengths when some observations are probably censored.",0
"One major goal in microarray studies is to identify genes having different expression levels across different classes/conditions. In order to achieve this goal, a study needs to have an adequate sample size to ensure the desired power. Owing to the importance of this topic, a number of approaches to sample size calculation have been developed. However, due to the cost and/or experimental difficulties in obtaining sufficient biological materials, it might be difficult to attain the required sample size. In this article, we address more practical questions for assessing power and false discovery rate (FDR) for a fixed sample size. The relationships between power, sample size and FDR are explored. We also conduct simulations and a real data study to evaluate the proposed findings.-Practical guidelines for assessing power and false discovery rate for a fixed sample size in microarray experiments.",0
"The community surveillance study of coronary heart disease (CHD) in Olmsted County, MN, is designed to estimate trends in myocardial infarction (MI) incidence, case fatality rate, and CHD mortality, while including all ages. A distinctive feature of this study is its ability to capture longitudinal data before and after index events via the medical record linkage system of the Rochester Epidemiology Project. The goal of this report is to describe the methods implemented to measure CHD trends, the implications of including elderly individuals on MI ascertainment and trends in prior CHD among persons with incident MI. The methods are based on standardized criteria involving the review of death certificate information and hospital records to identify CHD deaths, and incident MIs in Olmsted County. The medical record linkage system in place under the auspices of the Rochester Epidemiology Project was used to ascertain antecedent CHD and outcomes. Hospitalized MIs were screened from sampled events coded ICD9 codes 410-414 and classified using enzyme values, cardiac pain, and ECG coding. After screening 5,042 records, a cohort of 1,658 validated incident MIs was assembled 35% (575) among persons aged 75 years or greater. The proportion of MIs validated with cardiac pain and enzymes without Minnesota ECG coding was lower among the elderly than among persons less than 75 years of age (35 vs. 29%, respectively; P &lt;.001). The proportion of events validated without requiring ECG coding decreased over time in both age strata (P for trend.001). Reliability analyses indicated excellent agreement in event classification. More than half of the incident MIs did not have antecedent CHD, and this proportion increased overtime. These data indicate that the elderly contribute approximately one-third of the cases of incident MI, underscoring the importance of including all ages to fully characterize the burden of CHD. Cases among elderly persons more frequently require ECG coding for validation, but standardized ascertainment procedures are feasible and reliable in all age groups. More than half of the incident MIs occurred among persons with no prior CHD, and this proportion increased over time. The combination of standardized methodology and of the longitudinal data via the record linkage system of the Rochester Epidemiology Project will allow reliable measures of CHD trends and help define preventive strategies.-Coronary disease surveillance in Olmsted County objectives and methodology.",0
"Latent state-trait (LST) models (Steyer, Ferring, &amp; Schmitt, 1992) allow separating person-specific (trait) effects from (1) effects of the situation and person ? situation interactions, and (2) random measurement error in purely observational studies. Typical LST applications use measurement designs in which all situations are sampled randomly and do not have to be known for any individual. Limitations of conventional LST models for only random situations are that traits are implicitly assumed to generalize perfectly across situations, and that main effects of situations are inseparable from person ? situation interaction effects because both are measured by the same latent variable. In this article, we show how these limitations can be overcome by using measurement designs in which two or more random situations are nested within two or more fixed situations that are known for each individual. We present extended LST models for the combination of random and fixed situations (LST-RF approach) and show that the extensions allow (1) examining the extent to which traits are situation-specific and (2) isolating person ? situation interactions from situation main effects. We demonstrate that the LST-RF approach can be applied with both homogenous and heterogeneous indicators in either the single- or multilevel structural equation modeling frameworks. Advantages and limitations of the new models as well as their relation to other approaches for studying person ? situation interactions are discussed.-Analyzing person, situation and person ? situation interaction effects: Latent state-trait models for the combination of random and fixed situations.",0
The Ottawa Statement on the ethical design and conduct of cluster randomised trials: precis for researchers and research ethics committees.,1
"Epidemiologic studies often compare several groups of subjects for the presence or absence of a specified biological trait, where each subject in a group contributes two or more observations to the analysis. Examples occur in ophthalmologic studies, where each subject contributes observations on each of two eyes, and dental studies, where observations on each of several teeth may be contributed. Application of the standard Pearson chi-square test to such data is not valid, since the resulting sample observations are not statistically independent. In this paper we show how simple adjustments can be made to the Pearson chi-square statistic that adjust for the within-subject clustering. Application to other types of investigations involving clustered data is also discussed.-The statistical analysis of multiple binary measurements.",1
"In cluster randomized trials, intact social units such as schools, worksites or medical practices - rather than individuals themselves - are randomly allocated to intervention and control conditions, while the outcomes of interest are then observed on individuals within each cluster. Such trials are becoming increasingly common in the fields of health promotion and health services research. Attrition is a common occurrence in randomized trials, and a standard approach for dealing with the resulting missing values is imputation. We consider imputation strategies for missing continuous outcomes, focusing on trials with a completely randomized design in which fixed cohorts from each cluster are enrolled prior to random assignment. We compare five different imputation strategies with respect to Type I and Type II error rates of the adjusted two-sample t -test for the intervention effect. Cluster mean imputation is compared with multiple imputation, using either within-cluster data or data pooled across clusters in each intervention group. In the case of pooling across clusters, we distinguish between standard multiple imputation procedures which do not account for intracluster correlation and a specialized procedure which does account for intracluster correlation but is not yet available in standard statistical software packages. A simulation study is used to evaluate the influence of cluster size, number of clusters, degree of intracluster correlation, and variability among cluster follow-up rates. We show that cluster mean imputation yields valid inferences and given its simplicity, may be an attractive option in some large community intervention trials which are subject to individual-level attrition only; however, it may yield less powerful inferences than alternative procedures which pool across clusters especially when the cluster sizes are small and cluster follow-up rates are highly variable. When pooling across clusters, the imputation procedure should generally take intracluster correlation into account to obtain valid inferences; however, as long as the intracluster correlation coefficient is small, we show that standard multiple imputation procedures may yield acceptable type I error rates; moreover, these procedures may yield more powerful inferences than a specialized procedure, especially when the number of available clusters is small. Within-cluster multiple imputation is shown to be the least powerful among the procedures considered.-Imputation strategies for missing continuous outcomes in cluster randomized trials.",1
"The Spearman (rho(s)) and Kendall (tau) rank correlation coefficient are routinely used as measures of association between non-normally distributed random variables. However, confidence limits for rho(s) are only available under the assumption of bivariate normality and for tau under the assumption of asymptotic normality of tau. In this paper, we introduce another approach for obtaining confidence limits for rho(s) or tau based on the arcsin transformation of sample probit score correlations. This approach is shown to be applicable for an arbitrary bivariate distribution. The arcsin-based estimators for rho(s) and tau (denoted by rho(s,a), tau(a)) are shown to have asymptotic relative efficiency (ARE) of 9/pi2 compared with the usual estimators rho(s) and tau when rho(s) and tau are, respectively, 0. In some nutritional applications, the Spearman rank correlation between nutrient intake as assessed by a reference instrument versus nutrient intake as assessed by a surrogate instrument is used as a measure of validity of the surrogate instrument. However, if only a single replicate (or a few replicates) are available for the reference instrument, then the estimated Spearman rank correlation will be downwardly biased due to measurement error. In this paper, we use the probit transformation as a tool for specifying an ANOVA-type model for replicate ranked data resulting in a point and interval estimate of a measurement error corrected rank correlation. This extends previous work by Rosner and Willett for obtaining point and interval estimates of measurement error corrected Pearson correlations.-Interval estimation for rank correlation coefficients based on the probit transformation with extension to measurement error correction of correlated ranked data.",0
"We consider regulatory clinical trials that require a prespecified method for the comparison of two treatments for chronic diseases (e.g. Chronic Obstructive Pulmonary Disease) in which patients suffer deterioration in a longitudinal process until death occurs. We define a composite endpoint structure that encompasses both the longitudinal data for deterioration and the time-to-event data for death, and use multivariate time-to-event methods to assess treatment differences on both data structures simultaneously, without a need for parametric assumptions or modeling. Our method is straightforward to implement, and simulations show that the method has robust power in situations in which incomplete data could lead to lower than expected power for either the longitudinal or survival data. We illustrate the method on data from a study of chronic lung disease.-A robust method for comparing two treatments in a confirmatory clinical trial via multivariate time-to-event methods that jointly incorporate information from longitudinal and time-to-event data.",0
"Longitudinal zero-inflated count data arise frequently in substance use research when assessing the effects of behavioral and pharmacological interventions. Zero-inflated count models (e.g. zero-inflated Poisson or zero-inflated negative binomial) with random effects have been developed to analyze this type of data. In random effects zero-inflated count models, the random effects covariance matrix is typically assumed to be homogeneous (constant across subjects). However, in many situations this matrix may be heterogeneous (differ by measured covariates). In this paper, we extend zero-inflated count models to account for random effects heterogeneity by modeling their variance as a function of covariates. We show via simulation that ignoring intervention and covariate-specific heterogeneity can produce biased estimates of covariate and random effect estimates. Moreover, those biased estimates can be rectified by correctly modeling the random effects covariance structure. The methodological development is motivated by and applied to the Combined Pharmacotherapies and Behavioral Interventions for Alcohol Dependence (COMBINE) study, the largest clinical trial of alcohol dependence performed in United States with 1383 individuals.-Zero-inflated count models for longitudinal measurements with heterogeneous random effects.",0
"Randomized trials aimed at improving the quality of medical care often randomize the provider. Such trials are frequently embedded in health care systems with available automated records, which can be used to enhance the design of the trial. We consider how available information from automated records can address each of the following concerns in the design of a trial: whether to randomize individual providers or practices; clustering of outcomes among patients in the same practice and its impact on study size; expected heterogeneity in adherence and the response to the intervention; eligibility criteria and the trade-offs between generalizability and internal validity; and blocking or matching to alleviate covariate imbalance across practices. Investigators can use available information from an automated database to estimate the amount of clustering of patients within providers and practices, and these estimates can inform the decision on whether to randomize at the level of the patient, the provider, or the practice. We illustrate calculation of the anticipated design effect for a proposed cluster-randomized trial and its implications for sample size. With available claims data, investigators can apply focused eligibility criteria to exclude subjects and providers with expected low compliance or lower likelihood of benefit, although possibly at some loss of generalizability. Chance imbalances in covariates are more likely when randomization occurs at the level of the practice than at the level of the patient, so we propose a matching score to limit such imbalances by design. Challenges to compliance, expected small effects, and covariate imbalances are particularly likely in cluster-randomized trials of quality improvement interventions. When such trials are embedded in medical systems with available automated records, use of these data can enhance the design of the trial.-Design of cluster-randomized trials of quality improvement interventions aimed at medical care providers.",1
"In cluster randomized trials (CRTs), identifiable clusters rather than individuals are randomized to study groups. Resulting data often consist of a small number of clusters with correlated observations within a treatment group. Missing data often present a problem in the analysis of such trials, and multiple imputation (MI) has been used to create complete data sets, enabling subsequent analysis with well-established analysis methods for CRTs. We discuss strategies for accounting for clustering when multiply imputing a missing continuous outcome, focusing on estimation of the variance of group means as used in an adjusted t-test or ANOVA. These analysis procedures are congenial to (can be derived from) a mixed effects imputation model; however, this imputation procedure is not yet available in commercial statistical software. An alternative approach that is readily available and has been used in recent studies is to include fixed effects for cluster, but the impact of using this convenient method has not been studied. We show that under this imputation model the MI variance estimator is positively biased and that smaller intraclass correlations (ICCs) lead to larger overestimation of the MI variance. Analytical expressions for the bias of the variance estimator are derived in the case of data missing completely at random, and cases in which data are missing at random are illustrated through simulation. Finally, various imputation methods are applied to data from the Detroit Middle School Asthma Project, a recent school-based CRT, and differences in inference are compared.-Quantifying the impact of fixed effects modeling of clusters in multiple imputation for cluster randomized trials.",1
Efficiency loss due to varying cluster sizes in cluster randomized trials and how to compensate for it: comment on You et al. (2011).,1
"Cluster randomized controlled trials (CRCTs) are attractive in settings in which individual randomization is difficult or impossible. This issue is common when studying several health problems in developing countries. The authors aimed to assess empirically the extent to which the prerequisite design and analysis aspects of cluster randomization were taken into account and reported properly in CRCTs conducted in sub-Saharan Africa. CRCTs published in the last three decades were evaluated by using a checklist based on the Consolidated Standards of Reporting Trials (CONSORT) statement. The authors identified 51 eligible CRCTs; 40 of them (78%) had been published after 1990. Only 10 (20%) studies took clustering into account in sample size or power calculations, and only 19 (37%) took clustering into account in the analysis. Intracluster correlation coefficients and design effects were reported in only one (2%) and three (6%) trials, respectively. An increasing number of CRCTs are conducted in sub-Saharan Africa, but many are not analyzed and reported properly. The special features stemming from cluster randomization need to be addressed in the design, analysis, and reporting of these studies.-Evaluation of cluster randomized controlled trials in sub-Saharan Africa.",1
"We reviewed published individually randomized group treatment (IRGT) trials to assess researchers' awareness of within-group correlation and determine whether appropriate design and analytic methods were used to test for treatment effectiveness. We assessed sample size and analytic methods in IRGT trials published in 6 public health and behavioral health journals between 2002 and 2006. Our review included 34 articles; in 32 (94.1%) of these articles, inappropriate analytic methods were used. In only 1 article did the researchers claim that expected intraclass correlations (ICCs) were taken into account in sample size estimation; in most articles, sample size was not mentioned or ICCs were ignored in the reported calculations. Trials in which individuals are randomly assigned to study conditions and treatments administered in groups may induce within-group correlation, violating the assumption of independence underlying commonly used statistical methods. Methods that take expected ICCs into account should be used in reexamining past studies and planning future studies to ensure that interventions are not judged effective solely on the basis of statistical artifacts. We strongly encourage investigators to report ICCs from IRGT trials and describe study characteristics clearly to aid these efforts.-Individually randomized group treatment trials: a critical appraisal of frequently used design and analytic approaches.",2
"An important research objective in most psychiatric clinical trials of maintenance treatment is to find predictors of recurrence of illness. In those trials, patients are first admitted into an open treatment period also called acute treatment. If they respond to the treatment and are considered to have stable remission from the illness, they enter the second phase of the trial where they are randomized into different arms of the 'maintenance treatments'. Often, more than one response variable is measured longitudinally in the acute treatment phase to monitor treatment responses. Trajectories of these response measures are believed to have predictive ability for recurrences in the maintenance phase of the trial. By using a bivariate growth curve from two such longitudinal measures, we developed a method to use the estimated trajectories of each subject in a Cox regression model to predict recurrence in the maintenance phase. To adjust for the parameter estimation errors, we applied a full likelihood approach based on the conditional expectations of the predictors. Simulation studies indicate that the estimation error corrected estimators for the Cox model parameters are less biased when compared to the naive regression estimators without accounting for these errors. The uniqueness of this method lies in estimating trajectories from bivariate unequally spaced longitudinal response measures. An illustrative example is provided with data from a maintenance treatment trial for major depression in an elderly population. Visual Fortran 90 programs were developed to implement the algorithm.-Using trajectories from a bivariate growth curve as predictors in a Cox regression model.",0
"Although aspirin is recommended for the prevention of colorectal cancer, the specific individuals for whom the benefits outweigh the risks are not clearly defined. Moreover, the precise mechanisms by which aspirin reduces the risk of cancer are unclear. We recently launched the ASPirin Intervention for the REDuction of colorectal cancer risk (ASPIRED) trial to address these uncertainties. ASPIRED is a prospective, double-blind, multidose, placebo-controlled, biomarker clinical trial of aspirin use in individuals previously diagnosed with colorectal adenoma. Individuals (n = 180) will be randomized in a 1:1:1 ratio to low-dose (81?mg/day) or standard-dose (325?mg/day) aspirin or placebo. At two study visits, participants will provide lifestyle, dietary and biometric data in addition to urine, saliva and blood specimens. Stool, grossly normal colorectal mucosal biopsies and cytology brushings will be collected during a flexible sigmoidoscopy without bowel preparation. The study will examine the effect of aspirin on urinary prostaglandin metabolites (PGE-M; primary endpoint), plasma inflammatory markers (macrophage inhibitory cytokine-1 (MIC-1)), colonic expression of transcription factor binding (transcription factor 7-like 2 (TCF7L2)), colonocyte gene expression, including hydroxyprostaglandin dehydrogenase 15-(NAD) (HPGD) and those that encode Wnt signaling proteins, colonic cellular nanocytology and oral and gut microbial composition and function. Aspirin may prevent colorectal cancer through multiple, interrelated mechanisms. The ASPIRED trial will scrutinize these pathways and investigate putative mechanistically based risk-stratification biomarkers. This protocol is registered with the U.S. National Institutes of Health trial registry, ClinicalTrials.gov, under the identifier NCT02394769 . Registered on?16 March 2015.-ASPirin Intervention for the REDuction of colorectal cancer risk (ASPIRED): a study protocol for a randomized controlled trial.",0
"Stepped wedge and other multiple-period cluster randomized trials, which collect data from multiple clusters across multiple time periods, are being conducted with increasing frequency; statistical research into these designs has not kept apace. In particular, some stepped wedge designs with missing cluster-period ""cells"" have been proposed without any formal justification. Indeed there are no general guidelines regarding which cells of a stepped wedge design contribute the least information toward estimation of the treatment effect, and correspondingly which may be preferentially omitted. In this article, we define a metric of the information content of cluster-period cells, entire treatment sequences, and entire periods of the standard stepped wedge design as the increase in variance of the estimator of the treatment effect when that cell, sequence, or period is omitted. We show that the most information-rich cells are those that occur immediately before or after treatment switches, but also that there are additional cells that contribute almost as much to the estimation of the treatment effect. However, the information content patterns depend on the assumed correlation structure for the repeated measurements within a cluster.-Information content of cluster-period cells in stepped wedge trials.",3
"Despite the importance of HIV testing for controlling the HIV epidemic, testing rates remain low. Efforts to scale up testing coverage and frequency in hard-to-reach and at-risk populations commonly focus on home-based HIV testing. This study evaluates the effect of a gift (a US$5 food voucher for families) on consent rates for home-based HIV testing. We use data on 18 478 individuals (6 418 men and 12 060 women) who were successfully contacted to participate in the 2009 and 2010 population-based HIV surveillance carried out by the Wellcome Trust's Africa Health Research Institute in rural KwaZulu-Natal, South Africa. Of 18 478 potential participants contacted in both years, 35% (6 518) consented to test in 2009, and 41% (7 533) consented to test in 2010. Our quasi-experimental difference-in-differences approach controls for unobserved confounding in estimating the causal effect of the intervention on HIV-testing consent rates. Allocation of the gift to a family in 2010 increased the probability of family members consenting to test in the same year by 25 percentage points [95% confidence interval (CI) 21-30 percentage points; P  &lt; 0.001]. The intervention effect persisted, slightly attenuated, in the year following the intervention (2011). In HIV hyperendemic settings, a gift can be highly effective at increasing consent rates for home-based HIV testing. Given the importance of HIV testing for treatment uptake and individual health, as well as for HIV treatment-as-prevention strategies and for monitoring the population impact of the HIV response, gifts should be considered as a supportive intervention for HIV-testing initiatives where consent rates have been low.-Do gifts increase consent to home-based HIV testing? A difference-in-differences study in rural KwaZulu-Natal, South Africa.",0
The intracluster correlation coefficient in cluster randomisation.,1
"Cluster randomized trials are frequently used in health service evaluation. It is common practice to use an analysis model with a random effect to allow for clustering at the analysis stage. In designs where clusters are exposed to both control and treatment conditions, it may be of interest to examine treatment effect heterogeneity across clusters. In designs where clusters are not exposed to both control and treatment conditions, it can also be of interest to allow heterogeneity in the degree of clustering between arms. These two types of heterogeneity are related. It has been proposed in both parallel cluster trials, stepped-wedge, and other cross-over designs that this heterogeneity can be allowed for by incorporating additional random effect(s) into the model. Here, we show that the choice of model parameterization needs careful consideration as some parameterizations for additional heterogeneity induce unnecessary or implausible assumptions. We suggest more appropriate parameterizations, discuss their relative advantages, and demonstrate the implications of these model choices using a real example of a parallel cluster trial and a simulated stepped-wedge trial.-Modeling clustering and treatment effect heterogeneity in parallel and stepped-wedge cluster randomized trials.",3
Studies examining the relationship between neighborhood social disorder and health often rely on multiple informants. Such studies assume interchangeability of the latent constructs derived from multiple-informant data. Existing methods examining this assumption do not clearly delineate the uncertainty at individual levels from that at neighborhood levels. We propose a multilevel variance component factor model that allows this delineation. Data come from a survey of a representative sample of children born between 1983 and 1985 in the inner city of Detroit and nearby middle-class suburbs. Results indicate that the informant-level models tend to exaggerate the effect of places because of differences between persons. Our evaluations of different methodologies lead to the recommendation of the multilevel variance component factor model whenever multiple-informant reports can be aggregated at a neighborhood level.-Assessing interchangeability at cluster levels with multiple-informant data.,0
"Propensity score analysis is widely used in observational studies to adjust for confounding and estimate the causal effect of a treatment on the outcome. When the outcome is survival time, there are special considerations on the definition of the causal estimand, point, and variance estimation that have not been thoroughly studied in the literature. We investigate propensity score analysis of survival data with a class of weighting methods. We consider the following estimands in the two-sample context: average survival time, restricted average survival time, survival probability, survival quantile, and the marginal hazard ratio. We propose a unified analytic framework to obtain the point and variance estimators. The proposed methodology properly adjusts for the sampling variability in the estimated propensity scores. Extensive simulations show that the point and variance estimators possess desired finite sample properties and demonstrate better numerical performance than some existing weighting and matching methods commonly used in the literature. The proposed methodology is illustrated with data from a breast cancer study.-On the propensity score weighting analysis with survival outcome: Estimands, estimation, and inference.",0
"The high co-morbidity of mental disorders, particularly depression, with non-communicable diseases (NCDs) such as cardiovascular disease (CVD), is concerning given the rising burden of NCDs globally, and the role depression plays in confounding prevention and treatment of NCDs. The objective of this randomised control trial (RCT) is to determine the real-world effectiveness of strengthened depression identification and management on depression outcomes in hypertensive patients attending primary health care (PHC) facilities in South Africa (SA). The study design is a pragmatic, two-arm, parallel-cluster RCT, the unit of randomisation being the clinics, with outcomes being measured for individual participants. The 20 largest eligible clinics from one district in the North West Province are enrolled in the trial. Equal numbers of hypertensive patients (n = 50) identified as having depression using the Patient Health Questionnaire (PHQ-9) are enrolled from each clinic, making up a total of 1000 participants with 500 in each arm. The nurse clinicians in the control facilities receive the standard training in Primary Care 101 (PC101), a clinical decision support tool for integrated chronic care that includes guidelines for hypertension and depression care. Referral pathways available include referrals to PHC physicians, clinical or counselling psychologists and outpatient psychiatric and psychological services. In the intervention clinics, this training is supplemented with strengthened training in the depression components of PC101 as well as training in clinical communication skills for nurse-led chronic care. Referral pathways are strengthened through the introduction of a facility-based behavioural health counsellor, trained to provide structured manualised counselling for depression and adherence counselling for all chronic conditions. The primary outcome is defined as at least 50% reduction in PHQ-9 score measured at 6 months. This trial should provide evidence of the real world effectiveness of strengtheneddepression identification and collaborative management on health outcomes of hypertensive patients withcomorbid depression attending PHC facilities in South Africa. South African National Clinical Trial Register: SANCTR ( http://www.sanctr.gov.za/SAClinicalTrials ) (DOH-27-0916-5051). Registered on 9 April 2015. ClinicalTrials.gov : ID: NCT02425124 . Registered on 22 April 2015.-Collaborative care for the detection and management of depression among adults with hypertension in South Africa: study protocol for the PRIME-SA randomised controlled trial.",0
"Often in randomized clinical trials and observational cohort studies, a non-negative continuously distributed response variable is measured in treatment and control groups. In the presence of true zeros for the response variable, a two-part zero-inflated log-normal model (which assumes that the data has a probability mass at zero and a continuous response for values greater than zero) is usually recommended. However, in some environmental health and human immunodeficiency virus (HIV) studies, quantitative assays for metabolites of toxicants, or quantitative HIV RNA measurements are subject to left-censoring due to values falling below the limit of detection (LD). Here, a zero-inflated log-normal mixture model is often suggested since true zeros are indistinguishable from left-censored values due to the LD. When the probabilities of true zeros in the two groups are not restricted to be equal, the information contributed by values falling below LD is used only to estimate the probability of true zeros in the context of mixture distributions. We derived the required sample size to assess the effect of a treatment in the context of mixture models with equal and unequal variances based on the left-truncated log-normal distribution. Methods for calculation of statistical power are also presented. We calculate the required sample size and power for a recent study estimating the effect of oltipraz on reducing urinary levels of the hydroxylated metabolite aflatoxin M(1) (AFM(1)) in a randomized, placebo-controlled, double-blind phase IIa chemoprevention trial in Qidong, China. A Monte Carlo simulation study is conducted to investigate the performance of the proposed methods.-Sample size and statistical power assessing the effect of interventions in the context of mixture distributions with detection limits.",0
"Analyses of multicenter studies often need to account for center clustering to ensure valid inference. For binary outcomes, it is particularly challenging to properly adjust for center when the number of centers or total sample size is small, or when there are few events per center. Our objective was to evaluate the performance of generalized estimating equation (GEE) log-binomial and Poisson models, generalized linear mixed models (GLMMs) assuming binomial and Poisson distributions, and a Bayesian binomial GLMM to account for center effect in these scenarios. We conducted a simulation study with few centers (?30) and 50 or fewer subjects per center, using both a randomized controlled trial and an observational study design to estimate relative risk. We compared the GEE and GLMM models with a log-binomial model without adjustment for clustering in terms of bias, root mean square error (RMSE), and coverage. For the Bayesian GLMM, we used informative neutral priors that are skeptical of large treatment effects that are almost never observed in studies of medical interventions. All frequentist methods exhibited little bias, and the RMSE was very similar across the models. The binomial GLMM had poor convergence rates, ranging from 27% to 85%, but performed well otherwise. The results show that both GEE models need to use small sample corrections for robust SEs to achieve proper coverage of 95% CIs. The Bayesian GLMM had similar convergence rates but resulted in slightly more biased estimates for the smallest sample sizes. However, it had the smallest RMSE and good coverage across all scenarios. These results were very similar for both study designs. For the analyses of multicenter studies with a binary outcome and few centers, we recommend adjustment for center with either a GEE log-binomial or Poisson model with appropriate small sample corrections or a Bayesian binomial GLMM with informative priors.-Estimating relative risks in multicenter studies with a small number of centers - which methods to use? A simulation study.",1
"To assess risks of mortality associated with use of individual antipsychotic drugs in elderly residents in nursing homes. Population based cohort study with linked data from Medicaid, Medicare, the Minimum Data Set, the National Death Index, and a national assessment of nursing home quality. Nursing homes in the United States. 75,445 new users of antipsychotic drugs (haloperidol, aripiprazole, olanzapine, quetiapine, risperidone, ziprasidone). All participants were aged ? 65, were eligible for Medicaid, and lived in a nursing home in 2001-5. Cox proportional hazards models were used to compare 180 day risks of all cause and cause specific mortality by individual drug, with propensity score adjustment to control for potential confounders. Compared with risperidone, users of haloperidol had an increased risk of mortality (hazard ratio 2.07, 95% confidence interval 1.89 to 2.26) and users of quetiapine a decreased risk (0.81, 0.75 to 0.88). The effects were strongest shortly after the start of treatment, remained after adjustment for dose, and were seen for all causes of death examined. No clinically meaningful differences were observed for the other drugs. There was no evidence that the effect measure modification in those with dementia or behavioural disturbances. There was a dose-response relation for all drugs except quetiapine. Though these findings cannot prove causality, and we cannot rule out the possibility of residual confounding, they provide more evidence of the risk of using these drugs in older patients, reinforcing the concept that they should not be used in the absence of clear need. The data suggest that the risk of mortality with these drugs is generally increased with higher doses and seems to be highest for haloperidol and least for quetiapine.-Differential risk of death in older residents in nursing homes prescribed specific antipsychotic drugs: population based cohort study.",0
"Stepped wedge cluster randomised trials frequently involve a relatively small number of clusters. The most common frameworks used to analyse data from these types of trials are generalised estimating equations and generalised linear mixed models. A topic of much research into these methods has been their application to cluster randomised trial data and, in particular, the number of clusters required to make reasonable inferences about the intervention effect. However, for stepped wedge trials, which have been claimed by many researchers to have a statistical power advantage over the parallel cluster randomised trial, the minimum number of clusters required has not been investigated. We conducted a simulation study where we considered the most commonly used methods suggested in the literature to analyse cross-sectional stepped wedge cluster randomised trial data. We compared the per cent bias, the type I error rate and power of these methods in a stepped wedge trial setting with a binary outcome, where there are few clusters available and when the appropriate adjustment for a time trend is made, which by design may be confounding the intervention effect. We found that the generalised linear mixed modelling approach is the most consistent when few clusters are available. We also found that none of the common analysis methods for stepped wedge trials were both unbiased and maintained a 5% type I error rate when there were only three clusters. Of the commonly used analysis approaches, we recommend the generalised linear mixed model for small stepped wedge trials with binary outcomes. We also suggest that in a stepped wedge design with three steps, at least two clusters be randomised at each step, to ensure that the intervention effect estimator maintains the nominal 5% significance level and is also reasonably unbiased.-Minimum number of clusters and comparison of analysis methods for cross sectional stepped wedge cluster randomised trials with binary outcomes: A simulation study.",3
"Motivated by an ongoing study to develop a screening test able to identify patients with undiagnosed Sj?gren's Syndrome in a symptomatic population, we propose methodology to combine multiple biomarkers and evaluate their performance in a two-stage group sequential design that proceeds as follows: biomarker data is collected from first stage samples; the biomarker panel is built and evaluated; if the panel meets pre-specified performance criteria the study continues to the second stage and the remaining samples are assayed. The design allows us to conserve valuable specimens in the case of inadequate biomarker panel performance. We propose a nonparametric conditional resampling algorithm that uses all the study data to provide unbiased estimates of the biomarker combination rule and the sensitivity of the panel corresponding to specificity of 1-t on the receiver operating characteristic curve (ROC). The Copas and Corbett (2002) correction, for bias resulting from using the same data to derive the combination rule and estimate the ROC, was also evaluated and an improved version was incorporated. An extensive simulation study was conducted to evaluate finite sample performance and propose guidelines for designing studies of this type. The methods were implemented in the National Cancer Institutes Early Detection Network Urinary PCA3 Evaluation Trial.-Unbiased estimation of biomarker panel performance when combining training and testing data in a group sequential design.",0
"Investigators often gather repeated measures on study subjects to directly measure how a subject's response changes with changes in explanatory variables. This paper focuses on several statistical issues related to assessing change with longitudinal and clustered binary data. Many popular approaches for analyzing repeated binary outcomes measure cross-sectional or between-subject, rather than within-subject, effects of covariates. The class of models known as cluster specific measures within-subject effects of covariates on responses but are subject to additional statistical complications. It is useful to decompose covariates into between- and within-cluster components. This paper describes several approaches that yield consistent estimates of the within-subject covariate effects of interest. Example data from three studies illustrate the results.-Assessing change with longitudinal and clustered binary data.",1
"Pooled testing is a procedure commonly used to reduce the cost of screening a large number of individuals for infectious diseases. In its simplest form, pooled testing works by compositing a set of individual specimens (e.g., blood or urine) into a common pool. If the pool tests negative, all individuals within it are diagnosed as negative. If the pool tests positive, retesting is needed to decode the positive individuals from the negative individuals. Traditionally, pooled testing has assumed that each individual has the same probability of being positive. However, this assumption is often unrealistic, especially when known risk factors can be used to measure distinct probabilities of positivity for each individual. In this paper, we investigate new pooled-testing algorithms that exploit the heterogeneity among individual probabilities and subsequently reduce the total number of tests needed, while maintaining accuracy levels similar to standard algorithms that do not account for heterogeneity. We apply these algorithms to data from the Infertility Prevention Project, a nationally implemented program supported by the Centers for Disease Control and Prevention.-Pooled-testing procedures for screening high volume clinical specimens in heterogeneous populations.",0
"Adverse psychosocial exposures in early life, namely experiences such as child maltreatment, caregiver stress or depression, and domestic or community violence, have been associated in epidemiological studies with increased lifetime risk of adverse outcomes, including diabetes, heart disease, cancers, and psychiatric illnesses. Additional work has shed light on the potential molecular mechanisms by which early adversity becomes ""biologically embedded"" in altered physiology across body systems. This review surveys evidence on such mechanisms and calls on researchers, clinicians, policymakers, and other practitioners to act upon evidence. Childhood psychosocial adversity has wide-ranging effects on neural, endocrine, immune, and metabolic physiology. Molecular mechanisms broadly implicate disruption of central neural networks, neuroendocrine stress dysregulation, and chronic inflammation, among other changes. Physiological disruption predisposes individuals to common diseases across the life course. Reviewed evidence has important implications for clinical practice, biomedical research, and work across other sectors relevant to public health and child wellbeing. Warranted changes include increased clinical screening for exposures among children and adults, scale-up of effective interventions, policy advocacy, and ongoing research to develop new evidence-based response strategies.-Biological embedding of childhood adversity: from physiological mechanisms to clinical implications.",0
"The existing missing data literature does not provide a clear prescription for estimating interaction effects with missing data, particularly when the interaction involves a pair of continuous variables. In this article, we describe maximum likelihood and multiple imputation procedures for this common analysis problem. We outline 3 latent variable model specifications for interaction analyses with missing data. These models apply procedures from the latent variable interaction literature to analyses with a single indicator per construct (e.g., a regression analysis with scale scores). We also discuss multiple imputation for interaction effects, emphasizing an approach that applies standard imputation procedures to the product of 2 raw score predictors. We thoroughly describe the process of probing interaction effects with maximum likelihood and multiple imputation. For both missing data handling techniques, we outline centering and transformation strategies that researchers can implement in popular software packages, and we use a series of real data analyses to illustrate these methods. Finally, we use computer simulations to evaluate the performance of the proposed techniques.-Estimating interaction effects with incomplete predictor variables.",0
"It has long been recognized that sample size calculations for cluster randomized trials require consideration of the correlation between multiple observations within the same cluster. When measurements are taken at anything other than a single point in time, these correlations depend not only on the cluster but also on the time separation between measurements and additionally, on whether different participants (cross-sectional designs) or the same participants (cohort designs) are repeatedly measured. This is particularly relevant in trials with multiple periods of measurement, such as the cluster cross-over and stepped-wedge designs, but also to some degree in parallel designs. Several papers describing sample size methodology for these designs have been published, but this methodology might not be accessible to all researchers. In this article we provide a tutorial on sample size calculation for cluster randomized designs with particular emphasis on designs with multiple periods of measurement and provide a web-based tool, the Shiny CRT Calculator, to allow researchers to easily conduct these sample size calculations. We consider both cross-sectional and cohort designs and allow for a variety of assumed within-cluster correlation structures. We consider cluster heterogeneity in treatment effects (for designs where treatment is crossed with cluster), as well as individually randomized group-treatment trials with differential clustering between arms, for example designs where clustering arises from interventions being delivered in groups. The calculator will compute power or precision, as a function of cluster size or number of clusters, for a wide variety of designs and correlation structures. We illustrate the methodology and the flexibility of the Shiny CRT Calculator using a range of examples.-A tutorial on sample size calculation for cluster randomised multiple-period parallel, cross-over and stepped-wedge trials using the Shiny CRT Calculator",1
"Some experimental designs involve clustering within only one treatment group. Such designs may involve group tutoring, therapy administered by multiple therapists, or interventions administered by clinics for the treatment group, whereas the control group receives no treatment. In such cases, the data analysis often proceeds as if there were no clustering within the treatment group. A consequence is that the actual significance level of the treatment effects is larger (i.e., actual p values are larger) than nominal. Additionally, biases will be introduced in estimates of the effect sizes and their variances, leading to inflated effects and underestimated variances when clustering in the treatment group is not taken into account. These consequences of clustering can seriously compromise the interpretation of study results. This article shows how information on the intraclass correlation can be used to obtain a correction for biases in the effect sizes and their variances, and also to obtain an adjustment to the significance test for the effects of clustering.-Estimating effect size when there is clustering in one treatment group.",1
"To determine whether there are ethnic differences in preferences for surgery vs. medical treatment of knee osteoarthritis (OA). Cross-sectional in-person interviews using conjoint analysis methodology, a technique often used in marketing, involved individuals making choices between alternative hypothetical scenarios for medical or surgical treatment of knee OA. One hundred ninety-three individuals over the age of 20 were recruited through random digit dialing in Harris County, TX, and 198 individuals with knee OA were recruited from a large outpatient health care provider in Houston, TX. African Americans were significantly less likely to chose surgery than whites (odds ratio 0.63 [0.42, 0.93]). Women and older individuals were also less likely to choose surgery (0.69 [0.51, 0.94], 0.98 [0.97, 0.99]). Larger reductions in negative symptoms with surgery significantly increased the likelihood of choosing surgery. There was no difference between the public and patients, and no effect of income level. Disparities in knee replacement rates among ethnic groups may be partly due to differences in preferences for surgery. Conjoint analysis was shown to be a feasible methodology for collecting preferences in health research. This methodology has great promise in contributing to our knowledge of drivers of health care decision making in individuals.-Racial/ethnic differences in preferences for total knee replacement surgery.",0
"Data that have a multilevel structure occur frequently across a range of disciplines, including epidemiology, health services research, public health, education and sociology. We describe three families of regression models for the analysis of multilevel survival data. First, Cox proportional hazards models with mixed effects incorporate cluster-specific random effects that modify the baseline hazard function. Second, piecewise exponential survival models partition the duration of follow-up into mutually exclusive intervals and fit a model that assumes that the hazard function is constant within each interval. This is equivalent to a Poisson regression model that incorporates the duration of exposure within each interval. By incorporating cluster-specific random effects, generalised linear mixed models can be used to analyse these data. Third, after partitioning the duration of follow-up into mutually exclusive intervals, one can use discrete time survival models that use a complementary log-log generalised linear model to model the occurrence of the outcome of interest within each interval. Random effects can be incorporated to account for within-cluster homogeneity in outcomes. We illustrate the application of these methods using data consisting of patients hospitalised with a heart attack. We illustrate the application of these methods using three statistical programming languages (R, SAS and Stata).-A Tutorial on Multilevel Survival Analysis: Methods, Models and Applications.",1
"The fetal origins hypothesis emphasizes the life-long health impacts of prenatal conditions. Birth weight, birth length, and gestational age are indicators of the fetal environment. However, these variables often have missing data and are subject to random and systematic errors caused by delays in measurement, differences in measurement instruments, and human error. With data from the Cebu (Philippines) Longitudinal Health and Nutrition Survey, we use structural equation models, to explore random and systematic errors in these birth outcome measures, to analyze how maternal characteristics relate to birth outcomes, and to take account of missing data. We assess whether birth weight, birth length, and gestational age are influenced by a single latent variable that we call favorable fetal growth conditions (FFGC) and if so, which variable is most closely related to FFGC. We find that a model with FFGC as a latent variable fits as well as a less parsimonious model that has birth weight, birth length, and gestational age as distinct individual variables. We also demonstrate that birth weight is more reliably measured than is gestational age. FFGCs were significantly influenced by taller maternal stature, better nutritional stores indexed by maternal arm fat and muscle area during pregnancy, higher birth order, avoidance of smoking, and maternal age 20-35 years. Effects of maternal characteristics on newborn weight, length, and gestational age were largely indirect, operating through FFGC.-Are gestational age, birth weight, and birth length indicators of favorable fetal growth conditions? A structural equation analysis of Filipino infants.",0
"Clinical trials are often performed using a group sequential design in order to allow investigators to review the accumulating data sequentially and possibly terminate the trial early for efficacy or futility. Standard methods for comparing survival distributions have been shown under varying levels of generality to follow an independent increments structure. In the presence of competing risks, where the occurrence of one type of event precludes the occurrence of another type of event, researchers may be interested in inference on the cumulative incidence function, which describes the probability of experiencing a particular event by a given time. This manuscript shows that two commonly used tests for comparing cumulative incidence functions, a pointwise comparison at a single point, and Gray's test, also follow the independent increments structure when used in a group sequential setting. A simulation study confirms the theoretical derivations even for modest trial sample sizes. We used two examples of clinical trials in hematopoietic cell transplantation to illustrate the techniques.-The use of group sequential designs with?common competing risks tests.",0
"Countries with ongoing outbreaks of Zika virus have observed a notable rise in reported cases of Guillain-Barr? syndrome (GBS), with mounting evidence of a causal link between Zika virus infection and the neurological syndrome. However, the risk of GBS following a Zika virus infection is not well characterized. In this work, we used data from 11 locations with publicly available data to estimate the risk of GBS following an infection with Zika virus, as well as the location-specific incidence of infection and the number of suspect GBS cases reported per infection. We built a mathematical inference framework utilizing data from 11 locations that had reported suspect Zika and GBS cases, two with completed outbreaks prior to 2015 (French Polynesia and Yap) and nine others in the Americas covering partial outbreaks and where transmission was ongoing as of early 2017. We estimated that 2.0 (95% credible interval 0.5-4.5) reported GBS cases may occur per 10,000 Zika virus infections. The frequency of reported suspect Zika cases varied substantially and was highly uncertain, with a mean of 0.11 (95% credible interval 0.01-0.24) suspect cases reported per infection. These estimates can help efforts to prepare for the GBS cases that may occur during Zika epidemics and highlight the need to better understand the relationship between infection and the reported incidence of clinical disease.-Guillain-Barr? syndrome risk among individuals infected with Zika virus: a multi-country assessment.",0
"Genital infection by human papillomavirus (HPV) is a common sexually transmitted disease, with over 25 per cent prevalence among young women in the US. Infections are usually without symptoms and transient (or reversible), but a small proportion of infections persist and are believed to be responsible for nearly all cervical cancers and precursor lesions such as cervical intraepithelial neoplasia (CIN). Therefore, successful vaccines against persistent HPV infections could have a great impact in preventing cervical cancers. In trials being planned, ongoing, and recently completed, a log-rank or a similar test may be employed to assess a vaccine effect in comparison to placebo, with an infection 'event' defined to capture persistent but not transient infections. However, it is not clear how best to define such an event, because (1) diagnostic tests cannot distinguish a persistent from a transient infection, (2) participants are only examined periodically, and (3) there can be misclassification errors in the detection of infections. This paper evaluates several definitions of persistent infection that are based on periodically observed infection statuses by postulating a multi-state model for persistent and transient infections. The type I error and the power of tests on vaccine efficacy based on these operational definitions are then examined under various scenarios of how a vaccine might affect the infection-disease process. We find that none of the candidates performs satisfactorily, thus raising concerns that clinical trials based only on infection endpoints will not be reliable.-Evaluation of log-rank tests for infrequent observations from a multi-state process, with application to HPV vaccine efficacy.",0
"In cluster randomized trials (CRTs), clusters of individuals are randomized rather than the individuals themselves. For such trials, power depends in part on the degree of similarity among responses within a cluster, which is quantified by the intaclass correlation coefficient (ICC). Thus, for a fixed sample size, power decreases with increasing ICC. In reliability studies with two observers, dichotomizing a continuous outcome variable has been shown to reduce the ICC. We checked (by a simulation study) that this property still applies to CRTs, in which cluster sizes are variable and usually greater than in reliability studies and observations (within clusters) are exchangeable. Then, in a CRT, dichotomizing a continuous outcome actually induces two antagonistic effects: decreased power because of loss of information and increased power induced by attenuation of the ICC. Therefore, we aimed to assess the impact of dichotomizing a continuous outcome on power in a CRT. We derived an analytical formula for power based on a generalized estimating equation approach after dichotomizing a continuous outcome. This theoretical result was obtained by considering equal cluster sizes, and we then assessed its accuracy (by a simulation study) in the more realistic situation of varying cluster sizes. We showed that dichotomization is associated with decreased power: attenuation of the ICC does not compensate for the loss of power induced by loss of information. Loss of power is reduced with increased initial continuous-outcome ICC and/or prevalence of success for the dichotomized outcome approaching 50%.-Dichotomizing a continuous outcome in cluster randomized trials: impact on power.",1
"When randomizations are assigned at the cluster level for longitudinal cluster randomized trials (longitudinal-CRTs) with a continuous outcome, formulae for determining the required sample size to detect a two-way interaction effect between time and intervention are available. To show that (1) those same formulae can also be applied to longitudinal trials when randomizations are assigned at the subject level within clusters and (2) this property can be extended to 2-by-2 factorial longitudinal-CRTs with two treatments and different levels of randomization for which testing a three-way interaction between time and the two interventions is of primary interest. We show that slope estimates from different treatment arms are uncorrelated, regardless of whether randomization occurs at the third or second level and also regardless of whether slopes are considered fixed or random in the mixed-effects model for testing two-way or three-way interactions. Sample size formulae are extended to unbalanced designs. Simulation studies were applied to verify the findings. Sample size formulae for testing two-way and three-way interactions in longitudinal-CRTs with second-level randomization are identical to those for trials with third-level randomization. In addition, the total number of observations required for testing a three-way interaction is demonstrated to be four times as large as that required for testing a two-way interaction, regardless of the level of randomization for both fixed- and random-slope models. The findings may be only applicable to longitudinal-CRTs with normally distributed continuous outcome. All of the findings are validated by simulation studies and enable the design of longitudinal clinical trials to be more flexible in regard to the level of randomization and allocation of clusters and subjects.-Sample size requirements to detect a two- or three-way interaction in longitudinal cluster randomized clinical trials with second-level randomization.",1
"The use of outcome-dependent sampling with longitudinal data analysis has previously been shown to improve efficiency in the estimation of regression parameters. The motivating scenario is when outcome data exist for all cohort members but key exposure variables will be gathered only on a subset. Inference with outcome-dependent sampling designs that also incorporates incomplete information from those individuals who did not have their exposure ascertained has been investigated for univariate but not longitudinal outcomes. Therefore, with a continuous longitudinal outcome, we explore the relative contributions of various sources of information toward the estimation of key regression parameters using a likelihood framework. We evaluate the efficiency gains that alternative estimators might offer over random sampling, and we offer insight into their relative merits in select practical scenarios. Finally, we illustrate the potential impact of design and analysis choices using data from the Cystic Fibrosis Foundation Patient Registry.-Likelihood-based analysis of outcome-dependent sampling designs with longitudinal data.",0
"Repeated measures are common in clinical trials and epidemiological studies. Designing studies with repeated measures requires reasonably accurate specifications of the variances and correlations to select an appropriate sample size. Underspecifying the variances leads to a sample size that is inadequate to detect a meaningful scientific difference, while overspecifying the variances results in an unnecessarily large sample size. Both lead to wasting resources and placing study participants in unwarranted risk. An internal pilot design allows sample size recalculation based on estimates of the nuisance parameters in the covariance matrix. We provide the theoretical results that account for the stochastic nature of the final sample size in a common class of linear mixed models. The results are useful for designing studies with repeated measures and balanced design. Simulations examine the impact of misspecification of the covariance matrix and demonstrate the accuracy of the approximations in controlling the type I error rate and achieving the target power. The proposed methods are applied to a longitudinal study assessing early antiretroviral therapy for youth living with HIV.-Internal pilot design for balanced repeated measures.",0
"Cluster-randomized trials are often conducted to assess vaccine effects. Defining estimands of interest before conducting a trial is integral to the alignment between a study's objectives and the data to be collected and analyzed. This paper considers estimands and estimators for overall, indirect, and total vaccine effects in trials, where clusters of individuals are randomized to vaccine or control. The scenario is considered where individuals self-select whether to participate in the trial, and the outcome of interest is measured on all individuals in each cluster. Unlike the overall, indirect, and total effects, the direct effect of vaccination is shown in general not to be estimable without further assumptions, such as no unmeasured confounding. An illustrative example motivated by a cluster-randomized typhoid vaccine trial is provided.-Estimands and inference in cluster-randomized vaccine trials",1
"The objective of this simulation study is to compare the accuracy and efficiency of population-averaged (i.e. generalized estimating equations (GEE)) and cluster-specific (i.e. random-effects logistic regression (RELR)) models for analyzing data from cluster randomized trials (CRTs) with missing binary responses. In this simulation study, clustered responses were generated from a beta-binomial distribution. The number of clusters per trial arm, the number of subjects per cluster, intra-cluster correlation coefficient, and the percentage of missing data were allowed to vary. Under the assumption of covariate dependent missingness, missing outcomes were handled by complete case analysis, standard multiple imputation (MI) and within-cluster MI strategies. Data were analyzed using GEE and RELR. Performance of the methods was assessed using standardized bias, empirical standard error, root mean squared error (RMSE), and coverage probability. GEE performs well on all four measures--provided the downward bias of the standard error (when the number of clusters per arm is small) is adjusted appropriately--under the following scenarios: complete case analysis for CRTs with a small amount of missing data; standard MI for CRTs with variance inflation factor (VIF) &lt;3; within-cluster MI for CRTs with VIF?3 and cluster size&gt;50. RELR performs well only when a small amount of data was missing, and complete case analysis was applied. GEE performs well as long as appropriate missing data strategies are adopted based on the design of CRTs and the percentage of missing data. In contrast, RELR does not perform well when either standard or within-cluster MI strategy is applied prior to the analysis.-Comparison of population-averaged and cluster-specific models for the analysis of cluster randomized trials with missing binary outcomes: a simulation study.",1
"To draw valid inference about an indirect effect in a mediation model, there must be no omitted confounders. No omitted confounders means that there are no common causes of hypothesized causal relationships. When the no-omitted-confounder assumption is violated, inference about indirect effects can be severely biased and the results potentially misleading. Despite the increasing attention to address confounder bias in single-level mediation, this topic has received little attention in the growing area of multilevel mediation analysis. A formidable challenge is that the no-omitted-confounder assumption is untestable. To address this challenge, we first analytically examined the biasing effects of potential violations of this critical assumption in a two-level mediation model with random intercepts and slopes, in which all the variables are measured at Level 1. Our analytic results show that omitting a Level 1 confounder can yield misleading results about key quantities of interest, such as Level 1 and Level 2 indirect effects. Second, we proposed a sensitivity analysis technique to assess the extent to which potential violation of the no-omitted-confounder assumption might invalidate or alter the conclusions about the indirect effects observed. We illustrated the methods using an empirical study and provided computer code so that researchers can implement the methods discussed.-Assessing Omitted Confounder Bias in Multilevel Mediation Models.",1
"We propose a method for defining and empirically validating episodes of alcoholism treatment from health care utilization records. The study includes utilization records from 86,207 patients enrolled in a large managed behavioral care company who had at least one alcoholism encounter between 1991 and 1998. Treatment episodes are defined as a minimum number of alcoholism treatment encounters with the behavioral care company prior to a ""clear zone"" of no encounters. Statistical procedures to select a subset of episode definitions from a number of candidate definitions and methods for assessing the convergent and criterion validity of the definitions are presented. The percentage of patients having at least one episode of alcoholism treatment varies from 43% to 77%, with the results being more sensitive to the minimum number of encounters required than the length of the clear zone. Criterion validity does not reveal any clear ""winning"" definitions; positive predictive ability increases most rapidly when going from 2 to 3 encounters required. The most robust definitions of an alcoholism treatment episode entail 3 to 4 encounters with a clear zone of 3 to 4 months.-Alcoholism treatment episodes validly defined using mental health care utilization records.",0
"The Gubbio Study is a prospective epidemiological study on the population residing in the city of Gubbio, Italy. Original objectives of the study were the control of hypertension and the role of cellular electrolyte handling in hypertension. Other objectives were added during the 30-year activity of the study. The original target cohort consists of individuals aged ?5 years residing within the medieval walls of the city. To complete family genealogies, individuals residing outside the city were also included. Three active screenings (exams) were conducted. A total of 5376 individuals (response rate 92%) participated in Exam 1 which was performed in 1983-86. Follow-up exams were completed between 1989-92 and 2001-2007. Data categories included demographics, personal and family medical history, lifestyle habits, education, type of work, anthropometry, blood pressure, pulse rate, blood biochemistry, urine biochemistry and special investigations on cellular electrolyte handling. Electrocardiogram, echocardiogram, 24-h ambulatory blood pressure and uroflowmetry were performed in selected subgroups defined by age and/or sex. Data about hospitalizations, mortality and causes of death were collected starting from completion of Exam 1. The study shared the data with other studies.-Cohort profile: The Gubbio Population Study.",0
"The incidence and prevalence of autism have dramatically increased over the last 20 years. Decomposition of autism incidence rates into age, period and cohort effects disentangle underlying domains of causal factors linked to time trends. We estimate an age-period-cohort effect model for autism diagnostic incidence overall and by level of functioning. Data are drawn from sequential cohorts of all 6 501 262 individuals born in California from 1992 to 2003. Autism diagnoses from 1994 to 2005 were ascertained from the California Department of Development Services Client Development and Evaluation Report. Compared with those born in 1992, each successively younger cohort has significantly higher odds of an autism diagnosis than the previous cohort, controlling for age and period effects. For example, individuals born in 2003 have 16.6 times the odds of an autism diagnosis compared with those born in 1992 [95% confidence interval (CI) 7.8-35.3]. The cohort effect observed in these data is stronger for high than for low-functioning children with an autism diagnosis. Autism incidence in California exhibits a robust and linear positive cohort effect that is stronger among high-functioning children with an autism diagnosis. This finding indicates that the primary drivers of the increases in autism diagnoses must be factors that: (i) have increased linearly year-to-year; (ii) aggregate in birth cohorts; and (iii) are stronger among children with higher levels of functioning.-Cohort effects explain the increase in autism diagnosis among children born from 1992 to 2003 in California.",0
"We propose a Bayesian response-adaptive covariate-balanced (RC) randomization design for multiple-arm comparative clinical trials. The goal of the design is to skew the allocation probability to more efficacious treatment arms, while also balancing the distribution of the covariates across the arms. In particular, we first propose a new covariate-adaptive randomization (CA) method based on a prognostic score that naturally accommodates continuous and categorical prognostic factors and automatically assigns imbalance weights to covariates according to their importance in response prediction. We then incorporate this CA design into a group sequential response-adaptive randomization (RA) scheme. The resulting RC randomization design combines the advantages of both CA and RA randomizations and meets the design goal. We illustrate the proposed design through its application to a phase II leukemia clinical trial, and evaluate its operating characteristics through simulation studies.-A Bayesian response-adaptive covariate-balanced randomization design with application to a leukemia clinical trial.",0
"In a cluster randomized trial (CRT), groups of people are randomly assigned to different interventions. Existing parametric and semiparametric methods for CRTs rely on distributional assumptions or a large number of clusters to maintain nominal confidence interval (CI) coverage. Randomization-based inference is an alternative approach that is distribution-free and does not require a large number of clusters to be valid. Although it is well-known that a CI can be obtained by inverting a randomization test, this requires testing a non-zero null hypothesis, which is challenging with non-continuous and survival outcomes. In this article, we propose a general method for randomization-based CIs using individual-level data from a CRT. This approach accommodates various outcome types, can account for design features such as matching or stratification, and employs a computationally efficient algorithm. We evaluate this method's performance through simulations and apply it to the Botswana Combination Prevention Project, a large HIV prevention trial with an interval-censored time-to-event outcome.-Randomization-based confidence intervals for cluster randomized trials",1
"The method of generalized estimating equations has become almost standard for analysing longitudinal and other correlated response data. However, we have found that if binary responses have less than binomial variation over clusters, and are modelled using exchangeable correlations, prevailing software implementations may give unreliable results. Bounding the negative correlation away from its theoretical minimum may not always be a satisfactory solution. In such instances, using the independence working correlation structure and robust SEs is a more trustworthy alternative.-GEE analysis of negatively correlated binary responses: a caution.",1
"We examined the association between menstrual patterns and risk of developing adult-onset diabetes in a prospective study of 668 white, college-educated women who completed menstrual diaries throughout their reproductive years. We calculated summary measures of cycle length and variability and bleeding duration for ages &lt; or = 22, 23-27, 28-32, and 33-37 years. The analysis included 35,418 person-years of follow-up and 49 self-reported cases of diabetes (median age at diagnosis, 63 years). There was no association between diabetes risk and age at menarche, mean cycle length, cycle variability, or frequency of long cycles (&gt; 42 days). Longer bleeding periods in the mid- and late reproductive years were somewhat associated with an increased risk of diabetes (adjusted rate ratio 1.4, 95% confidence interval 1.0-1.8 per day increase in bleeding duration for menses during ages 28-32). These results do not support the association of long or irregular menstrual cycles with post-menopausal diabetes incidence, but do suggest a possible association of longer bleeding duration with subsequent onset of diabetes.-Menstrual patterns and risk of adult-onset diabetes mellitus.",0
"Cluster randomized trials are popular in health-related research due to the need or desire to randomize clusters of subjects to different trial arms as opposed to randomizing each subject individually. As outcomes from subjects within the same cluster tend to be more alike than outcomes from subjects within other clusters, an exchangeable correlation arises that is measured via the intra-cluster correlation coefficient. Intra-cluster correlation coefficient estimation is especially important due to the increasing awareness of the need to publish such values from studies in order to help guide the design of future cluster randomized trials. Therefore, numerous methods have been proposed to accurately estimate the intra-cluster correlation coefficient, with much attention given to binary outcomes. As marginal models are often of interest, we focus on intra-cluster correlation coefficient estimation in the context of fitting such a model with binary outcomes using generalized estimating equations. Traditionally, intra-cluster correlation coefficient estimation with generalized estimating equations has been based on the method of moments, although such estimators can be negatively biased. Furthermore, alternative estimators that work well, such as the analysis of variance estimator, are not as readily applicable in the context of practical data analyses with generalized estimating equations. Therefore, in this article we assess, in terms of bias, the readily available residual pseudo-likelihood approach to intra-cluster correlation coefficient estimation with the GLIMMIX procedure of SAS (SAS Institute, Cary, NC). Furthermore, we study a possible corresponding approach to confidence interval construction for the intra-cluster correlation coefficient. We utilize a simulation study and application example to assess bias in intra-cluster correlation coefficient estimates obtained from GLIMMIX using residual pseudo-likelihood. This estimator is contrasted with method of moments and analysis of variance estimators which are standards of comparison. The approach to confidence interval construction is assessed by examining coverage probabilities. Overall, the residual pseudo-likelihood estimator performs very well. It has considerably less bias than moment estimators, which are its competitor for general generalized estimating equation-based analyses, and therefore, it is a major improvement in practice. Furthermore, it works almost as well as analysis of variance estimators when they are applicable. Confidence intervals have near-nominal coverage when the intra-cluster correlation coefficient estimate has negligible bias. Our results show that the residual pseudo-likelihood estimator is a good option for intra-cluster correlation coefficient estimation when conducting a generalized estimating equation-based analysis of binary outcome data arising from cluster randomized trials. The estimator is practical in that it is simply a result from fitting a marginal model with GLIMMIX, and a confidence interval can be easily obtained. An additional advantage is that, unlike most other options for performing generalized estimating equation-based analyses, GLIMMIX provides analysts the option to utilize small-sample adjustments that ensure valid inference.-A readily available improvement over method of moments for intra-cluster correlation estimation in the context of cluster randomized trials and fitting a GEE-type marginal model for binary outcomes.",1
"Design for intervention studies may combine longitudinal data collected from sampled locations over several survey rounds and cross-sectional data from other locations in the study area. In this case, modeling the impact of the intervention requires an approach that can accommodate both types of data, accounting for the dependence between individuals followed up over time. Inadequate modeling can mask intervention effects, with serious implications for policy making. In this paper we use data from a large-scale larviciding intervention for malaria control implemented in Dar es Salaam, United Republic of Tanzania, collected over a period of almost 5 years. We apply a longitudinal Bayesian spatial model to the Dar es Salaam data, combining follow-up and cross-sectional data, treating the correlation in longitudinal observations separately, and controlling for potential confounders. An innovative feature of this modeling is the use of Ornstein-Uhlenbeck process to model random time effects. We contrast the results with other Bayesian modeling formulations, including cross-sectional approaches that consider individual-level random effects to account for subjects followed up in two or more surveys. The longitudinal modeling approach indicates that the intervention significantly reduced the prevalence of malaria infection in Dar es Salaam by 20% whereas the joint model did not suggest significance within the results. Our results suggest that the longitudinal model is to be preferred when longitudinal information is available at the individual level.-Joint spatial Bayesian modeling for studies combining longitudinal and cross-sectional data.",0
"In this paper, we develop methods to combine multiple biomarker trajectories into a composite diagnostic marker using functional data analysis (FDA) to achieve better diagnostic accuracy in monitoring disease recurrence in the setting of a prospective cohort study. In such studies, the disease status is usually verified only for patients with a positive test result in any biomarker and is missing in patients with negative test results in all biomarkers. Thus, the test result will affect disease verification, which leads to verification bias if the analysis is restricted only to the verified cases. We treat verification bias as a missing data problem. Under both missing at random (MAR) and missing not at random (MNAR) assumptions, we derive the optimal classification rules using the Neyman-Pearson lemma based on the composite diagnostic marker. We estimate thresholds adjusted for verification bias to dichotomize patients as test positive or test negative, and we evaluate the diagnostic accuracy using the verification bias corrected area under the ROC curves (AUCs). We evaluate the performance and robustness of the FDA combination approach and assess the consistency of the approach through simulation studies. In addition, we perform a sensitivity analysis of the dependency between the verification process and disease status for the approach under the MNAR assumption. We apply the proposed method on data from the Religious Orders Study and from a non-small cell lung cancer trial.-Combining biomarker trajectories to improve diagnostic accuracy in prospective cohort studies with verification bias.",0
"To describe multilevel recruitment strategies for an ongoing clinical trial in pediatric primary care settings, and assess adoption and reach of these strategies via the RE-AIM framework. This study is part of a larger pragmatic cluster randomized clinical trial focused on the effectiveness of interventions on the practice, provider, and caregiver levels on dental utilization for Medicaid-enrolled 3-6 year old children. Pediatric practices were recruited according to the proportion of Medicaid-eligible children, geographic region, and County. In accordance with the RE-AIM framework, providers reached were those approached directly and consented, and those who participated in the intervention training adopted to deliver the intervention. Caregivers reached were those approached and consented at their child's well-child visit to participate in the trial. Recruitment goals were met over a 21 month period, with an overall enrollment of 18 practices, 62 providers, and 1024 caregivers-child dyads. The majority of practices enrolled were small, suburban, and located in an urban county. The participation rates among approached providers and caregivers was 93% and 84% respectively. Enablers for recruitment was the one-on-one interaction with the provider and caregivers. Barriers to recruitment for caregivers included no-shows and cancellations at well-child visits. Adoption of intervention among providers was high, and caregiver reached were representative of the eligible target population. Active approaches to recruitment, such as utilizing opinion leaders, in-person recruitment, and building relationships with practice staff, can result in successful enrollment and imp lementation of a multi-level intervention in pediatric primary care settings.-Recruitment strategies for a pragmatic cluster randomized oral health trial in pediatric primary care settings.",1
"Cluster randomized trials increasingly are being used in health services research and in primary care, yet the majority of these trials do not account appropriately for the clustering in their analysis. We review the main implications of adopting a cluster randomized design in primary care and highlight the practical application of appropriate analytical techniques. The application of different analytical techniques is demonstrated through the use of empirical data from a primary care-based case study. Inappropriate analysis of cluster trials can lead to the presentation of inaccurate results and hence potentially misleading conclusions. We have demonstrated that adjustment for clustering can be applied to real-life data and we encourage more routine adoption of appropriate analytical techniques.-Analysis of cluster randomized trials in primary care: a practical approach.",1
"In multilevel settings such as individual participant data meta-analysis, a variable is 'systematically missing' if it is wholly missing in some clusters and 'sporadically missing' if it is partly missing in some clusters. Previously proposed methods to impute incomplete multilevel data handle either systematically or sporadically missing data, but frequently both patterns are observed. We describe a new multiple imputation by chained equations (MICE) algorithm for multilevel data with arbitrary patterns of systematically and sporadically missing variables. The algorithm is described for multilevel normal data but can easily be extended for other variable types. We first propose two methods for imputing a single incomplete variable: an extension of an existing method and a new two-stage method which conveniently allows for heteroscedastic data. We then discuss the difficulties of imputing missing values in several variables in multilevel data using MICE, and show that even the simplest joint multilevel model implies conditional models which involve cluster means and heteroscedasticity. However, a simulation study finds that the proposed methods can be successfully combined in a multilevel MICE procedure, even when cluster means are not included in the imputation models.-Multiple imputation by chained equations for systematically and sporadically missing multilevel data.",1
"Although widely available, little is known about the effectiveness of youth cessation treatments delivered in real-world settings. The authors recruited a nonprobability sample of 41 community-based group-format programs that treated at least 15 youth per year and included evidence-based treatment components. Data collection included longitudinal surveys of youth participants (n = 878); posttreatment surveys of program leaders (n = 77); and one-time surveys of organizational leaders (n = 64)and community leaders in education, health, and juvenile justice (n = 94). Information about smoking-related ordinances was collected at the state and local levels. The framework, evaluation design, and implementation strategies described in this article provide a template for large-scale real-world program evaluations.-A national evaluation of community-based youth cessation programs: design and implementation.",0
"The cluster-randomized trial is the methodology of choice for evaluating interventions administered at the group level such as public health and healthcare quality improvement interventions. Because of unique features of this design, it can be difficult to apply standard research ethics guidelines to cluster-randomized trials. The Ottawa Statement on the Ethical Design and Conduct of Cluster-Randomized Trials provides researchers and research ethics committees with comprehensive guidance on the ethical design, conduct and review of cluster-randomized trials. The Ottawa Statement supplements current national and international research ethics guidelines with guidance that is specific to cluster-randomized trials. In a recently published commentary, three examples drawn from the ClinicalTrials.gov registry were used to illustrate challenges associated with the cluster-randomized trial design. The commentary argued that the Ottawa Statement fails to provide comprehensive ethical guidance. In this article, we illustrate the application of the Ottawa Statement to the three trials. We challenge the conclusions reached in the commentary by demonstrating that an ethical analysis requires complete information. We correct some misperceptions about the cluster-randomized trial design. We collected essential additional information by contacting the authors of trials and by referring to published trial articles. We used the Ottawa Statement to conduct an ethical analysis of each trial and to address a number of substantive concerns raised regarding the identification of study participants, informed consent and harm benefit analysis. In the two cases in which we were able to obtain detailed study information, we were able to complete the ethical analysis prescribed by the Ottawa Statement. The Ottawa Statement does provide a useful framework for the ethical design, review and conduct of cluster-randomized trials.-Cluster-randomized trials: A closer look.",1
"When designing cluster randomized trials, it is important for researchers to be familiar with strategies to achieve valid study designs given limited resources. Constrained randomization is a technique to help ensure balance on pre-specified baseline covariates. The goal was to develop a randomization scheme that balanced 16 intervention and 16 control practices with respect to 7 factors that may influence improvement in study outcomes during a 4-year cluster randomized trial to improve colorectal cancer screening within a primary care practice-based research network. We used a novel approach that included simulating 30,000 randomization schemes, removing duplicates, identifying which schemes were sufficiently balanced, and randomly selecting one scheme for use in the trial. For a given factor, balance was considered achieved when the frequency of each factor's sub-classifications differed by no more than 1 between intervention and control groups. The population being studied includes approximately 32 primary care practices located in 19 states within the U.S. that care for approximately 56,000 patients at least 50 years old. Of 29,782 unique simulated randomization schemes, 116 were determined to be balanced according to pre-specified criteria for all 7 baseline covariates. The final randomization scheme was randomly selected from these 116 acceptable schemes. Using this technique, we were successfully able to find a randomization scheme that allocated 32 primary care practices into intervention and control groups in a way that preserved balance across 7 baseline covariates. This process may be a useful tool for ensuring covariate balance within moderately large cluster randomized trials.-An application of a modified constrained randomization process to a practice-based cluster randomized trial to improve colorectal cancer screening.",1
"The sandwich variance estimator of generalized estimating equations (GEE) may not perform well when the number of independent clusters is small. This could jeopardize the validity of the robust Wald test by causing inflated type I error and lower coverage probability of the corresponding confidence interval than the nominal level. Here, we investigate the small-sample performance of the robust score test for correlated data and propose several modifications to improve the performance. In a simulation study, we compare the robust score test to the robust Wald test for correlated Bernoulli and Poisson data, respectively. It is confirmed that the robust Wald test is too liberal whereas the robust score test is too conservative for small samples. To explain this puzzling operating difference between the two tests, we consider their applications to two special cases, one-sample and two-sample comparisons, thus motivating some modifications to the robust score test. A modification based on a simple adjustment to the usual robust score statistic by a factor of J/(J - 1) (where J is the number of clusters) reduces the conservativeness of the generalized score test. Simulation studies mimicking group-randomized clinical trials with binary and count responses indicated that it may improve the small-sample performance over that of the generalized score and Wald tests with test size closer to the nominal level. Finally, we demonstrate the utility of our proposal by applying it to a group-randomized clinical trial, trying alternative cafeteria options in schools (TACOS).-Small-sample performance of the robust score test and its modifications in generalized estimating equations.",1
"Cluster randomised trials (CRTs) are commonly analysed using mixed-effects models or generalised estimating equations (GEEs). However, these analyses do not always perform well with the small number of clusters typical of most CRTs. They can lead to increased risk of a type I error (finding a statistically significant treatment effect when it does not exist) if appropriate corrections are not used. We conducted a small simulation study to evaluate the impact of using small-sample corrections for mixed-effects models or GEEs in CRTs with a small number of clusters. We then reanalysed data from TRIGGER, a CRT with six clusters, to determine the effect of using an inappropriate analysis method in practice. Finally, we reviewed 100 CRTs previously identified by a search on PubMed in order to assess whether trials were using appropriate methods of analysis. Trials were classified as at risk of an increased type I error rate if they did not report using an analysis method which accounted for clustering, or if they had fewer than 40 clusters and performed an individual-level analysis without reporting the use of an appropriate small-sample correction. Our simulation study found that using mixed-effects models or GEEs without an appropriate correction led to inflated type I error rates, even for as many as 70 clusters. Conversely, using small-sample corrections provided correct type I error rates across all scenarios. Reanalysis of the TRIGGER trial found that inappropriate methods of analysis gave much smaller P values (P ? 0.01) than appropriate methods (P = 0.04-0.15). In our review, of the 99 trials that reported the number of clusters, 64 (65?%) were at risk of an increased type I error rate; 14 trials did not report using an analysis method which accounted for clustering, and 50 trials with fewer than 40 clusters performed an individual-level analysis without reporting the use of an appropriate correction. CRTs with a small or medium number of clusters are at risk of an inflated type I error rate unless appropriate analysis methods are used. Investigators should consider using small-sample corrections with mixed-effects models or GEEs to ensure valid results.-Increased risk of type I errors in cluster randomised trials with small or medium numbers of clusters: a review, reanalysis, and simulation study.",1
"Where treatments are administered to groups of patients or delivered by therapists, outcomes for patients in the same group or treated by the same therapist may be more similar, leading to clustering. Trials of such treatments should take account of this effect. Where such a treatment is compared with an un-clustered treatment, the trial has a partially nested design. This paper compares statistical methods for this design where the outcome is binary. Investigation of consistency reveals that a random coefficient model with a random effect for group or therapist is not consistent with other methods for a null treatment effect, and so this model is not recommended for this design. Small sample performance of a cluster-adjusted test of proportions, a summary measures test and logistic generalised estimating equations and random intercept models are investigated through simulation. The expected treatment effect is biased for the logistic models. Empirical test size of two-sided tests is raised only slightly, but there are substantial biases for one-sided tests. Three formulae are proposed for calculating sample size and power based on (i) the difference of proportions, (ii) the log-odds ratio or (iii) the arc-sine transformation of proportions. Calculated power from these formulae is compared with empirical power from a simulations study. Logistic models appeared to perform better than those based on proportions with the likelihood ratio test performing best in the range of scenarios considered. For these analyses, the log-odds ratio method of calculation of power gave an approximate lower limit for empirical power.-Design and analysis of trials with a partially nested design and a binary outcome measure.",2
"Pragmatic cluster randomized trials (CRTs) offer an opportunity to improve health care by answering important questions about the comparative effectiveness of treatments using a trial design that can be embedded in routine care. There is a lack of empirical research that addresses ethical issues generated by pragmatic CRTs in hemodialysis. To identify stakeholder perceptions of ethical issues in pragmatic CRTs conducted in hemodialysis. Qualitative study using semi-structured interviews. In-person or telephone interviews with an international group of stakeholders. Stakeholders (clinical investigators, methodologists, ethicists and research ethics committee members, and other knowledge users) who had been involved in the design or conduct of a pragmatic individual patient or cluster randomized trial in hemodialysis, or their role would require them to review and evaluate pragmatic CRTs in hemodialysis. Interviews were conducted in-person or over the telephone and were audio-recorded with consent. Recorded interviews were transcribed verbatim prior to analysis. Transcripts and field notes were analyzed using a thematic analysis approach. Sixteen interviews were conducted with 19 individuals. Interviewees were largely drawn from North America (84%) and were predominantly clinical investigators (42%). Six themes were identified in which pragmatic CRTs in hemodialysis raise ethical issues: (1) patients treated with hemodialysis as a vulnerable population, (2) appropriate approaches to informed consent, (3) research burdens, (4) roles and responsibilities of gatekeepers, (5) inequities in access to research, and (6) advocacy for patient-centered research and outcomes. Participants were largely from North America and did not include research staff, who may have differing perspectives. The six themes reflect concerns relating to individual rights, but also the need to consider population-level issues. To date, concerns regarding inequity of access to research and the need for patient-centered research have received less coverage than other, well-known, issues such as consent. Pragmatic CRTs offer a potential approach to address equity concerns and we suggest future ethical analyses and guidance for pragmatic CRTs in hemodialysis embed equity considerations within them. We further note the potential for the co-creation of health data infrastructure with patients which would aid care but also facilitate patient-centered research. These present results will inform planned future guidance in relation to the ethical design and conduct of pragmatic CRTs in hemodialysis. Registration is not applicable as this is a qualitative study.-Ethical Issues in the Design and Conduct of Pragmatic Cluster Randomized Trials in Hemodialysis Care: An Interview Study With Key Stakeholders.",1
Design and Analysis Issues in Community Trials,1
"Existing methods for the analysis of clustered, ordinal data are inappropriate for certain applications. We propose latent variable models for clustered ordinal data which are derived as natural extensions of latent variable models for clustered binary data (Qu, Williams, Beck, and Medendorp, 1992. Biometrics 48, 1095-1102). These models can be applied to repeated measures data, familial data, longitudinal data, and data with both cluster specific and occasion specific covariates with a wide range of correlation structures.-Latent variable models for clustered ordinal data.",1
"This paper investigates estimating and testing treatment effects in randomized control trials where imperfect diagnostic device is used to assign subjects to treatment and control group(s). The paper focuses on pre-post design and proposes two new methods for estimating and testing treatment effects. Furthermore, methods for computing sample sizes for such design accounting for misclassification of the subjects are devised. The methods are compared with each other and with a traditional method that ignores the imperfection of the diagnostic device. In particular, the likelihood-based approach shows a significant advantage in terms of power, coverage probability and, consequently, in reduction of the required sample size. The application of the results are illustrated with data from an aging trial for dementia and data from electroencephalogram (EEG) recordings of alcoholic and non-alcoholic subjects. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Assessing treatment efficacy in the presence of diagnostic errors.",0
"Compared to healthy controls, people with Alzheimer's disease (AD) have been shown to receive less pain medication and report pain less frequently. It is unknown if these findings reflect less perceived pain in AD, an inability to recognize pain, or an inability to communicate pain. To further examine aspects of pain processing in AD, we conducted a cross-sectional study of sex-matched adults ?65 years old with and without AD (AD: n = 40, female = 20, median age = 75; control: n = 40, female = 20, median age = 70) to compare the psychophysical response to contact-evoked perceptual heat thresholds of warmth, mild pain, and moderate pain, and self-reported unpleasantness for each percept. When compared to controls, participants with AD required higher temperatures to report sensing warmth (Cohen's d = 0.64, p = 0.002), mild pain (Cohen's d = 0.51, p = 0.016), and moderate pain (Cohen's d = 0.45, p = 0.043). Conversely, there were no significant between-group differences in unpleasantness ratings (p &gt; 0.05). The between-group findings demonstrate that when compared to controls, people with AD are less sensitive to the detection of thermal pain but do not differ in affective response to the unpleasant aspects of thermal pain. These findings suggest that people with AD may experience greater levels of pain and potentially greater levels of tissue or organ damage prior to identifying and reporting injury. This finding may help to explain the decreased frequency of pain reports and consequently a lower administration of analgesics in AD.-Contact heat sensitivity and reports of unpleasantness in communicative people with mild to moderate cognitive impairment in Alzheimer's disease: a cross-sectional study.",0
"Clustered randomised controlled trials (CRCTs) are increasingly common in primary care. Outcomes within the same cluster tend to be correlated with one another. In sample size calculations, estimates of the intra-cluster correlation coefficient (ICC) are needed to allow for this nonindependence. In studies with observations over more than one time period, estimates of the inter-period correlation (IPC) and the within-period correlation (WPC) are also needed. This is a retrospective cross-sectional study of all patients aged 18 or over with a diagnosis of type-2 diabetes, from The Health Improvement Network (THIN) database, between 1 October 2007 and 31 March 2010. We report estimates of the ICC, IPC, and WPC for typical outcomes using unadjusted and adjusted generalised linear mixed models with cluster and cluster by period random effects. For binary outcomes we report on the proportions scale, which is the appropriate scale for trial design. Estimated ICCs were compared to those reported from a systematic search of CRCTs undertaken in primary care in the UK in type-2 diabetes. Data from 430 general practices, with a median [IQR] number of diabetics per practice of 241 [150-351], were analysed. The ICC for HbA1c was 0.032 (95 % CI 0.026-0.038). For a two-period (each of 12 months) design, the WPC for HbA1c was 0.035 (95 % CI 0.030-0.040) and the IPC was 0.019 (95 % CI 0.014-0.026). The difference between the WPC and the IPC indicates a decay of correlation over time. Following dichotomisation at 7.5 %, the ICC for HbA1c was 0.026 (95 % CI 0.022-0.030). ICCs for other clinical measurements and clinical outcomes are presented. A systematic search of ICCs used in the design of CRCTs involving type-2 diabetes with HbA1c (undichotomised) as the outcome found that published trials tended to use more conservative ICC values (median 0.047, IQR 0.047-0.050) than those reported here. These estimates of ICCs, IPCs, and WPCs for a variety of outcomes commonly used in diabetes trials can be useful for the design of CRCTs. In studies with observations taken at different time-points, the correlation of observations may decay over time, as reflected in lower values for the IPC than for the ICC. The IPC and WPC estimates are the first reported for UK primary care data.-Intra-cluster and inter-period correlation coefficients for cross-sectional cluster randomised controlled trials for type-2 diabetes in UK primary care.",3
Individuals randomized to receive treatment in groups:  Implications for analysis,1
"Subjects in randomized controlled trials do not always comply to the treatment condition they have been assigned to. This may cause the estimated effect of the intervention to be biased and also affect efficiency, coverage of confidence intervals, and statistical power. In cluster randomized trials non-compliance may occur at the subject level but also at the cluster level. In the latter case, all subjects within the same cluster have the same compliance status. The purpose of this study is to investigate the statistical implications of non-compliance in cluster randomized trials. A simulation study was conducted with varying degrees of non-compliance at either the cluster level or subject level. The probability of non-compliance depends on a covariate at the cluster or subject level. Various realistic values of the intraclass correlation coefficient and cluster size are used. The data are analyzed by intention to treat, as treated, per protocol and the instrumental variable approach. The results show non-compliance may result in downward biased estimates of the intervention effect and an under- or overestimate of its standard deviation. The coverage of the confidence intervals may be too small, and in most cases, empirical power is too small. The results are more severe when the probability of non-compliance increases and the covariate that affects compliance is unobserved. It is advocated to avoid non-compliance. If this is not possible, compliance status and covariates that affect compliance should be measured and included in the statistical model.-What are the statistical implications of treatment non-compliance in cluster randomized trials: A simulation study",1
"Pooling biospecimens prior to performing laboratory assays is a useful tool to reduce costs, achieve minimum volume requirements and mitigate assay measurement error. When estimating the risk of a continuous, pooled exposure on a binary outcome, specialized statistical techniques are required. Current methods include a regression calibration approach, where the expectation of the individual-level exposure is calculated by adjusting the observed pooled measurement with additional covariate data. While this method employs a linear regression calibration model, we propose an alternative model that can accommodate log-linear relationships between the exposure and predictive covariates. The proposed model permits direct estimation of the relative risk associated with a log-transformation of an exposure measured in pools. Published 2016. This article is a U.S. Government work and is in the public domain in the USA.-Estimating relative risk of a log-transformed exposure measured in pools.",0
"Crohn's disease (CD) is a life-long condition associated with recurrent relapses characterized by abdominal pain, weight loss, anemia, and persistent diarrhea. In the US, there are approximately 780?000 CD patients and 33?000 new cases added each year. In this article, we propose a new network meta-regression approach for modeling ordinal outcomes in order to assess the efficacy of treatments for CD. Specifically, we develop regression models based on aggregate covariates for the underlying cut points of the ordinal outcomes as well as for the variances of the random effects to capture heterogeneity across trials. Our proposed models are particularly useful for indirect comparisons of multiple treatments that have not been compared head-to-head within the network meta-analysis framework. Moreover, we introduce Pearson residuals and construct an invariant test statistic to evaluate goodness-of-fit in the setting of ordinal outcome data. A detailed case study demonstrating the usefulness of the proposed methodology is carried out using aggregate ordinal outcome data from 16 clinical trials for treating CD.-Network meta-regression for ordinal outcomes: Applications in comparing Crohn's disease treatments.",0
"We propose using a class of community-level measures--environmental indicators--as part of the evaluation of community-based health-promotion programs. Community-level measures of health-related behavior can be divided into three conceptual categories. The first two categories comprise statistical aggregates (e.g., means, sums, percentages) of measurements made on individuals, distinguished by whether individual-level covariates are also available. Individual-disaggregated measures include covariates, such as a comprehensive survey of health-risk behavior, including demographic information; individual-aggregated measures, such as sales data, do not. Our main focus is on the third category, environmental indicators, derived from observations of the community environment. Environmental indicators measures aspects of the physical, legal, social, and economic environment in a community that reflect, and likely influence, the attitudes and behavior of individual community members. They also measure an important intermediate step in community-based health-promotion interventions, namely environmental factors that programs target to modify individual attitudes and behavior. We present examples of environmental indicators for tobacco use and diet.-Environmental indicators: a tool for evaluating community-based health-promotion programs.",1
"Stepped wedge designs (SWDs) have received considerable attention recently, as they are potentially a useful way to assess new treatments in areas such as health services implementation. Because allocation is usually by cluster, SWDs are often viewed as a form of cluster-randomized trial. However, since the treatment within a cluster changes during the course of the study, they can also be viewed as a form of crossover design. This article explores SWDs from the perspective of crossover trials and designed experiments more generally. We show that the treatment effect estimator in a linear mixed effects model can be decomposed into a weighted mean of the estimators obtained from (1) regarding an SWD as a conventional row-column design and (2) a so-called vertical analysis, which is a row-column design with row effects omitted. This provides a precise representation of ""horizontal"" and ""vertical"" comparisons, respectively, which to date have appeared without formal description in the literature. This decomposition displays a sometimes surprising way the analysis corrects for the partial confounding between time and treatment effects. The approach also permits the quantification of the loss of efficiency caused by mis-specifying the correlation parameter in the mixed-effects model. Optimal extensions of the vertical analysis are obtained, and these are shown to be highly inefficient for values of the within-cluster dependence that are likely to be encountered in practice. Some recently described extensions to the classic SWD incorporating multiple treatments are also compared using the experimental design framework.-Stepped wedge designs: insights from a design of experiments perspective.",3
Hepatitis C in the criminal justice system: opportunities for global action in the era of viral hepatitis elimination.,0
"In many community-based surveys, multi-level sampling is inherent in the design. In the design of these studies, especially to calculate the appropriate sample size, investigators need good estimates of intra-class correlation coefficient (ICC), along with the cluster size, to adjust for variation inflation due to clustering at each level. The present study used data on the assessment of clinical vitamin A deficiency and intake of vitamin A-rich food in children in a district in India. For the survey, 16 households were sampled from 200 villages nested within eight randomly-selected blocks of the district. ICCs and components of variances were estimated from a three-level hierarchical random effects analysis of variance model. Estimates of ICCs and variance components were obtained at village and block levels. Between-cluster variation was evident at each level of clustering. In these estimates, ICCs were inversely related to cluster size, but the design effect could be substantial for large clusters. At the block level, most ICC estimates were below 0.07. At the village level, many ICC estimates ranged from 0.014 to 0.45. These estimates may provide useful information for the design of epidemiological studies in which the sampled (or allocated) units range in size from households to large administrative zones.-Intra-class correlation estimates for assessment of vitamin A intake in children.",1
"University of Pennsylvania 11th annual conference on statistical issues in clinical trials: Estimands, missing data and sensitivity analysis (afternoon panel session).",0
"Analytic approaches, including the structural equation model (autoregressive panel model), hierarchical linear model, latent growth curve model, survival/event history analysis, latent transition model, and time-series analysis (interrupted time series, multivariate time-series analysis) are discussed for their applicability to data of different structures and their utility in evaluating temporal effects of treatment. Methods are illustrated by presenting applications of the various approaches in previous studies examining temporal patterns of treatment effects. Recent advancements in these longitudinal modeling approaches and the accompanying computer software development offer tremendous flexibility in examining long-term treatment effects through longitudinal data with varying numbers and intervals of assessment and types of measures. A multimethod assessment will contribute to a more complete understanding of the complex phenomena of the long-term courses of substance use and its treatment.-Analytic approaches for assessing long-term treatment effects. Examples of empirical applications and findings.",0
"The cluster randomized trial (CRT) is used increasingly in knowledge translation research, quality improvement research, community based intervention studies, public health research, and research in developing countries. However, cluster trials raise difficult ethical issues that challenge researchers, research ethics committees, regulators, and sponsors as they seek to fulfill responsibly their respective roles. Our project will provide a systematic analysis of the ethics of cluster trials. Here we have outlined a series of six areas of inquiry that must be addressed if the cluster trial is to be set on a firm ethical foundation: 1. Who is a research subject? 2. From whom, how, and when must informed consent be obtained? 3. Does clinical equipoise apply to CRTs? 4. How do we determine if the benefits outweigh the risks of CRTs? 5. How ought vulnerable groups be protected in CRTs? 6. Who are gatekeepers and what are their responsibilities? Subsequent papers in this series will address each of these areas, clarifying the ethical issues at stake and, where possible, arguing for a preferred solution. Our hope is that these papers will serve as the basis for the creation of international ethical guidelines for the design and conduct of cluster randomized trials.-Ethical issues posed by cluster randomized trials in health research.",1
"The stepped-wedge cluster randomised trial (SW-CRT) is a complex design, for which many decisions about key design parameters must be made during the planning. These include the number of steps and the duration of time needed to embed the intervention. Feasibility studies are likely to be useful for informing these decisions and increasing the likelihood of the main trial's success. However, the number of feasibility studies being conducted for SW-CRTs is currently unknown. This review aims to establish the number of feasibility studies being conducted for SW-CRTs and determine which feasibility issues are commonly investigated. Fully published feasibility studies for SW-CRTs will be identified, according to predefined inclusion criteria, from searches conducted in Ovid MEDLINE, Scopus, Embase and PsycINFO. To also identify and gain information on unpublished feasibility studies the following will be contacted: authors of published SW-CRTs (identified from the most recent systematic reviews); contacts for registered SW-CRTs (identified from clinical trials registries); lead statisticians of UK registered clinical trials units and researchers known to work in the area of SW-CRTs.Data extraction will be conducted independently by two reviewers. For the fully published feasibility studies, data will be extracted on the study characteristics, the rationale for the study, the process for determining progression to a main trial, how the study informed the main trial and whether the main trial went ahead. The researchers involved in the unpublished feasibility studies will be contacted to elicit the same information.A narrative synthesis will be conducted and provided alongside a descriptive analysis of the study characteristics. This review does not require ethical approval, as no individual patient data will be used. The results of this review will be published in an open-access peer-reviewed journal.-The use of feasibility studies for stepped-wedge cluster randomised trials: protocol for a review of impact and scope.",3
"In this work, we delineate an altered study design of a pre-existing clinical trial that is currently being implemented in the Department of Pediatrics at the University of North Carolina at Chapel Hill. The purpose of the ongoing investigation of the desensitized pediatric cohort is to address the effectiveness of sublingual immunotherapy in achieving sustained unresponsiveness (SU) as assessed by repeated double-blind placebo-controlled food challenges (DBPCFC). With scarce published literature characterizing SU, the length of time off-therapy that would represent clinically meaningful benefit remains undefined. We use the new design features to assess time to loss of SU, an important efficacy endpoint, that to our knowledge, no prior study has investigated. Our work has two-fold objectives: first is to propose and discuss aspects of the altered design that would allow us to study SU and second is to explore methodology to evaluate the time to loss of SU and its association with risk factors in the context of the data originating from the trial. The salient feature of the new design is the allocation scheme of study subjects to staggered sampling timepoints when a subsequent DBPCFC is administered. Due to this feature, the time to loss of SU is either left or right censored. Additionally, some participants at study entry fail the DBPCFC, leading to what can be construed as an instantaneous failure. Through in-depth numerical studies, we examine the performance and power of a recently proposed mixture proportional hazards model specifically designed for the analysis of interval-censored data subject to instantaneous failures.-Study design with staggered sampling times for evaluating sustained unresponsiveness to peanut sublingual immunotherapy.",0
Sample size estimation in cluster randomized studies with varying cluster size,1
"Drug abusers vary considerably in their drug use and criminal behavior over time, and these trajectories are likely to influence drug treatment participation and treatment outcomes. Drawing on longitudinal natural history data from three samples of adult male drug users, we identify four groups with distinctive drug use and crime trajectories over the 5 years prior to their first treatment episode. The groups' characteristics of initial treatment are compared. The trajectory groups are then included in Poisson growth curve models to predict drug use, incarceration, and employment over the 5 years following first treatment. Findings indicate that posttreatment drug use decreased and posttreatment employment increased. There was little change in posttreatment incarceration. Posttreatment trajectories for drug use, incarceration, and employment were significantly different across the four trajectory groups.-Patterns of Crime and Drug Use Trajectories in Relation to Treatment Initiation and 5-Year Outcomes: An Application of Growth Mixture Modeling Across Three Datasets.",0
"Structural interventions addressing macro-social determinants of risk have been suggested as potentially important adjuncts to biomedical and behavioural interventions for the prevention of HIV and other diseases. A few interventions of this type have been evaluated using randomised controlled trials (RCTs), the most rigorous design to evaluate the effects of biomedical and behavioural interventions. The appropriateness of applying RCTs to structural interventions is however debated. This paper considers whether issues of ethics, feasibility and utility preclude the use of RCTs in evaluations of structural interventions for HIV prevention. We conclude there is nothing particular to this category of interventions prohibiting use of RCTs. However, we suggest that RCTs may prove unacceptable, unfeasible or not useful in certain circumstances, such as where an intervention brings important benefits other than HIV prevention (such as increased income); where leaders of clusters do not allow decisions about macro-social policies to be determined randomly; where the unit of social organization addressed by an intervention is so large that recruitment of adequate numbers of clusters is impossible; and where the period required to trial interventions extends beyond practical decision-making time-scales. In such cases, alternative evaluative designs must be assessed for their ability to provide evidence of intervention effectiveness.-Should structural interventions be evaluated using RCTs? The case of HIV prevention.",1
"Many measures have been proposed to summarize the prognostic ability of the Cox proportional hazards (CPH) survival model, although none is universally accepted for general use. By contrast, little work has been done to summarize the prognostic ability of the stratified CPH model; such measures would be useful in analyses of individual participant data from multiple studies, data from multi-centre studies, and in single study analysis where stratification is used to avoid making assumptions of proportional hazards. We have chosen three measures developed for the unstratified CPH model (Schemper and Henderson's V , Harrell's C-index and Royston and Sauerbrei's D), adapted them for use with the stratified CPH model and demonstrated how their values can be represented over time. Although each of these measures is promising in principle, we found the measure of explained variation V very difficult to apply when data are combined from several studies with differing durations of participant follow-up. The two other measures considered, D and the C-index, were more applicable under such circumstances. We illustrate the methods using individual participant data from several prospective epidemiological studies of chronic disease outcomes.-Measures to assess the prognostic ability of the stratified Cox proportional hazards model.",0
"For utilizing electronic health records to help design and conduct clinical trials, an essential first step is to select eligible patients from electronic health records, that is, electronic health record phenotyping. We present two novel statistical methods that can be used in the context of electronic health record phenotyping. One mitigates the requirement for gold-standard control patients in developing phenotyping algorithms, and the other effectively corrects for bias in downstream analysis introduced by study samples contaminated by ineligible subjects.-Phenotyping issues for exploring electronic health records to design clinical trials.",0
"Cluster randomized trials occur when groups or clusters of individuals, rather than the individuals themselves, are randomized to intervention and control groups and outcomes are measured on individuals within those clusters. Within primary care, between 1997 and 2000, there has been a virtual doubling in the number of published cluster randomized trials. A recent systematic review, specifically within primary care, found study quality to be both generally lower than that reported elsewhere and not to have shown any recent quality improvement. To discuss the design, conduct and analysis of cluster randomized trials within primary care in terms of the appropriate expertise required, potential bias, ethical considerations and expense. Compared with trials that involve the randomization of individual participants, cluster randomized trials are more complex to design and analyse and, for a given sample size, have decreased power and a broadening of confidence intervals. Cluster randomized trials are specifically prone to potential bias at two levels-the cluster and individual. Regarding the former, it is recommended that cluster allocation be undertaken by a party independent to the research team and careful consideration be given to ensure minimal cluster attrition. Bias at the individual level can be overcome by identifying trial participants before randomization and at this time obtaining consent for intervention, data collection or both. A unique ethical aspect to cluster randomized trials is that cluster leaders may consent to the trial on behalf of potential cluster members. Additional costs of cluster randomized trials include the increased number of patients required, the complexity in their design and conduct and, usually, the need to recruit clusters de novo. Cluster randomized trials are a powerful and increasingly popular research tool. They are uniquely placed for the conduct of research within primary-care clusters where intracluster contamination can occur. Associated methodological issues are straightforward and surmountable and just need careful consideration and management.-Cluster randomized controlled trials in primary care: an introduction.",1
"Ordered categorical data summarized in a 2 x K table usually consist of two-sample multinomial or K-sample binomial observations. In analysing these data, we usually assign scores to the K columns and perform a testing for the equality of two multinomial distributions in the former case and no trend among K binomial proportions in the latter case. Among the most popular score tests are the Wilcoxon rank sum test and the Armitage's linear trend test. In this paper we extend the score tests to be used for clustered data under diverse study designs. Our methods do not require correct specification of the dependence structure within clusters. The proposed tests are based on the asymptotic normality for large number of clusters and are a generalization of the standard tests used for independent data. Simulation studies are conducted to investigate the finite-sample performance of the new methods. The proposed methods are applied to real-life data.-Tests for 2 x K contingency tables with clustered ordered categorical data.",1
"Identification of key factors associated with the risk of developing cardiovascular disease and quantification of this risk using multivariable prediction algorithms are among the major advances made in preventive cardiology and cardiovascular epidemiology in the 20th century. The ongoing discovery of new risk markers by scientists presents opportunities and challenges for statisticians and clinicians to evaluate these biomarkers and to develop new risk formulations that incorporate them. One of the key questions is how best to assess and quantify the improvement in risk prediction offered by these new models. Demonstration of a statistically significant association of a new biomarker with cardiovascular risk is not enough. Some researchers have advanced that the improvement in the area under the receiver-operating-characteristic curve (AUC) should be the main criterion, whereas others argue that better measures of performance of prediction models are needed. In this paper, we address this question by introducing two new measures, one based on integrated sensitivity and specificity and the other on reclassification tables. These new measures offer incremental information over the AUC. We discuss the properties of these new measures and contrast them with the AUC. We also develop simple asymptotic tests of significance. We illustrate the use of these measures with an example from the Framingham Heart Study. We propose that scientists consider these types of measures in addition to the AUC when assessing the performance of newer biomarkers.-Evaluating the added predictive ability of a new marker: from area under the ROC curve to reclassification and beyond.",0
"We consider estimation of various probabilities after termination of a group sequential phase II trial. A motivating example is that the stopping rule of a phase II oncologic trial is determined solely based on response to a drug treatment, and at the end of the trial estimating the rate of toxicity and response is desirable. The conventional maximum likelihood estimator (sample proportion) of a probability is shown to be biased, and two alternative estimators are proposed to correct for bias, a bias-reduced estimator obtained by using Whitehead's bias-adjusted approach, and an unbiased estimator from the Rao-Blackwell method of conditioning. All three estimation procedures are shown to have certain invariance property in bias. Moreover, estimators of a probability and their bias and precision can be evaluated through the observed response rate and the stage at which the trial stops, thus avoiding extensive computation.-Supplementary analysis of probabilities at the termination of a group sequential phase II trial.",0
"Cluster randomization trials, in which intact social units are randomized to different interventions, have become popular in the last 25 years. Outcomes from these trials in many cases are positively skewed, following approximately lognormal distributions. When inference is focused on the difference between treatment arm arithmetic means, existent confidence interval procedures either make restricting assumptions or are complex to implement. We approach this problem by assuming log-transformed outcomes from each treatment arm follow a one-way random effects model. The treatment arm means are functions of multiple parameters for which separate confidence intervals are readily available, suggesting that the method of variance estimates recovery may be applied to obtain closed-form confidence intervals. A simulation study showed that this simple approach performs well in small sample sizes in terms of empirical coverage, relatively balanced tail errors, and interval widths as compared to existing methods. The methods are illustrated using data arising from a cluster randomization trial investigating a critical pathway for the treatment of community acquired pneumonia.-Confidence intervals for a difference between lognormal means in cluster randomization trials.",1
"Large field trials that randomize naturally occurring clusters such as communities, worksites, or schools are becoming widely accepted for evaluating complex interventions. The within-cluster measurement of individuals typically uses a cohort, followed throughout the trial or cross-sectional samples selected independently at each time point. The relative costs of these approaches is of concern in designing such trials. This paper takes the unified model for analyzing large cluster unit trials developed by Feldman and McKinlay (Stat Med 1993) and combines the resulting expression for the variance of the treatment effect with a simple cost function into an algorithm that produces the optimal trial design in terms of the number of clusters and the number of observations per cluster. Using the unified model developed in the prior paper, this algorithm also allows direct comparison of the cost of designs with equivalent precision. In particular, designs that use cohorts in each cluster unit and observe cohort members over time are contrasted with designs that draw independent cross-sectional samples from each cluster at each time point. Using the algorithm and a realistic design problem, it is demonstrated that cohort designs are more cost efficient for short trials and high (&gt; or = 0.75) autocorrelations. The power of the algorithm in designing cost-efficient cluster unit trials is well demonstrated. Estimates of variance and cost components from prior trials need to be readily accessible for use in the algorithm, for planning subsequent trials.-Cost-efficient designs of cluster unit trials.",1
QT interval increased after single dose of lofexidine.,0
"To be relevant for public health, a context (e.g., neighborhood, school, hospital) should influence or affect the health status of the individuals included in it. The greater the influence of the shared context, the higher the correlation of subject outcomes within that context is likely to be. This intra-context or intra-class correlation is of substantive interest and has been used to quantify the magnitude of the general contextual effect (GCE). Furthermore, ignoring the intra-class correlation in a regression analysis results in spuriously narrow 95% confidence intervals around the estimated regression coefficients of the specific contextual variables entered as covariates and, thereby, overestimates the precision of the estimated specific contextual effects (SCEs). Multilevel regression analysis is an appropriate methodology for investigating both GCEs and SCEs. However, frequently researchers only report SCEs and disregard the study of the GCE, unaware that small GCEs lead to more precise estimates of SCEs so, paradoxically, the less relevant the context is, the easier it is to detect (and publish) small but ""statistically significant"" SCEs. We describe this paradoxical situation and encourage researchers performing multilevel regression analysis to consider simultaneously both the GCE and SCEs when interpreting contextual influences on individual health.-General and specific contextual effects in multilevel regression analyses and their paradoxical relationship: A conceptual tutorial.",1
"Cluster randomized trials (CRTs) are widely used in epidemiological and public health studies assessing population-level effect of group-based interventions. One important application of CRTs is the control of vector-borne disease, such as malaria. However, a particular challenge for designing these trials is that the primary outcome involves counts of episodes that are subject to right truncation. While sample size formulas have been developed for CRTs with clustered counts, they are not directly applicable when the counts are right truncated. To address this limitation, we discuss two marginal modeling approaches for the analysis of CRTs with truncated counts and develop two corresponding closed-form sample size formulas to facilitate the design of such trials. The proposed sample size formulas allow investigators to explore the power under a large number of scenarios without computationally intensive simulations. The proposed formulas are validated in extensive simulations. We further explore the implication of right truncation on power and apply the proposed formulas to illustrate the power calculation for a malaria control CRT where the primary outcome is subject to right?truncation.-Sample size and power considerations for cluster randomized trials with count outcomes subject to right truncation.",1
"Markov models used to analyze transition patterns in discrete longitudinal data are based on the limiting assumption that individuals follow the common underlying transition process. However, when one is interested in diseases with different disease or severity subtypes, explicitly modeling subpopulation-specific transition patterns may be appropriate. We propose a model which captures heterogeneity in the transition process through a finite mixture model formulation and provides a framework for identifying subpopulations at different risks. We apply the procedure to longitudinal bacterial vaginosis study data and demonstrate that the model fits the data well. Further, we show that under the mixture model formulation, we can make the important distinction between how covariates affect transition patterns unique to each of the subpopulations and how they affect which subgroup a participant will belong to. Practically, covariate effects on subpopulation-specific transition behavior and those on subpopulation membership can be interpreted as effects on short-term and long-term transition behavior. We further investigate models with higher-order subpopulation-specific transition dependence.-A mixture of transition models for heterogeneous longitudinal ordinal data: with applications to longitudinal bacterial vaginosis data.",0
"In many medical studies, estimation of the association between treatment and outcome of interest is often of primary scientific interest. Standard methods for its evaluation in survival analysis typically require the assumption of independent censoring. This assumption might be invalid in many medical studies, where the presence of dependent censoring leads to difficulties in analyzing covariate effects on disease outcomes. This data structure is called ""semicompeting risks data,"" for which many authors have proposed an artificial censoring technique. However, confounders with large variability may lead to excessive artificial censoring, which subsequently results in numerically unstable estimation. In this paper, we propose a strategy for weighted estimation of the associations in the accelerated failure time model. Weights are based on propensity score modeling of the treatment conditional on confounder variables. This novel application of propensity scores avoids excess artificial censoring caused by the confounders and simplifies computation. Monte Carlo simulation studies and application to AIDS and cancer research are used to illustrate the methodology.-Covariate adjustment using propensity scores for dependent censoring problems in the accelerated failure time model.",0
"We review recent developments in the estimation of an optimal treatment strategy or regime from longitudinal data collected in an observational study. We also propose novel methods for using the data obtained from an observational database in one health-care system to determine the optimal treatment regime for biologically similar subjects in a second health-care system when, for cultural, logistical, or financial reasons, the two health-care systems differ (and will continue to differ) in the frequency of, and reasons for, both laboratory tests and physician visits. Finally, we propose a novel method for estimating the optimal timing of expensive and/or painful diagnostic or prognostic tests. Diagnostic or prognostic tests are only useful in so far as they help a physician to determine the optimal dosing strategy, by providing information on both the current health state and the prognosis of a patient because, in contrast to drug therapies, these tests have no direct causal effect on disease progression. Our new method explicitly incorporates this no direct effect restriction.-Estimation and extrapolation of optimal treatment and testing strategies.",0
Comments on 'Efficiency loss because of varying cluster size in cluster randomized trials is smaller than literature suggests'.,1
Cluster randomised trials with repeated cross sections: alternatives to parallel group designs.,3
Timeline cluster: a graphical tool to identify risk of bias in cluster randomised trials.,1
"Multilevel nested, correlated data often arise in biomedical research. Examples include teeth nested within quadrants in a mouth or students nested within classrooms in schools. In some settings, cluster sizes may be large relative to the number of independent clusters and the degree of correlation may vary across clusters. When cluster sizes are large, fitting marginal regression models using Generalized Estimating Equations with flexible correlation structures that reflect the nested structure may fail to converge and result in unstable covariance estimates. Also, the use of patterned, nested working correlation structures may not be efficient when correlation varies across clusters. This paper describes a flexible marginal regression modeling approach based on an optimal combination of estimating equations. Particular within-cluster and between-cluster data contrasts are used without specification of the working covariance structure and without estimation of covariance parameters. The method involves estimation of the covariance matrix only for the vector of component estimating equations (which is typically of small dimension) rather than the covariance matrix of the observations within a cluster (which may be of large dimension). In settings where the number of clusters is large relative to the cluster size, the method is stable and is highly efficient, while maintaining appropriate coverage levels. Performance of the method is investigated with simulation studies and an application to a periodontal study.-Optimal combination of estimating equations in the analysis of multilevel nested correlated data.",1
"Cluster randomized controlled trials (CRCT) can be susceptible to a wide range of methodological problems. Many of these problems are not commonly recognized by researchers. This article is focused on one potential problem, the issue of impure clustering (multiple patient membership) within the CRCT structures and how it can lead to possible misunderstanding and bias in the results of the trial. A solution to this problem is presented using a multiple membership random effects model (MMREM). A simulated example of a three-level CRCT is presented and modeled with and without multiple patient membership data. Results indicate underestimation of higher level variances, and overestimation of lower-level variances, while also indicating underestimation of level predictors where the multiple membership occurs.-Modeling impure clusters in a cluster randomized controlled trial.",1
"The design and analysis of cluster randomized trials has been a recurrent theme in Statistics in Medicine since the early volumes. In celebration of 25 years of Statistics in Medicine, this paper reviews recent developments, particularly those that featured in the journal. Issues in design such as sample size calculations, matched paired designs, cohort versus cross-sectional designs, and practical design problems are covered. Developments in analysis include modification of robust methods to cope with small numbers of clusters, generalized estimation equations, population averaged and cluster specific models. Finally, issues on presenting data, some other clustering issues and the general problem of evaluating complex interventions are briefly mentioned.-Developments in cluster randomized trials and Statistics in Medicine.",1
"The overall perspective of this article is the need for researchers of school-based health promotion to make more progressive use of existing statistical methods to improve both the rigor and the efficiency of school-based experiments. Investigative teams working in the school setting have an advantage over health education researchers working in communities or worksites in that they have greater choice of the experimental unit and usually easier access to large clusters of units. They are, therefore, in a position to make optimal use of a wide variety of experimental designs and observation strategies. In order to make full use of this advantage, however, researchers in this field need to assemble multidisciplinary teams, including researchers from other fields who are not restricted by the traditional approaches evident in many of the current school studies. Such teams should include statisticians and epidemiologists who have broadly-based experimental design and survey experience to work with the health educators and other behavioral and biomedical scientists.-Research design and analysis issues.",1
How to design efficient cluster randomised trials.,1
"Increasing attention has been given recently to the methodological issues associated with randomization of clusters rather than individuals in lifestyle intervention trials. These issues are explored through an empirical study of the 'effective sample size' imposed by randomization of three experimental units frequently considered in epidemiological research: the spouse pair, the general practice, and the large geographic area (county). The measurement of within-cluster dependence for a dichotomous outcome variable is also discussed, and a relationship shown between Fleiss's kappa and Cornfield's inflation factor.-An empirical study of cluster randomization.",1
"The authors disagree with M. Siemer and J. Joormann's assertion that therapist should be a fixed effect in psychotherapy treatment outcome studies. If treatment is properly standardized, therapist effects can be examined in preliminary tests and the therapist term deleted from analyses if such differences approach zero. If therapist effects are anticipated and either cannot be minimized through standardization or are specifically of interest because of the nature of the research question, the study has to be planned with adequate statistical power for including therapist as a random term. Simulation studies conducted by Siemer and Joormann confounded bias due to small sample size and inconsistent estimates.-Therapists as fixed versus random effects - Some statistical and conceptual issues: A comment on Siemer and Joormann (2003)",2
Clinical technology specialists.,0
"The reduced efficiency of the cluster randomized trial design may be compensated by implementing a multi-period design. The trial then becomes longitudinal, with a risk of intermittently missing observations and dropout. This paper studies the effect of missing data on design efficiency in trials where the periods are the days of the week and clusters are followed for at least one week. The multilevel model with a decaying correlation structure is used to relate outcome to period and treatment condition. The variance of the treatment effect estimator is used to measure efficiency. When there is no data loss, efficiency increases with increasing number of subjects per day and number of weeks. Different weekly measurement schemes are used to evaluate the impact of planned missing data designs: the loss of efficiency due to measuring on fewer days is largest for few subjects per day and few weeks. Dropout is modeled by the Weibull survival function. The loss of efficiency due to dropout increases when more clusters drop out during the course of the trial, especially if the risk of dropout is largest at the beginning of the trial. The largest loss is observed for few subjects per day and a large number of weeks. An example of the effect of waiting room environments in reducing stress in dental care shows how different design options can be compared. An R Shiny app allows researchers to interactively explore various design options and to choose the best design for their trial.-The effect of missing data on design efficiency in repeated cross-sectional multi-period two-arm parallel cluster randomized trials.",1
"To examine how a healthy lifestyle is related to life expectancy that is free from major chronic diseases. Prospective cohort study. The Nurses' Health Study (1980-2014; n=73 196) and the Health Professionals Follow-Up Study (1986-2014; n=38 366). Five low risk lifestyle factors: never smoking, body mass index 18.5-24.9, moderate to vigorous physical activity (?30 minutes/day), moderate alcohol intake (women: 5-15 g/day; men 5-30 g/day), and a higher diet quality score (upper 40%). Life expectancy free of diabetes, cardiovascular diseases, and cancer. The life expectancy free of diabetes, cardiovascular diseases, and cancer at age 50 was 23.7 years (95% confidence interval 22.6 to 24.7) for women who adopted no low risk lifestyle factors, in contrast to 34.4 years (33.1 to 35.5) for women who adopted four or five low risk factors. At age 50, the life expectancy free of any of these chronic diseases was 23.5 (22.3 to 24.7) years among men who adopted no low risk lifestyle factors and 31.1 (29.5 to 32.5) years in men who adopted four or five low risk lifestyle factors. For current male smokers who smoked heavily (?15 cigarettes/day) or obese men and women (body mass index ?30), their disease-free life expectancies accounted for the lowest proportion (?75%) of total life expectancy at age 50. Adherence to a healthy lifestyle at mid-life is associated with a longer life expectancy free of major chronic diseases.-Healthy lifestyle and life expectancy free of cancer, cardiovascular disease, and type 2 diabetes: prospective cohort study.",0
"We conducted a group randomized trial of 2 South African school-based smoking prevention programs and examined possible sources and implications of why our actual intraclass correlation coefficients (ICCs) were significantly higher than the ICC of 0.02 used to compute initial sample size requirements. Thirty-six South African high schools were randomly assigned to 1 of 3 experimental groups. On 3 occasions, students completed questionnaires on tobacco and drug use attitudes and behaviors. We used mixed-effects models to partition individual and school-level variance components, with and without covariate adjustment. For 30-day smoking, unadjusted ICCs ranged from 0.12 to 0.17 across the 3 time points. For lifetime smoking, ICCs ranged from 0.18 to 0.22; for other drug use variables, 0.02 to 0.10; and for psychosocial variables, 0.09 to 0.23. Covariate adjustment substantially reduced most ICCs. The unadjusted ICCs we observed for smoking behaviors were considerably higher than those previously reported. This effectively reduced our sample size by a factor of 17. Future studies that anticipate significant cluster-level racial homogeneity may consider using higher-value ICCs in sample-size calculations to ensure adequate statistical power.-When intraclass correlation coefficients go awry: a case study from a school-based smoking prevention study in South Africa.",1
"Partially clustered designs, where clustering occurs in some conditions and not others, are common in psychology, particularly in prevention and intervention trials. This article reports results from a simulation comparing 5 approaches to analyzing partially clustered data, including Type I errors, parameter bias, efficiency, and power. Results indicate that multilevel models adapted for partially clustered data are relatively unbiased and efficient and consistently maintain the nominal Type I error rate when using appropriate degrees of freedom. To attain sufficient power in partially clustered designs, researchers should attend primarily to the number of clusters in the study. An illustration using data from a partially clustered eating disorder prevention trial is provided.-Evaluating models for partially clustered designs.",2
"Outcome data from dental caries clinical trials have a naturally hierarchical structure, with surfaces clustered within teeth, clustered within individuals. Data are often aggregated into the DMF index for each individual, losing tooth- and surface-specific information. If these data are to be analysed by tooth or surface, allowing exploration of effects of interventions on different teeth and surfaces, appropriate methods must be used to adjust for the clustered nature of the data. Multilevel modelling allows analysis of clustered data using individual observations without aggregating data, and has been little used in the field of dental caries. A simulation study was conducted to investigate the performance of multilevel modelling methods and standard caries increment analysis. Data sets were simulated from a three-level binomial distribution based on analysis of a caries clinical trial in Scottish adolescents, with varying sample sizes, treatment effects and random tooth level effects based on trials reported in Cochrane reviews of topical fluoride, and analysed to compare the power of multilevel models and traditional analysis. 40,500 data sets were simulated. Analysis showed that estimated power for the traditional caries increment method was similar to that for multilevel modelling, with more variation in smaller data sets. Multilevel modelling may not allow significant reductions in the number of participants required in a caries clinical trial, compared to the use of traditional analyses, but investigators interested in exploring the effect of their intervention in more detail may wish to consider the application of multilevel modelling to their clinical trial data.-Statistical power of multilevel modelling in dental caries clinical trials: a simulation study.",1
"Non-adherence to assigned treatment is a common issue in cluster randomised trials. In these settings, the efficacy estimand may also be of interest. Many methodological contributions in recent years have advocated using instrumental variables to identify and estimate the local average treatment effect. However, the clustered nature of randomisation in cluster randomised trials adds to the complexity of such analyses. In this paper, we show that the local average treatment effect can be estimated via two-stage least squares regression using cluster-level summaries of the outcome and treatment received under certain assumptions. We propose the use of baseline variables to adjust the cluster-level summaries before performing two-stage least squares in order to improve efficiency. Implementation needs to account for the reduced sample size, as well as the possible heteroscedasticity, to obtain valid inferences. Simulations are used to assess the performance of two-stage least squares of cluster-level summaries under cluster-level or individual-level non-adherence, with and without weighting and robust standard errors. The impact of adjusting for baseline covariates and of appropriate degrees of freedom correction for inference is also explored. The methods are then illustrated by re-analysing a cluster randomised trial carried out in a specific UK primary care setting. Two-stage least squares estimation using cluster-level summaries provides estimates with small to negligible bias and coverage close to nominal level, provided the appropriate small sample degrees of freedom correction and robust standard errors are used for inference.-Estimating cluster-level local average treatment effects in cluster randomised trials with non-adherence",1
"Many school-based research efforts require active parental consent for student participation. Maximizing rates of consent form return and agreement is an important issue, because sample representativeness may be compromised when these rates are low. This article compares two methods for obtaining active parental consent: return of consent forms in the mail versus return by students to their classrooms. The methods were tested in a pilot study of 46 schools (1,058 students), with half of the schools randomly allocated to each of the alternative methods. A hierarchical nonlinear model of consent form return and agreement rates suggests that the student-delivered method is more successful at producing higher rates of consent form return and agreement to participate in the study, after controlling for school-level characteristics. The authors discuss the findings and their implications for other researchers engaged in school-based research with adolescents.-A comparison of methods to obtain active parental consent for an international student survey.",0
"We provide optimal treatment allocation schemes when the outcome variance varies across the treatment groups and our objectives are to estimate treatment effects with equal or unequal interest. Unlike other optimal designs, such as A-optimal designs, the proposed designs can be found without an iterative scheme. We evaluate robustness properties of the optimal designs to mis-specification in the expected variance from each group and identify situations when popular allocation schemes have poor efficiencies. An application to design a randomized rheumatoid arthritis trial is discussed, along with a potential application to design a cancer screening trial when the main outcome is a continuous variable.-Optimum treatment allocation rules under a variance heterogeneity model.",0
"Non-parametric procedures such as the Wilcoxon rank-sum test, or equivalently the Mann-Whitney test, are often used to analyse data from clinical trials. These procedures enable testing for treatment effect, but traditionally do not account for covariates. We adapt recently developed methods for receiver operating characteristic (ROC) curve regression analysis to extend the Mann-Whitney test to accommodate covariate adjustment and evaluation of effect modification. Our approach naturally extends use of the Mann-Whitney statistic in a fashion that is analogous to how linear models extend the t-test. We illustrate the methodology with data from clinical trials of a therapy for Cystic Fibrosis.-Using the ROC curve for gauging treatment effect in clinical trials.",0
This article compares four mixed-model analyses valid for group-randomized trials (GRTs) involving a nested cohort design with a single pretest and posttest. This study makes estimates of intraclass correlations (ICCs) available to investigators planning GRTs addressing dietary outcomes. It also provides formulae demonstrating the potential benefits to the standard error of the intervention effect (sigma(delta)) from adjustments for both fixed and time-varying covariates and correlations over time. These estimates will allow other researchers using these variables to plan their studies by estimating a priori detectable differences and sample size requirements for any of the four analytic options. These methods are demonstrated using data from the Teens Eating for Energy and Nutrition at School study. Mixed-model analyses of covariance proved to be the most powerful analysis in that data set. The formulae may be applied to any dependent variable in any GRT given corresponding information for those variables on the parameters that define the formulae.-Assessing intervention effects in a school-based nutrition intervention trial: which analytic model is most powerful?,1
"In this article, we consider comparing the areas under correlated receiver operating characteristic (ROC) curves of diagnostic biomarkers whose measurements are subject to a limit of detection (LOD), a source of measurement error from instruments' sensitivity in epidemiological studies. We propose and examine the likelihood ratio tests with operating characteristics that are easily obtained by classical maximum likelihood methodology.-Maximum likelihood ratio tests for comparing the discriminatory ability of biomarkers subject to limit of detection.",0
"This article proposes a joint modeling framework for longitudinal insomnia measurements and a stochastic smoking cessation process in the presence of a latent permanent quitting state (i.e., 'cure'). We use a generalized linear mixed-effects model and a stochastic mixed-effects model for the longitudinal measurements of insomnia symptom and for the smoking cessation process, respectively. We link these two models together via the latent random effects. We develop a Bayesian framework and Markov Chain Monte Carlo algorithm to obtain the parameter estimates. We formulate and compute the likelihood functions involving time-dependent covariates. We explore the within-subject correlation between insomnia and smoking processes. We apply the proposed methodology to simulation studies and the motivating dataset, that is, the Alpha-Tocopherol, Beta-Carotene Lung Cancer Prevention study, a large longitudinal cohort study of smokers from Finland.-Joint analysis of stochastic processes with application to smoking patterns and insomnia.",0
"Cluster randomized trials (CRTs) are a design used to test interventions where individual randomization is not appropriate. The mixed model for repeated measures (MMRM) is a popular choice for individually randomized trials with longitudinal continuous outcomes. This model's appeal is due to avoidance of model misspecification and its unbiasedness for data missing completely at random or at random. We extended the MMRM to cluster randomized trials by adding a random intercept for the cluster and undertook a simulation experiment to investigate statistical properties when data are missing at random. We simulated cluster randomized trial data where the outcome was continuous and measured at baseline and three post-intervention time points. We varied the number of clusters, the cluster size, the intra-cluster correlation, missingness and the data-generation models. We demonstrate the MMRM-CRT with an example of a cluster randomized trial on cardiovascular disease prevention among diabetics. When simulating a treatment effect at the final time point we found that estimates were unbiased when data were complete and when data were missing at random. Variance components were also largely unbiased. When simulating under the null, we found that type I error was largely nominal, although for a few specific cases it was as high as 0.081. Although there have been assertions that this model is inappropriate when there are more than two repeated measures on subjects, we found evidence to the contrary. We conclude that the MMRM for CRTs is a good analytic choice for cluster randomized trials with a continuous outcome measured longitudinally. ClinicalTrials.gov, ID: NCT02804698.-The mixed model for repeated measures for cluster randomized trials: a simulation study investigating bias and type I error with missing continuous data",1
"This study reports results from an evaluation of the experimental Rio Hondo driving under the influence (DUI) court of Los Angeles County, California. Interviews and official record checks with 284 research participants who were randomly assigned to a DUI court or a traditional criminal court were assessed at baseline and at 24-month follow-up. The interviews assessed the impact of the DUI court on self-reported drunk driving behavior, the completion of treatment, time spent in jail, alcohol use, and stressful life events. Official record checks assessed the impact of the DUI court on subsequent arrests for driving under the influence and other drinking-related behaviors. Few differences on any outcomes were observed between participants in the experimental DUI court and those assigned to the traditional court. The results suggest that the DUI court model had little additional therapeutic or public safety benefit over the traditional court process. The implication of these findings for the popularity of specialized courts for treating social problems is discussed.-The efficacy of the Rio Hondo DUI court: a 2-year field experiment.",0
"The Nuremberg code defines the general ethical framework of medical research with participant consent as its cornerstone. In cluster randomized trials (CRT), obtaining participant informed consent raises logistic and methodologic concerns. First, with randomization of large clusters such as geographical areas, obtaining individual informed consent may be impossible. Second, participants in randomized clusters cannot avoid certain interventions, which implies that participant informed consent refers only to data collection, not administration of an intervention. Third, complete participant information may be a source of selection bias, which then raises methodological concerns. We assessed whether participant informed consent was required in such trials, which type of consent was required, and whether the trial was at risk of selection bias because of the very nature of participant information. We systematically reviewed all reports of CRT published in MEDLINE in 2008 and surveyed corresponding authors regarding the nature of the informed consent and the process of participant inclusion. We identified 173 reports and obtained an answer from 113 authors (65.3%). In total, 23.7% of the reports lacked information on ethics committee approval or participant consent, 53.1% of authors declared that participant consent was for data collection only and 58.5% that the group allocation was not specified for participants. The process of recruitment (chronology of participant recruitment with regard to cluster randomization) was rarely reported, and we estimated that only 56.6% of the trials were free of potential selection bias. For CRTs, the reporting of ethics committee approval and participant informed consent is less than optimal. Reports should describe whether participants consented for administration of an intervention and/or data collection. Finally, the process of participant recruitment should be fully described (namely, whether participants were informed of the allocation group before being recruited) for a better appraisal of the risk of selection bias.-Participant informed consent in cluster randomized trials: review.",1
"Group randomized trials (GRTs) in public health research typically use a small number of randomized groups with a relatively large number of participants per group. Two fundamental features characterize GRTs: a positive correlation of outcomes within a group, and the small number of groups. Appropriate consideration of these fundamental features is essential for design and analysis. This paper presents the fundamental features of GRTs and the importance of considering these features in design and analysis. It also reviews and contrasts the main analytic methods proposed for GRTs, emphasizing the assumptions required to make these methods valid and efficient. Also discussed are various design issues, along with guidelines for choosing among them. A real data example illustrates these issues and methods.-Selected statistical issues in group randomized trials.",1
The efficiency of stepped wedge vs. cluster randomized trials: stepped wedge studies do not always require a smaller sample size.,3
The Intra-Cluster Correlation Coefficient in Cluster Randomized Trials: A Review of Definitions,1
"Motivated by a study of surgical operating time and post-operative outcomes for lung cancer, we consider the estimation of causal effects of continuous point-exposure treatments. To investigate causality, the standard paradigm postulates a series of treatment-specific counterfactual outcomes and establishes conditions under which we may learn about them from observational study data. While many choices are possible, causal effects are typically defined in terms of variation of the mean of counterfactual outcomes in hypothetical worlds in which specific treatment strategies are 'applied' to all individuals. For example, one might compare two worlds: one where each individual receives some specific dose and a second where each individual receives some other dose. For our motivating study, defining causal effects in this way corresponds to (hypothetical) interventions that could not conceivably be implemented in the real world. In this work, we consider an alternative, complimentary framework that investigates variation in the mean of counterfactual outcomes under hypothetical treatment strategies where each individual receives a treatment dose corresponding to that actually received but modified in some pre-specified way. Quantification of this variation is defined in terms of contrasts for specific interventions as well as in terms of the parameters of a new class of marginal structural mean models. Within this framework, we propose three estimators: an outcome regression estimator, an inverse probability of treatment weighted estimator and a doubly robust estimator. We illustrate the methods with an analysis of the motivating data.-Estimation of the effect of interventions that modify the received treatment.",0
"Multilevel analysis is an appropriate tool for the analysis of hierarchically structured data. There may, however, be reasons to ignore one of the levels of nesting in the data analysis. In this article a three level model with one predictor variable is used as a reference model and the top or intermediate level is ignored in the data analysis. Analytical results show that this has an effect on the estimated variance components and that standard errors of regression coefficients estimators may be overestimated, leading to a lower power of the test of the effect of the predictor variable. The magnitude of these results depends on the ignored level and the level at which the predictor variable varies, and on the values of the variance components and the sample sizes.-The Consequence of Ignoring a Level of Nesting in Multilevel Analysis.",1
"In this paper, we develop a Bayesian semiparametric accelerated failure time model for survival data with cluster structures. Our model allows distributional heterogeneity across clusters and accommodates their relationships through a density ratio approach. Moreover, a nonparametric mixture of Dirichlet processes prior is placed on the baseline distribution to yield full distributional flexibility. We illustrate through simulations that our model can greatly improve estimation accuracy by effectively pooling information from multiple clusters, while taking into account the heterogeneity in their random error distributions. We also demonstrate the implementation of our method using analysis of Mayo Clinic Trial in Primary Biliary Cirrhosis.-Semiparametric Bayesian analysis of accelerated failure time models with cluster structures.",1
Varieties of uncertainty and the validity of informed consent.,0
"Clustered data are common in medical research. Typically, one is interested in a regression model for the association between an outcome and covariates. Two complications that can arise when analysing clustered data are informative cluster size (ICS) and confounding by cluster (CBC). ICS and CBC mean that the outcome of a member given its covariates is associated with, respectively, the number of members in the cluster and the covariate values of other members in the cluster. Standard generalised linear mixed models for cluster-specific inference and standard generalised estimating equations for population-average inference assume, in general, the absence of ICS and CBC. Modifications of these approaches have been proposed to account for CBC or ICS. This article is a review of these methods. We express their assumptions in a common format, thus providing greater clarity about the assumptions that methods proposed for handling CBC make about ICS and vice versa, and about when different methods can be used in practice. We report relative efficiencies of methods where available, describe how methods are related, identify a previously unreported equivalence between two key methods, and propose some simple additional methods. Unnecessarily using a method that allows for ICS/CBC has an efficiency cost when ICS and CBC are absent. We review tools for identifying ICS/CBC. A strategy for analysis when CBC and ICS are suspected is demonstrated by examining the association between socio-economic deprivation and preterm neonatal death in Scotland.-Review of methods for handling confounding by cluster and informative cluster size in clustered data.",1
"The sexual behaviors of HIV/sexually transmitted infection (STI) prevention intervention participants can be assessed on a partner-by-partner basis: in aggregate (i.e., total numbers of sex acts, collapsed across partners) or using a combination of these two methods (e.g., assessing five partners in detail and any remaining partners in aggregate). There is a natural trade-off between the level of sexual behavior detail and the precision of HIV/STI acquisition risk estimates. The results of this study indicate that relatively simple aggregate data collection techniques suffice to adequately estimate HIV risk. For highly infectious STIs, in contrast, accurate STI risk assessment requires more intensive partner-by-partner methods.-Aggregate versus individual-level sexual behavior assessment: how much detail is needed to accurately estimate HIV/STI risk?",0
"For randomised population trials the unit of randomisation is normally the individual person. In some situations, however, investigators take other groups as basic unit and one such design is cluster randomisation. Considerable attention has been given to this design recently in statistical and epidemiological literature. The Edinburgh randomised trial of breast cancer screening is an example which takes general practices as clusters of patients. The experience of this trial is reported here. Mortality from all causes, cardiovascular disease and lung cancer over the first 5 year period of follow up are examined. We found that spurious mortality differences were present in the analyses, which do not allow for socio-economic status. From examination of methods of adjusting for this, we conclude that allowance can be made at the analysis stage, and it is intended that this approach will be adopted when breast cancer mortality is analysed in the Edinburgh trial. Nevertheless, we recommend that for future studies with outcome related to socio-economic status, randomisations which use this design be stratified by socio-economic criteria where this is feasible.-Randomisation by cluster and the problem of social class bias.",1
"Rapid Early Action for Coronary Treatment (REACT) was a multisite trial testing a community intervention to reduce the delay between onset of symptoms of acute myocardial infarction (MI) and patients' arrival at a hospital emergency department. The study employed a group-randomized trial design, with ten communities randomized from within matched pairs to each of two conditions. REACT also employed continuous data collection, based on surveillance of heart attack patients in community emergency departments. They analysed their data by comparing the mean slope for delay time in the ten intervention communities to the mean slope estimated in the ten control communities. Because no estimates of slope variation were available a priori, REACT was sized using approximations based on more traditional designs. In this paper, we present the slope and residual error variances as estimated from the REACT data and examine their influence on the power of the trial post hoc. We also examine the power of the trial as it would have been given a more traditional pretest-post-test design with analysis via a comparison of the net difference in condition means pretest vs post-test.-Components of variance in a group-randomized trial analysed via a random-coefficients model: the Rapid Early Action for Coronary Treatment (REACT) trial.",1
"We develop a weighted cumulative sum (WCUSUM) to evaluate and monitor pre-transplant waitlist mortality of facilities in the context where transplantation is considered to be dependent censoring. Waitlist patients are evaluated multiple times in order to update their current medical condition as reflected in a time-dependent variable called the Model for End-Stage Liver Disease (MELD) score. Higher MELD scores are indicative of higher pre-transplant death risk. Moreover, under the current liver allocation system, patients with higher MELD scores receive higher priority for liver transplantation. To evaluate the waitlist mortality of transplant centers, it is important to take this dependent censoring into consideration. We assume a 'standard' transplant practice through a transplant model and utilize inverse probability censoring weights to construct a WCUSUM. We evaluate the properties of a weighted zero-mean process as the basis of the proposed WCUSUM. We then discuss a resampling technique to obtain control limits. The proposed WCUSUM is illustrated through the analysis of national transplant registry data.-A weighted cumulative sum (WCUSUM) to monitor medical outcomes with dependent censoring.",0
"Normality (symmetry) of the model random errors is a routine assumption for mixed-effects models in many longitudinal studies, but it may be unrealistically obscuring important features of subject variations. Covariates are usually introduced in the models to partially explain inter-subject variations, but some covariates such as CD4 cell count may be often measured with substantial errors. This paper formulates a class of models in general forms that considers model errors to have skew-normal distributions for a joint behavior of longitudinal dynamic processes and time-to-event process of interest. For estimating model parameters, we propose a Bayesian approach to jointly model three components (response, covariate, and time-to-event processes) linked through the random effects that characterize the underlying individual-specific longitudinal processes. We discuss in detail special cases of the model class, which are offered to jointly model HIV dynamic response in the presence of CD4 covariate process with measurement errors and time to decrease in CD4/CD8 ratio, to provide a tool to assess antiretroviral treatment and to monitor disease progression. We illustrate the proposed methods using the data from a clinical trial study of HIV treatment. The findings from this research suggest that the joint models with a skew-normal distribution may provide more reliable and robust results if the data exhibit skewness, and particularly the results may be important for HIV/AIDS studies in providing quantitative guidance to better understand the virologic responses to antiretroviral treatment.-Bayesian inference on joint models of HIV dynamics for time-to-event and longitudinal data with skewness and covariate measurement errors.",0
"We sought to promote cervical cancer screening among Vietnamese American women in Santa Clara County, Calif. In 2001-2004, we recruited and randomized 1005 Vietnamese American women into 2 groups: lay health worker outreach plus media-based education (combined intervention) or media-based education only. Lay health workers met with the combined intervention group twice over 3 to 4 months to promote Papanicolaou (Pap) testing. We used questionnaires to measure changes in awareness, knowledge, and Pap testing. Testing increased among women in both the combined intervention (65.8% to 81.8%; P&lt;.001) and media-only (70.1% to 75.5%; P&lt;.001) groups, but significantly more in the combined intervention group (P=.001). Among women never previously screened, significantly more women in the combined intervention group (46.0%) than in the media-only group (27.1%) obtained tests (P&lt;.001). Significantly more women in the combined intervention group obtained their first Pap test or obtained one after an interval of more than 1 year (became up-to-date; 45.7% to 67.3%, respectively; P&lt;.001) than did those in the media-only group (50.9% to 55.7%, respectively; P=.035). Combined intervention motivated more Vietnamese American women to obtain their first Pap tests and to become up-to-date than did media education alone.-Effective lay health worker outreach and media-based education for promoting cervical cancer screening among Vietnamese American women.",0
"Various factors contribute to the effective implementation of evidence-based treatments (EBTs). In this study, cognitive processing therapy (CPT) was administered in a Veterans Affairs (VA) posttraumatic stress disorder (PTSD) specialty clinic in which training and supervision were provided following VA implementation guidelines. The aim was to (a) estimate the proportion of variability in outcome attributable to therapists and (b) identify characteristics of those therapists who produced better outcomes. We used an archival database of veterans (n = 192) who completed 12 sessions of CPT by therapists (n = 25) who were trained by 2 nationally recognized trainers, 1 of whom also provided weekly group supervision. Multilevel modeling was used to estimate therapist effects, with therapists treated as a random factor. The supervisor was asked to retrospectively rate each therapist in terms of perceived effectiveness based on supervision interactions. Using single case study design, the supervisor was interviewed to determine what criteria she used to rate the therapists and emerging themes were coded. When initial level of severity on the PTSD Checklist (PCL; McDonald &amp; Calhoun, 2010; Weathers, Litz, Herman, Huska, &amp; Keane, 1993) was taken into account, approximately 12% of the variability in the PCL at the end of treatment was due to therapists. The trainer, blind to the results, identified the following characteristics and actions of effective therapists: effectively addressing patient avoidance, language used in supervision, flexible interpersonal style, and ability to develop a strong therapeutic alliance. This study adds to the growing body of literature documenting the importance of the individual therapist as an important factor in the change process.-Uniformity of evidence-based treatments in practice? Therapist effects in the delivery of cognitive processing therapy for PTSD.",2
"To clarify and illustrate sample size calculations for the cross-sectional stepped wedge cluster randomized trial (SW-CRT) and to present a simple approach for comparing the efficiencies of competing designs within a unified framework. We summarize design effects for the SW-CRT, the parallel cluster randomized trial (CRT), and the parallel cluster randomized trial with before and after observations (CRT-BA), assuming cross-sectional samples are selected over time. We present new formulas that enable trialists to determine the required cluster size for a given number of clusters. We illustrate by example how to implement the presented design effects and give practical guidance on the design of stepped wedge studies. For a fixed total cluster size, the choice of study design that provides the greatest power depends on the intracluster correlation coefficient (ICC) and the cluster size. When the ICC is small, the CRT tends to be more efficient; when the ICC is large, the SW-CRT tends to be more efficient and can serve as an alternative design when the CRT is an infeasible design. Our unified approach allows trialists to easily compare the efficiencies of three competing designs to inform the decision about the most efficient design in a given scenario.-Sample size calculations for stepped wedge and cluster randomised trials: a unified approach.",3
"This study examined (1) the relationship between income inequality and mortality among all counties in the contiguous United States to ascertain whether the relationships found for states and metropolitan areas extend to smaller geographic units and (2) the influence of minority racial concentration on the inequality-mortality linkage. This county-level ecologic analysis used data from the Compressed Mortality Files and the US Census. Weighted least squares regression models of age-, sex-, and race-adjusted county mortality rates were estimated to examine the additive and interactive effects of income inequality and minority racial concentration. Higher income inequality at the county level was significantly associated with higher total mortality. Higher minority racial concentration also was significantly related to higher mortality and interacted with income inequality. The relationship between income inequality and mortality is robust for counties in the United States. Minority concentration interacts with income inequality, resulting in higher mortality in counties with low inequality and a high percentage of Blacks than in counties with high inequality and a high percentage of Blacks.-Income inequality and mortality in US counties: does minority racial concentration matter?",0
"This study compared different methods for assigning confidence intervals to the analysis of variance estimator of the intraclass correlation coefficient (rho). The context of the comparison was the use of rho to estimate the variance inflation factor when planning cluster randomized trials. The methods were compared using Monte Carlo simulations of unbalanced clustered data and data from a cluster randomized trial of an intervention to improve the management of asthma in a general practice setting. The coverage and precision of the intervals were compared for data with different numbers of clusters, mean numbers of subjects per cluster and underlying values of rho. The performance of the methods was also compared for data with Normal and non-Normally distributed cluster specific effects. Results of the simulations showed that methods based upon the variance ratio statistic provided greater coverage levels than those based upon large sample approximations to the standard error of rho. Searle's method provided close to nominal coverage for data with Normally distributed random effects. Adjusted versions of Searle's method to allow for lack of balance in the data generally did not improve upon it either in terms of coverage or precision. Analyses of the trial data, however, showed that limits provided by Thomas and Hultquist's method may differ from those of the other variance ratio statistic methods when the arithmetic mean differs markedly from the harmonic mean cluster size. The simulation results demonstrated that marked non-Normality in the cluster level random effects compromised the performance of all methods. Confidence intervals for the methods were generally wide relative to the underlying size of rho suggesting that there may be great uncertainty associated with sample size calculations for cluster trials where large clusters are randomized. Data from cluster based studies with sample sizes much larger than those typical of cluster randomized trials are required to estimate rho with a reasonable degree of precision.-A comparison of confidence interval methods for the intraclass correlation coefficient in cluster randomized trials.",1
"Ahnn and Anderson derived sample size formulae for unstratified and stratified designs assuming equal allocation of subjects to three or more treatment groups. We generalize the sample size formulae to allow for unequal allocation. In addition, we define the overall probability of death to be equal to one minus the censored proportion for the stratified design. This definition also leads to a slightly different definition of the non-centrality parameter than that of Ahnn and Anderson for the stratified case. Assuming proportional hazards, sample sizes are determined for a prespecified power, significance level, hazard ratios, allocation of subjects to several treatment groups, and known censored proportion. In the proportional hazards setting, three cases are considered: (1) exponential failures--exponential censoring, (2) exponential failures--uniform censoring, and (3) Weibull failures (assuming same shape parameter for all groups)--uniform censoring. In all three cases of the unstratified case, it is assumed that the censoring distribution is the same for all of the treatment groups. For the stratified log-rank test, it is assumed the same censoring distribution across the treatment groups and the strata. Further, formulae have been developed to provide approximate powers for the test, based upon the first two or first four-moments of the asymptotic distribution. We observe the following two major findings based on the simulations. First, the simulated power of the log-rank test does not depend on the censoring mechanism. Second, for a significance level of 0.05 and power of 0.80, the required sample size n is independent of the censoring pattern. Moreover, there is very close agreement between the exact (asymptotic) and simulated powers when a sequence of alternatives is close to the null hypothesis. Two-moment and four-moment power series approximations also yield powers in close agreement with the exact (asymptotic) power. With unequal allocations, our simulations show that the empirical powers are consistently above the target value of prespecified power of 0.80 when 50 per cent of the patients are allocated to the treatment group with the smallest hazard.-Sample size determination for comparing several survival curves with unequal allocations.",0
"""Cross-sectional"" stepped wedge designs always reduce the required sample size when there is no time effect.",3
"The aetiology of ischaemic heart disease (IHD) is complex and is influenced by a spectrum of environmental factors and susceptibility genes. Traditional statistical modelling considers such factors to act independently in an additive manner. The Patient Rule-Induction Method (PRIM) is a multi-model building strategy for evaluating risk attributable to context-dependent gene and environmental effects. PRIM was applied to 9073 participants from the prospective Copenhagen City Heart Study (CCHS). Gender-specific cumulative incidences were estimated for subgroups defined by categories of age, smoking, hypertension, diabetes, body mass index, total cholesterol, high-density lipoprotein cholesterol and triglycerides and by 94 single nucleotide variants (SNVs).Cumulative incidences for subgroups were validated using an independently ascertained sample of 58 240 participants from the Copenhagen General Population Study (CGPS). In the CCHS the overall cumulative incidences were 0.17 in women and 0.21 in men. PRIM identified six and four mutually exclusive subgroups in women and men, respectively, with cumulative incidences of IHD ranging from 0.02 to 0.34. Cumulative incidences of IHD generated by PRIM in the CCHS were validated in four of the six subgroups of women and two of the four subgroups of men in the CGPS. PRIM identified high-risk subgroups characterized by specific contexts of selected values of traditional risk factors and genetic variants. These subgroups were validated in an independently ascertained cohort study. Thus, a multi-model strategy may identify groups of individuals with substantially higher risk of IHD than the overall risk for the general population.-Subgroups at high risk for ischaemic heart disease:identification and validation in 67?000 individuals from the general population.",0
"A longer leukocyte telomere length (LTL) in women than men has been attributed to a slow rate of LTL attrition in women, perhaps due to high estrogen exposure during the premenopausal period. To test this premise we performed a longitudinal study (an average follow-up of 12 years) in a subset of the population-based Danish National Twin Registry. Participants consisted of 405 women, aged 37.5 (range 18.0-64.3) years, and 329 men, aged 38.8 (range 18.0-58.5) years, at baseline examination. Women showed a longer LTL [kb ? standard error(SE)] than men (baseline: 7.01 ? 0.03 vs 6.87 ? 0.04; follow-up: 6.79 ? 0.03 vs 6.65 ? 0.03; both P = 0.005). Women displayed deceleration of LTL attrition (bp/years ? SE), as they transitioned from the premenopausal period (20.6 ? 1.0) through the perimenopausal period (16.5 ? 1.3) to the postmenopausal period (15.1 ? 1.7). Age was not associated with LTL attrition in women after statistical control for menopausal status. Men, in contrast, displayed a trend for age-dependent increase in the rate of LTL attrition, which differed significantly from the pattern in women (P for interaction = 0.01). Results indicate that the premenopausal period is expressed in a higher rate of LTL attrition than the postmenopausal period. They further suggest that the sex gap in LTL stems from earlier ages-the period of growth and development. The higher rate of LTL attrition in premenopausal women, we propose, might relate to estrogen-mediated increased turnover of erythrocytes, menstrual bleeding or both.-Leukocyte telomere length dynamics in women and men: menopause vs age effects.",0
"We used theoretical and simulation-based approaches to study Type I error rates for one-stage and two-stage analytic methods for cluster-randomized designs. The one-stage approach uses the observed data as outcomes and accounts for within-cluster correlation using a general linear mixed model. The two-stage model uses the cluster specific means as the outcomes in a general linear univariate model. We demonstrate analytically that both one-stage and two-stage models achieve exact Type I error rates when cluster sizes are equal. With unbalanced data, an exact size ? test does not exist, and Type I error inflation may occur. Via simulation, we compare the Type I error rates for four one-stage and six two-stage hypothesis testing approaches for unbalanced data. With unbalanced data, the two-stage model, weighted by the inverse of the estimated theoretical variance of the cluster means, and with variance constrained to be positive, provided the best Type I error control for studies having at least six clusters per arm. The one-stage model with Kenward-Roger degrees of freedom and unconstrained variance performed well for studies having at least 14 clusters per arm. The popular analytic method of using a one-stage model with denominator degrees of freedom appropriate for balanced data performed poorly for small sample sizes and low intracluster correlation. Because small sample sizes and low intracluster correlation are common features of cluster-randomized trials, the Kenward-Roger method is the preferred one-stage approach.-Recommendations for choosing an analysis method that controls Type I error for unbalanced cluster sample designs with Gaussian outcomes.",1
Community-based health intervention trials: an overview of methodological issues.,1
"Cluster randomization design is increasingly used for the evaluation of health-care, screening or educational interventions. At the planning stage, sample size calculations usually consider an average cluster size without taking into account any potential imbalance in cluster size. However, there may exist high discrepancies in cluster sizes. We performed simulations to study the impact of an imbalance in cluster size on power. We determined by simulations to which extent four methods proposed to adapt the sample size calculations to a pre-specified imbalance in cluster size could lead to adequately powered trials. We showed that an imbalance in cluster size can be of high influence on the power in the case of severe imbalance, particularly if the number of clusters is low and/or the intraclass correlation coefficient is high. In the case of a severe imbalance, our simulations confirmed that the minimum variance weights correction of the variation inflaction factor (VIF) used in the sample size calculations has the best properties. Publication of cluster sizes is important to assess the real power of the trial which was conducted and to help designing future trials. We derived an adaptation of the VIF from the minimum variance weights correction to be used in case the imbalance can be a priori formulated such as ""a proportion (gamma) of clusters actually recruit a proportion (tau) of subjects to be included (gamma &lt; or = tau)"".-Planning a cluster randomized trial with unequal cluster sizes: practical issues involving continuous outcomes.",1
Cluster-randomized controlled trials: Part 2.,1
"Testing anti-cancer agents with multiple disease subtypes is challenging and it becomes more complicated when the subgroups have different types of endpoints (such as binary endpoints of tumor response and progression-free survival endpoints). When this occurs, one common approach in oncology is to conduct a series of small screening trials in specific patient subgroups, and these trials are typically run in parallel, independent of each other. However, this approach does not consider the possibility that some of the patient subpopulations respond similarly to therapy. In this article, we developed a simple approach to jointly model subgroups with mixed-type endpoints, which allows borrowing strength across subgroups for efficient estimation of treatment effects.-Integrating subgroups with mixed-type endpoints in early phase oncology trials.",0
"Stromal-epithelial interactions are of particular significance in breast tissue as misregulation of these interactions can promote tumorigenesis and invasion. Moreover, collagen-dense breast tissue increases the risk of breast carcinoma, although the relationship between collagen density and tumorigenesis is not well understood. As little is known about epithelial-stromal interactions in vivo, it is necessary to visualize the stroma surrounding normal epithelium and mammary tumors in intact tissues to better understand how matrix organization, density, and composition affect tumor formation and progression. Epithelial-stromal interactions in normal mammary glands, mammary tumors, and tumor explants in three-dimensional culture were studied with histology, electron microscopy, and nonlinear optical imaging methodologies. Imaging of the tumor-stromal interface in live tumor tissue ex vivo was performed with multiphoton laser-scanning microscopy (MPLSM) to generate multiphoton excitation (MPE) of endogenous fluorophores and second harmonic generation (SHG) to image stromal collagen. We used both laser-scanning multiphoton and second harmonic generation microscopy to determine the organization of specific collagen structures around ducts and tumors in intact, unfixed and unsectioned mammary glands. Local alterations in collagen density were clearly seen, allowing us to obtain three-dimensional information regarding the organization of the mammary stroma, such as radiating collagen fibers that could not have been obtained using classical histological techniques. Moreover, we observed and defined three tumor-associated collagen signatures (TACS) that provide novel markers to locate and characterize tumors. In particular, local cell invasion was found predominantly to be oriented along certain aligned collagen fibers, suggesting that radial alignment of collagen fibers relative to tumors facilitates invasion. Consistent with this observation, primary tumor explants cultured in a randomly organized collagen matrix realigned the collagen fibers, allowing individual tumor cells to migrate out along radially aligned fibers. The presentation of these tumor-associated collagen signatures allowed us to identify pre-palpable tumors and see cells at the tumor-stromal boundary invading into the stroma along radially aligned collagen fibers. As such, TACS should provide indications that a tumor is, or could become, invasive, and may serve as part of a strategy to help identify and characterize breast tumors in animal and human tissues.-Collagen reorganization at the tumor-stromal interface facilitates local invasion.",0
"Doubly censored data often arise in medical studies of disease progression involving two related events for which both an originating and a terminating event are interval-censored. Although regression modeling for such doubly censored data may be complicated, we propose a simple semiparametric regression modeling strategy based on jackknife pseudo-observations obtained using nonparametric estimators of the survival function. Inference is carried out via generalized estimating equations. Simulations studies show that the proposed method produces virtually unbiased covariate effect estimates, even for moderate sample sizes. A prostate cancer study example illustrates the practical advantages of the proposed approach.-A flexible semiparametric modeling approach for doubly censored data with an application to prostate cancer.",0
"The ""Age of Blood in Children in Pediatric Intensive Care Unit"" (ABC PICU) study is a randomized controlled trial (RCT) that aims to determine if red blood cell (RBC) unit storage age affects outcomes in critically ill children. While RBCs can be stored for up to 42?days in additive solutions, their efficacy and safety after long-term storage have been challenged. Preclinical and clinical observational evidence suggests loss of efficacy and lack of safety of older RBC units, especially in more vulnerable populations such as critically ill children. Because there is a belief that shorter storage will improve outcomes, some physicians and institutions systematically transfuse fresh RBCs to children. Conversely, the standard practice of blood banks is to deliver the oldest available RBC unit (first-in, first-out policy) in order to decrease wastage. The ABC PICU study, is a double-blind superiority trial comparing the development of ""New or Progressive Multiple Organ Dysfunction Syndrome"" (NPMODS) in 1538 critically ill children randomized to either transfusion with RBCs stored for ? 7?days or to standard-issue RBCs (oldest in inventory). Patients are being recruited from 52 centers in the US, Canada, France, Italy, and Israel. The ABC PICU study should have significant implications for blood procurement services. A relative risk reduction of 33% is postulated in the short-storage arm. If a difference is found, this will indicate that fresher RBCs do improve outcomes in the pediatric intensive care unit population and would justify that use in critically ill children. If no difference is found, this will reassure clinicians and transfusion medicine specialists regarding the safety of the current system of allocating the oldest RBC unit in inventory and will discourage clinicians from preferentially requesting fresher blood for critically ill children. ClinicalTrials.gov, ID: NCT01977547 . Registered on 6 November 2013.-The age of blood in pediatric intensive care units (ABC PICU): study protocol for a randomized controlled trial.",0
"Cluster randomized trials (CRTs) are often prone to selection bias despite randomization. Using a simulation study, we investigated the use of propensity score (PS) based methods in estimating treatment effects in CRTs with selection bias when the outcome is quantitative. Of four PS-based methods (adjustment on PS, inverse weighting, stratification, and optimal full matching method), three successfully corrected the bias, as did an approach using classical multivariable regression. However, they showed poorer statistical efficiency than classical methods, with higher standard error for the treatment effect, and type I error much smaller than the 5% nominal level.-Propensity scores used for analysis of cluster randomized trials with selection bias: a simulation study.",1
"Structural balance theory (SBT) has maintained a venerable status in the psychological literature for more than 5 decades. One important problem pertaining to SBT is the approximation of structural or generalized balance via the partitioning of the vertices of a signed graph into K clusters. This K-balance partitioning problem also has more general psychological applications associated with the analysis of similarity/dissimilarity relationships among stimuli. Accordingly, K-balance partitioning can be gainfully used in a wide variety of SBT applications, such as attraction and child development, evaluation of group membership, marketing and consumer issues, and other psychological contexts not necessarily related to SBT. We present a branch-and-bound algorithm for the K-balance partitioning problem. This new algorithm is applied to 2 synthetic numerical examples as well as to several real-world data sets from the behavioral sciences literature.-K-balance partitioning: an exact method with applications to generalized structural balance and other psychological contexts.",0
"Zhao, Rahardja and Qu consider sample size calculation for Wilcoxon-Mann-Whitney (WMW) tests for data with ties, and present a straightforward formula. We observe that the 'exemplary data set' approach, usually applied in more complex situations, has a close relationship to the Zhao-Rahardja-Qu method for WMW sample size estimation and they are asymptotically equivalent. Therefore, the exemplary data set approach can be used to easily obtain estimates similar to those that the closed formula gives. We illustrate application of both methods for a WMW sample size estimation example, and also extend the simulation study presented by Zhao et al. We find that the Zhao-Rahardja-Qu formula (and by extension the exemplary data set method) can give estimates just as accurate as those obtained using either the Kolassa approach (via nQuery Advisor) or the O'Brien-Castelloe approach (via SAS 9.2 PROC POWER), for 1:1 and 1:2 allocation ratios. However, the latter two methods can be more accurate for a ratio of 1:4 or 1:19. Finally, we note the general utility of the exemplary data set approach for sample size estimation, even in other situations where closed-form sample size formulae exist.-Exemplary data set sample size calculation for Wilcoxon-Mann-Whitney tests.",0
"Multicenter study designs have several advantages, but the possibility of non-random measurement error resulting from procedural differences between the centers is a special concern. While it is possible to address and correct for some measurement error through statistical analysis, proactive data monitoring is essential to ensure high-quality data collection. In this article, we describe quality assurance efforts aimed at reducing the effect of measurement error in a recent follow-up of a large cluster-randomized controlled trial through periodic evaluation of intraclass correlation coefficients (ICCs) for continuous measurements. An ICC of 0 indicates the variance in the data is not due to variation between the centers, and thus the data are not clustered by center. Through our review of early data downloads, we identified several outcomes (including sitting height, waist circumference, and systolic blood pressure) with higher than expected ICC values. Further investigation revealed variations in the procedures used by pediatricians to measure these outcomes. We addressed these procedural inconsistencies through written clarification of the protocol and refresher training workshops with the pediatricians. Further data monitoring at subsequent downloads showed that these efforts had a beneficial effect on data quality (sitting height ICC decreased from 0.92 to 0.03, waist circumference from 0.10 to 0.07, and systolic blood pressure from 0.16 to 0.12). We describe a simple but formal mechanism for identifying ongoing problems during data collection. The calculation of the ICC can easily be programmed and the mechanism has wide applicability, not just to cluster randomized controlled trials but to any study with multiple centers or with multiple observers.-Ongoing monitoring of data clustering in multicenter studies.",1
"Generalized estimating equations (GEE) can be highly influenced by the presence of unusual data points. A generalization of the GEE procedure, which yields parameter estimates and fitted values that are resistant to influential data, is introduced. Resistant generalized estimating equations (REGEE) include weights in the estimating equations to downweight influential observations or clusters. Influential observations are downweighted according to their leverage or residual in an example of correlated binary regression applied to 137 urinary incontinent elderly patients from 38 medical practices.-Robust regression for clustered data with application to binary responses.",1
"Prostate cancer grade, assessed with the Gleason score, describes how abnormal the tumor tissue and cells appear, and it is an important prognostic indicator of disease progression. Whether prostate tumors change grade is a question that has implications for screening and treatment. Empirical data on tumor grade over time have become available from men biopsied regularly as part of active surveillance (AS). However, biopsy (BX) grade is subject to misclassification. In this article, we develop a model that allows for estimation of the time of grade change while accounting for the misclassification error from BX grade. We use misclassification rates from studies of prostate cancer BXs followed by radical prostatectomy. Estimation of the transition times from true low-grade to high-grade disease is conducted within a Bayesian framework. We apply our model to serial observations on BX grade among 627 cases enrolled in a cohort of AS patients at Johns Hopkins University who were biopsied annually and referred to treatment if there was any evidence of disease progression on BX. We consider different prior distributions for the time to true grade progression. The estimated likelihood of grade progression within 10 years of study entry ranges from 12% to 24% depending on the prior. We conclude that knowledge of rates of grade misclassification allows for determination of true grade progression rates among men with serial BXs on AS. Although our results are sensitive to prior specifications, they indicate that in a nontrivial fraction of the patient population, tumor grade can progress.-Modeling grade progression in an active surveillance study.",0
"Random forests are a popular nonparametric tree ensemble procedure with broad applications to data analysis. While its widespread popularity stems from its prediction performance, an equally important feature is that it provides a fully nonparametric measure of variable importance (VIMP). A current limitation of VIMP, however, is that no systematic method exists for estimating its variance. As a solution, we propose a subsampling approach that can be used to estimate the variance of VIMP and for constructing confidence intervals. The method is general enough that it can be applied to many useful settings, including regression, classification, and survival problems. Using extensive simulations, we demonstrate the effectiveness of the subsampling estimator and in particular find that the delete-d jackknife variance estimator, a close cousin, is especially effective under low subsampling rates due to its bias correction properties. These 2 estimators are highly competitive when compared with the .164 bootstrap estimator, a modified bootstrap procedure designed to deal with ties in out-of-sample data. Most importantly, subsampling is computationally fast, thus making it especially attractive for big data settings.-Standard errors and confidence intervals for variable importance in random forest regression, classification, and survival.",0
"A pilot test of a survey of grocery store product displays was conducted to measure the amount of health-education information provided and the proportion of the display devoted to ""healthier"" products. Inter-rater reliability ranged between 0.73 and 0.78 for the healthiness indices and between 0.30 and 0.67 for the health education measures. Test-retest reliability ranged from 0.44 to 1.0.-Evaluating community-based nutrition programs: assessing the reliability of a survey of grocery store product displays.",1
"The development of obesity in childhood is considered a major determinant of cardiovascular risk. Currently the body mass index (BMI = weight/height(2)) is widely used as a measure of obesity. However, since BMI is associated with height during childhood, a weight for height index (weight/height(p)) that is independent of height is thought to be more appropriate. Therefore, to compare the utility of such weight/height(p) index with BMI in assessing adiposity and its relation to cardiovascular risk variable data from the Bogalusa Heart Study participants aged 6 months to 21 years were examined. A total of 31,796 observations on 12,827 subjects was used in the data analysis. Study variables include height, weight, subscapular and triceps skinfolds, blood pressure, serum lipids and lipoproteins, and plasma glucose and insulin. The optimal exponential for the weight/height(p) index started from 2.42 in the 6 month olds, decreased to 1.86 in 2 to 3 year olds, increased to 3.29 among 10 to 11 year olds, and then decreased to 2.15 in the 20 to 21 year olds. The BMI showed slightly higher correlations than weight/height(p) index with subscapular skinfold in children. Both in children and young adults BMI also showed a slightly higher correlation with other cardiovascular risk factor variables regardless of age-race-sex groups. These results indicate that weight/height(p) index is not superior to BMI as an indicator of adiposity and related cardiovascular risk factors during childhood.-Comparison of weight-for-height indices as a measure of adiposity and cardiovascular risk from childhood to young adulthood: the Bogalusa heart study.",0
"Screening and diagnostic tests are important in disease prevention or control. The predictive values of positive and negative (PPV and NPV) test results are two of four operational characteristics of a screening test. We review an existing method based on the generalized estimating equation (GEE) methodology for comparing predictive values from the same sample of subjects and propose two Wald test statistics derived from the weighted least squares (WLS) method for the analysis of categorical data. Using these results, we propose sample size calculation formulae for this problem. Simulation studies are conducted to compare the performances of the two Wald test statistics (one based on the difference of two PPVs or NPVs, another based on the logarithm of the ratio of two PPVs or NPVs) and the score/Wald test statistic derived from GEE. We recommend using the difference-based WLS approach.-Comparison of predictive values of two diagnostic tests from the same sample of subjects using weighted least squares.",0
"This study evaluated an intervention designed to improve behavioral and mental health outcomes among adolescents and their parents with AIDS. Parents with AIDS (n = 307) and their adolescent children (n = 412) were randomly assigned to an intensive intervention or a standard care control condition. Ninety-five percent of subjects were reassessed at least once annually over 2 years. Adolescents in the intensive intervention condition reported significantly lower levels of emotional distress, of multiple problem behaviors, of conduct problems, and of family-related stressors and higher levels of self-esteem than adolescents in the standard care condition. Parents with AIDS in the intervention condition also reported significantly lower levels of emotional distress and multiple problem behaviors. Coping style, levels of disclosure regarding serostatus, and formation of legal custody plans were similar across intervention conditions. Interventions can reduce the long-term impact of parents' HIV status on themselves and their children.-An intervention for parents with AIDS and their adolescent children.",0
"Cluster randomized trials (CRTs) present unique ethical challenges. In the absence of a uniform standard for their ethical design and conduct, problems such as variability in procedures and requirements by different research ethics committees will persist. We aimed to assess the need for ethics guidelines for CRTs among research ethics chairs internationally, investigate variability in procedures for research ethics review of CRTs within and among countries, and elicit research ethics chairs' perspectives on specific ethical issues in CRTs, including the identification of research subjects. The proper identification of research subjects is a necessary requirement in the research ethics review process, to help ensure, on the one hand, that subjects are protected from harm and exploitation, and on the other, that reviews of CRTs are completed efficiently. A web-based survey with closed- and open-ended questions was administered to research ethics chairs in Canada, the United States, and the United Kingdom. The survey presented three scenarios of CRTs involving cluster-level, professional-level, and individual-level interventions. For each scenario, a series of questions was posed with respect to the type of review required (full, expedited, or no review) and the identification of research subjects at cluster and individual levels. A total of 189 (35%) of 542 chairs responded. Overall, 144 (84%, 95% CI 79 to 90%) agreed or strongly agreed that there is a need for ethics guidelines for CRTs and 158 (92%, 95% CI 88 to 96%) agreed or strongly agreed that research ethics committees could be better informed about distinct ethical issues surrounding CRTs. There was considerable variability among research ethics chairs with respect to the type of review required, as well as the identification of research subjects. The cluster-cluster and professional-cluster scenarios produced the most disagreement. Research ethics committees identified a clear need for ethics guidelines for CRTs and education about distinct ethical issues in CRTs. There is disagreement among committees, even within the same countries, with respect to key questions in the ethics review of CRTs. This disagreement reflects variability of opinion and practices pointing toward possible gaps in knowledge, and supports the need for explicit guidelines for the ethical conduct and review of CRTs.-Variability in research ethics review of cluster randomized trials: a scenario-based survey in three countries.",1
"The aims of this Patient Reported Outcome Measurement Information System (PROMIS) study were to (1) conceptualize children's subjective well-being (SWB) and (2) produce item pools with excellent content validity for calibration and use in computerized adaptive testings (CATs). Children's SWB was defined through semistructured interviews with experts, children (aged 8-17 years), parents, and a systematic literature review to identify item concepts comprehensively covering the full spectrum of SWB. Item concepts were transformed into item expressions and evaluated for comprehensibility using cognitive interviews, reading level analysis, and translatability review. Children's SWB comprises affective (positive affect) and global evaluation components (life satisfaction). Input from experts, children, parents, and the literature indicated that the eudaimonic dimension of SWB-that is, a sense of meaning and purpose-could be evaluated. Item pools for life satisfaction (56 items), positive affect (53 items), and meaning and purpose (55 items) were produced. Small differences in comprehensibility of some items were observed between children and adolescents. The SWB measures for children are the first to assess both the hedonic and eudaimonic aspects of SWB. Both children and youth seem to understand the concepts of a meaningful life, optimism, and goal orientation.-Subjective well-being measures for children were developed within the PROMIS project: presentation of first results.",0
"In cluster randomised cross-over (CRXO) trials, clusters receive multiple treatments in a randomised sequence over time. In such trials, there is usual correlation between patients in the same cluster. In addition, within a cluster, patients in the same period may be more similar to each other than to patients in other periods. We demonstrate that it is necessary to account for these correlations in the analysis to obtain correct Type I error rates. We then use simulation to compare different methods of analysing a binary outcome from a two-period CRXO design. Our simulations demonstrated that hierarchical models without random effects for period-within-cluster, which do not account for any extra within-period correlation, performed poorly with greatly inflated Type I errors in many scenarios. In scenarios where extra within-period correlation was present, a hierarchical model with random effects for cluster and period-within-cluster only had correct Type I errors when there were large numbers of clusters; with small numbers of clusters, the error rate was inflated. We also found that generalised estimating equations did not give correct error rates in any scenarios considered. An unweighted cluster-level summary regression performed best overall, maintaining an error rate close to 5% for all scenarios, although it lost power when extra within-period correlation was present, especially for small numbers of clusters. Results from our simulation study show that it is important to model both levels of clustering in CRXO trials, and that any extra within-period correlation should be accounted for. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Choosing appropriate analysis methods for cluster randomised cross-over trials with a binary outcome.",1
"Low-income, Mexican-American patients with diabetes exhibit high rates of medication nonadherence, poor blood sugar control and serious complications, and often have difficulty communicating their concerns about the medication regimen to physicians. Interventions led by community health workers, non-professional community members who are trained to work with patients to improve engagement and communication during the medical visit, have had mixed success in improving outcomes. The primary objective of this project is to pilot test a prototype software toolkit called ""EMPATHy"" that a community health worker can administer to help patients identify the most important barriers to adherence that they face and discuss these barriers with their doctor. The EMPATHy toolkit will be piloted in an ongoing intervention (Coached Care) in which community health workers are trained to be ""coaches"" to meet with patients before the medical visit and help them prepare a list of important questions for the doctor. A total of 190 Mexican-American patients with poorly controlled type 2 diabetes will be recruited from December 2014 through June 2015 and will be randomly assigned to complete either a single Coached Care intervention visit with no software tools or a Coached Care visit incorporating the EMPATHy software toolkit. The primary endpoints are (1) the development of a ""contextualized plan of care"" (i.e., a plan of care that addresses a barrier to medication adherence in the patient's daily life) with the doctor, determined from an audio recording of the medical visit, and (2) attainment of a concrete behavioral goal set during the intervention session, assessed in a 2-week follow-up phone call to the patient. The statistical analysis will include logistic regression models and is powered to detect a 50% increase in the primary endpoints. The study will provide evidence regarding the effectiveness and feasibility of a software tool to help patients communicate with doctors about problems they face with their medications. ClinicalTrials.gov NCT02324036 Registered 16 December 2014.-Effectiveness and feasibility of a software tool to help patients communicate with doctors about problems they face with their medication regimen (EMPATHy): study protocol for a randomized controlled trial.",0
"The Consolidated Standards of Reporting Trials extension for the stepped-wedge cluster randomized trial (SW-CRT) is a recently published reporting guideline for SW-CRTs. We assess the quality of reporting of a recent sample of SW-CRTs. Quality of reporting was asssessed according to the 26 items in the new guideline using a novel crowd sourcing methodology conducted independently and in duplicate, with random assignment, by 50 reviewers. We assessed reliability of the quality assessments, proposing this as a novel way to assess robustness of items in reporting guidelines. Several items were well reported. Some items were very poorly reported, including several items that have unique requirements for the SW-CRT, such as the rationale for use of the design, description of the design, identification and recruitment of participants within clusters, and concealment of cluster allocation (not reported in more than 50% of the reports). Agreement across items was moderate (median percentage agreement was 76% [IQR 64 to 86]). Agreement was low for several items including the description of the trial design and why trial ended or stopped for example. When reporting SW-CRTs, authors should pay particular attention to ensure clear reporting on the exact format of the design with justification, as well as how clusters and individuals were identified for inclusion in the study, and whether this was done before or after randomization of the clusters, which are crucial for risk of bias assessments. Some items, including why the trial ended, might either not be relevant to SW-CRTs or might be unclearly described in the statement.-Quality of stepped-wedge trial reporting can be reliably assessed using an updated CONSORT: crowd-sourcing systematic review.",3
"Pediatric cardiac surgery may lead to poor outcomes such as acute kidney injury (AKI) and prolonged hospital length of stay (LOS). Plasma and urine biomarkers may help with early identification and prediction of these adverse clinical outcomes. In a recent multi-center study, 311 children undergoing cardiac surgery were enrolled to evaluate multiple biomarkers for diagnosis and prognosis of AKI and other clinical outcomes. LOS is often analyzed as count data, thus Poisson regression and negative binomial (NB) regression are common choices for developing predictive models. With many correlated prognostic factors and biomarkers, variable selection is an important step. The present paper proposes new variable selection methods for Poisson and NB regression. We evaluated regularized regression through penalized likelihood function. We first extend the elastic net (Enet) Poisson to two penalized Poisson regression: Mnet, a combination of minimax concave and ridge penalties; and Snet, a combination of smoothly clipped absolute deviation (SCAD) and ridge penalties. Furthermore, we extend the above methods to the penalized NB regression. For the Enet, Mnet, and Snet penalties (EMSnet), we develop a unified algorithm to estimate the parameters and conduct variable selection simultaneously. Simulation studies show that the proposed methods have advantages with highly correlated predictors, against some of the competing methods. Applying the proposed methods to the aforementioned data, it is discovered that early postoperative urine biomarkers including NGAL, IL18, and KIM-1 independently predict LOS, after adjusting for risk and biomarker variables.-Penalized count data regression with application to hospital stay after pediatric cardiac surgery.",0
"Court documents have proven that a manufacturer-orchestrated strategy tried to promote gabapentin by distorting evidence in randomized trials. Given this background, we aimed to assess whether implausibly large treatment effects for gabapentin and for a similar gabapentinoid, pregabalin may have been published. We identified meta-analyses on gabapentin or pregabalin on any outcome from Google Scholar, PubMed, and EMBASE. We explored excess of significance in meta-analyses and whether outlier studies with extreme results (differing &gt;0.8 standard deviations from the summary effect of the meta-analysis) were scrutinized. All 10 evaluated meta-analyses showed statistically significant favorable findings. Heterogeneity I2 estimates exceeding 90% were noted in four meta-analyses of postoperative pain. In these four meta-analyses, 77 studies had estimates differing &gt;0.8 standard deviations from the summary estimate. Thirty-nine of 77 represented extremely favorable results, and 33 of them came from less developed countries with no tradition of clinical research, 22 reported no information on funding, and 20 reported no conflicts of interest. Conversely, 27 of 38 studies with unfavorable results came from more developed countries. Extremely favorable outlier studies in the meta-analyzed literature of gabapentin and pregabalin may be a footprint of bias in studies done in less developed countries.-Extremely large outlier treatment effects may be a footprint of bias in trials from less developed countries: randomized trials of gabapentinoids.",0
The stepped wedge design does not inherently have more power than a cluster randomized controlled trial.,3
"Behavioral strategies are recommended for menopausal symptoms, but little evidence exists regarding efficacy. Describe design and methodology of a randomized controlled 3 by 2 factorial trial of yoga, exercise and omega-3 fatty acids. Women from three geographic areas with a weekly average of ?14 hot flashes/night sweats, who met exclusion/inclusion criteria, were randomized to 12weeks of: 1) yoga classes and daily home practice; 2) supervised, facility-based aerobic exercise training; or 3) usual activity. Women in each arm were further randomized to either omega-3 supplement or placebo. Standardized training, on-going monitoring, and site visits were adopted to ensure consistency across sites and fidelity to the intervention. Participant adherence to the intervention protocol was monitored continuously, and retention was actively encouraged by staff. Information on adverse events was systematically collected. Of 7377 women who responded to mass mailings, 355 (4.8%) were randomized; mean age was 54.7 (sd=3.7), 26.2% were African American, 81.7% were post-menopausal, and mean baseline frequency of daily hot flashes/night sweats was 7.6 (sd=3.8). Adherence of ?80% was 59% for yoga, 77% for exercise training, and 80% for study pills. Final week 12 data were collected from 95.2% Conducting a multi-site, multi-behavioral randomized trial for menopausal symptoms is challenging but feasible. Benefits included cost-effective study design, centralized recruitment, and methodologic standardization.-Design and methods of a multi-site, multi-behavioral treatment trial for menopausal symptoms: the MsFLASH experience.",0
"Cluster randomized trials are increasingly common. Obtaining informed patient consent to participation in these trials raises practical challenges and ethical issues. The aims of this paper were to 1) develop a typology of interventions employed in cluster randomized trials in primary care; 2) assess whether the likelihood of seeking individual consent to participation varies by intervension type; 3) assess whether this likelihood has increased over time; 4) assess evidence for under reporting of consent procedures; 5) articulate reasons for not obtaining consent; and 6) make recommendations for future trial investigators. We collected data on trial interventions and consent procedures from reports of 152 recently published trials, and 47 unpublished trials. We develop a typology of interventions based on reasons for adopting a clustered design. We examine proportions seeking individual consent to participation among trials involving different types of intervention, in different periods, and among published and unpublished trials. Two-thirds of the trials had multifaceted interventions. Trials involving different types of intervention had different propensities to seek consent, largely because of practical obstacles to obtaining consent. Obtaining consent can compromise internal validity. More recent trials are no more likely to obtain consent than past trials. There was no evidence of under-reporting of consent procedures in publications. In conclusion, future trial investigators should consider both practical reasons and scientific arguments for not obtaining individual patient consent for all interventions in their trials. Where feasible, they should allow patients to opt out of the trial. Lay individuals should represent trial participants as part of the process of cluster consent to participation, and lay individuals could also be involved in considering ethical issues during trial planning. A more public debate may clarify the general acceptability of not obtaining consent in certain situations.-Informed patient consent to participation in cluster randomized trials: an empirical exploration of trials in primary care.",1
"Hypotension is associated with serious complications, including myocardial infarction, acute kidney injury, and mortality. Consequently, predicting and preventing hypotension may improve outcomes. We will therefore determine if use of a novel hypotension prediction tool reduces the duration and severity of hypotension in patients having non-cardiac surgery. We will conduct a two-center, pragmatic, randomized controlled trial (N = 213) in noncardiac surgical patients &gt; 45 years old who require intra-arterial blood pressure monitoring. All participating patients will be connected to a Flortrac IQ sensor and EV1000 platform (Edwards Lifesciences, Irvine). They will be randomly assigned to blinded or unblinded arms. The Hypotension Prediction Index (HPI) and advanced hemodynamic information will be universally recorded, but will only be available to clinicians when patients are assigned to unblinded monitoring. The primary outcome will be the effect of HPI software guidance on intraoperative time-weighted average mean arterial pressure under a threshold of 65 mmHg, which will be assessed with a Wilcoxon-Mann-Whitney 2-sample, two-tailed test. Our trial will determine whether the Hypotension Prediction Index and associated hemodynamic information substantively reduces hypotension during non-cardiac surgery. ClinicalTrials.gov, NCT03610165 . Registered on 1 August 2018.-Hypotension Prediction Index software for management of hypotension during moderate- to high-risk noncardiac surgery: protocol for a randomized trial.",0
"It is widely recognized that public health interventions benefit from community engagement and leadership, yet there are challenges to evaluating complex, community-led interventions assuming hierarchies of evidence derived from laboratory experimentation and clinical trials. Particular challenges include, first, the inconsistency of the intervention across sites and, second, the absence of researcher control over the sampling frame and methodology. This report highlights these challenges as they played out in the evaluation of a community-organized health project in South London. The project aimed to benefit maternal mental health, health literacy, and social capital, and especially to engage local populations known to have reduced contact with statutory services. We evaluated the project using two studies with different designs, sampling frames, and methodologies. In one, the sampling frame and methodology were under community control, permitting a comparison of change in outcomes before and after participation in the project. In the other, the sampling frame and methodology were under researcher control, permitting a case-control design. The two evaluations led to different results, however: participants in the community-controlled study showed benefits, while participants in the researcher-controlled study did not. The principal conclusions are that while there are severe challenges to evaluating a community-led health intervention using a controlled design, the measurement of pre-/post-participation changes in well-defined health outcomes should typically be a minimum evaluation requirement, and confidence in attributing causation of any positive changes to participation can be increased by use of interventions in the project and in the engagement process itself that have a credible theoretical and empirical basis.-Evaluation of a Community-Led Intervention in South London: How Much Standardization Is Possible?",1
"Delivering effective tobacco dependence treatment that is feasible within lung cancer screening (LCS) programs is crucial for realizing the health benefits and cost savings of screening. Large-scale trials and systematic reviews have demonstrated that digital cessation interventions (i.e. web-based and text message) are effective, sustainable over the long-term, scalable, and cost-efficient. Use of digital technologies is commonplace among older adults, making this a feasible approach within LCS programs. Use of cessation treatment has been improved with models that proactively connect smokers to treatment rather than passive referrals. Proactive referral to cessation treatment has been advanced through healthcare systems changes such as modifying the electronic health record to automatically link smokers to treatment. This study evaluates the impact of a proactive enrollment strategy that links LCS-eligible smokers with an evidence-based intervention comprised of a web-based (WEB) program and integrated text messaging (TXT) in a three-arm randomized trial with repeated measures at one, three, six, and 12?months post randomization. The primary outcome is biochemically confirmed abstinence at 12?months post randomization. We will randomize 1650 smokers who present for a clinical LCS to: (1) a usual care control condition (UC) which consists of Ask-Advise-Refer; (2) a digital (WEB + TXT) cessation intervention; or (3) a digital cessation intervention combined with tobacco treatment specialist (TTS) counseling (WEB + TXT + TTS). The scalability and sustainability of a digital intervention may represent the most cost-effective and feasible approach for LCS programs to proactively engage large numbers of smokers in effective cessation treatment. We will also evaluate the impact and cost-effectiveness of adding proven clinical intervention provided by a TTS. We expect that a combined digital/clinical intervention will yield higher quit rates than digital alone, but that it may not be as cost-effective or feasible for LCS programs to implement. This study is innovative in its use of interoperable, digital technologies to deliver a sustainable, scalable, high-impact cessation intervention and to facilitate its integration within clinical practice. It will add to the growing knowledge base about the overall effectiveness of digital interventions and their role in the healthcare delivery system. ClinicalTrials.gov, NCT03084835 . Registered on 9 March 2017.-An integrated digital/clinical approach to smoking cessation in lung cancer screening: study protocol for a randomized controlled trial.",0
"We reviewed group-randomized trials (GRTs) published in the American Journal of Public Health and Preventive Medicine from 1998 through 2002 and estimated the proportion of GRTs that employ appropriate methods for design and analysis. Of 60 articles, 9 (15.0%) reported evidence of using appropriate methods for sample size estimation. Of 59 articles in the analytic review, 27 (45.8%) reported at least 1 inappropriate analysis and 12 (20.3%) reported only inappropriate analyses. Nineteen (32.2%) reported analyses at an individual or subgroup level, ignoring group, or included group as a fixed effect. Hence increased vigilance is needed to ensure that appropriate methods for GRTs are employed and that results based on inappropriate methods are not published.-Design and analysis of group-randomized trials: a review of recent practices.",1
This paper discusses statistical techniques for the analysis of dichotomous data arising from a design in which the investigator randomly assigns each of two clusters of possibly varying size to interventions within strata. The problem addressed is that of assessing the statistical significance of the intervention effect over all strata. We propose a weighted paired t-test based on the empirical logistic transform for designs that randomize large aggregate clusters in each of several strata.-Analysis of data arising from a stratified design with the cluster as unit of randomization.,1
Intraoperative assessment of axillary lymph nodes in patients with breast cancer.,0
Education section - Cluster trials,1
"Influenza causes substantial morbidity and annual vaccination is the most important prevention strategy. Accurately measuring vaccine effectiveness (VE) is difficult. The clinical syndrome most closely associated with influenza virus infection, influenza-like illness (ILI), is not specific. In addition, laboratory confirmation is infrequently done, and available rapid diagnostic tests are imperfect. The objective of this study was to estimate the joint impact of rapid diagnostic test sensitivity and specificity on VE for three types of study designs: a cohort study, a traditional case-control study, and a case-control study that used as controls individuals with ILI who tested negative for influenza virus infection. We developed a mathematical model with five input parameters: true VE, attack rates (ARs) of influenza-ILI and non-influenza-ILI and the sensitivity and specificity of the diagnostic test. With imperfect specificity, estimates from all three designs tended to underestimate true VE, but were similar except if fairly extreme inputs were used. Only if test specificity was 95% or more or if influenza attack rates doubled that of background illness did the case-control method slightly overestimate VE. The case-control method usually produced the highest and most accurate estimates, followed by the test-negative design. The bias toward underestimating true VE introduced by low test specificity increased as the AR of influenza- relative to non-influenza-ILI decreases and, to a lesser degree, with lower test sensitivity. Demonstration of a high influenza VE using tests with imperfect sensitivity and specificity should provide reassurance that the program has been effective in reducing influenza illnesses, assuming adequate control of confounding factors.-Methodologic issues regarding the use of three observational study designs to assess influenza vaccine effectiveness.",0
"Longitudinal behavioral intervention trials to reduce HIV transmission risk collect complex multilevel and multivariate data longitudinally for each subject with important correlation structures across time, level, and variables. Accurately assessing the effects of these trials are critical for determining which interventions are effective. Both numbers of partners and numbers of sex acts with each partner are reported at each time point. Sex acts with each partner are further differentiated into protected and unprotected acts with correspondingly differing risks of HIV/STD transmission. These trials generally also have eligibility criteria limiting enrollment to participants with some minimal level of risky sexual behavior tied directly to the outcome of interest. The combination of these factors makes it difficult to quantify sexual behaviors and the effects of intervention. We propose a multivariate multilevel count model that simultaneously models the number of partners, acts within partners, and accounts for recruitment eligibility. Our methods are useful in the evaluation of intervention trials and provide a more accurate and complete model for sexual behavior. We illustrate the contributions of our model by examining seroadaptive behavior defined as risk reducing behavior that depends on the serostatus of the partner. Several forms of seroadaptive risk reducing behavior are quantified and distinguished from nonseroadaptive risk reducing behavior.-Modeling seroadaptation and sexual behavior among HIV+ study participants with a simultaneously multilevel and multivariate longitudinal count model.",0
"Stratified randomized designs are popular in cluster randomized trials (CRTs) because they increase the chance of the intervention groups being well balanced in terms of identified prognostic factors at baseline and may increase statistical power. The objective of this paper is to assess the gains in power obtained by stratifying randomization by cluster size, when cluster size is associated with an important cluster level factor which is otherwise unaccounted for in data analysis. A simulation study was carried out using a CRT where UK general practices were the randomized units as a template. The results show that when cluster size is strongly associated with a cluster level factor which is predictive of outcome, the stratified randomized design has superior power results to the completely randomized design and that the superiority is related to the number of clusters.-Comparing completely and stratified randomized designs in cluster randomized trials when the stratifying factor is cluster size: a simulation study.",1
Randomization by cluster. Sample size requirements and analysis.,1
"Objectives: With the emerging trends, more cluster randomized trials will be conducted in older adults, where facilities are randomized rather than individuals. Similarity of individuals from a facility (intraclass correlation coefficient/ICC) plays a critical role, but not readily available. We document ICCs for measures commonly used in community-dwelling older adults and discuss implications. Method: Secondary analysis of a range of baseline measures from the On the Move cluster randomized trial, whose ICCs were computed using a linear mixed model. Results: Self-reported disability measures related to facility characteristics and sense of community had the greatest ICCs (&gt;0.10), while mobility performance measures had 0.05 to 0.10, and cognitive measure 0.11. Discussion: The ICCs for measures commonly used in older adults are of a sufficient magnitude to have a substantial impact on planned sample size of a study and credibility of results, and should be taken into consideration in study planning and data analysis.-Intraclass Correlation Coefficients for Planning Cluster Randomized Trials in Community-Dwelling Older Adults",1
"Stepped wedge cluster trials are an increasingly popular alternative to traditional parallel cluster randomized trials. Such trials often utilize a small number of clusters and numerous time intervals, and these components must be considered when choosing an analysis method. A generalized linear mixed model containing a random intercept and fixed time and intervention covariates is the most common analysis approach. However, the sole use of a random intercept applies a constant intraclass correlation coefficient structure, which is an assumption that is likely to be violated given stepped wedge trials (SWTs) have multiple time intervals. Alternatively, generalized estimating equations (GEE) are robust to the misspecification of the working correlation structure, although it has been shown that small-sample adjustments to standard error estimates and the use of appropriate degrees of freedom are required to maintain the validity of inference when the number of clusters is small. In this article, we show, using an extensive simulation study based on a motivating example and a more general design, the use of GEE can maintain the validity of inference in small-sample SWTs with binary outcomes. Furthermore, we show which combinations of bias corrections to standard error estimates and degrees of freedom work best in terms of attaining nominal type I error rates.-Maintaining the validity of inference in small-sample stepped wedge cluster randomized trials with binary outcomes when using generalized estimating equations",3
"In many scientific problems involving high-throughput technology, inference must be made involving several hundreds or thousands of hypotheses. Recent attention has focused on how to address the multiple testing issue; much focus has been devoted toward the use of the false discovery rate. In this article, we consider an alternative estimation procedure titled shrunken p-values for assessing differential expression (SPADE). The estimators are motivated by risk considerations from decision theory and lead to a completely new method for adjustment in the multiple testing problem. In addition, the decision-theoretic framework can be used to derive a decision rule for controlling the number of false positive results. Some theoretical results are outlined. The proposed methodology is illustrated using simulation studies and with application to data from a prostate cancer gene expression profiling study.-Shrunken p-values for assessing differential expression with applications to genomic data analysis.",0
"Person mobility is an inescapable fact of life for most cluster-randomized (e.g., schools, hospitals, clinic, cities, state) cohort prevention trials. Mobility rates are an important substantive consideration in estimating the effects of an intervention. In cluster-randomized trials, mobility rates are often correlated with ethnicity, poverty and other variables associated with disparity. This raises the possibility that estimated intervention effects may generalize to only the least mobile segments of a population and, thus, create a threat to external validity. Such mobility can also create threats to the internal validity of conclusions from randomized trials. Researchers must decide how to deal with persons who leave study clusters during a trial (dropouts), persons and clusters that do not comply with an assigned intervention, and persons who enter clusters during a trial (late entrants), in addition to the persons who remain for the duration of a trial (stayers). Statistical techniques alone cannot solve the key issues of internal and external validity raised by the phenomenon of person mobility. This commentary presents a systematic, Campbellian-type analysis of person mobility in cluster-randomized cohort prevention trials. It describes four approaches for dealing with dropouts, late entrants and stayers with respect to data collection, analysis and generalizability. The questions at issue are: 1) From whom should data be collected at each wave of data collection? 2) Which cases should be included in the analyses of an intervention effect? and 3) To what populations can trial results be generalized? The conclusions lead to recommendations for the design and analysis of future cluster-randomized cohort prevention trials.-Person mobility in the design and analysis of cluster-randomized cohort prevention trials.",1
"Near infrared spectroscopy (NIRS) is an imaging-based diagnostic tool that provides non-invasive and continuous evaluation of regional tissue oxygenation in real-time. In recent years, NIRS has shown promise as a useful monitoring technology to help detect relative tissue ischemia that could lead to significant morbidity and mortality in preterm infants. However, some issues inherent in NIRS technology use on neonates, such as wide fluctuation in signals, signal dropout and low limit of detection of the device, pose challenges that may obscure reliable interpretation of the NIRS measurements using current methods of analysis. In this paper, we propose new nonparametric statistical methods to analyze mesenteric rSO2 (regional oxygenation) produced by NIRS to evaluate oxygenation in intestinal tissues and investigate oxygenation response to red blood cell transfusion (RBC) in preterm infants. Specifically, we present a mean area under the curve (MAUC) measure and a slope measure to capture the mean rSO2 level and temporal trajectory of rSO2, respectively. We develop estimation methods for the measures based on multiple imputation and spline smoothing and further propose novel nonparametric testing procedures to detect RBC-related changes in mesenteric oxygenation in preterm infants. Through simulation studies, we show that the proposed methods demonstrate improved accuracy in characterizing the mean level and changing pattern of mesenteric rSO2 and also increased statistical power in detecting RBC-related changes, as compared with standard approaches. We apply our methods to a NIRS study in preterm infants receiving RBC transfusion from Emory University to evaluate the pre- and post-transfusion mesenteric oxygenation in preterm infants.-Statistical methods for characterizing transfusion-related changes in regional oxygenation using near-infrared spectroscopy (NIRS) in preterm infants.",0
"Objectives. To examine negative police encounters and police avoidance as mediators of incarceration history and depressive symptoms among US Black men and to assess the role of unemployment as a moderator of these associations.Methods. Data were derived from the quantitative phase of Menhood, a 2015-2016 study based in Washington, DC. Participants were 891 Black men, 18 to 44 years of age, who completed computer surveys. We used moderated mediation to test the study's conceptual model.Results. The results showed significant indirect effects of incarceration history on depressive symptoms via negative police encounters and police avoidance. Unemployment moderated the indirect effect via police avoidance. Participants with a history of incarceration who were unemployed reported significantly higher police avoidance and, in turn, higher depressive symptoms. Moderation of unemployment on the indirect effect via negative police encounters was not significant.Conclusions. There is a critical need to broaden research on the health impact of mass incarceration to include other aspects of criminal justice involvement (e.g., negative police encounters and police avoidance) that negatively affect Black men's mental health.-Negative Police Encounters and Police Avoidance as Pathways to Depressive Symptoms Among US Black Men, 2015-2016.",0
"We extend the pattern-mixture approach to handle missing continuous outcome data in longitudinal cluster randomized trials, which randomize groups of individuals to treatment arms, rather than the individuals themselves. Individuals who drop out at the same time point are grouped into the same dropout pattern. We approach extrapolation of the pattern-mixture model by applying multilevel multiple imputation, which imputes missing values while appropriately accounting for the hierarchical data structure found in cluster randomized trials. To assess parameters of interest under various missing data assumptions, imputed values are multiplied by a sensitivity parameter, k, which increases or decreases imputed values. Using simulated data, we show that estimates of parameters of interest can vary widely under differing missing data assumptions. We conduct a sensitivity analysis using real data from a cluster randomized trial by increasing k until the treatment effect inference changes. By performing a sensitivity analysis for missing data, researchers can assess whether certain missing data assumptions are reasonable for their cluster randomized trial.-A pattern-mixture model approach for handling missing continuous outcome data in longitudinal cluster randomized trials.",1
"The Skin Cancer after Organ Transplant (SCOT) study was designed to investigate the link between genus beta human papillomavirus (HPV) and squamous cell skin cancer (SCSC). We focused on a population receiving immunosuppressive therapy for extended periods, transplant patients, as they are at extremely high risk for developing SCSC. Two complementary projects were conducted in the Seattle area: (i) a retrospective cohort with interview data from 2004 recipients of renal or cardiac transplants between 1995 and 2010 and (ii) a prospective cohort with interview data from 328 people on the transplant waiting lists between 2009 and 2011. Within the retrospective cohort, we developed a nested case-control study (172 cases and 337 control subjects) to assess risk of SCSC associated with markers of HPV in SCSC tumour tissue and eyebrow hair bulb DNA (HPV genotypes) and blood (HPV antibodies). In the prospective cohort, 135 participants had a 1-year post-transplant visit and 71 completed a 2-year post-transplant visit. In both arms of the cohort, we collected samples to assess markers of HPV infection such as acquisition of new types, proportion positive for each type, persistence of types at consecutive visits and number of HPV types detected. In the prospective cohort, we will also examine these HPV markers in relation to levels of cell-mediated immunity. The goal of the SCOT study is to use the data we collected to gain a more complete understanding of the role of immune suppression in HPV kinetics and of genus beta HPV types in SCSC. For more information, please contact the principal investigator through the study website: http://www.fhcrc.org/science/phs/cerc/The_SCOT_Study.html.-Cohort profile: the skin cancer after organ transplant study.",0
"This review identifies 10 common errors and problems in the statistical analysis, design, interpretation, and reporting of obesity research and discuss how they can be avoided. The 10 topics are: 1) misinterpretation of statistical significance, 2) inappropriate testing against baseline values, 3) excessive and undisclosed multiple testing and ""P-value hacking,"" 4) mishandling of clustering in cluster randomized trials, 5) misconceptions about nonparametric tests, 6) mishandling of missing data, 7) miscalculation of effect sizes, 8) ignoring regression to the mean, 9) ignoring confirmation bias, and 10) insufficient statistical reporting. It is hoped that discussion of these errors can improve the quality of obesity research by helping researchers to implement proper statistical practice and to know when to seek the help of a statistician.-Common scientific and statistical errors in obesity research.",1
"Bilirubin, a byproduct of hemoglobin breakdown and purported anti-oxidant, is thought to be cancer preventive. We conducted complementary serological and Mendelian randomization (MR) analyses to investigate whether alterations in circulating levels of bilirubin are associated with risk of colorectal cancer (CRC). We decided a priori to perform analyses separately in men and women based on suggestive evidence that associations may differ by sex. In a case-control study nested in the European Prospective Investigation into Cancer and Nutrition (EPIC), pre-diagnostic unconjugated bilirubin (UCB, the main component of total bilirubin) concentrations were measured by high-performance liquid chromatography in plasma samples of 1386 CRC cases and their individually matched controls. Additionally, 115 single-nucleotide polymorphisms (SNPs) robustly associated (P &lt; 5 ? 10-8) with circulating total bilirubin were instrumented in a 2-sample MR to test for a potential causal effect of bilirubin on CRC risk in 52,775 CRC cases and 45,940 matched controls in the Genetics and Epidemiology of Colorectal Cancer Consortium (GECCO), the Colon Cancer Family Registry (CCFR), and the Colorectal Transdisciplinary (CORECT) study. The associations between circulating UCB levels and CRC risk differed by sex (Pheterogeneity = 0.008). Among men, higher levels of UCB were positively associated with CRC risk (odds ratio [OR] = 1.19, 95% confidence interval [CI] = 1.04-1.36; per 1-SD increment of log-UCB). In women, an inverse association was observed (OR = 0.86 (0.76-0.97)). In the MR analysis of the main UGT1A1 SNP (rs6431625), genetically predicted higher levels of total bilirubin were associated with a 7% increase in CRC risk in men (OR = 1.07 (1.02-1.12); P = 0.006; per 1-SD increment of total bilirubin), while there was no association in women (OR = 1.01 (0.96-1.06); P = 0.73). Raised bilirubin levels, predicted by instrumental variables excluding rs6431625, were suggestive of an inverse association with CRC in men, but not in women. These differences by sex did not reach formal statistical significance (Pheterogeneity ? 0.2). Additional insight into the relationship between circulating bilirubin and CRC is needed in order to conclude on a potential causal role of bilirubin in CRC development.-Circulating bilirubin levels and risk of colorectal cancer: serological and Mendelian randomization analyses.",0
"Automated storage and analysis of the results of serial haematologic studies are now technically feasible with present-day laboratory instruments and devices for data storage and processing. In current practice, physicians mentally compare a laboratory result with previous values and use their clinical judgement to determine the significance of any change. To provide a statistical basis for this process, we describe a new approach for the detection of changes in patient-specific sequential measurements of standard haematologic laboratory tests. These methods include hierarchical multiple regression modelling, with a weighted minimum risk criteria for model selection, to choose models indicating changes in mean values over time. This study is the first to analyse sequential patient-specific distributions of laboratory measurements, utilizing mixture distribution modelling with systematic selection of starting values for the EM algorithm. To evaluate these statistical methods under controlled conditions, we studied 11 healthy human volunteers who were depleted of iron by serial phlebotomy to iron-deficiency anaemia, then treated with oral iron supplements to replete iron stores and correct the anaemia. Application of sequential patient-specific analyses of haemoglobin, haematocrit, and mean cell volume showed that significant departures from past values could be identified, in many cases, even when values were still within the population reference ranges. Additionally, for all subjects sequential alterations in red blood cell volume distributions during development of iron-deficiency anaemia could be characterized and quantified. These methods promise to provide more sensitive techniques for improved diagnostic evaluation of developing anaemia and serial monitoring of response to therapy.-Patient-specific analysis of sequential haematological data by multiple linear regression and mixture distribution modelling.",0
"In clinical studies, multiple endpoints are often measured for each patient longitudinally. The multivariate random-effects or random coefficient model has been a useful method for analysis. However, medical research problems may impose restrictions on the model parameters of interests. For example, in a paediatric brain tumour study on radiation therapy, there is a natural ordering in the white matter relaxation time of brain tissues among different regions surrounding the primary tumour, i.e. the closer a specific region of brain tissues is to the centre of primary tumour, the shorter is the relaxation time. Such parameter constraints should be accounted for in the analysis. This article proposes a class of multivariate random coefficient models with restricted parameters and derives its maximum likelihood estimates (MLE). We propose a modified EM algorithm for the quadratic optimalization with linear inequality constraints necessary in deriving the MLE. The method is applied to analysing the paediatric brain tumour study.-A multivariate random-effects model with restricted parameters: application to assessing radiation therapy for brain tumours.",0
"Inference regarding the inclusion or exclusion of random effects in linear mixed models is challenging because the variance components are located on the boundary of their parameter space under the usual null hypothesis. As a result, the asymptotic null distribution of the Wald, score, and likelihood ratio tests will not have the typical ?(2) distribution. Although it has been proved that the correct asymptotic distribution is a mixture of ?(2) distributions, the appropriate mixture distribution is rather cumbersome and nonintuitive when the null and alternative hypotheses differ by more than one random effect. As alternatives, we present two permutation tests, one that is based on the best linear unbiased predictors and one that is based on the restricted likelihood ratio test statistic. Both methods involve weighted residuals, with the weights determined by the among- and within-subject variance components. The null permutation distributions of our statistics are computed by permuting the residuals both within and among subjects and are valid both asymptotically and in small samples. We examine the size and power of our tests via simulation under a variety of settings and apply our test to a published data set of chronic myelogenous leukemia patients.-Permutation tests for random effects in linear mixed models.",0
"A recent, nationwide study of 54 million births reported increasing trends toward more prenatal resource utilization from 1981 to 1995, when other indicators have shown worsening trends in birth outcomes. The Adequacy of Prenatal Care Utilization (APNCU) Index was used to measure resource utilization, but the Index appears to be biased because women grouped in the intensive category have the highest rates of low birth weight (LBW). The objective of this paper is to provide a systematic examination of the Index and to uncover biases that may preclude its use in analyzing the association between resource utilization and birth outcomes. This is a cross-sectional study including all singleton live births in 1993 through 1996 (n = 591,403) in Ohio. Birth certificate data are used to derive the Index, which categorizes women as follows: Adequate Plus (A+), Adequate, Intermediate, and Inadequate. The Index is based on the ratio of observed to expected (O/E) number of prenatal visits. The expected number of visits is based on the American College of Obstetricians and Gynecologists (ACOG) recommendations. The Index also considers the month of initiation of prenatal care. The outcome measures are low birth weight (LBW) and small-for-gestational age (SGA). The LBW rate is 11.8% in the (A+) category, compared to 9.4% in the Inadequate category, and 3.3% and 3.5% in each of the Intermediate and Adequate categories, respectively. Preterm births are disproportionately represented in the (A+) category: 61.2% of births prior to 37 weeks are (A+), whereas only 18.9% of term births are (A+). This apparent bias results from the fact that the ACOG schedule of prenatal visits allocates nearly one third of the total visits to the last 4-5 weeks of gestation. A shorter gestational age implies fewer number of expected visits, a smaller denominator in the O/E ratio, and O/E ratios exceeding 100% by large margins. In fact, the observed number of visits exceeds the expected number of visits by only one or two in 40.1% of all births grouped in the (A+) category. Consequently, the Index yields misleading results indicating that women grouped in the (A+) category (or O/E ratios &gt; 110%) are most likely to deliver LBW infants. Contrary to the results obtained through the APNCU Index, our gestational age-specific analysis showed that increasing number of prenatal visits is associated with improved birth outcomes. We recommend that the use of the APNCU Index to study the association between prenatal resource utilization and LBW be discontinued.-The ""Adequacy of Prenatal Care Utilization"" (APNCU) index to study low birth weight: is the index biased?",0
"Discriminant analysis is commonly used to evaluate the ability of candidate biomarkers to separate patients into pre-defined groups. Recent extension of discriminant analysis to longitudinal data enables us to improve the classification accuracy based on biomarker profiles rather than on a single biomarker measurement. However, the biomarker measurement is often limited by the sensitivity of the given assay, resulting in data that are censored at either the lower or the upper limit of detection. Inappropriate handling of censored data may affect the classification accuracy of biomarker and hinder the evaluation of its potential discrimination power. We develop a discriminant analysis method for censored longitudinal biomarker data based on mixed models and evaluate its performance by area under the receiver operation characteristic curve. Through the simulation study, we show that our method is better than the simple substitution methods in terms of parameter estimation and evaluating biomarker performance. Application to a biomarker study of patients with acute kidney injury demonstrates that our method may shed light on the potential clinical utility of biomarkers by taking into account both longitudinal trajectory and limit of detection issues.-Classification using longitudinal trajectory of biomarker in the presence of detection limits.",0
"Motivated by a previously published study of HIV treatment, we simulated data subject to time-varying confounding affected by prior treatment to examine some finite-sample properties of marginal structural Cox proportional hazards models. We compared (a) unadjusted, (b) regression-adjusted, (c) unstabilized, and (d) stabilized marginal structural (inverse probability-of-treatment [IPT] weighted) model estimators of effect in terms of bias, standard error, root mean squared error (MSE), and 95% confidence limit coverage over a range of research scenarios, including relatively small sample sizes and 10 study assessments. In the base-case scenario resembling the motivating example, where the true hazard ratio was 0.5, both IPT-weighted analyses were unbiased, whereas crude and adjusted analyses showed substantial bias towards and across the null. Stabilized IPT-weighted analyses remained unbiased across a range of scenarios, including relatively small sample size; however, the standard error was generally smaller in crude and adjusted models. In many cases, unstabilized weighted analysis showed a substantial increase in standard error compared with other approaches. Root MSE was smallest in the IPT-weighted analyses for the base-case scenario. In situations where time-varying confounding affected by prior treatment was absent, IPT-weighted analyses were less precise and therefore had greater root MSE compared with adjusted analyses. The 95% confidence limit coverage was close to nominal for all stabilized IPT-weighted but poor in crude, adjusted, and unstabilized IPT-weighted analysis. Under realistic scenarios, marginal structural Cox proportional hazards models performed according to expectations based on large-sample theory and provided accurate estimates of the hazard ratio.-A simulation study of finite-sample properties of marginal structural Cox proportional hazards models.",0
"The purpose of this research was to determine the extent to which elevated gravitational-force event rates predict crashes and near crashes. Accelerometers, global positioning systems, cameras, and other technology were installed in vehicles driven by 42 newly licensed Virginia teenage drivers for a period of 18 months between 2006 and 2009. Elevated gravitational force and crash and near-crash events were identified, and rates per miles driven were calculated. (One mile = 1.6 km.) The correlation between crashes and near crashes and elevated gravitational-force event rates was 0.60. Analyses were done by using generalized estimating equations with logistic regression. Higher elevated gravitational-force event rates in the past month substantially increased the risk of a crash in the subsequent month (odds ratio = 1.07, 95% confidence interval: 1.02, 1.12). Although the difference in this relation did not vary significantly by time, it was highest in the first 6 months compared with the second and third 6-month periods. With a receiver operating characteristic curve, the risk models showed relatively high predictive accuracy with an area under the curve of 0.76. The authors conclude that elevated gravitational-force event rates can be used to assess risk and to show high predictive accuracy of a near-future crash.-Do elevated gravitational-force events while driving predict crashes and near crashes?",0
"Novel rationales for randomizing clusters rather than individuals appear to be emerging from the push for more pragmatic trials, for example, to facilitate trial recruitment, reduce the costs of research, and improve external validity. Such rationales may be driven by a mistaken perception that choosing cluster randomization lessens the need for informed consent. We reviewed a random sample of published cluster randomized trials involving only individual-level health care interventions to determine (a) the prevalence of reporting a rationale for the choice of cluster randomization; (b) the types of explicit, or if absent, apparent rationales for the use of cluster randomization; (c) the prevalence of reporting patient informed consent for study interventions; and (d) the types of justifications provided for waivers of consent. We considered cluster randomized trials for evaluating exclusively the individual-level health care interventions to focus on clinical trials where individual randomization is only theoretically possible and where there is a general expectation of informed consent. A random sample of 40 cluster randomized trials were identified by implementing a validated electronic search filter in two electronic databases (Ovid MEDLINE and Embase), with two reviewers independently extracting information from each trial. Inclusion criteria were the following: primary report of a cluster randomized trial, evaluating exclusively an individual-level health care intervention, published between 2007 and 2016, and conducted in Canada, the United States, European Union, Australia, or low- and middle-income country settings. Twenty-five trials (62.5%, 95% confidence interval = 47.5%-77.5%) reported an explicit rationale for the use of cluster randomization. The most commonly reported rationales were those with logistical or administrative convenience (15 trials, 60%) and those that need to avoid contamination (13 trials, 52%); five trials (20%) were cited rationales related to the push for more pragmatic trials. Twenty-one trials (52.5%, 95% confidence interval = 37%-68%) reported written informed consent for the intervention, two (5%) reported verbal consent, and eight (20%) reported waivers of consent, while in nine trials (22.5%) consent was unclear or not mentioned. Reported justifications for waivers of consent included that study interventions were already used in clinical practice, patients were not randomized individually, and the need to facilitate the pragmatic nature of the trial. Only one trial reported an explicit and appropriate justification for waiver of consent based on minimum criteria in international research ethics guidelines, namely, infeasibility and minimal risk. Rationales for adopting cluster over individual randomization and for adopting consent waivers are emerging, related to the need to facilitate pragmatic trials. Greater attention to clear reporting of study design rationales, informed consent procedures, as well as justification for waivers is needed to ensure that such trials meet appropriate ethical standards.-Cluster over individual randomization: are study design choices appropriately justified? Review of a random sample of trials",1
"for assessing the level of understanding of trial-related information during the informed consent (IC) process in developing countries are lacking. To assess the understanding and retention of trial-related information presented in the IC process by administering an informed consent assessment instrument (ICAI) to participants in a clinical trial for a new tuberculosis (TB) regimen being conducted in Rio de Janeiro (Brazil). Methods The format of the ICAI was based on the language and structure of the United States National Cancer Institute's IC comprehension checklist. The ICAI was designed to assess points of the RioMAR study IC process that addressed the principles of research ethics requested by Brazilian Regulatory Authority: autonomy, beneficence, non-maleficence, and justice. Briefly, (1) Is the respondent participating in a clinical trial? (2) Are two different treatments being evaluated? (3) Is the treatment arm chosen by chance? (4) Is an HIV test required? (5) Are liver function tests required? (6) Can participants leave the study at any time? (7) Are the risks and benefits of taking part in the study clear? (8) May pregnant women participate in the study? (9) Can one of the study drugs reduce the effectiveness of contraceptives? (10) Are patients paid to participate in the study? The ICAI was applied at two time points: immediately after enrollment in the clinical trial and 2 months later. A total of 61 patients who enrolled in the RioMAR study participated in this study. The percentage of correct answers to all questions was 82% at the time of the first ICAI; 31 participants (51%) did not recall that an HIV test was required (question 4) and 43 (70%) did not know that they could leave the study (question 6). Other individual questions were answered correctly by at least 76% of participants. There was no association between incorrect answers and age, gender, monthly family income, neighborhood, or level of education (p &gt; 0.07). When the responses to the first and the second ICAI questions were compared, 15% or more of participants had conflicting answers to 5 of the 10 questions. The ICAI uses dichotomous responses, leading to a 50% chance of guessing the correct answers. Two questions were asked only of women. Finally, only 6 of the 10 questions on the current version of the ICAI apply to most trials; others are trial-specific. The ICAI may be adapted to an individual trial and may prove to be a useful tool following a consent discussion to identify issues not fully understood by the research participants, thus prompting study staff to re-explain topics, possibly in a more elementary manner.-Understanding and retention of trial-related information among participants in a clinical trial after completing the informed consent process.",0
"The Anglia menorrhagia education study tests the effectiveness of an education package for the treatment of menorrhagia given to doctors at a primary care level. General practices were randomized to receive or not receive the package. It is hoped that this intervention will reduce the proportion of women suffering from menorrhagia that are referred to hospital. Data are available on the treatment and referral of women in the practices in the education and control groups, both pre- and post-intervention. We define and demonstrate a random effects logistic regression model that includes pre-intervention data for calculating the effectiveness of the intervention.-Randomization at the level of primary care practice: use of pre-intervention data and random effects models.",1
There are some circumstances where the stepped-wedge cluster randomized trial is preferable to the alternative: no randomized trial at all. Response to the commentary by Kotz and colleagues.,3
"Classical methods for fitting a varying intercept logistic regression model to stratified data are based on the conditional likelihood principle to eliminate the stratum-specific nuisance parameters. When the outcome variable has multiple ordered categories, a natural choice for the outcome model is a stratified proportional odds or cumulative logit model. However, classical conditioning techniques do not apply to the general K-category cumulative logit model (K&gt;2) with varying stratum-specific intercepts as there is no reduction due to sufficiency; the nuisance parameters remain in the conditional likelihood. We propose a methodology to fit stratified proportional odds model by amalgamating conditional likelihoods obtained from all possible binary collapsings of the ordinal scale. The method allows for categorical and continuous covariates in a general regression framework. We provide a robust sandwich estimate of the variance of the proposed estimator. For binary exposures, we show equivalence of our approach to the estimators already proposed in the literature. The proposed recipe can be implemented very easily in standard software. We illustrate the methods via three real data examples related to biomedical research. Simulation results comparing the proposed method with a random effects model on the stratification parameters are also furnished.-Fitting stratified proportional odds models by amalgamating conditional likelihoods.",0
"Cluster randomized designs are frequently employed in pragmatic clinical trials which test interventions in the full spectrum of everyday clinical settings in order to maximize applicability and generalizability. In this study, we propose to directly incorporate pragmatic features into power analysis for cluster randomized trials with count outcomes. The pragmatic features considered include arbitrary randomization ratio, overdispersion, random variability in cluster size, and unequal lengths of follow-up over which the count outcome is measured. The proposed method is developed based on generalized estimating equation (GEE) and it is advantageous in that the sample size formula retains a closed form, facilitating its implementation in pragmatic trials. We theoretically explore the impact of various pragmatic features on sample size requirements. An efficient Jackknife algorithm is presented to address the problem of underestimated variance by the GEE sandwich estimator when the number of clusters is small. We assess the performance of the proposed sample size method through extensive simulation and an application example to a real clinical trial is presented.-Incorporating pragmatic features into power analysis for cluster randomized trials with a count outcome",1
"In the modern era, cardiovascular biomarkers are often measured in the presence of medication use, such that the observed biomarker value for the treated participants is different than their underlying natural history value. However, for certain predictors (e.g. age, gender, and genetic exposures) the observed biomarker value is not of primary interest. Rather, we are interested in estimating the association between these predictors and the natural history of the biomarker that would have occurred in the absence of treatment. Nonrandom medication use obscures our ability to estimate this association in cross-sectional observational data. Structural equation methodology (e.g. the treatment effects model), while historically used to estimate treatment effects, has been previously shown to be a reasonable way to correct endogeneity bias when estimating natural biomarker associations. However, the assumption that the effects of medication use on the biomarker are uniform across participants on medication is generally not thought to be reasonable. We derive an extension of the treatment effects model to accommodate effect modification. Based on several simulation studies and an application to data from the Multi-Ethnic Study of Atherosclerosis, we show that our extension substantially improves bias in estimating associations of interest, particularly when effect modifiers are associated with the biomarker or with medication use, without a meaningful cost of efficiency.-A method to account for covariate-specific treatment effects when estimating biomarker associations in the presence of endogenous medication use.",0
Small sample inference for clustered data,1
"Group-administered interventions often create statistical dependencies, which, if ignored, increase the rate of Type I errors. The authors analyzed data from two randomized trials involving group interventions to document the impact of statistical dependency on tests of intervention effects and to provide estimates of statistical dependency. Intraclass correlations ranged from .02 to .12. Adjusting for dependencies increased p values for the tests of intervention effects. The increase in the p values depended on the magnitude of the statistical dependence and available degrees of freedom. Results suggest that the literature may overstate the efficacy of group interventions and imply that it will be important to study why groups create dependencies. The authors discuss how dependencies impact statistical power and how researchers can address this concern.-Statistical analysis of group-administered intervention data: reanalysis of two randomized trials.",2
"A method for modeling survival data with multilevel clustering is described. The Cox partial likelihood is incorporated into the generalized linear mixed model (GLMM) methodology. Parameter estimation is achieved by maximizing a log likelihood analogous to the likelihood associated with the best linear unbiased prediction (BLUP) at the initial step of estimation and is extended to obtain residual maximum likelihood (REML) estimators of the variance component. Estimating equations for a three-level hierarchical survival model are developed in detail, and such a model is applied to analyze a set of chronic granulomatous disease (CGD) data on recurrent infections as an illustration with both hospital and patient effects being considered as random. Only the latter gives a significant contribution. A simulation study is carried out to evaluate the performance of the REML estimators. Further extension of the estimation procedure to models with an arbitrary number of levels is also discussed.-Multilevel models for survival analysis with random effects.",1
"The association between psychosocial risk factors and retinal microvascular signs was examined in the Multi-Ethnic Study of Atherosclerosis. Subjects were recruited from Baltimore, Maryland; Chicago, Illinois; Forsyth County, North Carolina; Los Angeles County, California; New York, New York; and St. Paul, Minnesota. Levels of depressive symptoms, trait anger, trait anxiety, chronic burdens, emotional support, and cynical distrust were assessed by questionnaire (from July 2000 to July 2002). Digital retinal images (from August 2002 to January 2004) from 6,147 participants were used to evaluate retinopathy and retinal vascular caliber. After controlling for potential confounding factors, the authors found that subjects without access to emotional support (Enriched Social Support Instrument score of &lt;19 vs. &gt; or = 19) had 60% greater odds of retinopathy (odds ratio = 1.6, 95% confidence interval (CI): 1.3, 2.0). Subjects with high Spielberger trait-anxiety scale scores (&gt; or = 22 vs. &lt; or = 14) and subjects with high depressive symptoms (Center for Epidemiology Studies Depression Scale score, &gt; or = 16 vs. &lt;16) were also more likely to have retinopathy (odds ratio = 1.4, 95% CI: 1.1, 1.9 and odds ratio = 1.5, 95% CI: 1.2, 1.9), respectively. In this cross-sectional study, lack of emotional support, increased trait anxiety, and more depressive symptoms were associated with retinopathy signs, independently of other known risk factors.-Psychosocial risk factors and retinal microvascular signs: the multi-ethnic study of atherosclerosis.",0
"To quantify the association between a combination of healthy lifestyle factors before pregnancy (healthy body weight, healthy diet, regular exercise, and not smoking) and the risk of gestational diabetes. Prospective cohort study. Nurses' Health Study II, United States. 20,136 singleton live births in 14,437 women without chronic disease. Self reported incident gestational diabetes diagnosed by a physician, validated by medical records in a previous study. Incident first time gestational diabetes was reported in 823 pregnancies. Each lifestyle factor measured was independently and significantly associated with risk of gestational diabetes. The combination of three low risk factors (non-smoker, ? 150 minutes a week of moderate to vigorous physical activity, and healthy eating (top two fifths of Alternate Healthy Eating Index-2010 adherence score)) was associated with a 41% lower risk of gestational diabetes compared with all other pregnancies (relative risk 0.59, 95% confidence interval 0.48 to 0.71). Addition of body mass index (BMI) &lt;25 before pregnancy (giving a combination of four low risk factors) was associated with a 52% lower risk of gestational diabetes compared with all other pregnancies (relative risk 0.48, 0.38 to 0.61). Compared with pregnancies in women who did not meet any of the low risk lifestyle factors, those meeting all four criteria had an 83% lower risk of gestational diabetes (relative risk 0.17, 0.12 to 0.25). The population attributable risk percentage of the four risk factors in combination (smoking, inactivity, overweight, and poor diet) was 47.5% (95% confidence interval 35.6% to 56.6%). A similar population attributable risk percentage (49.2%) was observed when the distributions of the four low risk factors from the US National Health and Nutrition Examination Survey (2007-10) data were applied to the calculation. Adherence to a low risk lifestyle before pregnancy is associated with a low risk of gestational diabetes and could be an effective strategy for the prevention of gestational diabetes.-Adherence to healthy lifestyle and risk of gestational diabetes mellitus: prospective cohort study.",0
"Cluster randomized trials (CRTs) pose ethical challenges for investigators and ethics committees. This study describes the views and experiences of CRT researchers with respect to: (1) ethical challenges in CRTs; (2) the ethics review process for CRTs; and (3) the need for comprehensive ethics guidelines for CRTs. Descriptive qualitative analysis of interviews conducted with a purposive sample of 20 experienced CRT researchers. Informants expressed concern over the potential for bias that may result from requirements to obtain informed consent from research participants in CRTs. Informants suggested that the need for informed consent ought to be related to the type of intervention under study in a CRT. Informants rarely expressed concern regarding risks to research participants in CRTs, other than risks to privacy. Important issues identified in the research ethics literature, including fair subject selection and other justice issues, were not mentioned by informants. The ethics review process has had positive and negative impacts on CRT conduct. Informants stated that variability in ethics review between jurisdictions, and increasingly stringent ethics review in recent years, have hampered their ability to conduct CRTs. Many informants said that comprehensive ethics guidelines for CRTs would be helpful to researchers and research ethics committees. Informants identified key ethical challenges in the conduct of CRTs, specifically relating to identifying subjects, seeking informed consent, and the use of gatekeepers. These data have since been used to identify topics for in-depth ethical analysis and to guide the development of comprehensive ethics guidelines for CRTs.-Researchers' perceptions of ethical challenges in cluster randomized trials: a qualitative analysis.",1
"Comparative studies of the accuracy of diagnostic tests often involve designs according to which each study participant is examined by two or more of the tests and the diagnostic examinations are interpreted by several readers. Tests are then compared on the basis of a summary index, such as the (full or partial) area under the receiver operating characteristic (ROC) curve, averaged over the population of readers. The design and analysis of such studies naturally need to take into account the correlated nature of the diagnostic test results and interpretations. In this paper, we describe the use of hierarchical modelling for ROC summary measures derived from multi-reader, multi-modality studies. The models allow the variance of the estimates to depend on the actual value of the index and account for the correlation in the data both explicitly via parameters and implicitly via the hierarchical structure. After showing how the hierarchical models can be employed in the analysis of data from multi-reader, multi-modality studies, we discuss the design of such studies using the simulation-based, Bayesian design approach of Wang and Gelfand (Stat. Sci. 2002; 17(2):193-208). The methodology is illustrated via the analysis of data from a study conducted to evaluate a computer-aided diagnosis tool for screen film mammography and via the development of design considerations for a multi-reader study comparing display modes for digital mammography. The hierarchical model methodology described in this paper is also applicable to the meta-analysis of ROC studies.-Hierarchical models for ROC curve summary measures: design and analysis of multi-reader, multi-modality studies of medical tests.",0
"We present a method for the simultaneous estimation of the basic reproductive number, R(0), and the serial interval for infectious disease epidemics, using readily available surveillance data. These estimates can be obtained in real time to inform an appropriate public health response to the outbreak. We show how this methodology, in its most simple case, is related to a branching process and describe similarities between the two that allow us to draw parallels which enable us to understand some of the theoretical properties of our estimators. We provide simulation results that illustrate the efficacy of the method for estimating R(0) and the serial interval in real time. Finally, we implement our proposed method with data from three infectious disease outbreaks.-A likelihood-based method for real-time estimation of the serial interval and reproductive number of an epidemic.",0
"Propensity score-based estimators are increasingly used for causal inference in observational studies. However, model selection for propensity score estimation in high-dimensional data has received little attention. In these settings, propensity score models have traditionally been selected based on the goodness-of-fit for the treatment mechanism itself, without consideration of the causal parameter of interest. Collaborative minimum loss-based estimation is a novel methodology for causal inference that takes into account information on the causal parameter of interest when selecting a propensity score model. This ""collaborative learning"" considers variable associations with both treatment and outcome when selecting a propensity score model in order to minimize a bias-variance tradeoff in the estimated treatment effect. In this study, we introduce a novel approach for collaborative model selection when using the LASSO estimator for propensity score estimation in high-dimensional covariate settings. To demonstrate the importance of selecting the propensity score model collaboratively, we designed quasi-experiments based on a real electronic healthcare database, where only the potential outcomes were manually generated, and the treatment and baseline covariates remained unchanged. Results showed that the collaborative minimum loss-based estimation algorithm outperformed other competing estimators for both point estimation and confidence interval coverage. In addition, the propensity score model selected by collaborative minimum loss-based estimation could be applied to other propensity score-based estimators, which also resulted in substantive improvement for both point estimation and confidence interval coverage. We illustrate the discussed concepts through an empirical example comparing the effects of non-selective nonsteroidal anti-inflammatory drugs with selective COX-2 inhibitors on gastrointestinal complications in a population of Medicare beneficiaries.-Collaborative-controlled LASSO for constructing propensity score-based estimators in high-dimensional data.",0
"This Monte Carlo study compares performance of the linear and the logistic mixed-model analyses of simulated community trials having event rates of 37%, 13%, or 5%, intraclass correlations between 0.01 and 0.05, and 17 or 5 denominator degrees of freedom. Type I or Type II error rates showed no essential difference between the two analysis methods. They showed depressed error rates when the event rate or the denominator degrees of freedom were small. The authors conclude that in studies with adequate denominator degrees of freedom, the researcher may use either method of analysis but should accept negative estimates of components of variance to avoid depression of error rates.-Gauss or Bernoulli? A Monte Carlo comparison of the performance of the linear mixed-model and the logistic mixed-model analyses in simulated community trials with a dichotomous outcome variable at the individual level.",1
"We develop statistical methods for designing and analyzing arm-in-cage experiments used to test the efficacy of insect repellents and other topical treatments. In these experiments, a controlled amount of the treatment is applied to a volunteer's forearm, which then is exposed to the insects by being placed into a special cage. Arms are not kept in the cages continuously, but rather placed there periodically for a brief period of time, during which it is noted whether an insect lands (but does not bite) or (lands and) bites. Efficacy of a repellent can be described using a progressive three-state model in which the first two states represent varying degrees of protection (no landing and landing without biting) and the third state occurs once protection is completely lost (biting). Because subjects within a treatment group follow the same cage visit schedule, transition times between states are interval censored into one of several fixed intervals. We develop an approach that uses a mixture of nonparametric and parametric techniques for estimating the parameters of interest when sojourn times are dependent. Design considerations for arm-in-cage experiments are addressed and the proposed methods are illustrated on data from a recent arm-in-cage experiment as well as simulated data.-Design and analysis of arm-in-cage experiments: inference for three-state progressive disease models with common periodic observation times.",0
"Stallard (1998, Biometrics 54, 279-294) recently used Bayesian decision theory for sample-size determination in phase II trials. His design maximizes the expected financial gains in the development of a new treatment. However, it results in a very high probability (0.65) of recommending an ineffective treatment for phase III testing. On the other hand, the expected gain using his design is more than 10 times that of a design that tightly controls the false positive error (Thall and Simon, 1994, Biometrics 50, 337-349). Stallard's design maximizes the expected gain per phase II trial, but it does not maximize the rate of gain or total gain for a fixed length of time because the rate of gain depends on the proportion of treatments forwarding to the phase III study. We suggest maximizing the rate of gain, and the resulting optimal one-stage design becomes twice as efficient as Stallard's one-stage design. Furthermore, the new design has a probability of only 0.12 of passing an ineffective treatment to phase III study.-A Bayesian decision approach for sample size determination in phase II trials.",0
"Schizophrenia is a severe, disabling and prevalent mental disorder without cure and with a variable, incomplete pharmacotherapeutic response. Prior to onset in adolescence or young adulthood a prodromal period of abnormal symptoms lasting weeks to years has been identified and operationalized as clinically high risk (CHR) for schizophrenia. However, only a minority of subjects prospectively identified with CHR convert to schizophrenia, thereby limiting enthusiasm for early intervention(s). This study utilized objective resting electroencephalogram (EEG) quantification to determine whether CHR constitutes a cohesive entity and an evoked potential to assess CHR cortical auditory processing. This study constitutes an EEG-based quantitative neurophysiological comparison between two unmedicated subject groups: 35 neurotypical controls (CON) and 22 CHR patients. After artifact management, principal component analysis (PCA) identified EEG spectral and spectral coherence factors described by associated loading patterns. Discriminant function analysis (DFA) determined factors' discrimination success between subjects in the CON and CHR groups. Loading patterns on DFA-selected factors described CHR-specific spectral and coherence differences when compared to controls. The frequency modulated auditory evoked response (FMAER) explored functional CON-CHR differences within the superior temporal gyri. Variable reduction by PCA identified 40 coherence-based factors explaining 77.8% of the total variance and 40 spectral factors explaining 95.9% of the variance. DFA demonstrated significant CON-CHR group difference (P &lt;0.00001) and successful jackknifed subject classification (CON, 85.7%; CHR, 86.4% correct). The population distribution plotted along the canonical discriminant variable was clearly bimodal. Coherence factors delineated loading patterns of altered connectivity primarily involving the bilateral posterior temporal electrodes. However, FMAER analysis showed no CON-CHR group differences. CHR subjects form a cohesive group, significantly separable from CON subjects by EEG-derived indices. Symptoms of CHR may relate to altered connectivity with the posterior temporal regions but not to primary auditory processing abnormalities within these regions.-Neurophysiological differences between patients clinically at high risk for schizophrenia and neurotypical controls--first steps in development of a biomarker.",0
"Randomized experiments are often considered the strongest designs to study the impact of educational interventions. Perhaps the most prevalent class of designs used in large-scale education experiments is the cluster randomized design in which entire schools are assigned to treatments. In cluster randomized trials that assign schools to treatments within a set of school districts, the statistical power of the test for treatment effects depends on the within-district school-level intraclass correlation (ICC). Hedges and Hedberg (2014) recently computed within-district ICC values in 11 states using three-level models (students in schools in districts) that pooled results across all the districts within each state. Although values from these analyses are useful when working with a representative sample of districts, they may be misleading for other samples of districts because the magnitude of ICCs appears to be related to district size. To plan studies with small or nonrepresentative samples of districts, better information are needed about the relation of within-district school-level ICCs to district size. Our objective is to explore the relation between district size and within-district ICCs to provide reference values for math and reading achievement for Grades 3-8 by district size, poverty level, and urbanicity level. These values are not derived from pooling across all districts within a state as in previous work but are based on the direct calculation of within-district school-level ICCs for each school district. We use mixed models to estimate over 7,000 district-specific ICCs for math and reading achievement in 11 states and for Grades 3-8. We then perform a random effects meta-analysis on the estimated within-district ICCs. Our analysis is performed by grade and subject for different strata designated by district size (number of schools), urbanicity, and poverty rates.-Reference values of within-district intraclass correlations of academic achievement by district characteristics: results from a meta-analysis of district-specific values.",1
"To evaluate the association between non-alcoholic fatty liver disease and all cause and cause specific mortality in a representative sample of the US general population. Prospective cohort study. US Third National Health and Nutrition Examination Survey (NHANES III: 1988-94) with follow-up of mortality to 2006. 11,371 adults aged 20-74 participating in the Third National Health and Nutrition Examination Survey, with assessment of hepatic steatosis. Mortality from all causes, cardiovascular disease, cancer, and liver disease (up to 18 years of follow-up). The prevalence of non-alcoholic fatty liver disease with and without increased levels of liver enzymes in the population was 3.1% and 16.4%, respectively. Compared with participants without steatosis, those with non-alcoholic fatty liver disease but normal liver enzyme levels had multivariate adjusted hazard ratios for deaths from all causes of 0.92 (95% confidence interval 0.78 to 1.09), from cardiovascular disease of 0.86 (0.67 to 1.12), from cancer of 0.92 (0.67 to 1.27), and from liver disease of 0.64 (0.12 to 3.59). Compared with participants without steatosis, those with non-alcoholic fatty liver disease and increased liver enzyme levels had adjusted hazard ratios for deaths from all causes of 0.80 (0.52 to 1.22), from cardiovascular disease of 0.59 (0.29 to 1.20), from cancer of 0.53 (0.26 to 1.10), and from liver disease of 1.17 (0.15 to 8.93). Non-alcoholic fatty liver disease was not associated with an increased risk of death from all causes, cardiovascular disease, cancer, or liver disease.-Non-alcoholic fatty liver disease and mortality among US adults: prospective cohort study.",0
"In this article, we examine the development and use of covariate models where the relation with explanantory covariates is spatially adaptive. In this way space is regarded as an effect modifier. We examine the possibility of discrete groupings of coefficients (clustering of coefficients). Our application is to prostate cancer survival based on the SEER cancer registry for the state of Louisiana, USA. This registry holds individual records linked to vital outcomes and is geo-coded at county level. We examine a range of potential prior distributions for groupings of regression coefficients in application to these data.-Prior choice in discrete latent modeling of spatially referenced cancer survival.",0
"Pooling DNA samples of multiple individuals has been advocated as a method to reduce genotyping costs. Under such a scheme, only the allele counts at each locus, not the haplotype information, are observed. We develop a systematic way for handling such data by formulating the problem in terms of contingency tables, where pooled allele counts are expressed as the margins and the haplotype counts correspond to the unobserved cell counts. We show that the cell frequencies can be uniquely determined from the marginal frequencies under the usual Hardy-Weinberg equilibrium (HWE) assumption and that the maximum likelihood estimates of haplotype frequencies are consistent and asymptotically normal as the number of pools increases. The limiting covariance matrix is shown to be closely related to the extended hypergeometric distribution. Our results are used to derive Wald-type tests for linkage disequilibrium (LD) coefficient using pooled data. It is discovered that pooling is not efficient in testing weak LD despite its efficiency in estimating haplotype frequencies. We also show by simulations that the proposed LD tests are robust to slight deviation from HWE and to minor genotype error. Applications to two real angiotensinogen gene data sets are also provided.-Testing linkage disequilibrium from pooled DNA: a contingency table perspective.",0
"Most adolescent smoking prevention studies employ designs in which classrooms, schools, school districts, or sometimes whole communities are assigned to treatment conditions while observations are made on individual students. The critical design feature in such community trials is the nesting of intact social groups within treatment conditions. This combination requires that the treatment effect be assessed against the between-group variance; unfortunately, that variance is usually larger than for randomly constituted groups and its precision is usually less than that for the within-group variance. These factors often combine to reduce power so that it is almost impossible to detect important treatment effects in an otherwise well designed and properly executed study. To address these problems, investigators need good estimates of the intraclass correlation for the variables of interest, which together with the number of observations per unit determine the magnitude of the extra variation in the nested design. The purpose of this paper is to describe the methods and results from a study designed to generate estimates of intraclass correlation for common outcomes in adolescent smoking prevention studies and to discuss the use of these estimates in the planning of new studies.-Intraclass correlation among common measures of adolescent smoking: estimates, correlates, and applications in smoking prevention studies.",1
"In this paper, we propose an alternative covariance estimator to the robust covariance estimator of generalized estimating equations (GEE). Hypothesis tests using the robust covariance estimator can have inflated size when the number of independent clusters is small. Resampling methods, such as the jackknife and bootstrap, have been suggested for covariance estimation when the number of clusters is small. A drawback of the resampling methods when the response is binary is that the methods can break down when the number of subjects is small due to zero or near-zero cell counts caused by resampling. We propose a bias-corrected covariance estimator that avoids this problem. In a small simulation study, we compare the bias-corrected covariance estimator to the robust and jackknife covariance estimators for binary responses for situations involving 10-40 subjects with equal and unequal cluster sizes of 16-64 observations. The bias-corrected covariance estimator gave tests with sizes close to the nominal level even when the number of subjects was 10 and cluster sizes were unequal, whereas the robust and jackknife covariance estimators gave tests with sizes that could be 2-3 times the nominal level. The methods are illustrated using data from a randomized clinical trial on treatment for bone loss in subjects with periodontal disease.-A covariance estimator for GEE with improved small-sample properties.",1
"Mixed-effects linear regression models have become more widely used for analysis of repeatedly measured outcomes in clinical trials over the past decade. There are formulae and tables for estimating sample sizes required to detect the main effects of treatment and the treatment by time interactions for those models. A formula is proposed to estimate the sample size required to detect an interaction between two binary variables in a factorial design with repeated measures of a continuous outcome. The formula is based, in part, on the fact that the variance of an interaction is fourfold that of the main effect. A simulation study examines the statistical power associated with the resulting sample sizes in a mixed-effects linear regression model with a random intercept. The simulation varies the magnitude (?) of the standardized main effects and interactions, the intraclass correlation coefficient (? ), and the number (k) of repeated measures within-subject. The results of the simulation study verify that the sample size required to detect a 2 ? 2 interaction in a mixed-effects linear regression model is fourfold that to detect a main effect of the same magnitude.-Sample Sizes Required to Detect Interactions between Two Binary Fixed-Effects in a Mixed-Effects Linear Regression Model.",1
Data Resource Profile: Cardiovascular H3Africa Innovation Resource (CHAIR).,0
"The focus of this paper is regression analysis of clustered data. Although the presence of intracluster correlation (the tendency for items within a cluster to respond alike) is typically viewed as an obstacle to good inference, the complex structure of clustered data offers significant analytic advantages over independent data. One key advantage is the ability to separate effects at the individual (or item-specific) level and the group (or cluster-specific) level. We review different approaches for the separation of individual-level and cluster-level effects on response, their appropriate interpretation and give recommendations for model fitting based on the intent of the data analyst. Unlike many earlier papers on this topic, we place particular emphasis on the interpretation of the cluster-level covariate effect. The main ideas of the paper are highlighted in an analysis of the relationship between birth weight and IQ using sibling data from a large birth cohort study.-Separation of individual-level and cluster-level covariate effects in regression analysis of correlated data.",1
"Longitudinal trajectories for HIV risk were examined over 5 years following treatment among 1,393 patients who participated in the nationwide Drug Abuse Treatment Outcome Studies. Both injection drug use and sexual risk behavior declined over time, with most of the decline occurring between intake and the first-year follow-up. However, results of the application of growth mixture models for both sets of trajectories indicated that a subgroup of individuals reverted to a high-risk behavior over time, with a higher level of risk at the 5-year follow-up than their original risk level at intake. Of clients who were engaged in regular injection drug use at intake, 76% continued to inject drug at a moderate-stable or increased rate during the 5-year follow-up.-Longitudinal HIV risk behavior among the Drug Abuse Treatment Outcome Studies (DATOS) adult sample.",0
"Lack of adherence to recommended treatment for obstructive sleep apnea remains an ongoing public health challenge. Despite evidence that continuous positive airway pressure (CPAP) is effective and improves overall quality of life, adherence with the use of CPAP in certain racial/ethnic groups, especially blacks, is suboptimal. Evidence indicates that the incidence and prevalence of obstructive sleep apnea are higher among blacks, relative to whites, and blacks are less likely to adhere to recommended treatment compared with other racial/ethnic groups. Using a two-arm randomized controlled design, this study will evaluate the effectiveness of a culturally and linguistically tailored telephone-delivered intervention to promote adherence to physician-recommended sleep apnea assessment and treatment among blacks with metabolic syndrome, versus an attention-control arm. The intervention is designed to foster adherence to recommended sleep apnea care using the stages-of-change model. The intervention will be delivered entirely over the telephone. Participants in the intervention arm will receive 10 phone calls to address challenges and barriers to recommended care. Outcomes will be assessed at baseline, and at 6- and 12-months post-randomization. This tailored behavioral intervention will improve adherence to sleep apnea assessment and treatment among blacks with metabolic syndrome. We expect to demonstrate that this intervention modality is feasible in terms of time and cost and can be replicated in populations with similar racial/ethnic backgrounds. The study is registered at clinicaltrials.gov NCT01946659 (February 2013).-Telephone-delivered behavioral intervention among blacks with sleep apnea and metabolic syndrome: study protocol for a randomized controlled trial.",0
"When the outcome of interest is a quantity whose value may be altered through the use of medications, estimation of associations with this outcome is a challenging statistical problem. For participants taking medication the treated value is observed, but the underlying 'untreated' value may be the measure that is truly of interest. Problematically, those with the highest untreated values may have some of the lowest observed measurements due to the effectiveness of medications. In this paper we propose an approach in which we parametrically estimate the underlying untreated variable of interest as a function of the observed treated value, and dose and type of medication. Multiple imputation is used to incorporate the variability induced by the estimation. We show that this approach yields more realistic parameter estimates than other more traditional approaches to the problem and that study conclusions may be altered in a meaningful way by using the imputed values.-Estimation of risk factor associations when the response is influenced by medication use: an imputation approach.",0
"Severe anemia due to Plasmodium falciparum malaria is a major cause of mortality among young children in western Kenya. The factors that lead to the age-specific incidence of this anemia are unknown. Previous studies have shown an age-related expression of red cell complement regulatory proteins, which protect erythrocytes from autologous complement attack and destruction. Our primary objective was to determine whether in a malaria-endemic area red cells with low levels of complement regulatory proteins are at increased risk for complement (C3b) deposition in vivo. Secondarily, we studied the relationship between red cell complement regulatory protein levels and hemoglobin levels. Three hundred and forty-two life-long residents of a malaria-holoendemic region of western Kenya were enrolled in a cross-sectional study and stratified by age. We measured red cell C3b, CR1, CD55, and immune complex binding capacity by flow cytometry. Individuals who were positive for malaria were treated and blood was collected when they were free of parasitemia. Analysis of variance was used to identify independent variables associated with the %C3b-positive red cells and the hemoglobin level. Individuals between the ages of 6 and 36 months had the lowest red cell CR1, highest %C3b-positive red cells, and highest parasite density. Malaria prevalence also reached its peak within this age group. Among children &lt;/= 24 months of age the %C3b-positive red cells was usually higher in individuals who were treated for malaria than in uninfected individuals with similarly low red cell CR1 and CD55. The variables that most strongly influenced the %C3b-positive red cells were age, malaria status, and red cell CD55 level. Although it did not reach statistical significance, red cell CR1 was more important than red cell CD55 among individuals treated for malaria. The variables that most strongly influenced the hemoglobin level were age, the %C3b-positive red cells, red cell CR1, and red cell CD55. Increasing malaria prevalence among children &gt;6 to &lt;or= 36 months of age in western Kenya, together with low red cell CR1 and CD55 levels, results in increased C3b deposition on red cells and low hemoglobin. The strong contribution of age to C3b deposition suggests that there are still additional unidentified age-related factors that increase the susceptibility of red cells to C3b deposition and destruction.-Increased deposition of C3b on red cells with low CR1 and CD55 in a malaria-endemic region of western Kenya: implications for the development of severe anemia.",0
"A simple method for comparing independent groups of clustered binary data with group-specific covariates is proposed. It is based on the concepts of design effect and effective sample size widely used in sample surveys, and assumes no specific models for the intracluster correlations. It can be implemented using any standard computer program for the analysis of independent binary data after a small amount of preprocessing. The method is applied to a variety of problems involving clustered binary data: testing homogeneity of proportions, estimating dose-response models and testing for trend in proportions, and performing the Mantel-Haenszel chi-squared test for independence in a series of 2 x 2 tables and estimating the common odds ratio and its variance. Illustrative applications of the method are also presented.-A simple method for the analysis of clustered binary data.",1
"Multiple-period cluster randomised trials, such as stepped wedge or cluster cross-over trials, are being conducted with increasing frequency. In the design and analysis of these trials, it is necessary to specify the form of the within-cluster correlation structure, and a common assumption is that the correlation between the outcomes of any pair of subjects within a cluster is identical. More complex models that allow for correlations within a cluster to decay over time have recently been suggested. However, most software packages cannot fit these models. As a result, practitioners may choose a simpler model. We analytically examine the impact of incorrectly omitting a decay in correlation on the variance of the treatment effect estimator and show that misspecification of the within-cluster correlation structure can lead to incorrect conclusions regarding estimated treatment effects for stepped wedge and cluster crossover trials.-Inference for the treatment effect in multiple-period cluster randomised trials when random effect correlation structure is misspecified.",3
"To review patterns of publication of clinical trials funded by US National Institutes of Health (NIH) in peer reviewed biomedical journals indexed by Medline. Cross sectional analysis. Clinical trials funded by NIH and registered within ClinicalTrials.gov (clinicaltrials.gov), a trial registry and results database maintained by the US National Library of Medicine, after 30 September 2005 and updated as having been completed by 31 December 2008, allowing at least 30 months for publication after completion of the trial. Publication and time to publication in the biomedical literature, as determined through Medline searches, the last of which was performed in June 2011. Among 635 clinical trials completed by 31 December 2008, 294 (46%) were published in a peer reviewed biomedical journal, indexed by Medline, within 30 months of trial completion. The median period of follow-up after trial completion was 51 months (25th-75th centiles 40-68 months), and 432 (68%) were published overall. Among published trials, the median time to publication was 23 months (14-36 months). Trials completed in either 2007 or 2008 were more likely to be published within 30 months of study completion compared with trials completed before 2007 (54% (196/366) v 36% (98/269); P&lt;0.001). Despite recent improvement in timely publication, fewer than half of trials funded by NIH are published in a peer reviewed biomedical journal indexed by Medline within 30 months of trial completion. Moreover, after a median of 51 months after trial completion, a third of trials remained unpublished.-Publication of NIH funded trials registered in ClinicalTrials.gov: cross sectional analysis.",0
"It is not uncommon to set the sample size in a clinical trial to attain specified power at a value for the treatment effect deemed likely by the experimenters, even though a smaller treatment effect would still be clinically important. Recent papers have addressed the situation where such a study produces only weak evidence of a positive treatment effect at an interim stage and the organizers wish to modify the design in order to increase the power to detect a smaller treatment effect than originally expected. Raising the power at a small treatment effect usually leads to considerably higher power than was first specified at the original alternative. Several authors have proposed methods which are not based on sufficient statistics of the data after the adaptive redesign of the trial. We discuss these proposals and show in an example how the same objectives can be met while maintaining the sufficiency principle, as long as the eventuality that the treatment effect may be small is considered at the design stage. The group sequential designs we suggest are quite standard in many ways but unusual in that they place emphasis on reducing the expected sample size at a parameter value under which extremely high power is to be achieved. Comparisons of power and expected sample size show that our proposed methods can out-perform L. Fisher's 'variance spending' procedure. Although the flexibility to redesign an experiment in mid-course may be appealing, the cost in terms of the number of observations needed to correct an initial design may be substantial.-Mid-course sample size modification in clinical trials based on the observed treatment effect.",0
"For group-randomized trials, randomization inference based on rank statistics provides robust, exact inference against nonnormal distributions. However, in a matched-pair design, the currently available rank-based statistics lose significant power compared to normal linear mixed model (LMM) test statistics when the LMM is true. In this article, we investigate and develop an optimal test statistic over all statistics in the form of the weighted sum of signed Mann-Whitney-Wilcoxon statistics under certain assumptions. This test is almost as powerful as the LMM even when the LMM is true, but it is much more powerful for heavy tailed distributions. A simulation study is conducted to examine the power.-A powerful and robust test statistic for randomization inference in group-randomized trials with matched pairs of groups.",1
"Many patients with poorly controlled multiple chronic conditions (MCC) also have unhealthy behaviors, mental health challenges, and unmet social needs. Medical management of MCC may have limited benefit if patients are struggling to address their basic life needs. Health systems and communities increasingly recognize the need to address these issues and are experimenting with and investing in new models for connecting patients with needed services. Yet primary care clinicians, whose regular contact with patients makes them more familiar with patients' needs, are often not included in these systems. We are starting a clinician-level cluster-randomized controlled trial to evaluate how primary care clinicians can participate in these community and hospital solutions and whether doing so is effective in controlling MCC. Sixty clinicians in the Virginia Ambulatory Care Outcomes Research Network will be matched by age and sex and randomized to usual care (control condition) or enhanced care planning with clinical-community linkage support (intervention). From the electronic health record we will identify all patients with MCC, including cardiovascular disease or risks, diabetes, obesity, or depression. A baseline assessment will be mailed to up to 50 randomly selected patients for each clinician (3000 total). Ten respondents per clinician (600 patients total) with uncontrolled MCC will be randomly selected for study inclusion, with oversampling of minorities. The intervention includes two components. First, we will use an enhanced care planning tool, My Own Health Report (MOHR), to screen patients for health behavior, mental health, and social needs. Patients will be supported by a patient navigator, who will help patients prioritize needs, create care plans, and write a personal narrative to guide the care team. Patients will update care plans every 1 to 2 weeks. Second, we will create community-clinical linkage to help address patients' needs. The linkage will include community resource registries, personnel to span settings (patient navigators and a community health worker), and care team coordination across team members through MOHR. This study will help inform efforts by primary care clinicians to help address unhealthy behaviors, mental health needs, and social risks as a strategy to better control MCC. ClinicalTrials.gov: NCT03885401. Registered on 19 September 2019.-Enhanced care planning and clinical-community linkages versus usual care to address basic needs of patients with multiple chronic conditions: a clinician-level randomized controlled trial.",0
"To describe the association between values for a proportion and the intraclass correlation coefficient (ICC). Analysis of data obtained from the General Practice Research Database (GPRD) for variation between United Kingdom general practices and results from a Health Technology Assessment (HTA) review for a range of outcomes in community and health services settings. There were 188 ICCs from the GPRD, the median prevalence was 13.1% (interquartile range IQR 3.5 to 28.4%) and median ICC 0.051 (IQR 0.011 to 0.094). There were 136 ICCs from the HTA review, with median prevalence 6.5% (IQR 0.4 to 20.7%) and median ICC 0.006 (IQR 0.0003 to 0.036). There was a linear association of log ICC with log prevalence in both datasets (GPRD, regression coefficient 0.61, 95% confidence interval 0.53 to 0.69, P &lt; 0.001; HTA, 0.91, 0.81 to 1.01, P &lt; 0.001). When the prevalence was 1% the predicted ICC was 0.008 from the GPRD or 0.002 from the HTA, but when the prevalence was 40% the predicted ICC was 0.075 (GPRD) or 0.046 (HTA). The prevalence of an outcome may be used to make an informed assumption about the magnitude of the intraclass correlation coefficient.-Intraclass correlation coefficient and outcome prevalence are associated in clustered binary data.",1
"Owing to the rapid development of biomarkers in clinical trials, joint modeling of longitudinal and survival data has gained its popularity in the recent years because it reduces bias and provides improvements of efficiency in the assessment of treatment effects and other prognostic factors. Although much effort has been put into inferential methods in joint modeling, such as estimation and hypothesis testing, design aspects have not been formally considered. Statistical design, such as sample size and power calculations, is a crucial first step in clinical trials. In this paper, we derive a closed-form sample size formula for estimating the effect of the longitudinal process in joint modeling, and extend Schoenfeld's sample size formula to the joint modeling setting for estimating the overall treatment effect. The sample size formula we develop is quite general, allowing for p-degree polynomial trajectories. The robustness of our model is demonstrated in simulation studies with linear and quadratic trajectories. We discuss the impact of the within-subject variability on power and data collection strategies, such as spacing and frequency of repeated measurements, in order to maximize the power. When the within-subject variability is large, different data collection strategies can influence the power of the study in a significant way. Optimal frequency of repeated measurements also depends on the nature of the trajectory with higher polynomial trajectories and larger measurement error requiring more frequent measurements.-Sample size and power determination in joint modeling of longitudinal and survival data.",0
"Technical reasons are presented as to why therapist should be included as a random design factor in the nested analysis of (co)variance (AN[C]OVA) design commonly used in psychotherapy research. Incorrect specification of the ANOVA design can, under some circumstances, result in incorrect estimation of the error term, overly liberal F ratios, and an unacceptably high risk of Type I errors. Review of studies indicates that the great majority of investigators continue to ignore this issue. Computer simulation studies revealed that considerable bias can be introduced by not specifying therapist as a random term. Finally, a reanalysis is presented of data from 10 psychotherapy outcome studies that indicated that therapist effects vary considerably and at times can be large. More recent studies that implement better quality controls appear to demonstrate less variance due to therapist. The implications of these results for the design of future studies are discussed.-Implications of therapist effects for the design and analysis of comparative studies of psychotherapies",2
"Kawasaki disease is an acute vasculitis of infants and young children that is recognized through a constellation of clinical signs that can mimic other benign conditions of childhood. The etiology remains unknown and there is no specific laboratory-based test to identify patients with Kawasaki disease. Treatment to prevent the complication of coronary artery aneurysms is most effective if administered early in the course of the illness. We sought to develop a diagnostic algorithm to help clinicians distinguish Kawasaki disease patients from febrile controls to allow timely initiation of treatment. Urine peptidome profiling and whole blood cell type-specific gene expression analyses were integrated with clinical multivariate analysis to improve differentiation of Kawasaki disease subjects from febrile controls. Comparative analyses of multidimensional protein identification using 23 pooled Kawasaki disease and 23 pooled febrile control urine peptide samples revealed 139 candidate markers, of which 13 were confirmed (area under the receiver operating characteristic curve (ROC AUC 0.919)) in an independent cohort of 30 Kawasaki disease and 30 febrile control urine peptidomes. Cell type-specific analysis of microarrays (csSAM) on 26 Kawasaki disease and 13 febrile control whole blood samples revealed a 32-lymphocyte-specific-gene panel (ROC AUC 0.969). The integration of the urine/blood based biomarker panels and a multivariate analysis of 7 clinical parameters (ROC AUC 0.803) effectively stratified 441 Kawasaki disease and 342 febrile control subjects to diagnose Kawasaki disease. A hybrid approach using a multi-step diagnostic algorithm integrating both clinical and molecular findings was successful in differentiating children with acute Kawasaki disease from febrile controls.-A diagnostic algorithm combining clinical and molecular data distinguishes Kawasaki disease from other febrile illnesses.",0
A comparison study of general linear mixed moedl and permutation tests in group-randomized trials under non-normal error distributions,1
"Composite outcomes, which combine multiple types of clinical events into a single outcome, are common in clinical trials. The usual analysis considers the time to first occurrence of any event in the composite. The major criticisms of such an approach are (1) this implicitly treats the outcomes as if they were of equal importance, but they often vary in terms of clinical relevance and severity, (2) study participants often experience more than one type of event, and (3) often less severe events occur before more severe ones, but the usual analysis disregards any information beyond that first event. A novel approach, referred to as the win ratio, which addresses the aforementioned criticisms of composite outcomes, is illustrated with a re-analysis of data on fatal and non-fatal cardiovascular disease time-to-event outcomes reported for the Multiple Risk Factor Intervention Trial. In this trial, 12,866 participants were randomized to a special intervention group (n = 6428) or a usual care (n = 6438) group. Non-fatal outcomes were ranked by risk of cardiovascular disease death up to 20 years after trial. In one approach, participants in the special intervention and usual care groups were first matched on coronary heart disease risk at baseline and time of enrollment. Each matched pair was categorized as a winner or loser depending on which one experienced a cardiovascular disease death first. If neither died of cardiovascular disease causes, they were evaluated on the most severe non-fatal outcome. This process continued for all the non-fatal outcomes. A second win ratio statistic, obtained from Cox partial likelihood, was also estimated. This statistic provides a valid estimate of the win ratio using multiple events if the marginal and conditional survivor functions of each outcome satisfy proportional hazards. Loss ratio statistics (inverse of win ratios) are compared to hazard ratios from the usual first event analysis. A larger 11-event composite was also considered. For the 7-event cardiovascular disease composite, the previously reported first event analysis based on 581 events in the special intervention group and 652 events in the usual care group yielded a hazard ratio (95% confidence interval) of 0.89 (0.79-0.99), compared to 0.86 (0.77-0.97) and 0.91 (0.81-1.02) for the severity ranked estimates. Results for the 11-event composite also confirmed the findings of the first event analysis. The win ratio analysis was able to leverage information collected past the first experienced event and rank events by severity. The results were similar to and confirmed previously reported traditional first event analysis. The win ratio statistic is a useful adjunct to the traditional first event analysis for trials with composite outcomes.-A win ratio approach to the re-analysis of Multiple Risk Factor Intervention Trial.",0
Phase II trial of web-based tailored asthma management intervention in adolescents at clinics.,0
"A Monte Carlo study examined the statistical performance of single sample and bootstrap methods that can be used to test and form confidence interval estimates of indirect effects in two cluster randomized experimental designs. The designs were similar in that they featured random assignment of clusters to one of two treatment conditions and included a single intervening variable and outcome, but they differed in whether the mediator was measured at the participant or site level. A bias-corrected bootstrap had the best statistical performance for each design and was closely followed by the empirical-Mtest, either of which is recommended for testing and estimating indirect effects in multilevel designs. In addition, consistent with previous research, the commonly used z test had relatively poor performance.-A Comparison of Single Sample and Bootstrap Methods to Assess Mediation in Cluster Randomized Trials.",1
"In disease screening and prevention trials, subjects in the experimental condition are frequently nested within therapy groups, whereas subjects in the control group receive individual or no therapy and are therefore not nested within groups. Outcomes of subjects within the same therapy group are expected to be more alike than outcomes of subjects within different therapy groups. Ignoring this dependency in the design stage may result in less powerful designs. This paper presents a multilevel model for analyzing such trials and sample size formulae for continuous and binary outcomes with unequal variances and costs across groups. The proposed optimal design ensures that there is adequate power to detect a treatment effect with either minimal cost or a minimal number of subjects. We apply our strategy and design an improved trial where all subjects with musculoskeletal pain received conventional therapy and subjects in the intervention arm participated in a group-learning program.-Sample size formulae for trials comparing group and individual treatments in a multilevel model.",2
"The South African Government has outlined detailed plans for antiretroviral (ART) rollout in KwaZulu-Natal Province, but has not created a plan to address treatment accessibility in rural areas in KwaZulu-Natal. Here, we calculate the distance that People Living With HIV/AIDS (PLWHA) in rural areas in KwaZulu-Natal would have to travel to receive ART. Specifically, we address the health policy question 'How far will we need to go to reach PLWHA in rural KwaZulu-Natal?'. We developed a model to quantify treatment accessibility in rural areas; the model incorporates heterogeneity in spatial location of HCFs and patient population. We defined treatment accessibility in terms of the number of PLWHA that have access to an HCF. We modeled the treatment-accessibility region (i.e. catchment area) around an HCF by using a two-dimensional function, and assumed that treatment accessibility decreases as distance from an HCF increases. Specifically, we used a distance-discounting measure of ART accessibility based upon a modified form of a two-dimensional gravity-type model. We calculated the effect on treatment accessibility of: (1) distance from an HCF, and (2) the number of HCFs. In rural areas in KwaZulu-Natal even substantially increasing the size of a small catchment area (e.g. from 1 km to 20 km) around an HCF would have a negligible impact (~2%) on increasing treatment accessibility. The percentage of PLWHA who can receive ART in rural areas in this province could be as low as ~16%. Even if individuals were willing (and able) to travel 50 km to receive ART, only ~50% of those in need would be able to access treatment. Surprisingly, we show that increasing the number of available HCFs for ART distribution ~ threefold does not lead to a threefold increase in treatment accessibility in rural KwaZulu-Natal. Our results show that many PLWHA in rural KwaZulu-Natal are unlikely to have access to ART, and that the impact of an additional 37 HCFs on treatment accessibility in rural areas would be less substantial than might be expected. There is a great length to go before we will be able to reach many PLWHA in rural areas in South Africa, and specifically in KwaZulu-Natal.-How far will we need to go to reach HIV-infected people in rural South Africa?",0
"This study describes a method for incorporating external estimates of intraclass correlation to improve the precision for the analysis of an existing group-randomized trial. The authors use a random-effects meta-analytic approach to pool the information across studies, which takes into account any interstudy heterogeneity that may exist. This approach can be used in several different situations to estimate the degrees of freedom available for an adjusted test of the intervention effect in a study where the challenges of group-randomized trials were not fully considered when the study was planned. The authors discuss the limitations of this approach and the circumstances in which it is likely to be helpful.-Increasing the degrees of freedom in existing group randomized trials: the df* approach.",1
"Severe neonatal jaundice and its progression to kernicterus is a leading cause of death and disability among newborns in poorly-resourced countries, particularly in sub-Saharan Africa. The standard treatment for jaundice using conventional phototherapy (CPT) with electric artificial blue light sources is often hampered by the lack of (functional) CPT devices due either to financial constraints or erratic electrical power. In an attempt to make phototherapy (PT) more readily available for the treatment of pathologic jaundice in underserved tropical regions, we set out to test the hypothesis that filtered sunlight phototherapy (FS-PT), in which potentially harmful ultraviolet and infrared rays are appropriately screened, will be as efficacious as CPT. This prospective, non-blinded randomized controlled non-inferiority trial seeks to enroll infants with elevated total serum/plasma bilirubin (TSB, defined as 3 mg/dl below the level recommended by the American Academy of Pediatrics for high-risk infants requiring PT) who will be randomly and equally assigned to receive FS-PT or CPT for a total of 616 days at an inner-city maternity hospital in Lagos, Nigeria. Two FS-PT canopies with pre-tested films will be used. One canopy with a film that transmits roughly 33% blue light (wavelength range: 400 to 520 nm) will be used during sunny periods of a day. Another canopy with a film that transmits about 79% blue light will be used during overcast periods of the day. The infants will be moved from one canopy to the other as needed during the day with the goal of keeping the blue light irradiance level above 8 ?W/cm?/nm. FS-PT will be as efficacious as CPT in reducing the rate of rise in bilirubin levels. Secondary outcome: The number of infants requiring exchange transfusion under FS-PT will not be more than those under CPT. This novel study offers the prospect of an effective treatment for infants at risk of severe neonatal jaundice and avoidable exchange transfusion in poorly-resourced settings without access to (reliable) CPT in the tropics. ClinicalTrials.gov Identifier: NCT01434810.-Treatment of neonatal jaundice with filtered sunlight in Nigerian neonates: study protocol of a non-inferiority, randomized controlled trial.",0
"This paper discusses a new class of multiple testing procedures, tree-structured gatekeeping procedures, with clinical trial applications. These procedures arise in clinical trials with hierarchically ordered multiple objectives, for example, in the context of multiple dose-control tests with logical restrictions or analysis of multiple endpoints. The proposed approach is based on the principle of closed testing and generalizes the serial and parallel gatekeeping approaches developed by Westfall and Krishen (J. Statist. Planning Infer. 2001; 99:25-41) and Dmitrienko et al. (Statist. Med. 2003; 22:2387-2400). The proposed testing methodology is illustrated using a clinical trial with multiple endpoints (primary, secondary and tertiary) and multiple objectives (superiority and non-inferiority testing) as well as a dose-finding trial with multiple endpoints.-Tree-structured gatekeeping tests in clinical trials with hierarchically ordered multiple objectives.",0
"Bridging clinical trials are sometimes designed to evaluate whether a proposed dose for use in one population, for example, children, gives similar pharmacokinetic (PK) levels, or has similar effects on a surrogate marker as an established effective dose used in another population, for example, adults. For HIV bridging trials, because of the increased risk of viral resistance to drugs at low PK levels, the goal is often to determine whether the doses used in different populations result in similar percentages of patients with low PK levels. For example, it may be desired to evaluate that a proposed pediatric dose gives approximately 10% of children with PK levels below the 10th percentile of PK levels for the established adult dose. However, the 10th percentile for the adult dose is often imprecisely estimated in studies of relatively small size. Little attention has been given to the statistical framework for such bridging studies. In this article, a formal framework for the design and analysis of quantile-based bridging studies is proposed. The methodology is then developed for normally distributed outcome measures from both frequentist and Bayesian directions. Sample size and other design considerations are discussed.-A statistical framework for quantile equivalence clinical trials with application to pharmacokinetic studies that bridge from HIV-infected adults to children.",0
"This paper concerns the issue of cluster randomization in primary care practice intervention trials. We present information on the cluster effect of measuring the performance of various preventive maneuvers between groups of physicians based on a successful trial. We discuss the intracluster correlation coefficient of determining the required sample size and the implications for designing randomized controlled trials where groups of subjects (e.g., physicians in a group practice) are allocated at random. We performed a cross-sectional study involving data from 46 participating practices with 106 physicians collected using self-administered questionnaires and a chart audit of 100 randomly selected charts per practice. The population was health service organizations (HSOs) located in Southern Ontario. We analyzed performance data for 13 preventive maneuvers determined by chart review and used analysis of variance to determine the intraclass correlation coefficient. An index of ""up-to-datedness"" was computed for each physician and practice as the number of a recommended preventive measure done divided by the number of eligible patients. An index called ""inappropriateness"" was computed in the same manner for the not-recommended measures. The intraclass correlation coefficients for 2 key study outcomes (up-to-datedness and inappropriateness) were also calculated and compared. The mean up-to-datedness score for the practices was 53.5% (95% confidence interval [CI], 51.0%-56.0%), and the mean inappropriateness score was 21.5% (95% CI, 18.1%-24.9%). The intraclass correlation for up-to-datedness was 0.0365 compared with inappropriateness at 0.1790. The intraclass correlation for preventive maneuvers ranged from 0.005 for blood pressure measurement to 0.66 for chest radiographs of smokers, and as a consequence required the sample size ranged from 20 to 42 physicians per group. Randomizing by practice clusters and analyzing at the level of the physician has important implications for sample size requirements. Larger intraclass correlations indicate interdependence among the physicians within a cluster; as a consequence, variability within clusters is reduced, and the required sample size increased. The key finding that many potential outcome measures perform differently in terms of the intracluster correlation reinforces the need for researchers to carefully consider the selection of outcome measures and adjust sample sizes accordingly when the unit of analysis and randomization are not the same.-The effect of cluster randomization on sample size in prevention research.",1
"Factorial experimental designs have many potential advantages for behavioral scientists. For example, such designs may be useful in building more potent interventions by helping investigators to screen several candidate intervention components simultaneously and to decide which are likely to offer greater benefit before evaluating the intervention as a whole. However, sample size and power considerations may challenge investigators attempting to apply such designs, especially when the population of interest is multilevel (e.g., when students are nested within schools, or when employees are nested within organizations). In this article, we examine the feasibility of factorial experimental designs with multiple factors in a multilevel, clustered setting (i.e., of multilevel, multifactor experiments). We conduct Monte Carlo simulations to demonstrate how design elements-such as the number of clusters, the number of lower-level units, and the intraclass correlation-affect power. Our results suggest that multilevel, multifactor experiments are feasible for factor-screening purposes because of the economical properties of complete and fractional factorial experimental designs. We also discuss resources for sample size planning and power estimation for multilevel factorial experiments. These results are discussed from a resource management perspective, in which the goal is to choose a design that maximizes the scientific benefit using the resources available for an investigation.-Multilevel factorial experiments for developing behavioral interventions: power, sample size, and resource considerations",1
"Semi-parametric methods are often used for the estimation of intervention effects on correlated outcomes in cluster-randomized trials (CRTs). When outcomes are missing at random (MAR), Inverse Probability Weighted (IPW) methods incorporating baseline covariates can be used to deal with informative missingness. Also, augmented generalized estimating equations (AUG) correct for imbalance in baseline covariates but need to be extended for MAR outcomes. However, in the presence of interactions between treatment and baseline covariates, neither method alone produces consistent estimates for the marginal treatment effect if the model for interaction is not correctly specified. We propose an AUG-IPW estimator that weights by the inverse of the probability of being a complete case and allows different outcome models in each intervention arm. This estimator is doubly robust (DR); it gives correct estimates whether the missing data process or the outcome model is correctly specified. We consider the problem of covariate interference which arises when the outcome of an individual may depend on covariates of other individuals. When interfering covariates are not modeled, the DR property prevents bias as long as covariate interference is not present simultaneously for the outcome and the missingness. An R package is developed implementing the proposed method. An extensive simulation study and an application to a CRT of HIV risk reduction-intervention in South Africa illustrate the method.-Accounting for interactions and complex inter-subject dependency in estimating treatment effect in cluster-randomized trials with missing outcomes.",1
"The authors review the common methods for measuring strength of contingency between 2 behaviors in a behavioral sequence, the binomial z score and the adjusted cell residual, and point out a number of limitations of these approaches. They present a new approach using log odds ratios and empirical Bayes estimation in the context of hierarchical modeling, an approach not constrained by these limitations. A series of hierarchical models is presented to test the stationarity of behavioral sequences, the homogeneity of sequences across a sample of episodes, and whether covariates can account for variation in sequences across the sample. These models are applied to observational data taken from a study of the behavioral interactions of 254 couples to illustrate their use.-Hierarchical modeling of sequential behavioral data: an empirical Bayesian approach.",0
"Women have higher rates of depression and often experience depression symptoms during critical reproductive periods, including adolescence, pregnancy, postpartum, and menopause. Collaborative care intervention models for mood disorders in patients receiving care in an OB-GYN clinic setting have not been evaluated. Study design and methodology for a randomized controlled trial of collaborative care depression management versus usual care in OB-GYN clinics and the details of the adapted collaborative care intervention and model implementation are described in this paper. Women over age 18 years with clinically significant symptoms of depression, as measured by a Patient Health Questionnaire-9 (PHQ-9) score ?10 and a clinical diagnosis of major depression or dysthymia, were randomized to the study intervention or to usual care and were followed for 18 months. The primary outcome assessed was change over time in the SCL-20 depression scale between baseline and 12 months. Two hundred five women were randomized: 57% white, 20% African American, 9% Asian or Pacific Islander, 7% Hispanic, and 6% Native American. Mean age was 39 years. 4.6% were pregnant and 7.5% were within 12 months postpartum. The majority were single (52%), and 95% had at least the equivalent of a high school diploma. Almost all patients met DSM IV criteria for major depression (99%) and approximately 33% met criteria for dysthymia. An OB-GYN collaborative care team, including a social worker, a psychiatrist, and an OB-GYN physician, who met weekly and used an electronic tracking system for patients was the essential element of the proposed depression care treatment model described here. Further study of models that improve quality of depression care that are adapted to the unique OB-GYN setting is needed.-Improving depression treatment for women: integrating a collaborative care depression intervention into OB-GYN care.",0
Introduction to Special Issue on Design Parameters for Cluster Randomized Trials in Education.,1
"?To assess the effect of the FTO genotype on weight loss after dietary, physical activity, or drug based interventions in randomised controlled trials. ?Systematic review and random effects meta-analysis of individual participant data from randomised controlled trials. ?Ovid Medline, Scopus, Embase, and Cochrane from inception to November 2015. ?Randomised controlled trials in overweight or obese adults reporting reduction in body mass index, body weight, or waist circumference by FTO genotype (rs9939609 or a proxy) after dietary, physical activity, or drug based interventions. Gene by treatment interaction models were fitted to individual participant data from all studies included in this review, using allele dose coding for genetic effects and a common set of covariates. Study level interactions were combined using random effect models. Metaregression and subgroup analysis were used to assess sources of study heterogeneity. ?We identified eight eligible randomised controlled trials for the systematic review and meta-analysis (n=9563). Overall, differential changes in body mass index, body weight, and waist circumference in response to weight loss intervention were not significantly different between FTO genotypes. Sensitivity analyses indicated that differential changes in body mass index, body weight, and waist circumference by FTO genotype did not differ by intervention type, intervention length, ethnicity, sample size, sex, and baseline body mass index and age category. ?We have observed that carriage of the FTO minor allele was not associated with differential change in adiposity after weight loss interventions. These findings show that individuals carrying the minor allele respond equally well to dietary, physical activity, or drug based weight loss interventions and thus genetic predisposition to obesity associated with the FTO minor allele can be at least partly counteracted through such interventions. ?PROSPERO CRD42015015969.-FTO genotype and weight loss: systematic review and meta-analysis of 9563 individual participant data from eight randomised controlled trials.",0
"Recent reviews have shown that while clustering is extremely common in individually randomised trials (for example, clustering within centre, therapist, or surgeon), it is rarely accounted for in the trial analysis. Our aim is to develop a general framework for assessing whether potential sources of clustering must be accounted for in the trial analysis to obtain valid type I error rates (non-ignorable clustering), with a particular focus on individually randomised trials. A general framework for assessing clustering is developed based on theoretical results and a case study of a recently published trial is used to illustrate the concepts. A simulation study is used to explore the impact of not accounting for non-ignorable clustering in practice. Clustering is non-ignorable when there is both correlation between patient outcomes within clusters, and correlation between treatment assignments within clusters. This occurs when the intraclass correlation coefficient is non-zero, and when the cluster has been used in the randomisation process (e.g. stratified blocks within centre) or when patients are assigned to clusters after randomisation with different probabilities (e.g. a surgery trial in which surgeons treat patients in only one arm). A case study of an individually randomised trial found multiple sources of clustering, including centre of recruitment, attending surgeon, and site of rehabilitation class. Simulations show that failure to account for non-ignorable clustering in trial analyses can lead to type I error rates over 20% in certain cases; conversely, adjusting for the clustering in the trial analysis gave correct type I error rates. Clustering is common in individually randomised trials. Trialists should assess potential sources of clustering during the planning stages of a trial, and account for any sources of non-ignorable clustering in the trial analysis.-Assessing potential sources of clustering in individually randomised trials.",2
"In this paper, we propose a Bayesian method for group randomized trials with multiple observation times and multiple outcomes of different types. We jointly model these outcomes using latent multivariate normal linear regression, which allows treatment effects to change with time and accounts for (i) intraclass correlation within groups; (ii) the correlation between different outcomes measured on the same subject; and (iii) the over-time correlation of each outcome. Moreover, we develop a set of innovative priors for the variance components, which yield direct inference on the correlations, avoid undesirable constraints, and allow utilization of information from previous studies. We illustrate through simulations that our model can improve estimation efficiency (lower posterior standard deviations) of intraclass correlations and treatment effects relative to single outcome models and models with diffuse priors on the variance components. We also demonstrate the methodology using body composition data collected in the Trial of Activity in Adolescent Girls.-Efficient Bayesian joint models for group randomized trials with multiple observation times and multiple outcomes.",1
"Various predictive diagnostic tests are highly demanded to guide optimal treatments for individual patients, as individual patients with the same disease such as cancer frequently exhibit dramatically different therapeutic responses to multiple available treatment options. A large number of clinical trials have thus been performed to test the predictive ability and utility of various therapeutic biomarker tests. However, in these trial designs the conventional optimization criteria such as positive predictive value or negative predictive value cannot reflect each patient's true chance of success associated with continuous predictive biomarker scores. We have developed a novel statistical concept, point success rate (PSR), to overcome deficiencies in these conventional methods for optimizing biomarker-based clinical trials. We demonstrate statistical superiority as well as clinical improvement by a PSR-based treatment selection both with simulated and breast cancer patient data.-Point success rate for patient therapeutic response prediction by continuous biomarker scores.",0
"Prevalence of depression is associated inversely with some indicators of socioeconomic position, and the stress of social disadvantage is hypothesized to mediate this relation. Relative to whites, blacks have a higher burden of most physical health conditions but, unexpectedly, a lower burden of depression. This study evaluated an etiologic model that integrates mental and physical health to account for this counterintuitive patterning. The Baltimore Epidemiologic Catchment Area Study (Maryland, 1993-2004) was used to evaluate the interaction between stress and poor health behaviors (smoking, alcohol use, poor diet, and obesity) and risk of depression 12 years later for 341 blacks and 601 whites. At baseline, blacks engaged in more poor health behaviors and had a lower prevalence of depression compared with whites (5.9% vs. 9.2%). The interaction between health behaviors and stress was nonsignificant for whites (odds ratio (OR = 1.04, 95% confidence interval: 0.98, 1.11); for blacks, the interaction term was significant and negative (?: -0.18, P &lt; 0.014). For blacks, the association between median stress and depression was stronger for those who engaged in zero (OR = 1.34) relative to 1 (OR = 1.12) and ?2 (OR = 0.94) poor health behaviors. Findings are consistent with the proposed model of mental and physical health disparities.-Reconsidering the role of social disadvantage in physical and mental health: stressful life events, health behaviors, race, and depression.",0
"The aim of a phase I oncology trial is to identify a dose with an acceptable safety profile. Most phase I designs use the dose-limiting toxicity, a binary endpoint, to assess the unacceptable level of toxicity. The dose-limiting toxicity might be incomplete for investigating molecularly targeted therapies as much useful toxicity information is discarded. In this work, we propose a quasi-continuous toxicity score, the total toxicity profile (TTP), to measure quantitatively and comprehensively the overall severity of multiple toxicities. We define the TTP as the Euclidean norm of the weights of toxicities experienced by a patient, where the weights reflect the relative clinical importance of each grade and toxicity type. We propose a dose-finding design, the quasi-likelihood continual reassessment method (CRM), incorporating the TTP score into the CRM, with a logistic model for the dose-toxicity relationship in a frequentist framework. Using simulations, we compared our design with three existing designs for quasi-continuous toxicity score (the Bayesian quasi-CRM with an empiric model and two nonparametric designs), all using the TTP score, under eight different scenarios. All designs using the TTP score to identify the recommended dose had good performance characteristics for most scenarios, with good overdosing control. For a sample size of 36, the percentage of correct selection for the quasi-likelihood CRM ranged from 80% to 90%, with similar results for the quasi-CRM design. These designs with TTP score present an appealing alternative to the conventional dose-finding designs, especially in the context of molecularly targeted agents.-Dose-finding designs using a novel quasi-continuous endpoint for multiple toxicities.",0
"School- and community-based alcohol prevention programs are often evaluated using a group-randomized trial (GRT) design with a single pretest and a single posttest survey. To size such studies properly, investigators need accurate estimates of the variance and intraclass correlation that will be operative in their analyses. Until recently, the only available estimates were based on cross-sectional analyses. A recent report suggests that values from cross-sectional analyses may overestimate the intraclass correlation operative in pretest-posttest analyses. The purpose of this article is to review these issues, present estimates of intraclass correlation for a variety of alcohol-related endpoints based on cross-sectional analyses and to compare those estimates to estimates based on pretest-posttest analyses. We will also show how these estimates can be used to establish optimal sample sizes for GRTs to evaluate school- and community-based alcohol prevention programs. Data were collected from 18 to 20 year olds and high-school seniors as part of an alcohol prevention effort employing a group-randomized trial design with a single pretest and a single posttest survey. Data were analyzed via mixed-model regression methods to estimate components of variance. Those components were then used to compute the intraclass correlations operative in both cross-sectional analyses and in pretest-posttest analyses. Results indicate that intraclass correlations operative in pretest-posttest analyses are much smaller than are those operative in cross-sectional analyses. Our findings indicate that future alcohol-prevention studies employing a group-randomized trial design with a single pretest and single posttest survey may not need to be as large as previously suggested by intraclass correlation estimates based on cross-sectional data. This holds true even if they are analyzed to reflect the extra variation typical of group-randomized trials.-Intraclass correlations from a community-based alcohol prevention study: the effect of repeat observations on the same communities.",1
"Mutations in either of two genes comprising the STSL locus, ATP-binding cassette (ABC)-transporters ABCG5 (encoding sterolin-1) and ABCG8 (encoding sterolin-2), result in sitosterolemia, a rare autosomal recessive disorder of sterol trafficking characterized by increased plasma plant sterol levels. Based upon the genetics of sitosterolemia, ABCG5/sterolin-1 and ABCG8/sterolin-2 are hypothesized to function as obligate heterodimers. No phenotypic difference has yet been described in humans with complete defects in either ABCG5 or ABCG8. These proteins, based upon the defects in humans, are responsible for regulating dietary sterol entry and biliary sterol secretion. In order to mimic the human disease, we created, by a targeted disruption, a mouse model of sitosterolemia resulting in Abcg8/sterolin-2 deficiency alone. Homozygous knockout mice are viable and exhibit sitosterolemia. Mice deficient in Abcg8 have significantly increased plasma and tissue plant sterol levels (sitosterol and campesterol) consistent with sitosterolemia. Interestingly, Abcg5/sterolin-1 was expressed in both liver and intestine in Abcg8/sterolin-2 deficient mice and continued to show an apical expression. Remarkably, Abcg8 deficient mice had an impaired ability to secrete cholesterol into bile, but still maintained the ability to secrete sitosterol. We also report an intermediate phenotype in the heterozygous Abcg8+/- mice that are not sitosterolemic, but have a decreased level of biliary sterol secretion relative to wild-type mice. These data indicate that Abcg8/sterolin-2 is necessary for biliary sterol secretion and that loss of Abcg8/sterolin-2 has a more profound effect upon biliary cholesterol secretion than sitosterol. Since biliary sitosterol secretion is preserved, although not elevated in the sitosterolemic mice, this observation suggests that mechanisms other than by Abcg8/sterolin-2 may be responsible for its secretion into bile.-A mouse model of sitosterolemia: absence of Abcg8/sterolin-2 results in failure to secrete biliary cholesterol.",0
"Plasma branched-chain amino acids (BCAAs, including leucine, isoleucine and valine) were recently related to risk of type 2 diabetes (T2D). Dietary intake is the only source of BCAAs; however, little is known about whether habitual dietary intake of BCAAs affects risk of T2D. We assessed associations between cumulative consumption of BCAAs and risk of T2D among participants from three prospective cohorts: the Nurses' Health Study (NHS; followed from 1980 to 2012); NHS II (followed from 1991 to 2011); and the Health Professionals Follow-up Study (HPFS; followed from 1986 to 2010). We documented 16?097 incident T2D events during up to 32 years of follow-up. After adjustment for demographics and traditional risk factors, higher total BCAA intake was associated with an increased risk of T2D in men and women. In the meta-analysis of all cohorts, comparing participants in the highest quintile with those in the lowest quintile of intake, hazard ratios (95%confidence intervals) were for leucine 1.13 (1.07-1.19), for isoleucine 1.13 (1.07-1.19) and for valine 1.11 (1.05-1.17) (all P for trend &lt; 0.001). In a healthy subsample, higher dietary BCAAs were significantly associated with higher plasma levels of these amino acids (P for trend = 0.01). Our data suggest that high consumption of BCAAs is associated with an increased risk of T2D.-Cumulative consumption of branched-chain amino acids and incidence of type 2 diabetes.",0
"Cluster randomized trials in health care may involve three instead of two levels, for instance, in trials where different interventions to improve quality of care are compared. In such trials, the intervention is implemented in health care units (""clusters"") and aims at changing the behavior of health care professionals working in this unit (""subjects""), while the effects are measured at the patient level (""evaluations""). Within the generalized estimating equations approach, we derive a sample size formula that accounts for two levels of clustering: that of subjects within clusters and that of evaluations within subjects. The formula reveals that sample size is inflated, relative to a design with completely independent evaluations, by a multiplicative term that can be expressed as a product of two variance inflation factors, one that quantifies the impact of within-subject correlation of evaluations on the variance of subject-level means and the other that quantifies the impact of the correlation between subject-level means on the variance of the cluster means. Power levels as predicted by the sample size formula agreed well with the simulated power for more than 10 clusters in total, when data were analyzed using bias-corrected estimating equations for the correlation parameters in combination with the model-based covariance estimator or the sandwich estimator with a finite sample correction.-Sample size considerations for GEE analyses of three-level cluster randomized trials.",1
Researchers should convince policy makers to perform a classic cluster randomized controlled trial instead of a stepped wedge design when an intervention is rolled out.,3
"In settings where multiple HIV prevention trials are conducted in close proximity, trial participants may attempt to enroll in more than one trial simultaneously. Co-enrollment impacts on participant's safety and validity of trial results. We describe our experience, remedial action taken, inter-organizational collaboration and lessons learnt following the identification of co-enrolled participants. Between February and April 2008, we identified 185 of the 398 enrolled participants as ineligible. In violation of the study protocol exclusion criteria, there was simultaneous enrollment in another HIV prevention trial (ineligible co-enrolled, n=135), and enrollment of women who had participated in a microbicide trial within the past 12 months (ineligible not co-enrolled, n=50). Following a complete audit of all enrolled participants, ineligible participants were discontinued via study exit visits from trial follow-up. Custom-designed education program on co-enrollment impacting on participants' safety and validity of the trial results was implemented. Shared electronic database between research units was established to enable verification of each volunteer's trial participation and to prevent future co-enrollments. Interviews with ineligible enrolled women revealed that high-quality care, financial incentives, altruistic motives, preference for sex with gel, wanting to increase their likelihood of receiving active gel, perceived low risk of discovery and peer pressure are the reasons for their enrollment in the CAPRISA 004 trial. Instituting education programs based on the reasons reported by women for seeking enrollment in more than one trial and using a shared central database system to identify co-enrollments have effectively prevented further co-enrollments.-Co-enrollment in multiple HIV prevention trials - experiences from the CAPRISA 004 Tenofovir gel trial.",0
"Prior research has investigated design parameters for assessing average program impacts on achievement outcomes with cluster randomized trials (CRTs). Less is known about parameters important for assessing differential impacts. This article develops a statistical framework for designing CRTs to assess differences in impact among student subgroups and presents initial estimates of critical parameters. Effect sizes and minimum detectable effect sizes for average and differential impacts are calculated before and after conditioning on effects of covariates using results from several CRTs. Relative sensitivities to detect average and differential impacts are also examined. Student outcomes from six CRTs are analyzed. Achievement in math, science, reading, and writing. The ratio of between-cluster variation in the slope of the moderator divided by total variance-the ""moderator gap variance ratio""-is important for designing studies to detect differences in impact between student subgroups. This quantity is the analogue of the intraclass correlation coefficient. Typical values were .02 for gender and .04 for socioeconomic status. For studies considered, in many cases estimates of differential impact were larger than of average impact, and after conditioning on effects of covariates, similar power was achieved for detecting average and differential impacts of the same size. Measuring differential impacts is important for addressing questions of equity, generalizability, and guiding interpretation of subgroup impact findings. Adequate power for doing this is in some cases reachable with CRTs designed to measure average impacts. Continuing collection of parameters for assessing differential impacts is the next step.-An Empirical Study of Design Parameters for Assessing Differential Impacts for Students in Group Randomized Trials.",1
"Multivariate meta-analysis, which involves jointly analyzing multiple and correlated outcomes from separate studies, has received a great deal of attention. One reason to prefer the multivariate approach is its ability to account for the dependence between multiple estimates from the same study. However, nearly all the existing methods for analyzing multivariate meta-analytic data require the knowledge of the within-study correlations, which are usually unavailable in practice. We propose a simple non-iterative method that can be used for the analysis of multivariate meta-analysis datasets, that has no convergence problems, and does not require the use of within-study correlations. Our approach uses standard univariate methods for the marginal effects but also provides valid joint inference for multiple parameters. The proposed method can directly handle missing outcomes under missing completely at random assumption. Simulation studies show that the proposed method provides unbiased estimates, well-estimated standard errors, and confidence intervals with good coverage probability. Furthermore, the proposed method is found to maintain high relative efficiency compared with conventional multivariate meta-analyses where the within-study correlations are known. We illustrate the proposed method through two real meta-analyses where functions of the estimated effects are of interest.-Inference for correlated effect sizes using multiple univariate meta-analyses.",0
"Cost-effectiveness analyses (CEAs) may use data from cluster randomized trials (CRTs), where the unit of randomization is the cluster, not the individual. However, most studies use analytical methods that ignore clustering. This article compares alternative statistical methods for accommodating clustering in CEAs of CRTs. Our simulation study compared the performance of statistical methods for CEAs of CRTs with 2 treatment arms. The study considered a method that ignored clustering--seemingly unrelated regression (SUR) without a robust standard error (SE)--and 4 methods that recognized clustering--SUR and generalized estimating equations (GEEs), both with robust SE, a ""2-stage"" nonparametric bootstrap (TSB) with shrinkage correction, and a multilevel model (MLM). The base case assumed CRTs with moderate numbers of balanced clusters (20 per arm) and normally distributed costs. Other scenarios included CRTs with few clusters, imbalanced cluster sizes, and skewed costs. Performance was reported as bias, root mean squared error (rMSE), and confidence interval (CI) coverage for estimating incremental net benefits (INBs). We also compared the methods in a case study. Each method reported low levels of bias. Without the robust SE, SUR gave poor CI coverage (base case: 0.89 v. nominal level: 0.95). The MLM and TSB performed well in each scenario (CI coverage, 0.92-0.95). With few clusters, the GEE and SUR (with robust SE) had coverage below 0.90. In the case study, the mean INBs were similar across all methods, but ignoring clustering underestimated statistical uncertainty and the value of further research. MLMs and the TSB are appropriate analytical methods for CEAs of CRTs with the characteristics described. SUR and GEE are not recommended for studies with few clusters.-Developing appropriate methods for cost-effectiveness analysis of cluster randomized trials.",1
"Interaction effects have been consistently found important in explaining the variation in outcomes in many scientific research fields. Yet, in practice, variable selection including interactions is complicated due to the limited sample size, conflicting philosophies regarding model interpretability, and accompanying amplified multiple-testing problems. The lack of statistically sound algorithms for automatic variable selection with interactions has discouraged activities in exploring important interaction effects. In this article, we investigated issues of selecting interactions from three aspects: (1) What is the model space to be searched? (2) How is the hypothesis-testing performed? (3) How to address the multiple-testing issue? We propose loss functions and corresponding decision rules that control FDR in a Bayesian context. Properties of the decision rules are discussed and their performance in terms of power and FDR is compared through simulations. Methods are illustrated on data from a colorectal cancer study assessing the chemotherapy treatments and data from a diffuse large-B-cell lymphoma study assessing the prognostic effect of gene expressions.-A false-discovery-rate-based loss framework for selection of interactions.",0
"This paper presents the first estimates of school-level intraclass correlation (ICC) for smoking-related variables from an urban and largely African American population. Seventh graders (n = 6967) from 39 middle schools in Memphis, TN, were measured at baseline in 1994 and annually through 1997. Mixed model regression methods were used to estimate variance components for school and residual error. School-level ICCs were large enough, if ignored, to substantially inflate the Type I error rate in an analysis of treatment effects. We show how those correlations can be reduced using regression adjustments and used to determine sample size for future school-based smoking prevention studies.-Intraclass correlation among measures related to cigarette use by adolescents: estimates from an urban and largely African American cohort.",1
"In this paper, we propose a novel Gaussian quadrature estimation method in various frailty proportional hazards models. We approximate the unspecified baseline hazard by a piecewise constant one, resulting in a parametric model that can be fitted conveniently by Gaussian quadrature tools in standard software such as SAS Proc NLMIXED. We first apply our method to simple frailty models for correlated survival data (e.g. recurrent or clustered failure times), then to joint frailty models for correlated failure times with informative dropout or a dependent terminal event such as death. Simulation studies show that our method compares favorably with the well-received penalized partial likelihood method and the Monte Carlo EM (MCEM) method, for both normal and Gamma frailty models. We apply our method to three real data examples: (1) the time to blindness of both eyes in a diabetic retinopathy study, (2) the joint analysis of recurrent opportunistic diseases in the presence of death for HIV-infected patients, and (3) the joint modeling of local, distant tumor recurrences and patients survival in a soft tissue sarcoma study. The proposed method greatly simplifies the implementation of the (joint) frailty models and makes them much more accessible to general statistical practitioners.-The use of Gaussian quadrature for estimation in frailty proportional hazards models.",0
"Identification of gene-environment (G??E) interactions associated with disease phenotypes has posed a great challenge in high-throughput cancer studies. The existing marginal identification methods have suffered from not being able to accommodate the joint effects of a large number of genetic variants, while some of the joint-effect methods have been limited by failing to respect the ""main effects, interactions"" hierarchy, by ignoring data contamination, and by using inefficient selection techniques under complex structural sparsity. In this article, we develop an effective penalization approach to identify important G??E interactions and main effects, which can account for the hierarchical structures of the 2 types of effects. Possible data contamination is accommodated by adopting the least absolute deviation loss function. The advantage of the proposed approach over the alternatives is convincingly demonstrated in both simulation and a case study on lung cancer prognosis with gene expression measurements and clinical covariates under the accelerated failure time model.-Dissecting gene-environment interactions: A penalized robust approach accounting for hierarchical structures.",0
"Increased prevalence of overweight and obesity among Appalachian residents may contribute to increased cancer rates in this region. This manuscript describes the design, components, and participant baseline characteristics of a faith-based study to decrease overweight and obesity among Appalachian residents. A group randomized study design was used to assign 13 churches to an intervention to reduce overweight and obesity (Walk by Faith) and 15 churches to a cancer screening intervention (Ribbons of Faith). Church members with a body mass index (BMI) ?25 were recruited from these churches in Appalachian counties in five states to participate in the study. A standard protocol was used to measure participant characteristics at baseline. The same protocol will be followed to obtain measurements after completion of the active intervention phase (12months) and the sustainability phase (24months). Primary outcome is change in BMI from baseline to 12months. Secondary outcomes include changes in blood pressure, waist-to-hip ratio, and fruit and vegetable consumption, as well as intervention sustainability. Church members (n=664) from 28 churches enrolled in the study. At baseline 64.3% of the participants were obese (BMI?30), less than half (41.6%) reported regular exercise, and 85.5% reported consuming less than 5 servings of fruits and vegetables per day. Church members recruited to participate in a faith-based study across the Appalachian region reported high rates of unhealthy behaviors. We have demonstrated the feasibility of developing and recruiting participants to a faith-based intervention aimed at improving diet and increasing exercise among underserved populations.-Study design, intervention, and baseline characteristics of a group randomized trial involving a faith-based healthy eating and physical activity intervention (Walk by Faith) to reduce weight and cancer risk among overweight and obese Appalachian adults.",1
"Cluster randomized trials are increasingly used in research into health care and health services. Ethics of individual patient randomized trials have been elucidated in a number of different codes, but less attention has been given to the ethical issues raised by cluster randomized trials. I assess the challenges raised by cluster randomized controlled trials by considering three questions: What are the essential elements of ethical medical research, particularly experiments on people? What are the features which distinguish cluster randomized controlled trials from ordinary RCTs? Do the distinctive features of cluster randomized trials entail new ethical principles, or careful application of existing principles? I conclude that cluster randomized controlled trials raise new issues on the nature and practice of informed consent, because of the levels at which consent can be sought, and for which it can be sought. In addition, careful consideration of the principles relating to the quality of the scientific design and analysis, balance of risk and benefit, liberty to leave a trial, early stopping of a trial and the power to exclude people from potential benefits is required.-Are distinctive ethical principles required for cluster randomized controlled trials?",1
Cluster randomised trials,1
"There appears to be a preclinical stage of physical disability which precedes onset of task difficulty (disability) in those who develop disability progressively as a result of chronic disease. Such a stage provides a basis for identifying older adults at risk of becoming disabled. This cross-sectional study evaluated whether a preclinical stage of physical function identified by self-report is associated with decrements in objective physical performance measures or increases in disease; that is, whether these measures, in those with preclinical disability, are intermediate between individuals who report no difficulty and no preclinical changes and those who report difficulty. The Women's Health and Aging Study II, an observational study of 436 women 70-80 years of age who were among the two-thirds least disabled living in the community. Participants were sampled from the HCFA Medicare eligibility lists and were determined eligible if they reported no difficulty, or difficulty in only one of four domains of physical function: mobility, upper extremity, IADL and ADL tasks. At the first follow-up (18 months after baseline), participants completed questionnaires on physical functioning for tasks in each of these domains, with possible answer options for each task: they had (1) difficulty (disabled); (2) no difficulty and no modification of task performance (High Function); or (3) no difficulty but reported modification and/or change in frequency of task performance (a self-report measure of preclinical disability predictive of incident difficulty). At the same visit, standardized, objective measures of function and disease were obtained, including measured walk; chair stands; strength: hip flexion, knee extension, ankle dorsiflexion, and grip; balance: function reach, single leg stand, tandem stand; joint exam: hip pain on passive motion and knee pain or tenderness; spirometry; ankle:arm blood pressure ratio; visual function: acuity, contrast sensitivity, stereopsis; and graded treadmill exercise test. Data were analyzed from the first follow-up examination. Physical performance decreased, and disease frequency increased, in association with decreasing self-reported mobility function (in walking one-half mile and climbing 10 steps), across three self-report categories: High Function, Preclinical Disability (Task Modification but No Difficulty) and Disability (Difficulty). These findings pertained for measures of walking speed, balance, strength, and knee and hip osteoarthritis. Self-reported level of function predicted differences in ranges as well as means for walking speed, balance and strength. These findings indicate a physiologic basis for self-reported function, including preclinical disability, specifically that different levels of disease severity, impairments and physical performance are concurrently associated with different categories of self-reported function. They also suggest new avenues for screening and intervention to prevent disability.-Self-reported preclinical disability identifies older women with early declines in performance and early disease.",0
"A note on ""Design and analysis of stepped wedge cluster randomized trials"".",3
"A mixed-effects multinomial logistic regression model is described for analysis of clustered or longitudinal nominal or ordinal response data. The model is parameterized to allow flexibility in the choice of contrasts used to represent comparisons across the response categories. Estimation is achieved using a maximum marginal likelihood (MML) solution that uses quadrature to numerically integrate over the distribution of random effects. An analysis of a psychiatric data set, in which homeless adults with serious mental illness are repeatedly classified in terms of their living arrangement, is used to illustrate features of the model.-A mixed-effects multinomial logistic regression model.",1
"Prior studies in critically ill patients suggest the supra-physiologic chloride concentration of 0.9% (""normal"") saline may be associated with higher risk of renal failure and death compared to physiologically balanced crystalloids. However, the comparative effects of 0.9% saline and balanced fluids are largely unexamined among patients outside the intensive care unit, who represent the vast majority of patients treated with intravenous fluids. This study, entitled Saline Against Lactated Ringer's or Plasma-Lyte in the Emergency Department (SALT-ED), is a pragmatic, cluster, multiple-crossover trial at a single institution evaluating clinical outcomes of adults treated with 0.9% saline versus balanced crystalloids for intravenous fluid resuscitation in the emergency department. All adults treated in the study emergency department receiving at least 500?mL of isotonic crystalloid solution during usual clinical care and subsequently hospitalized outside the intensive care unit are included. Treatment allocation of 0.9% saline versus balanced crystalloids is assigned by calendar month, with study patients treated during the same month assigned to the same fluid type. The first month (January 2016) was randomly assigned to balanced crystalloids, with each subsequent month alternating between 0.9% saline and balanced crystalloids. For balanced crystalloid treatment, clinicians can choose either Lactated Ringer's or Plasma-Lyte A?. The study period is set at 16?months, which will result in an anticipated estimated sample size of 15,000 patients. The primary outcome is hospital-free days to day 28, defined as the number of days alive and out of the hospital from the index emergency department visit until 28?days later. Major secondary outcomes include proportion of patients who develop acute kidney injury by creatinine measurements; major adverse kidney events by hospital discharge or day 30 (MAKE30), which is a composite outcome of death, new renal replacement therapy, and persistent creatinine elevation &gt;200% of baseline; and in-hospital mortality. This ongoing pragmatic trial will provide the most comprehensive evaluation to date of clinical outcomes associated with 0.9% saline compared to physiologically balanced fluids in patients outside the intensive care unit. ClinicalTrials.gov, NCT02614040 . Registered on 18 November 2015.-Saline versus balanced crystalloids for intravenous fluid therapy in the emergency department: study protocol for a cluster-randomized, multiple-crossover trial.",0
Comment: The Essential Role of Pair Matching,1
"Three-level data occur frequently in behaviour and medical sciences. For example, in a multi-centre trial, subjects within a given site are randomly assigned to treatments and then studied over time. In this example, the repeated observations (level-1) are nested within subjects (level-2) who are nested within sites (level-3). Similarly, in twin studies, repeated measurements (level-1) are taken on each twin (level-2) within each twin pair (level-3). A three-level mixed-effects regression model is described here. Random effects at the second and third level are included in the model. Additionally, both proportional odds and non-proportional odds models are developed. The latter allows the effects of explanatory variables to vary across the cumulative logits of the model. A maximum marginal likelihood (MML) solution is described and Gauss-Hermite numerical quadrature is used to integrate over the distribution of random effects. The random effects are normally distributed in this instance. Features of this model are illustrated using data from a school-based smoking prevention trial and an Alzheimer's disease clinical trial.-A mixed-effects regression model for three-level ordinal response data.",1
"As medical applications for cluster randomization designs become more common, investigators look for guidance on optimal methods for estimating the effect of group-based interventions over time. This study examines two distinct cluster randomization designs: (1) the repeated cross-sectional design in which centres are followed over time but patients change, and (2) the longitudinal design in which individual patients are followed over time within treatment clusters. Simulations of each study design stipulated a multiplicative treatment effect (on the log odds scale), between 5 and 15 clusters in each of two treatment arms, and followed over two time periods. Estimation options included linear mixed effects models using restricted maximum likelihood (REML), generalized estimating equations (GEE), mixed effects logistic regression using both penalized quasi likelihood (PQL) and numerical integration, and Bayesian Monte Carlo analysis. For the repeated cross-sectional designs, most methods performed well in terms of bias and coverage when clusters were numerous (30) and variability across clusters of baseline risk and treatment effect was modest. With few clusters (two groups of five) and higher variability, only the Bayesian methods maintained coverage. In the longitudinal designs, the common methods of REML, GEE, or PQL performed poorly when compared to numerical integration, while Bayesian methods demonstrated less bias and better coverage for estimates of both log odds ratios and risk differences. The performance of common statistical tools for the analysis of cluster randomization designs depends heavily on the precise design, the number of clusters, and the variability of baseline outcomes and treatment effects across centres.-Longitudinal and repeated cross-sectional cluster-randomization designs using mixed effects regression for binary outcomes: bias and coverage of frequentist and Bayesian methods.",1
"The authors sought to quantify the overall and race/ethnic-specific relations between prepregnancy body mass index and both preterm birth and vaginal inflammation. Data from a cohort of 11,392 women who enrolled in the multicenter Vaginal Infections and Prematurity Study (1984-1989) at 23-26 weeks' gestation were used. Compared with a prepregnancy body mass index of 22, a body mass index of 16 increased the risk of preterm birth by 90% (odds ratio = 1.9, 95% confidence interval (CI): 1.5, 2.6), and a body mass index of 18 increased the risk by 40% (odds ratio = 1.4, 95% CI: 1.2, 1.7). Ethnicity substantially modified the magnitude of the body mass index effect and the shape of the preterm birth risk curve, with underweight having a greater impact on preterm birth among Blacks and Hispanics than among Whites. Low body mass index increased the risk of a high level of neutrophils (&gt; 5 per oil immersion field) and a high vaginal pH measurement (&gt; or = 5.0) among Black women; for a body mass index of 16 versus 22, the odds ratio = 1.7 (95% CI: 1.1, 2.6). Compared with Black women with a body mass index of 22, Blacks with a body mass index of 16 had a 1.7-fold increased risk for a high level of neutrophils and a high vaginal pH measurement, while those with a body mass index of 18 had a 1.3-fold increased risk.-Prepregnancy body mass index, vaginal inflammation, and the racial disparity in preterm birth.",0
"Over the last few years, three-level longitudinal models have become more common in psychotherapy research, particularly in therapist-effect or group-effect studies. Thus far, limited attention has been paid to power analysis in these models. This article demonstrates the effects of intraclass correlation, level of randomization, sample size, covariates and drop-out on power, using data from a routine outcome monitoring study. Results indicate that randomization at the patient level is the most efficient, and that increasing the number of measurements does not increase power much. Adding a covariate or having a 25% drop-out rate had limited effects on study power in our data. In addition, the results demonstrate that sufficient power can be reached with small sample sizes, but that larger sample sizes are needed to prevent estimation bias for the model parameters and standard errors.-A priori power analysis in longitudinal three-level multilevel models: an example with therapist effects.",1
"We present a method for computing sample size for cluster-randomized studies involving a large number of clusters with relatively small numbers of observations within each cluster. For multivariate survival data, only the marginal bivariate distribution is assumed to be known. The validity of this assumption is also discussed.-Sample size estimation for survival outcomes in cluster-randomized studies with small cluster sizes.",1
"Medical costs data with administratively censored observations often arise in cost-effectiveness studies of treatments for life-threatening diseases. Mean of medical costs incurred from the start of a treatment until death or a certain time point after the implementation of treatment is frequently of interest. In many situations, due to the skewed nature of the cost distribution and non-uniform rate of cost accumulation over time, the currently available normal approximation confidence interval has poor coverage accuracy. In this paper, we propose a bootstrap confidence interval for the mean of medical costs with censored observations. In simulation studies, we show that the proposed bootstrap confidence interval had much better coverage accuracy than the normal approximation one when medical costs had a skewed distribution. When there is light censoring on medical costs (&lt; or =25 per cent), we found that the bootstrap confidence interval based on the simple weighted estimator is preferred due to its simplicity and good coverage accuracy. For heavily censored cost data (censoring rate &gt; or =30 per cent) with larger sample sizes (n &gt; or =200), the bootstrap confidence intervals based on the partitioned estimator has superior performance in terms of both efficiency and coverage accuracy. We also illustrate the use of our methods in a real example.-Bootstrap confidence intervals for medical costs with censored observations.",0
"Three issues concerning the design and analysis of randomized behavioral intervention studies are illustrated and discussed within the framework of a tobacco and alcohol prevention trial among migrant Latino adolescents. The first issue arises when subjects are randomized in clusters rather than individually. Because subject observations cannot be assumed to be independent, information pertaining to the degree of clustering must be reported, and analyses must take the clustering into account. The second issue concerns the impact of compliance to the intervention and the importance of measuring compliance in the experimental and attention-control groups. A compliance analysis should control for participant contact with study personnel. Investigators must consider ways of constructing a compliance measure that is common to both conditions. Third, because outcomes are measured repeatedly over time, we illustrate the importance of assessing the impact of missing-data patterns on outcomes and the extent to which the patterns may modify the treatment effect.-Some methodologic issues in analyzing data from a randomized adolescent tobacco and alcohol use prevention trial.",1
"Trial investigators often have a primary interest in the estimation of the survival curve in a population for which there exists acceptable historical information from which to borrow strength. However, borrowing strength from a historical trial that is non-exchangeable with the current trial can result in biased conclusions. In this article we propose a fully Bayesian semiparametric method for the purpose of attenuating bias and increasing efficiency when jointly modeling time-to-event data from two possibly non-exchangeable sources of information. We illustrate the mechanics of our methods by applying them to a pair of post-market surveillance datasets regarding adverse events in persons on dialysis that had either a bare metal or drug-eluting stent implanted during a cardiac revascularization surgery. We finish with a discussion of the advantages and limitations of this approach to evidence synthesis, as well as directions for future work in this area. The article's Supplementary Materials offer simulations to show our procedure's bias, mean squared error, and coverage probability properties in a variety of settings.-Semiparametric Bayesian commensurate survival model for post-market medical device surveillance with non-exchangeable historical data.",0
"There is an abundance of guidance for the interim monitoring of individually randomised trials. While methodological literature exists on how to extend these methods to cluster randomised trials, there is little guidance on practical implementation. Cluster trials have many features which make their monitoring needs different. We outline the methodological and practical challenges of interim monitoring of cluster trials; and apply these considerations to a case study. The E-MOTIVE study is an 80-cluster randomised trial of a bundle of interventions to treat postpartum haemorrhage. The proposed data monitoring plan includes (1) monitor sample size assumptions, (2) monitor for evidence of selection bias, and (3) an interim assessment of the primary outcome, as well as monitoring data completeness. The timing of the sample size monitoring is chosen with both consideration of statistical precision and to allow time to recruit more clusters. Monitoring for selection bias involves comparing individual-level characteristics and numbers recruited between study arms to identify any post-randomisation participant identification bias. An interim analysis of outcomes presented with 99.9% confidence intervals using the Haybittle-Peto approach should mitigate any concern regarding the inflation of type-I error. The pragmatic nature of the trial means monitoring for adherence is not relevant, as it is built into a process evaluation. The interim analyses of cluster trials have a number of important differences to monitoring individually randomised trials. In cluster trials, there will often be a greater need to monitor nuisance parameters, yet there will often be considerable uncertainty in their estimation. This means the utility of sample size re-estimation can be questionable particularly when there are practical or funding difficulties associated with making any changes to planned sample sizes. Perhaps most importantly interim monitoring has the potential to identify selection bias, particularly in trials with post-randomisation identification or recruitment. Finally, the pragmatic nature of cluster trials might mean that the utility of methods to allow for interim monitoring of outcomes based on statistical testing, or monitoring for adherence to study interventions, are less relevant. Our intention is to facilitate the planning of future cluster randomised trials and to promote discussion and debate to improve monitoring of these studies.-Interim data monitoring in cluster randomised trials: Practical issues and a case study.",1
"This work has investigated under what conditions confidence intervals around the differences in mean costs from a cluster RCT are suitable for estimation using a commonly used cluster-adjusted bootstrap in preference to methods that utilise the Huber-White robust estimator of variance. The bootstrap's main advantage is in dealing with skewed data, which often characterise patient costs. However, it is insufficiently well recognised that one method of adjusting the bootstrap to deal with clustered data is only valid in large samples. In particular, the requirement that the number of clusters randomised should be large would not be satisfied in many cluster RCTs performed to date. The performances of confidence intervals for simple differences in mean costs utilising a robust (cluster-adjusted) standard error and from two cluster-adjusted bootstrap procedures were compared in terms of confidence interval coverage in a large number of simulations. Parameters varied included the intracluster correlation coefficient, the sample size and the distributions used to generate the data. The bootstrap's advantage in dealing with skewed data was found to be outweighed by its poor confidence interval coverage when the number of clusters was at the level frequently found in cluster RCTs in practice. Simulations showed that confidence intervals based on robust methods of standard error estimation achieved coverage rates between 93.5% and 94.8% for a 95% nominal level whereas those for the bootstrap ranged between 86.4% and 93.8%. In general, 24 clusters per treatment arm is probably the minimum number for which one would even begin to consider the bootstrap in preference to traditional robust methods, for the parameter combinations investigated here. At least this number of clusters and extremely skewed data would be necessary for the bootstrap to be considered in favour of the robust method. There is a need for further investigation of more complex bootstrap procedures if economic data from cluster RCTs are to be analysed appropriately.-Use of the bootstrap in analysing cost data from cluster randomised trials: some simulation results.",1
"The log-rank test is widely used to compare two survival distributions in a randomized clinical trial, while partial likelihood (Cox, 1975) is the method of choice for making inference about the hazard ratio under the Cox (1972) proportional hazards model. The Wald 95% confidence interval of the hazard ratio may include the null value of 1 when the p-value of the log-rank test is less than 0.05. Peto et al. (1977) provided an estimator for the hazard ratio based on the log-rank statistic; the corresponding 95% confidence interval excludes the null value of 1 if and only if the p-value of the log-rank test is less than 0.05. However, Peto's estimator is not consistent, and the corresponding confidence interval does not have correct coverage probability. In this article, we construct the confidence interval by inverting the score test under the (possibly stratified) Cox model, and we modify the variance estimator such that the resulting score test for the null hypothesis of no treatment difference is identical to the log-rank test in the possible presence of ties. Like Peto's method, the proposed confidence interval excludes the null value if and only if the log-rank test is significant. Unlike Peto's method, however, this interval has correct coverage probability. An added benefit of the proposed confidence interval is that it tends to be more accurate and narrower than the Wald confidence interval. We demonstrate the advantages of the proposed method through extensive simulation studies and a colon cancer study.-On confidence intervals for the hazard ratio in randomized clinical trials.",0
"Microarrays have become an indispensable tool in biomedical research. This powerful technology not only makes it possible to quantify a large number of nucleic acid molecules simultaneously, but also produces data with many sources of noise. A number of preprocessing steps are therefore necessary to convert the raw data, usually in the form of hybridisation images, to measures of biological meaning that can be used in further statistical analysis. Preprocessing of oligonucleotide arrays includes image processing, background adjustment, data normalisation/transformation and sometimes summarisation when multiple probes are used to target one genomic unit. In this article, we review the issues encountered in each preprocessing step and introduce the statistical models and methods in preprocessing.-A review of statistical methods for preprocessing oligonucleotide microarrays.",0
"Although antiretroviral therapy (ART) is known to be protective against HIV-related mortality, the expected magnitude of effect is unclear because existing estimates of the effect of ART may not directly generalize to recently HIV-diagnosed persons. In this study, we estimated 5-year mortality risks for immediate versus no ART initiation among patients (n = 12,547) in the Centers for AIDS Research Network of Integrated Clinical Systems (CNICS) using the complement of adjusted Kaplan-Meier survival functions. We subsequently standardized estimates to persons diagnosed with HIV in the USA between 2009 and 2011, who were enumerated using national surveillance data. The 5-year mortality, had all patients in the CNICS immediately initiated ART, was 10.6% [95% confidence interval (CI): 9.3%, 11.9%] compared with 28.3% (95% CI: 19.1%, 37.5%) had ART initiation been delayed at least 5 years. The 5-year mortality risk difference due to ART among patients in the CNICS was -17.7% (95% CI: -27.0%, -8.4%). Based on methods for generalizing an estimate from a study sample to a different target population, the expected risk difference due to ART initiation among recently HIV-diagnosed persons in the USA was -19.1% (95% CI: -30.5%, -7.8%). Immediate ART initiation substantially lowers mortality among persons in the CNICS and this benefit is expected to be similar among persons recently diagnosed with HIV in the USA. We demonstrate a method by which concerns about generalizability can be addressed and evaluated quantitatively.-The effect of antiretroviral therapy on all-cause mortality, generalized to persons diagnosed with HIV in the USA, 2009-11.",0
"Despite the ample interest in the measurement of substance abuse and dependence, obtaining biological samples from participants as a means to validate a scale is considered time and cost intensive and is, subsequently, largely overlooked. To report the psychometric properties of the drug use disorder (DUD) questionnaire including oral fluid and blood sample screening indicators measuring the three most commonly used illicit substances--marijuana, cocaine, and extramedicinal painkillers. Participants were a subset (N = 2,702) of the 2007 U.S. National Roadside Survey that was administered to daytime and nighttime weekend drivers in the 48 contiguous states to examine the prevalence of substance use and misuse. Participants completed demographic and substance use questions as well as the DUD--a 12-item measure assessing substance abuse and dependence. Participants could potentially have completed the DUD three times for each of the three substances. Subscales of abuse and dependence were created using Diagnostic and Statistical Manual of Mental Disorders (Fourth Edition Text Revision [DSM-IV-TR]) criteria of these diagnoses. The DUD displayed adequate internal consistency on both subscales of substance abuse and dependence (Cronbach's ? ranging from .71 to .84 and .83 to .92, respectively). The DUD also demonstrated construct validity in comparison to biological markers of each substance. The DUD is a biologically validated instrument that is both easy to utilize and may have valuable implications as a research tool among both clinical and nonclinical populations.-Drug use disorder (DUD) questionnaire: scale development and validation.",0
"Several gene- or set-based association tests have been proposed recently in the literature. Powerful statistical approaches are still highly desirable in this area. In this paper we propose a novel statistical association test, which uses information of the burden component and its complement from the genotypes. This new test statistic has a simple null distribution, which is a special and simplified variance-gamma distribution, and its p-value can be easily calculated. Through a comprehensive simulation study, we show that the new test can control type I error rate and has superior detecting power compared with some popular existing methods. We also apply the new approach to a real data set; the results demonstrate that this test is promising.-A novel gene-set association test based on variance-gamma distribution.",0
"The disproportionately high prevalence of hypertension and its associated mortality and morbidity in minority older adults is a major public health concern in the United States. Despite compelling evidence supporting the beneficial effects of therapeutic lifestyle changes on blood pressure reduction, these approaches remain largely untested among minority elders in community-based settings. The Counseling Older Adults to Control Hypertension trial is a two-arm randomized controlled trial of 250 African-American and Latino seniors, 60 years and older with uncontrolled hypertension, who attend senior centers. The goal of the trial is to evaluate the effect of a therapeutic lifestyle intervention delivered via group classes and individual motivational interviewing sessions versus health education, on blood pressure reduction. The primary outcome is change in systolic and diastolic blood pressure from baseline to 12 months. The secondary outcomes are blood pressure control at 12 months; changes in levels of physical activity; body mass index; and number of daily servings of fruits and vegetables from baseline to 12 months. The intervention group will receive 12 weekly group classes followed by individual motivational interviewing sessions. The health education group will receive an individual counseling session on healthy lifestyle changes and standard hypertension education materials. Findings from this study will provide needed information on the effectiveness of lifestyle interventions delivered in senior centers. Such information is crucial in order to develop implementation strategies for translation of evidence-based lifestyle interventions to senior centers, where many minority elders spend their time, making the centers a salient point of dissemination.-The Counseling Older Adults to Control Hypertension (COACH) trial: design and methodology of a group-based lifestyle intervention for hypertensive minority older adults.",0
"Prospective disease surveillance has gained increasing attention, particularly in light of recent concern for quick detection of bioterrorist events. Monitoring of health events has the potential for the detection of such events, but the benefits of surveillance extend much more broadly to the quick detection of change in public health. In this paper, univariate and multivariate cumulative sum methods for disease surveillance are compared. Although the univariate method has been previously used in the context of health surveillance, the multivariate method has not. The univariate approach consists of simultaneously and independently monitoring the disease rate in each region; the multivariate approach accounts explicitly for any covariation between regions. The univariate approaches are limited by their lack of ability to account for the spatial autocorrelation of regional data; the multivariate methods are limited by the difficulty in accurately specifying the multiregional covariance structure. The methods are illustrated using both simulated data and county-level data on breast cancer in the northeastern United States. When the degree of spatial autocorrelation is low, the univariate method is generally better at detecting changes in rates that occur in a small number of regions; the multivariate is better when change occurs in a large number of regions.-Monitoring change in spatial patterns of disease: comparing univariate and multivariate cumulative sum approaches.",0
"Alcohol use disorder (AUD) is a highly prevalent, chronic relapsing disorder with a high disease burden in the USA. Pharmacotherapy is a promising treatment method for AUD; however, the few FDA-approved medications are only modestly effective. Medications development for AUD is a high priority research area, but the cumbersome drug development process hinders many potential compounds from reaching approval. One area with major opportunities for improvement is the process of screening novel compounds for initial efficacy, also known as early phase 2 trials. Early phase 2 trials incorporate human laboratory paradigms to assess relevant clinical constructs, such as craving and subjective responses to alcohol. However, these controlled paradigms often lack the ecological validity of clinical trials. Therefore, early phase 2 trials can be more efficient and clinically meaningful if they combine the internal validity of experimental laboratory testing with the external validity of clinical trials. To that end, the current study aims to develop and validate a novel early efficacy paradigm, informed by smoking cessation literature, to screen novel medications for AUD. As an established AUD medication, naltrexone will serve as an active control to test both the practice quit attempt model and the efficacy of a promising AUD pharmacotherapy, varenicline. Individuals with current AUD reporting intrinsic motivation to change their drinking will complete a week-long ""practice quit attempt"" while on study medication. Participants are randomized and blinded to either naltrexone, varenicline, or placebo. During the practice quit attempt, participants will complete daily visits over the phone and fill out online questionnaires regarding their drinking, alcohol craving, and mood. Additionally, participants will undergo two alcohol cue-reactivity sessions. The successful completion of this study will advance medications development by proposing and validating a novel early efficacy model for screening AUD pharmacotherapies, which in turn can serve as an efficient strategy for making go/no-go decisions as to whether to proceed with clinical trials. ClinicalTrials.gov NCT04249882 . Registered on 31 January 2020.-A novel human laboratory model for screening medications for alcohol use disorder.",0
"Menstrual cycle length (MCL) has been shown to play an important role in couple fecundity, which is the biologic capacity for reproduction irrespective of pregnancy intentions. However, a comprehensive assessment of its role requires a fecundity model that accounts for male and female attributes and the couple's intercourse pattern relative to the ovulation day. To this end, we employ a Bayesian joint model for MCL and pregnancy. MCLs follow a scale multiplied (accelerated) mixture model with Gaussian and Gumbel components; the pregnancy model includes MCL as a covariate and computes the cycle-specific probability of pregnancy in a menstrual cycle conditional on the pattern of intercourse and no previous fertilization. Day-specific fertilization probability is modeled using natural, cubic splines. We analyze data from the Longitudinal Investigation of Fertility and the Environment Study (the LIFE Study), a couple based prospective pregnancy study, and find a statistically significant quadratic relation between fecundity and menstrual cycle length, after adjustment for intercourse pattern and other attributes, including male semen quality, both partner's age, and active smoking status (determined by baseline cotinine level 100 ng/mL). We compare results to those produced by a more basic model and show the advantages of a more comprehensive approach.-A Bayesian joint model of menstrual cycle length and fecundity.",0
"In experimental designs with nested structures, entire groups (such as schools) are often assigned to treatment conditions. Key aspects of the design in these cluster-randomized experiments involve knowledge of the intraclass correlation structure, the effect size, and the sample sizes necessary to achieve adequate power to detect the treatment effect. However, the units at each level of the hierarchy have a cost associated with them and thus researchers need to decide on sample sizes given a certain budget, when designing their studies. This article provides methods for computing power within an optimal design framework that incorporates costs of units in all three levels for three-level cluster-randomized balanced designs with two levels of nesting at the second and third level. The optimal sample sizes are a function of the variances at each level and the cost of each unit. Overall, larger effect sizes, smaller intraclass correlations at the second and third level, and lower cost of Level 3 and Level 2 units result in higher estimates of power.-Incorporating cost in power analysis for three-level cluster-randomized designs.",1
"To assess the perceptions of patients with stable coronary artery disease of the urgency and benefits of elective percutaneous coronary intervention and to examine how they vary across centers and by providers. Cross sectional study. 10 US academic and community hospitals performing percutaneous coronary interventions between 2009 and 2011. 991 patients with stable coronary artery disease undergoing elective percutaneous coronary intervention. Patients' perceptions of the urgency and benefits of percutaneous coronary intervention, assessed by interview. Multilevel hierarchical logistic regression models examined the variation in patients' understanding across centers and operators after adjusting for patient characteristics, using median odds ratios. The most common reported benefits from percutaneous coronary intervention were to extend life (90%, n=892; site range 80-97%) and to prevent future heart attacks (88%, n=872; site range 79-97%). Although nearly two thirds of patients (n=661) reported improvement of symptoms as a benefit of percutaneous coronary intervention (site range 52-87%), only 1% (n=9) identified this as the only benefit. Substantial variability was noted in the ways informed consent was obtained at each site. After adjusting for patient and operator characteristics, the median odds ratios showed significant variation in patients' perceptions of percutaneous coronary intervention across sites (range 1.4-3.1) but not across operators within a site. Patients have a poor understanding of the benefits of elective percutaneous coronary intervention, with significant variation across sites. No sites had a high proportion of patients accurately understanding the benefits. Coupled with the wide variability in the ways in which hospitals obtain informed consent, these findings suggest that hospital level interventions into the structure and processes of obtaining informed consent for percutaneous coronary intervention might improve patient comprehension and understanding.-Variation in patients' perceptions of elective percutaneous coronary intervention in stable coronary artery disease: cross sectional study.",0
"The belief remains widespread that medical research studies must have statistical power of at least 80% in order to be scientifically sound, and peer reviewers often question whether power is high enough. This requirement and the methods for meeting it have severe flaws. Notably, the true nature of how sample size influences a study's projected scientific or practical value precludes any meaningful blanket designation of &lt;80% power as ""inadequate"". In addition, standard calculations are inherently unreliable, and focusing only on power neglects a completed study's most important results: estimates and confidence intervals. Current conventions harm the research process in many ways: promoting misinterpretation of completed studies, eroding scientific integrity, giving reviewers arbitrary power, inhibiting innovation, perverting ethical standards, wasting effort, and wasting money. Medical research would benefit from alternative approaches, including established value of information methods, simple choices based on cost or feasibility that have recently been justified, sensitivity analyses that examine a meaningful array of possible findings, and following previous analogous studies. To promote more rational approaches, research training should cover the issues presented here, peer reviewers should be extremely careful before raising issues of ""inadequate"" sample size, and reports of completed studies should not discuss power. Common conventions and expectations concerning sample size are deeply flawed, cause serious harm to the research process, and should be replaced by more rational alternatives.-Current sample size conventions: flaws, harms, and alternatives.",0
"The Essential Role of Pair Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation",1
"The prevalence of chronic sleep deprivation is increasing in modern societies with negative health consequences. Recently, an association between short sleep and obesity has been reported. null To assess the feasibility of increasing sleep duration to a healthy length (approximately 7(1/2) h) and to determine the effect of sleep extension on body weight. To examine the long-term effects of sleep extension on endocrine (leptin and ghrelin) and immune (cytokines) parameters, the prevalence of metabolic syndrome, body composition, psychomotor vigilance, mood, and quality of life. One hundred-fifty obese participants who usually sleep less than 6(1/2) h, are being randomized at a 2:1 ratio to either an Intervention or to a Comparison Group. They are stratified by age (above and below 35) and the presence or absence of metabolic syndrome. During the first 12 months (Efficacy Phase) of the study, participants are evaluated at bi-monthly intervals: the Intervention Group is coached to increase sleep by at least 30-60 min/night, while the Comparison Group maintains baseline sleep duration. In the second (Effectiveness) phase, participants converge into the same group and are asked to increase (Comparison Group) or maintain (Intervention Group) sleep duration and are evaluated at 6-month intervals for an additional 3 years. Non-pharmacological and behavior-based interventions are being utilized to increase sleep duration. Endocrine, metabolic, and psychological effects are monitored. The sleep, energy expenditure, and caloric intake are assessed by activity monitors and food recall questionnaires. At yearly intervals, body composition, abdominal fat, and basal metabolic rate are measured by dual energy X-ray absorptiometry (DXA), computerized tomography (CT), and indirect calorimetry, respectively. As of January 2010, 109 participants had been randomized, 64 to the Intervention Group and 45 to the Comparison Group (76% women, 62% minorities, average age: 40.8 years; BMI: 38.5 kg/m(2)). Average sleep duration at screening was less than 6 h/night, 40.3 h/week. A total of 28 Intervention and 22 Comparison participants had completed the Efficacy Phase. The study is not blinded and the sample size is relatively small. This proof-of-concept study on a randomized sample will assess whether sleep extension is feasible and whether it influences BMI. Clinical Trials 2010; 7: 274-285. http://ctj.sagepub.com.-Treatment of obesity with extension of sleep duration: a randomized, prospective, controlled trial.",0
"Throughout the 1980s and 1990s cluster randomization trials have been increasingly used to evaluate effectiveness of health care intervention. Such trials have raised several methodologic challenges in analysis. Meta-analyses involving cluster randomization trials are becoming common in the area of health care intervention. However, as yet there has been no empirical evidence of current practice in the meta-analyses. Thus a review was performed to identify and examine synthesis approaches of meta-analyses involving cluster randomization trials in the published literature. Electronic databases were searched for meta-analyses involving cluster randomization trials from the earliest date available to 2000. Once a meta-analysis was identified, papers on the relevant cluster randomization trials included were also requested. Each of the original papers of cluster randomization trials included was examined for its randomized design and unit, and adjustment for clustering effect in analysis. Each of the selected meta-analyses was then evaluated as to its synthesis concerning clustering effect. In total, 25 eligible meta-analyses were reviewed. Of these, 15 meta-analyses reported simple conventional methods of the fixed-effect model as method of analysis, while six did not incorporate the cluster randomization trial results in the synthesis methods but described the trial results individually. Three meta-analyses attempted to account for the clustering effect in the synthesis methods but approaches were in arbitrary. Fifteen meta-analyses included more than one cluster randomization trial, each of which included cluster randomization trials with a mixture of randomized designs and units, and units of analysis. These mixture situations might increase heterogeneity, but have not been considered in any meta-analysis. Some methods dealing with a binary outcome for some specific situations have been discussed. In conclusion, some difficulties in the quantitative synthesis procedures were found in the meta-analyses involving cluster randomization trials. Recommendations in the applications of approaches to some specific situations in a binary outcome variable have also been provided. There are still, however, several methodologic issues of the meta-analyses involving cluster randomization trials that need to be investigated further.-Meta-analyses involving cluster randomization trials: a review of published literature in health care.",1
"In cluster randomized trials, the study units usually are not a simple random sample from some clearly defined target population. Instead, the target population tends to be hypothetical or ill-defined, and the selection of study units tends to be systematic, driven by logistical and practical considerations. As a result, the population average treatment effect (PATE) may be neither well defined nor easily interpretable. In contrast, the sample average treatment effect (SATE) is the mean difference in the counterfactual outcomes for the study units. The sample parameter is easily interpretable and arguably the most relevant when the study units are not sampled from some specific super-population of interest. Furthermore, in most settings, the sample parameter will be estimated more efficiently than the population parameter. To the best of our knowledge, this is the first paper to propose using targeted maximum likelihood estimation (TMLE) for estimation and inference of the sample effect in trials with and without pair-matching. We study the asymptotic and finite sample properties of the TMLE for the sample effect and provide a conservative variance estimator. Finite sample simulations illustrate the potential gains in precision and power from selecting the sample effect as the target of inference. This work is motivated by the Sustainable East Africa Research in Community Health (SEARCH) study, a pair-matched, community randomized trial to estimate the effect of population-based HIV testing and streamlined ART on the 5-year cumulative HIV incidence (NCT01864603). The proposed methodology will be used in the primary analysis for the SEARCH trial. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Targeted estimation and inference for the sample average treatment effect in trials with and without pair-matching.",1
"We present statistical considerations for the design of the Community Intervention Trial for Smoking Cessation (COMMIT). One outcome measurement, the quit rate in randomly selected cohorts of smokers, is compared with another outcome measurement, the decrease in smoking prevalence, in terms of statistical efficiency and interpretability. The COMMIT study uses both types of outcome measurements. The merits of pair-matching the communities are considered, and sample size calculations take into account heterogeneity among pair-matched communities. In addition to significance tests based on the permutational (randomization) distribution, we also describe approaches for covariate adjustment. The COMMIT design includes 11 pair-matched communities, which should provide good power to detect a 10% or greater difference in quit rates between the intervention and control communities in cohorts of heavy smokers and in cohorts of light or moderate smokers. The power is only moderate to detect intervention effects on the decreases in overall smoking prevalence or in the prevalence of heavy smoking.-Aspects of statistical design for the Community Intervention Trial for Smoking Cessation (COMMIT).",1
This article proposes and evaluates a method to test for mediation in multilevel data sets formed when an intervention administered to intact groups is designed to produce change in individual mediator and outcome variables. Simulated data of this form were used to compare ordinary least squares (OLS) and two multilevel estimators of the mediated effect. OLS and multilevel standard error approximations were also evaluated and recommendations given for optimal estimator choice. These methods were applied to data from an existing substance use intervention to show the impact multilevel mediation modeling can have on the conclusions drawn from real-world evaluation studies.-Multilevel mediation modeling in group-based intervention studies.,1
"Multilevel data occur frequently in many research areas like health services research and epidemiology. A suitable way to analyze such data is through the use of multilevel regression models. These models incorporate cluster-specific random effects that allow one to partition the total variation in the outcome into between-cluster variation and between-individual variation. The magnitude of the effect of clustering provides a measure of the general contextual effect. When outcomes are binary or time-to-event in nature, the general contextual effect can be quantified by measures of heterogeneity like the median odds ratio or the median hazard ratio, respectively, which can be calculated from a multilevel regression model. Outcomes that are integer counts denoting the number of times that an event occurred are common in epidemiological and medical research. The median (incidence) rate ratio in multilevel Poisson regression for counts that corresponds to the median odds ratio or median hazard ratio for binary or time-to-event outcomes respectively is relatively unknown and is rarely used. The median rate ratio is the median relative change in the rate of the occurrence of the event when comparing identical subjects from 2 randomly selected different clusters that are ordered by rate. We also describe how the variance partition coefficient, which denotes the proportion of the variation in the outcome that is attributable to between-cluster differences, can be computed with count outcomes. We illustrate the application and interpretation of these measures in a case study analyzing the rate of hospital readmission in patients discharged from hospital with a diagnosis of heart failure.-Measures of clustering and heterogeneity in multilevel Poisson regression analyses of rates/count data.",1
"The chronic and relapsing nature of substance abuse points to the need for continuing care after a primary phase of treatment. This article reviews the economic studies of continuing care, discusses research gaps, highlights some of the challenges of conducting rigorous economic evaluations of continuing care, and offers research guidelines and recommendations for future economic studies in this emerging field. Rigorous economic evaluations are needed by health care providers and policy makers to justify the allocation of scarce resources to continuing care interventions. The adoption of cost-effective continuing care services can reduce long-term consequences of addiction, thereby potentially increasing overall social welfare.-Economic evaluation of continuing care interventions in the treatment of substance abuse: recommendations for future research.",0
"Cluster randomized trials (CRT) are often used to evaluate therapies or interventions in situations where individual randomization is not possible or not desirable for logistic, financial or ethical reasons. While a significant and rapidly growing body of literature exists on CRTs utilizing a ""parallel"" design (i.e. I clusters randomized to each treatment), only a few examples of CRTs using crossover designs have been described. In this article we discuss the design and analysis of a particular type of crossover CRT - the stepped wedge - and provide an example of its use.-Design and analysis of stepped wedge cluster randomized trials.",3
"Alcohol use disorder (AUD) is a chronic and relapsing condition for which current pharmacological treatments are only modestly effective. The development of efficacious medications for AUD remains a high research priority with recent emphasis on identifying novel molecular targets for AUD treatment and to efficiently screen new compounds aimed at those targets. Ibudilast, a phosphodiesterase inhibitor, has been advanced as a novel addiction pharmacotherapy that targets neurotrophin signaling and neuroimmune function. This study will conduct a 12-week, double-blind, placebo controlled randomized clinical trial of ibudilast (50 mg BID) for AUD treatment. We will randomize 132 treatment-seeking men and women with current AUD. We will collect a number of alcohol consumption outcomes. Primary among these is percent heavy drinking days (PHDD); secondary drinking outcomes include drinks per day, drinks per drinking day, percent days abstinent, percent subjects with no heavy drinking days, and percent subjects abstinent, as well as measures of alcohol craving and negative mood. Additionally, participants will have the option to opt-in to a neuroimaging session in which we examine the effects of ibudilast on neural activation to psychosocial stress and alcohol cues. Finally, we will also collect plasma levels of proinflammatory markers, as well as subjective and biological (salivary cortisol) markers of stress response. This study will further develop ibudilast, a safe and promising novel compound with strong preclinical and clinical safety data for AUD, and will probe biological mechanisms underlying the effects of Ibudilast on stress, neuroinflammation, and alcohol cue-reactivity and craving. If ibudilast proves superior to placebo in this study, it will set the stage for a confirmatory multi-site trial leading to FDA approval of a novel AUD treatment. ClinicalTrials.gov NCT03594435 ""Ibudilast for the Treatment of Alcohol Use Disorder"". Registered on 20 July 2018.-Ibudilast for alcohol use disorder: study protocol for a phase II randomized clinical trial.",0
"The Northwestern University Center for Education and Research on Therapeutics (CERT), funded by the Agency for Healthcare Research and Quality, is one of seven such centers in the USA. The thematic focus of the Northwestern CERT is 'Tools for Optimizing Medication Safety.' Ensuring drug safety is essential, as many adults struggle to take medications, with estimates indicating that only half of adults take drugs as prescribed. This report describes the methods and rationale for one innovative project within the CERT: the 'Primary Care, Electronic Health Record-Based Strategy to Promote Safe and Appropriate Drug Use'. The overall objective of this 5-year study is to evaluate a health literacy-informed, electronic health record-based strategy for promoting safe and effective prescription medication use in a primary care setting. A total of 600 English and Spanish-speaking patients with diabetes will be consecutively recruited to participate in the study. Patients will be randomized to receive either usual care or the intervention; those in the intervention arm will receive a set of print materials designed to support medication use and prompt provider counseling and medication reconciliation. Participants will be interviewed in person after their index clinic visit and again one month later. Process outcomes related to intervention delivery will be recorded. A medical chart review will be performed at 6 months. Patient outcome measures include medication understanding, adherence and clinical measures (hemoglobin A1c, blood pressure, and cholesterol; exploratory outcomes only). Through this study, we will be able to examine the impact of a health literacy-informed, electronic health record-based strategy on medication understanding and adherence among diabetic primary care patients. The measurement of process outcomes will help inform how the strategy might ultimately be refined and disseminated to other sites. Strategies such as these are needed to address the multifaceted challenges related to medication self-management among patients with chronic conditions. Clinicaltrials.gov NCT01669473.-A primary care, electronic health record-based strategy to promote safe drug use: study protocol for a randomized controlled trial.",0
"de Boer and colleagues present a cost-effectiveness analysis based in the Netherlands of two vaccines available for the prevention of herpes zoster. Zostavax? was the first vaccine available for the prevention of herpes zoster in older adults. A live-attenuated vaccine, Zostavax is not free of limitations, which include a relatively low efficacy that wanes over time and its contraindication among immunocompromised individuals. The recently available adjuvanted herpes zoster subunit vaccine Shingrix? overcomes some of these limitations. The herpes zoster subunit vaccine is more efficacious than Zostavax, and it can be administered to immunosuppressed individuals. However, the herpes zoster subunit vaccine is considerably costlier and requires a booster injection. In order to clarify the value of each vaccine, de Boer and colleagues compare the cost-effectiveness of no vaccination, and of vaccination with Zostavax or the herpes zoster subunit vaccine in four cohorts of older adults from the perspective of the Netherlands. Whereas neither vaccine was cost-effective under the willingness-to-pay threshold of ?20,000 per quality-adjusted life year, the authors find the herpes zoster subunit vaccine to be cost-effective in some scenarios under a ?50,000 per quality-adjusted life year threshold.Please see related article: https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-018-1213-5.-Avoiding rash decisions about zoster vaccination: insights from cost-effectiveness evidence.",0
"Shrinkage-type variable selection procedures have recently seen increasing applications in biomedical research. However, their performance can be adversely influenced by outliers in either the response or the covariate space. This article proposes a weighted Wilcoxon-type smoothly clipped absolute deviation (WW-SCAD) method, which deals with robust variable selection and robust estimation simultaneously. The new procedure can be conveniently implemented with the statistical software R. We establish that the WW-SCAD correctly identifies the set of zero coefficients with probability approaching one and estimates the nonzero coefficients with the rate n(-1/2). Moreover, with appropriately chosen weights the WW-SCAD is robust with respect to outliers in both the x and y directions. The important special case with constant weights yields an oracle-type estimator with high efficiency in the presence of heavier-tailed random errors. The robustness of the WW-SCAD is partly justified by its asymptotic performance under local shrinking contamination. We propose a Bayesian information criterion type tuning parameter selector for the WW-SCAD. The performance of the WW-SCAD is demonstrated via simulations and by an application to a study that investigates the effects of personal characteristics and dietary factors on plasma beta-carotene level.-Weighted Wilcoxon-type smoothly clipped absolute deviation method.",0
"In clinical studies with time-to-event outcomes, the restricted mean survival time (RMST) has attracted substantial attention as a summary measurement for its straightforward clinical interpretation. When the data are subject to length-biased sampling, which is frequently encountered in observational cohort studies, existing methods to estimate the RMST are not applicable. In this article, we consider nonparametric and semiparametric regression methods to estimate the RMST under the setting of length-biased sampling. To assess the covariate effects on the RMST, a semiparametric regression model that directly relates the covariates and the RMST is assumed. Based on the model, we develop unbiased estimating equations to obtain consistent estimators of covariate effects by properly adjusting for informative censoring and length bias. Stochastic process theories are used to establish the asymptotic properties of the proposed estimators. We investigate the finite sample performance through simulations and illustrate the methods by analyzing a prevalent cohort study of dementia in Canada.-Analysis of restricted mean survival time for length-biased data.",0
"Heterogeneity of effect measures in intervention studies undermines the use of evidence to inform policy. Our objective was to develop a comprehensive algorithm to convert all types of effect measures to one standard metric, relative risk reduction (RRR). This work was conducted to facilitate synthesis of published intervention effects for our epidemic modeling of the health impact of human immunodeficiency virus [HIV testing and counseling (HTC)]. We designed and implemented an algorithm to transform varied effect measures to RRR, representing the proportionate reduction in undesirable outcomes. Our extraction of 55 HTC studies identified 473 effect measures representing unique combinations of intervention-outcome-population characteristics, using five outcome metrics: pre-post proportion (70.6%), odds ratio (14.0%), mean difference (10.2%), risk ratio (4.4%), and RRR (0.9%). Outcomes were expressed as both desirable (29.5%, eg, consistent condom use) and undesirable (70.5%, eg, inconsistent condom use). Using four examples, we demonstrate our algorithm for converting varied effect measures to RRR and provide the conceptual basis for advantages of RRR over other metrics. Our review of the literature suggests that RRR, an easily understood and useful metric to convey risk reduction associated with an intervention, is underused by original and review studies.-Relative risk reduction is useful metric to standardize effect size for public heath interventions for translational research.",0
"Subgroup identification (clustering) is an important problem in biomedical research. Gene expression profiles are commonly utilized to define subgroups. Longitudinal gene expression profiles might provide additional information on disease progression than what is captured by baseline profiles alone. Therefore, subgroup identification could be more accurate and effective with the aid of longitudinal gene expression data. However, existing statistical methods are unable to fully utilize these data for patient clustering. In this article, we introduce a novel clustering method in the Bayesian setting based on longitudinal gene expression profiles. This method, called BClustLonG, adopts a linear mixed-effects framework to model the trajectory of genes over time, while clustering is jointly conducted based on the regression coefficients obtained from all genes. In order to account for the correlations among genes and alleviate the high dimensionality challenges, we adopt a factor analysis model for the regression coefficients. The Dirichlet process prior distribution is utilized for the means of the regression coefficients to induce clustering. Through extensive simulation studies, we show that BClustLonG has improved performance over other clustering methods. When applied to a dataset of severely injured (burn or trauma) patients, our model is able to identify interesting subgroups. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-A Dirichlet process mixture model for clustering longitudinal gene expression data.",0
"To determine if pre-eclampsia is associated with reduced thyroid function during and after pregnancy. Nested case-control study during pregnancy and population based follow-up study after pregnancy. Calcium for Pre-eclampsia Prevention trial of healthy pregnant nulliparous women in the United States during 1992-5, and a Norwegian population based study (Nord-Trondelag Health Study or HUNT-2) during 1995-7 with linkage to the medical birth registry of Norway. All 141 women (cases) in the Calcium for Pre-eclampsia Prevention trial with serum measurements before 21 weeks' gestation (baseline) and after onset of pre-eclampsia (before delivery), 141 normotensive controls with serum measurements at similar gestational ages, and 7121 women in the Nord-Trondelag Health Study whose first birth had occurred in 1967 or later and in whom serum levels of thyroid stimulating hormone had been subsequently measured. Thyroid function tests and human chorionic gonadotrophin and soluble fms-like tyrosine kinase 1 concentrations in the Calcium for Pre-eclampsia Prevention cohort and odds ratios for levels of thyroid stimulating hormone above the reference range, according to pre-eclampsia status in singleton pregnancies before the Nord-Trondelag Health Study. In predelivery specimens of the Calcium for Pre-eclampsia Prevention cohort after the onset of pre-eclampsia, thyroid stimulating hormone levels increased 2.42 times above baseline compared with a 1.48 times increase in controls. The ratio of the predelivery to baseline ratio of cases to that of the controls was 1.64 (95% confidence interval 1.29 to 2.08). Free triiodothyronine decreased more in the women with pre-eclampsia than in the controls (case ratio to control ratio 0.96, 95% confidence interval 0.92 to 0.99). The predelivery specimens but not baseline samples from women with pre-eclampsia were significantly more likely than those from controls to have concentrations of thyroid stimulating hormone above the reference range (adjusted odds ratio 2.2, 95% confidence interval 1.1 to 4.4). Both in women who developed pre-eclampsia and in normotensive controls the increase in thyroid stimulating hormone concentration between baseline and predelivery specimens was strongly associated with increasing quarters of predelivery soluble fms-like tyrosine kinase 1 (P for trend 0.002 and &lt;0.001, respectively). In the Nord-Trondelag Health Study, women with a history of pre-eclampsia in their first pregnancy were more likely than other women (adjusted odds ratio 1.7, 95% confidence interval 1.1 to 2.5) to have concentrations of thyroid stimulating hormone above the reference range (&gt;3.5 mIU/l). In particular, they were more likely to have high concentrations of thyroid stimulating hormone without thyroid peroxidase antibodies (adjusted odds ratio 2.6, 95% confidence interval 1.3 to 5.0), suggesting hypothyroid function in the absence of an autoimmune process. This association was especially strong (5.8, 1.3 to 25.5) if pre-eclampsia had occurred in both the first and the second pregnancies. Increased serum concentration of soluble fms-like tyrosine kinase 1 during pre-eclampsia is associated with subclinical hypothyroidism during pregnancy. Pre-eclampsia may also predispose to reduced thyroid function in later years.-Pre-eclampsia, soluble fms-like tyrosine kinase 1, and the risk of reduced thyroid function: nested case-control and population based study.",0
"Public health interventions are often designed to target communities defined either geographically (e.g. cities, counties) or socially (e.g. schools or workplaces). The group randomized trial (GRT) is regarded as the gold standard for evaluating these interventions. However, community leaders may object to randomization as some groups may be denied a potentially beneficial intervention. Under a regression discontinuity design (RDD), individuals may be assigned to treatment based on the levels of a pretest measure, thereby allowing those most in need of the treatment to receive it. In this article, we consider analysis, power, and sample size issues in applying the RDD and related cutoff designs in community-based intervention studies. We examine the power of these designs as a function of intraclass correlation, number of groups, and number of members per group and compare results to the traditional GRT.-Cutoff designs for community-based intervention studies.",1
"Methods to develop core outcome sets, the minimum outcomes that should be measured in research in a topic area, vary. We applied social network analysis methods to understand outcome co-occurrence patterns in human immunodeficiency virus (HIV)/AIDS systematic reviews and identify outcomes central to the network of outcomes in HIV/AIDS. We examined all Cochrane reviews of HIV/AIDS as of June 2013. We defined a tie as two outcomes (nodes)?co-occurring in ?2 reviews. To identify central outcomes, we used normalized node betweenness centrality (nNBC) (the extent to which connections between other outcomes in a network rely on that outcome as an intermediary). We conducted a subgroup analysis by HIV/AIDS intervention type (i.e., clinical management, biomedical prevention, behavioral prevention, and health services). The 140 included reviews examined 1,140 outcomes, 294 of which were unique. The most central outcome overall was all-cause mortality (nNBC?=?23.9). The most central and most frequent outcomes differed overall and within subgroups. For example, ""adverse events (specified)"" was among the most central but not among the most frequent outcomes, overall. Social network analysis methods are a novel application to identify central outcomes, which provides additional information potentially useful for developing core outcome sets.-Social network analysis identified central outcomes for core outcome sets using systematic reviews of HIV/AIDS.",0
"Motivated by a study for soft tissue sarcoma, this article considers the analysis of diseases recurrence and survival. A multivariate frailty hazard model is established for joint modeling of three correlated time-to-event outcomes: local disease recurrence, distant disease recurrence (metastasis), and death. The goals are to find out (i) the effects of treatments on local and distant disease recurrences, and death, (ii) the effects of local and distant disease recurrences on death, and (iii) the correlation between local and distant recurrences. By our approach, all these three important questions, which are commonly asked in similar medical research studies, can be answered by a single model. We put the proposed joint frailty model in a Bayesian framework and use a hybrid Monte Carlo algorithm for the computation of posterior distributions. This hybrid algorithm relies on the evaluation of the gradient of target log density and a guided walk progress, and it combines these two strategies to suppress random walk behavior. A further distinction is that the hybrid algorithm can update all the components of a multivariate state vector simultaneously. Simulation studies are conducted to assess the proposed joint frailty model and the computation algorithm. The motivating soft tissue sarcoma data set is analyzed for illustration purpose. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-A Bayesian multivariate joint frailty model for disease recurrences and survival.",0
"We examine a class of multivariate meta-regression models in the presence of individual patient data. The methodology is well motivated from several studies of cholesterol-lowering drugs where the goal is to jointly analyze the multivariate outcomes, low density lipoprotein cholesterol, high density lipoprotein cholesterol, and triglycerides. These three continuous outcome measures are correlated and shed much light on a subject's lipid status. One of the main goals in lipid research is the joint analysis of these three outcome measures in a meta-regression setting. Since these outcome measures are not typically multivariate normal, one must consider classes of distributions that allow for skewness in one or more of the outcomes. In this paper, we consider a new general class of multivariate skew distributions for multivariate meta-regression and examine their theoretical properties. Using these distributions, we construct a Bayesian model for the meta-data and develop an efficient Markov chain Monte Carlo computational scheme for carrying out the computations. In addition, we develop a multivariate L measure for model comparison, Bayesian residuals for model assessment, and a Bayesian procedure for detecting outlying trials. The proposed multivariate L measure, Bayesian residuals, and Bayesian outlying trial detection procedure are particularly suitable and computationally attractive in the multivariate meta-regression setting. A detailed case study demonstrating the usefulness of the proposed methodology is carried out in an individual patient data multivariate meta-regression setting using 26 pivotal Merck clinical trials that compare statins (cholesterol-lowering drugs) in combination with ezetimibe and statins alone on treatment-na?ve patients and those continuing on statins at baseline.-Bayesian multivariate skew meta-regression models for individual patient data.",0
"Outcome reporting bias (ORB) is recognized as a threat to the validity of both pairwise and network meta-analysis (NMA). In recent years, multivariate meta-analytic methods have been proposed to reduce the impact of ORB in the pairwise setting. These methods have shown that multivariate meta-analysis can reduce bias and increase efficiency of pooled effect sizes. However, it is unknown whether multivariate NMA (MNMA) can similarly reduce the impact of ORB. Additionally, it is quite challenging to implement MNMA due to the fact that correlation between treatments and outcomes must be modeled; thus, the dimension of the covariance matrix and number of components to estimate grows quickly with the number of treatments and number of outcomes. To determine whether MNMA can reduce the effects of ORB on pooled treatment effect sizes, we present an extensive simulation study of Bayesian MNMA. Via simulation studies, we show that MNMA reduces the bias of pooled effect sizes under a variety of outcome missingness scenarios, including missing at random and missing not at random. Further, MNMA improves the precision of estimates, producing narrower credible intervals. We demonstrate the applicability of the approach via application of MNMA to a multi-treatment systematic review of randomized controlled trials of anti-depressants for the treatment of depression in older adults.-Multivariate network meta-analysis to mitigate the effects of outcome reporting bias.",0
"Physicians have an important role addressing the obesity epidemic. Lack of adequate teaching to provide weight management counseling (WMC) is cited as a reason for limited treatment. National guidelines have not been translated into an evidence-supported, competency-based curriculum in medical schools. Weight Management Counseling in Medical Schools: A Randomized Controlled Trial (MSWeight) is designed to determine if a multi-modal theoretically-guided WMC educational intervention improves observed counseling skills and secondarily improve perceived skills and self-efficacy among medical students compared to traditional education (TE). Eight U.S. medical schools were pair-matched and randomized in a group randomized controlled trial to evaluate whether a multi-modal education (MME) intervention compared to traditional education (TE) improves observed WMC skills. The MME intervention includes innovative components in years 1-3: a structured web-course; a role play exercise, WebPatientEncounter, and an enhanced outpatient internal medicine or family medicine clerkship. This evidence-supported curriculum uses the 5As framework to guide treatment and incorporates patient-centered counseling to engage the patient. The primary outcome is a comparison of scores on an Objective Structured Clinical Examination (OSCE) WMC case among third year medical students. The secondary outcome compares changes in scores of medical students from their first to third year on an assessment of perceived WMC skills and self-efficacy. MSWeight is the first RCT in medical schools to evaluate whether interventions integrated into the curriculum improve medical students' WMC skills. If this educational approach for teaching WMC is effective, feasible and acceptable it can affect how medical schools integrate WMC teaching into their curriculum.-Design and rationale of the medical students learning weight management counseling skills (MSWeight) group randomized controlled trial.",0
"Controlling hypertension rates and maintaining normal blood pressure, particularly in resource-constrained settings, represent ongoing challenges of effective and affordable implementation in health care. One of the strategies being largely advocated to improve high blood pressure calls for salt reduction strategies. This study aims to estimate the impact of a population-level intervention based on sodium reduction and potassium increase - in practice, introducing a low-sodium, high-potassium salt substitute - on adult blood pressure levels. The proposed implementation research study includes two components: Phase 1, an exploratory component, and Phase 2, an intervention component. The exploratory component involves a triangle taste test and a formative research study designed to gain an understanding of the best implementation methods. Phase 2 involves a pragmatic stepped wedge trial design where the intervention will be progressively implemented in several clusters starting the intervention randomly at different times. In addition, we will evaluate the implementation strategy using a cost-effectiveness analysis. This is the first project in a Latin-American setting to implement a salt substitution intervention at the population level to tackle high blood pressure. Data generated and lessons learnt from this study will provide a strong platform to address potential interventions applicable to other similar low- and middle-income settings. This study is registered in ClinicalTrials.gov NCT01960972.-Launching a salt substitute to reduce blood pressure at the population level: a cluster randomized stepped wedge trial in Peru.",0
"The statistical analysis of continuous longitudinal data may be complicated since quantitative levels of bioassay cannot always be determined. Values beyond the limits of detection (LOD) in the assays may not be observed and thus censored, rendering complexity to the analysis of such data. This article examines how both left-censoring and right censoring of HIV-1 plasma RNA measurements, collected for the study on AIDS-related Non-Hodgkin's lymphoma (AR-NHL) in East Africa, affects the quantification of viral load and explores the natural history of viral load measurements over time in AR-NHL patients receiving anticancer chemotherapy. Data analyses using Monte Carlo EM algorithm (MCEM) are compared to analyses where the LOD or LOD/2 (left censoring) value is substituted for the censored observations, and also to other methods such as multiple imputation, and maximum likelihood estimation for censored data (generalized Tobit regression). Simulations are used to explore the sensitivity of the results to changes in the model parameters. In conclusion, the antiretroviral treatment was associated with a significant decrease in viral load after controlling the effects of other covariates. A simulation study with finite sample size shows MCEM is the least biased method and the estimates are least sensitive to the censoring mechanism.-A comparative investigation of methods for longitudinal data with limits of detection through a case study.",0
"In many experiments, researchers would like to compare between treatments and outcome that only exists in a subset of participants selected after randomization. For example, in preventive HIV vaccine efficacy trials it is of interest to determine whether randomization to vaccine causes lower HIV viral load, a quantity that only exists in participants who acquire HIV. To make a causal comparison and account for potential selection bias we propose a sensitivity analysis following the principal stratification framework set forth by Frangakis and Rubin (2002, Biometrics58, 21-29). Our goal is to assess the average causal effect of treatment assignment on viral load at a given baseline covariate level in the always infected principal stratum (those who would have been infected whether they had been assigned to vaccine or placebo). We assume stable unit treatment values (SUTVA), randomization, and that subjects randomized to the vaccine arm who became infected would also have become infected if randomized to the placebo arm (monotonicity). It is not known which of those subjects infected in the placebo arm are in the always infected principal stratum, but this can be modeled conditional on covariates, the observed viral load, and a specified sensitivity parameter. Under parametric regression models for viral load, we obtain maximum likelihood estimates of the average causal effect conditional on covariates and the sensitivity parameter. We apply our methods to the world's first phase III HIV vaccine trial.-Sensitivity analyses comparing outcomes only existing in a subset selected post-randomization, conditional on covariates, with application to HIV vaccine trials.",0
"Over the past two decades, the lack of reliable empirical evidence concerning the effectiveness of educational interventions has motivated a new wave of research in education in sub-Saharan Africa (and across most of the world) that focuses on impact evaluation through rigorous research designs such as experiments. Often these experiments draw on the random assignment of entire clusters, such as schools, to accommodate the multilevel structure of schooling and the theory of action underlying many school-based interventions. Planning effective and efficient school randomized studies, however, requires plausible values of the intraclass correlation coefficient (ICC) and the variance explained by covariates during the design stage. The purpose of this study was to improve the planning of two-level school-randomized studies in sub-Saharan Africa by providing empirical estimates of the ICC and the variance explained by covariates for education outcomes in 15 countries. Our investigation drew on large-scale representative samples of sixth-grade students in 15 countries in sub-Saharan Africa and includes over 60,000 students across 2,500 schools. We examined two core education outcomes: standardized achievement in reading and mathematics. We estimated a series of two-level hierarchical linear models with students nested within schools to inform the design of two-level school-randomized trials. The analyses suggested that outcomes were substantially clustered within schools but that the magnitude of the clustering varied considerably across countries. Similarly, the results indicated that covariance adjustment generally reduced clustering but that the prognostic value of such adjustment varied across countries.-Intraclass Correlation Coefficients for Designing Cluster-Randomized Trials in Sub-Saharan Africa Education.",1
"Re: ""Mortality among Canadian women with cosmetic breast implants"".",0
"High-dimensional longitudinal data involving latent variables such as depression and anxiety that cannot be quantified directly are often encountered in biomedical and social sciences. Multiple responses are used to characterize these latent quantities, and repeated measures are collected to capture their trends over time. Furthermore, substantive research questions may concern issues such as interrelated trends among latent variables that can only be addressed by modeling them jointly. Although statistical analysis of univariate longitudinal data has been well developed, methods for modeling multivariate high-dimensional longitudinal data are still under development. In this paper, we propose a latent factor linear mixed model (LFLMM) for analyzing this type of data. This model is a combination of the factor analysis and multivariate linear mixed models. Under this modeling framework, we reduced the high-dimensional responses to low-dimensional latent factors by the factor analysis model, and then we used the multivariate linear mixed model to study the longitudinal trends of these latent factors. We developed an expectation-maximization algorithm to estimate the model. We used simulation studies to investigate the computational properties of the expectation-maximization algorithm and compare the LFLMM model with other approaches for high-dimensional longitudinal data analysis. We used a real data example to illustrate the practical usefulness of the model.-A latent factor linear mixed model for high-dimensional longitudinal data analysis.",0
"To evaluate the associations between maternal diabetes diagnosed before or during pregnancy and early onset cardiovascular disease (CVD) in offspring during their first four decades of life. Population based cohort study. Danish national health registries. All 2 432 000 liveborn children without congenital heart disease in Denmark during 1977-2016. Follow-up began at birth and continued until first time diagnosis of CVD, death, emigration, or 31 December 2016, whichever came first. Pregestational diabetes, including type 1 diabetes (n=22 055) and type 2 diabetes (n=6537), and gestational diabetes (n=26 272). The primary outcome was early onset CVD (excluding congenital heart diseases) defined by hospital diagnosis. Associations between maternal diabetes and risks of early onset CVD in offspring were studied. Cox regression was used to assess whether a maternal history of CVD or maternal diabetic complications affected these associations. Adjustments were made for calendar year, sex, singleton status, maternal factors (parity, age, smoking, education, cohabitation, residence at childbirth, history of CVD before childbirth), and paternal history of CVD before childbirth. The cumulative incidence was averaged across all individuals, and factors were adjusted while treating deaths from causes other than CVD as competing events. During up to 40 years of follow-up, 1153 offspring of mothers with diabetes and 91 311 offspring of mothers who did not have diabetes were diagnosed with CVD. Offspring of mothers with diabetes had a 29% increased overall rate of early onset CVD (hazard ratio 1.29 (95% confidence interval 1.21 to 1.37); cumulative incidence among offspring unexposed to maternal diabetes at 40 years of age 13.07% (12.92% to 13.21%), difference in cumulative incidence between exposed and unexposed offspring 4.72% (2.37% to 7.06%)). The sibship design yielded results similar to those of the unpaired design based on the whole cohort. Both pregestational diabetes (1.34 (1.25 to 1.43)) and gestational diabetes (1.19 (1.07 to 1.32)) were associated with increased rates of CVD in offspring. We also observed varied increased rates of specific early onset CVDs, particularly heart failure (1.45 (0.89 to 2.35)), hypertensive disease (1.78 (1.50 to 2.11)), deep vein thrombosis (1.82 (1.38 to 2.41)), and pulmonary embolism (1.91 (1.31 to 2.80)). Increased rates of CVD were seen in different age groups from childhood to early adulthood until age 40 years. The increased rates were more pronounced among offspring of mothers with diabetic complications (1.60 (1.25 to 2.05)). A higher incidence of early onset CVD in offspring of mothers with diabetes and comorbid CVD (1.73 (1.36 to 2.20)) was associated with the added influence of comorbid CVD but not due to the interaction between diabetes and CVD on the multiplicative scale (P value for interaction 0.94). Children of mothers with diabetes, especially those mothers with a history of CVD or diabetic complications, have increased rates of early onset CVD from childhood to early adulthood. If maternal diabetes does have a causal association with increased CVD rate in offspring, the prevention, screening, and treatment of diabetes in women of childbearing age could help to reduce the risk of CVD in the next generation.-Maternal diabetes during pregnancy and early onset of cardiovascular disease in offspring: population based cohort study with 40 years of follow-up.",0
"An accelerometer, a wearable motion sensor on the hip or wrist, is becoming a popular tool in clinical and epidemiological studies for measuring the physical activity. Such data provide a series of activity counts at every minute or even more often and displays a person's activity pattern throughout a day. Unfortunately, the collected data can include irregular missing intervals because of noncompliance of participants and therefore make the statistical analysis more challenging. The purpose of this study is to develop a novel imputation method to handle the multivariate count data, motivated by the accelerometer data structure. We specify the predictive distribution of the missing data with a mixture of zero-inflated Poisson and Log-normal distribution, which is shown to be effective to deal with the minute-by-minute autocorrelation as well as under- and over-dispersion of count data. The imputation is performed at the minute level and follows the principles of multiple imputation using a fully conditional specification with the chained algorithm. To facilitate the practical use of this method, we provide an R package accelmissing. Our method is demonstrated using 2003-2004 National Health and Nutrition Examination Survey data.-Missing value imputation for physical activity data measured by accelerometer.",0
"Group-randomized trials often involve repeat observations on the same participants. When there are no more than two observations from each participant, standard mixed-model regression methods for a pretest-posttest design can be used. When there are more than two observations from each participant, random coefficients models may be useful. This paper describes the random coefficients analysis appropriate to data from an extended nested cohort design and presents the methods for power analysis and sample size calculations based on that analysis. We provide estimates for the parameters required for those calculations for a number of adolescent health behaviours. We also show how the estimates can be used to plan a future trial.-Sizing a trial to alter the trajectory of health behaviours: methods, parameter estimates, and their application.",1
"Network meta-analysis is becoming a common approach to combine direct and indirect comparisons of several treatment arms. In recent research, there have been various developments and extensions of the standard methodology. Simultaneously, cluster randomized trials are experiencing an increased popularity, especially in the field of health services research, where, for example, medical practices are the units of randomization but the outcome is measured at the patient level. Combination of the results of cluster randomized trials is challenging. In this tutorial, we examine and compare different approaches for the incorporation of cluster randomized trials in a (network) meta-analysis. Furthermore, we provide practical insight on the implementation of the models. In simulation studies, it is shown that some of the examined approaches lead to unsatisfying results. However, there are alternatives which are suitable to combine cluster randomized trials in a network meta-analysis as they are unbiased and reach accurate coverage rates. In conclusion, the methodology can be extended in such a way that an adequate inclusion of the results obtained in cluster randomized trials becomes feasible. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Bayesian network meta-analysis for cluster randomized trials with binary outcomes.",1
"False-positive test results are among the most common harms of screening tests and may lead to more invasive and expensive diagnostic testing procedures. Estimating the cumulative risk of a false-positive screening test result after repeat screening rounds is, therefore, important for evaluating potential screening regimens. Existing estimators of the cumulative false-positive risk are limited by strong assumptions about censoring mechanisms and parametric assumptions about variation in risk across screening rounds. To address these limitations, we propose a semiparametric censoring bias model for cumulative false-positive risk that allows for dependent censoring without specifying a fixed functional form for variation in risk across screening rounds. Simulation studies demonstrated that the censoring bias model performs similarly to existing models under independent censoring and can largely eliminate bias under dependent censoring. We used the existing and newly proposed models to estimate the cumulative false-positive risk and variation in risk as a function of baseline age and family history of breast cancer after 10 years of annual screening mammography using data from the Breast Cancer Surveillance Consortium. Ignoring potential dependent censoring in this context leads to underestimation of the cumulative risk of false-positive results. Models that provide accurate estimates under dependent censoring are critical for providing appropriate information for evaluating screening tests.-A semiparametric censoring bias model for estimating the cumulative risk of a false-positive screening test under dependent censoring.",0
"To conduct a longitudinal evaluation of Patient-Reported Outcomes Measurement Information System (PROMIS) social function measures (satisfaction with participation in social roles and satisfaction with participation in discretionary social activities) in English-speaking people with chronic health conditions. Adults receiving treatment for chronic heart failure (CHF), chronic obstructive pulmonary disease (COPD), chronic back pain, or depression completed PROMIS computer-based measures of social health at two time points approximately 3?months apart and global ratings of change. Linear mixed effects models and standardized response means were estimated for the two social function measures. A total of 599 people participated: 79 with stable COPD, 46 COPD exacerbation, 60 with CHF, 196 with depression, and 218 with back pain. Four groups experienced improvement over time, one (COPD stable) changed very little. Those who reported better global ratings of change in overall health experienced larger changes in social function than those who reported the same or worse global health. This study provided support for responsiveness to change for two PROMIS social function measures. These results provide further evidence of the PROMIS goal to enable comparable measurement of universally relevant symptoms and experiences that apply to people with a variety of diseases.-The PROMIS satisfaction with social participation measures demonstrated responsiveness in diverse clinical populations.",0
"Power considerations for trials evaluating vaccines against infectious diseases are complicated by indirect protective effects of vaccination. While cluster-randomized controlled trials (cRCTs) are less statistically efficient than individually randomized controlled trials (iRCTs), a cRCT's ability to measure direct and indirect vaccine effects may mitigate the loss of efficiency due to clustering. Within cRCTs, the number and size of clusters affects 3 determinants of power: the effect size being measured, disease incidence, and intracluster correlation. We simulated trials conducted in a collection of small communities to assess how indirect protection and clustering affected the power of cRCTs and iRCTs during an emerging epidemic. Across diverse parameters, we found that within the same trial population, cRCTs were never more powerful than iRCTs, although the difference can be small. We also identified 2 effects that attenuated the loss of cRCT power traditionally associated with increased cluster size. First, if enrollment of fewer, larger clusters was performed to achieve higher vaccine coverage within vaccinated communities, this increased the effect to be measured and, consequently, power. Second, the greater rate of imported transmission in larger communities may increase the attack rate and similarly mitigate loss of power relative to a trial in many, smaller communities.-Competing Effects Of Indirect Protection And Clustering On The Power Of Cluster-Randomized Controlled Vaccine Trials.",1
"The field of neuroimaging dedicated to mapping connections in the brain is increasingly being recognized as key for understanding neurodevelopment and pathology. Networks of these connections are quantitatively represented using complex structures, including matrices, functions, and graphs, which require specialized statistical techniques for estimation and inference about developmental and disorder-related changes. Unfortunately, classical statistical testing procedures are not well suited to high-dimensional testing problems. In the context of global or regional tests for differences in neuroimaging data, traditional analysis of variance (ANOVA) is not directly applicable without first summarizing the data into univariate or low-dimensional features, a process that might mask the salient features of high-dimensional distributions. In this work, we consider a general framework for two-sample testing of complex structures by studying generalized within-group and between-group variances based on distances between complex and potentially high-dimensional observations. We derive an asymptotic approximation to the null distribution of the ANOVA test statistic, and conduct simulation studies with scalar and graph outcomes to study finite sample properties of the test. Finally, we apply our test to our motivating study of structural connectivity in autism spectrum disorder.-Distance-based analysis of variance for brain connectivity.",0
"This paper investigates the semiparametric statistical methods for recurrent events. The mean number of the recurrent events are modeled with the generalized semiparametric varying-coefficient model that can flexibly model three types of covariate effects: time-constant effects, time-varying effects, and covariate-varying effects. We assume that the time-varying effects are unspecified functions of time and the covariate-varying effects are parametric functions of an exposure variable specified up to a finite number of unknown parameters. Different link functions can be selected to provide a rich family of models for recurrent events data. The profile estimation methods are developed for the parametric and nonparametric components. The asymptotic properties are established. We also develop some hypothesis testing procedures to test validity of the parametric forms of covariate-varying effects. The simulation study shows that both estimation and hypothesis testing procedures perform well. The proposed method is applied to analyze a data set from an acyclovir study and investigate whether acyclovir treatment reduces the mean relapse recurrences.-Semiparametric varying-coefficient regression analysis of recurrent events with applications to treatment switching.",0
"Interventions in infectious diseases can have both direct effects on individuals who receive the intervention as well as indirect effects in the population. In addition, intervention combinations can have complex interactions at the population level, which are often difficult to adequately assess with standard study designs and analytical methods. Herein, we urge the adoption of a new paradigm for the design and interpretation of intervention trials in infectious diseases, particularly with regard to emerging infectious diseases, one that more accurately reflects the dynamics of the transmission process. In an increasingly complex world, simulations can explicitly represent transmission dynamics, which are critical for proper trial design and interpretation. Certain ethical aspects of a trial can also be quantified using simulations. Further, after a trial has been conducted, simulations can be used to explore the possible explanations for the observed effects. Much is to be gained through a multidisciplinary approach that builds collaborations among experts in infectious disease dynamics, epidemiology, statistical science, economics, simulation methods, and the conduct of clinical trials.-Simulations for designing and interpreting intervention trials in infectious diseases.",0
"The use of stepped wedge designs in cluster-randomized trials and implementation studies has increased rapidly in recent years but there remains considerable debate regarding the merits of the design. We discuss three key issues in the design and analysis of stepped wedge trials - time-on-treatment effects, treatment effect heterogeneity and cohort studies.-Current issues in the design and analysis of stepped wedge trials.",3
"Studies in which clusters of individuals are randomized to conditions are increasingly common in public health research. However, the designs utilized for such studies are often suboptimal and inefficient. We review strategies to improve the design of cluster randomized trials. We discuss both older but effective design concepts that are underutilized, such as stratification and factorial designs, as well as emergent ideas including fractional factorial designs and cluster randomized crossover studies. We draw examples from the recent literature and provide resources for sample size and power planning. Given the inherent inefficiencies of cluster randomized trials, these design strategies merit wider consideration and can lead to studies that are more cost-effective and potentially more rigorous than traditional approaches.-Improved Designs for Cluster Randomized Trials.",1
"Cluster randomized trials (CRTs) randomize participants in groups, rather than as individuals and are key tools used to assess interventions in health research where treatment contamination is likely or if individual randomization is not feasible. Two potential major pitfalls exist regarding CRTs, namely handling missing data and not accounting for clustering in the primary analysis. The aim of this review was to evaluate approaches for handling missing data and statistical analysis with respect to the primary outcome in CRTs. We systematically searched for CRTs published between August 2013 and July 2014 using PubMed, Web of Science, and PsycINFO. For each trial, two independent reviewers assessed the extent of the missing data and method(s) used for handling missing data in the primary and sensitivity analyses. We evaluated the primary analysis and determined whether it was at the cluster or individual level. Of the 86 included CRTs, 80 (93%) trials reported some missing outcome data. Of those reporting missing data, the median percent of individuals with a missing outcome was 19% (range 0.5 to 90%). The most common way to handle missing data in the primary analysis was complete case analysis (44, 55%), whereas 18 (22%) used mixed models, six (8%) used single imputation, four (5%) used unweighted generalized estimating equations, and two (2%) used multiple imputation. Fourteen (16%) trials reported a sensitivity analysis for missing data, but most assumed the same missing data mechanism as in the primary analysis. Overall, 67 (78%) trials accounted for clustering in the primary analysis. High rates of missing outcome data are present in the majority of CRTs, yet handling missing data in practice remains suboptimal. Researchers and applied statisticians should carry out appropriate missing data methods, which are valid under plausible assumptions in order to increase statistical power in trials and reduce the possibility of bias. Sensitivity analysis should be performed, with weakened assumptions regarding the missing data mechanism to explore the robustness of results reported in the primary analysis.-Statistical analysis and handling of missing data in cluster randomized trials: a systematic review.",1
"Novel statistical methods are constantly being developed within the context of biomedical research; however, the characteristics of biostatistics methods that have been adopted into the field of general/internal medicine (GIM) is unclear. This study highlights the statistical journal articles, the statistical journals, and the types of statistical methods that appear to be having the most direct impact on GIM research. Descriptive techniques, including analyses of articles' keywords and controlled vocabulary terms, were used to characterize the articles published in statistics and probability journals that were subsequently referenced within GIM journal articles during a recent 10-year period (2000-2009). From the 45 statistics and probability journals of interest, a total of 989 unique articles were identified as being cited by 2183 (out of a total of about 127 469) unique GIM journal articles. The most frequently cited statistical topics included general/other statistical methods, followed by randomized trials, epidemiologic methods, meta-analysis, generalized linear models, and computer simulation. As statisticians continue to develop and refine techniques, the promotion and adoption of these methods should also be addressed so that their efforts spent in developing the methods are not done in vain.-Characteristics of recent biostatistical methods adopted by researchers publishing in general/internal medicine journals.",0
"In this paper, we propose a Bayesian design framework for a biosimilars clinical program that entails conducting concurrent trials in multiple therapeutic indications to establish equivalent efficacy for a proposed biologic compared to a reference biologic in each indication to support approval of the proposed biologic as a biosimilar. Our method facilitates information borrowing across indications through the use of a multivariate normal correlated parameter prior (CPP), which is constructed from easily interpretable hyperparameters that represent direct statements about the equivalence hypotheses to be tested.?The CPP accommodates different endpoints and data types across indications (eg, binary and continuous) and can, therefore, be used in a wide context of models without having to modify the data (eg, rescaling) to provide reasonable information-borrowing properties. We illustrate how one can evaluate the design using Bayesian versions of the type I error rate and power with the objective of determining the sample size required for each indication such that the design has high power to demonstrate equivalent efficacy in each indication, reasonably high power to demonstrate equivalent efficacy simultaneously in all indications (ie, globally), and reasonable type I error control from a Bayesian perspective. We illustrate the method with several examples, including designing biosimilars trials for follicular lymphoma and rheumatoid arthritis using binary and continuous endpoints, respectively.-Bayesian design of biosimilars clinical programs involving multiple therapeutic indications.",0
"In some clinical trials, where the outcome is the time until development of a silent event, an unknown proportion of subjects who have already experienced the event will be unknowingly enrolled due to the imperfect nature of the diagnostic tests used to screen potential subjects. For example, commonly used diagnostic tests for evaluating HIV infection status in infants, such as DNA PCR and HIV Culture, have low sensitivity when given soon after infection. This can lead to the inclusion of an unknown proportion of HIV-infected infants into clinical trials aimed at the prevention of transmission from HIV-positive mothers to their infants through breastfeeding. The infection status of infants at the end of the trial, when they are more than a year of age, can be determined with certainty. For those infants found to be infected with HIV at the end of the trial, it cannot be determined whether this occurred during the study or whether they were already infected when they were enrolled. In these settings, estimates of the cumulative risk of the event by the end of the study will overestimate the true probability of event during the study period and hypothesis tests comparing two or more intervention strategies can also be biased. We present inference methods for the distribution of time until the event of interest in these settings, and investigate issues in the design of such trials when there is a choice of using both imperfect and perfect diagnostic tests.-Analyzing time-to-event data in a clinical trial when an unknown proportion of subjects has experienced the event at entry.",0
"Herpes zoster or shingles is a frequent occurrence in both elderly individuals and immunocompromised hosts. The pain associated with herpes zoster is the most debilitating complication of the disease. It can be described as acute pain and post-herpetic neuralgia or zoster associated pain (ZAP). The latter definition encompasses pain from the onset of disease through its resolution and provides a convenient analytic tool for evaluation of antiviral therapy. A heuristic examination of ZAP historical data suggests the existence of three phases of pain resolution: the acute, subacute and chronic phases. The subacute and chronic phases comprise the post-herpetic neuralgia (PHN) stage. Common analytic methods, such as a Kaplan-Meier survival function or a Cox's model, have been used to assess the pain. However, such approaches do not adequately allow for phase comparison. Notably, in the clinical trial setting the comparison of specific treatment effects on the latter stages of pain are of the greatest medical relevance since this is the most debilitating phase of the illness. In order to incorporate the phase-specific information in the modelling of time to cessation of ZAP, we assumed the hazard function was a stepwise constant. Utilizing the full likelihood function, we obtained the maximum likelihood estimate for the transition times (that is, change-points), and other parameters of medical importance. The standard error of the change-point estimates were obtained through a bootstrapping method. The asymptotic properties of the parameter estimates are also discussed. Hence, the rates of pain resolution across all phases can be examined in order to precisely define the existence of multiple phases. In addition, the covariates effect can be examined across phases and populations, thereby allowing us to translate potential efficacy of a standard therapy to different populations. These results can be utilized in the design of clinical trials or in targeting the outcome for a specific phase while controlling for the effect of other variables.-Phase specific analysis of herpes zoster associated pain data: a new statistical approach.",0
"Measures of low socioeconomic position have been associated with increased risk for coronary heart disease (CHD) among women. A more complete understanding of this association is gained when socioeconomic position is conceptualized from a life course perspective where socioeconomic position is measured both in early and later life. We examined various life course socioeconomic indicators in relation to CHD risk among women. The Stockholm Female Coronary Risk Study is a population-based case-control study, in which 292 women with CHD aged &lt; or =65 years and 292 age-matched controls were investigated using a wide range of socioeconomic, behavioural, psychosocial and physiological risk factors. Socioeconomic disadvantage in early life (large family size in childhood, being born last, low education), and in later life (housewife or blue-collar occupation at labour force entry, blue-collar occupation at examination, economic hardships prior to examination) was assessed. Exposure to early (OR = 2.65, 95% CI : 1.12-6.54) or later (OR = 5.38, 95% CI : 2.01-11.43) life socioeconomic disadvantage was associated with increased CHD risk as compared to not being exposed. After simultaneous adjustment for marital status and traditional CHD risk factors, early and later socioeconomic disadvantage, exposure to three instances of socioeconomic disadvantage in early life was associated with an increased CHD risk of 2.48 (95% CI : 0.90-6.83) as compared to not being exposed to any disadvantage. The corresponding adjusted risk associated with exposure to later life disadvantage was 3.22 (95% CI : 1.02-10.53). Further analyses did not show statistical evidence of interaction effects between early and later life exposures (P = 0.12), although being exposed to both resulted in a 4.2-fold (95% CI : 1.4-12.1) increased CHD risk. Exposure to cumulative socioeconomic disadvantage (combining both early and later life), across all stages in the life course showed strong, graded associations with CHD risk after adjusting for traditional CHD risk factors. Stratification of cumulative disadvantage by body height showed that exposure to more than three periods of cumulative socioeconomic disadvantage had a 1.7- (95% CI : 0.9-3.2) and 1.9- (95% CI : 1.0-7.7) fold increased CHD risk for taller and shorter women, respectively. The combination of both short stature and more than two periods of cumulative socioeconomic disadvantage resulted in a 4.4-fold (95% CI : 1.7-9.3) increased CHD risk. Both early and later exposure to socioeconomic disadvantage were associated with increased CHD risk in women. Later life exposure seems to be more harmful for women's cardiovascular health than early life exposure to socioeconomic disadvantage. However, being exposed to socioeconomic disadvantage in both early and later life magnified the risk for CHD in women. Cumulative exposure to socioeconomic disadvantage resulted in greater likelihood of CHD risk, even among women who were above median height. In terms of better understanding health inequalities among women, measures of socioeconomic disadvantage over the life course are both conceptually and empirically superior to using socioeconomic indicators from one point in time.-Women's exposure to early and later life socioeconomic disadvantage and coronary heart disease risk: the Stockholm Female Coronary Risk Study.",0
"The proportional subdistribution hazards model (i.e. Fine-Gray model) has been widely used for analyzing univariate competing risks data. Recently, this model has been extended to clustered competing risks data via frailty. To the best of our knowledge, however, there has been no literature on variable selection method for such competing risks frailty models. In this paper, we propose a simple but unified procedure via a penalized h-likelihood (HL) for variable selection of fixed effects in a general class of subdistribution hazard frailty models, in which random effects may be shared or correlated. We consider three penalty functions, least absolute shrinkage and selection operator (LASSO), smoothly clipped absolute deviation (SCAD) and HL, in our variable selection procedure. We show that the proposed method can be easily implemented using a slight modification to existing h-likelihood estimation approaches. Numerical studies demonstrate that the proposed procedure using the HL penalty performs well, providing a higher probability of choosing the true model than LASSO and SCAD methods without losing prediction accuracy. The usefulness of the new method is illustrated using two actual datasets from multi-center clinical trials.-Variable selection in subdistribution hazard frailty models with competing risks data.",0
"To ensure appropriate allocation of research funds, we need methods for identifying high-priority research needs. We developed and pilot tested a process to identify needs for primary clinical research using a systematic review in gestational diabetes mellitus. We conducted eight steps: abstract research gaps from a systematic review using the Population, Intervention, Comparison, Outcomes, and Settings (PICOS) framework; solicit feedback from the review authors; translate gaps into researchable questions using the PICOS framework; solicit feedback from multidisciplinary stakeholders at our institution; establish consensus among multidisciplinary external stakeholders on the importance of the research questions using the Delphi method; prioritize outcomes; develop conceptual models to highlight research needs; and evaluate the process. We identified 19 research questions. During the Delphi method, external stakeholders established consensus for 16 of these 19?questions (15 with ""high"" and 1 with ""medium"" clinical benefit/importance). We pilot tested an eight-step process to identify clinically important research needs. Before wider application of this process, it should be tested using systematic reviews of other diseases. Further evaluation should include assessment of the usefulness of the research needs generated using this process for primary researchers and funders.-Development and pilot test of a process to identify research needs from a systematic review.",0
"Components of repeatedly observed multivariate outcomes (for example, the two components of blood pressure measures (SBP(it), DBP(it)), obtained on subject i at arbitrarily spaced times t) are often analysed separately. We present a unified approach to regression analysis of such irregularly timed multivariate longitudinal data, with particular attention to assessment of the magnitude and durability of cross-component correlation. Maximum likelihood estimates are presented for component-specific regression parameters and autocorrelation and cross-correlation functions. The component-specific autocorrelation function has the 'damped exponential' form [see text], which generalizes the AR(1), MA(1) and random intercept models for univariate longitudinal outcomes. The cross-component correlation function (CCCF) has an analogous form, allowing damped-exponential decay of cross-component correlation as time between repeated measures elapses. Finite sample performance is assessed through simulation studies. The methods are illustrated through blood pressure modelling and construction of multivariate prediction regions.-Analysis of longitudinally observed irregularly timed multivariate outcomes: regression with focus on cross-component correlation.",0
"We determined whether adherence to recommendations for coronary angiography more than 12 h after symptom onset but prior to hospital discharge after acute myocardial infarction (AMI) resulted in better survival. Using propensity scores, we created a matched retrospective sample of 19,568 Medicare patients hospitalized with AMI during 1994-1995 in the United States. Twenty-nine percent, 36%, and 34% of patients were judged necessary, appropriate, or uncertain, respectively, for angiography while 60% of those judged necessary received the procedure during the hospitalization. The 3-year survival benefit was largest for patients rated necessary [mean survival difference (95% CI): 17.6% (15.1, 20.1)] and smallest for those rated uncertain [8.8% (6.8, 10.7)]. Angiography recommendations appear to select patients who are likely to benefit from the procedure and the consequent interventions. Because of the magnitude of the benefit and of the number of patients involved, steps should be taken to replicate these findings.-Validating recommendations for coronary angiography following acute myocardial infarction in the elderly: a matched analysis using propensity scores.",0
"Subject recruitment for medical research is challenging. Slow patient accrual leads to increased costs and delays in treatment advances. Researchers need reliable tools to manage and predict the accrual rate. The previously developed Bayesian method integrates researchers' experience on former trials and data from an ongoing study, providing a reliable prediction of accrual rate for clinical studies. In this paper, we present a user-friendly graphical user interface program developed in R. A closed-form solution for the total subjects that can be recruited within a fixed time is derived. We also present a built-in Android system using Java for web browsers and mobile devices. Using the accrual software, we re-evaluated the Veteran Affairs Cooperative Studies Program 558- ROBOTICS study. The application of the software in monitoring and management of recruitment is illustrated for different stages of the trial. This developed accrual software provides a more convenient platform for estimation and prediction of the accrual process.-Bayesian accrual prediction for interim review of clinical studies: open source R package and smartphone application.",0
"To estimate causal effects of vaccine on post-infection outcomes, Hudgens and Halloran (2006) defined a post-infection causal vaccine efficacy estimand VEI based on the principal stratification framework. They also derived closed forms for the maximum likelihood estimators of the causal estimand under some assumptions. Extending their research, we propose a Bayesian approach to estimating the causal vaccine effects on binary post-infection outcomes. The identifiability of the causal vaccine effect VEI is discussed under different assumptions on selection bias. The performance of the proposed Bayesian method is compared with the maximum likelihood method through simulation studies and two case studies - a clinical trial of a rotavirus vaccine candidate and a field study of pertussis vaccination. For both case studies, the Bayesian approach provided similar inference as the frequentist analysis. However, simulation studies with small sample sizes suggest that the Bayesian approach provides smaller bias and shorter confidence interval length.-A Bayesian approach to estimating causal vaccine effects on binary post-infection outcomes.",0
"Authors' reply to Quantrill, Benger, Ripley and colleagues, Roach, Rogers, and Haldar and colleagues.",0
"This article describes rates of missing item responses in personal digital assistant (PDA) assessments as compared to paper assessments. Data come from the evaluation of a classroom-based leisure, life skills, and sexuality education program delivered to high school students in Cape Town, South Africa. Analyses show that the paper assessments had much higher rates of missing-ness than PDA assessments. This association is moderated by item order. Certain analyses also suggest that paper assessments have higher rates of missingness for items pertaining to participants' sexual behavior. Implications of these results for evaluation research will be discussed.-Rates of missing responses in personal digital assistant (PDA) versus paper assessments.",0
"Between July 2005 and July 2007, the Oregon Supplemental Nutrition Program for Women, Infants and Children program conducted the largest randomized field experiment (RFE) ever in the United States to assess the effectiveness of a low-cost peer counseling intervention to promote exclusive breastfeeding. We undertook a within-study comparison of the intervention using unique administrative data between July 2005 and July 2010. We found no difference between experimental and nonexperimental estimates but failed to determine correspondence based on more stringent criteria. We show that tests for nonconsent bias in the benchmark RFE might provide an important signal as to confounding in the nonexperimental estimates.-Can Nonexperimental Methods Provide Unbiased Estimates of a Breastfeeding Intervention? A Within-Study Comparison of Peer Counseling in Oregon.",0
"The class of acetylcholinesterase inhibitors (ChEI), including donepezil, rivastigmine, and galantamine, have similar efficacy profiles in patients with mild to moderate Alzheimer's disease (AD). However, few studies have evaluated adherence to these agents. We sought to prospectively capture the rates and reasons for nonadherence to ChEI and determine factors influencing tolerability and adherence. We designed a pragmatic randomized clinical trial to evaluate the adherence to ChEIs among older adults with AD. Participants include AD patients receiving care within memory care practices in the greater Indianapolis area. Participants will be followed at 6-week intervals up to 18 weeks to measure the primary outcome of ChEI discontinuation and adherence rates and secondary outcomes of behavioral and psychological symptoms of dementia. The primary outcome will be assessed through two methods, a telephone interview of an informal caregiver and electronic medical record data captured from each healthcare system through a regional health information exchange. The secondary outcome will be measured by the Healthy Aging Brain Care Monitor and the Neuropsychiatric Inventory. In addition, the trial will conduct an exploratory evaluation of the pharmacogenomic signatures for the efficacy and the adverse effect responses to ChEIs. We hypothesized that patient-specific factors, including pharmacogenomics and pharmacokinetic characteristics, may influence the study outcomes. This pragmatic trial will engage a diverse population from multiple memory care practices to evaluate the adherence to and tolerability of ChEIs in a real world setting. Engaging participants from multiple healthcare systems connected through a health information exchange will capture valuable clinical and non-clinical influences on the patterns of utilization and tolerability of a class of medications with a high rate of discontinuation. Clinicaltrials.gov: NCT01362686.-Medication adherence and tolerability of Alzheimer's disease medications: study protocol for a randomized controlled trial.",0
"The September 11, 2001, terrorist attacks were the largest human-made disaster in the United States since the Civil War. Studies after earlier disasters have reported rates of psychological disorders in the acute postdisaster period. However, data on postdisaster increases in substance use are sparse. A random digit dial telephone survey was conducted to estimate the prevalence of increased cigarette smoking, alcohol consumption, and marijuana use among residents of Manhattan, New York City, 5-8 weeks after the attacks. Among 988 persons included, 28.8% reported an increase in use of any of these three substances, 9.7% reported an increase in smoking, 24.6% reported an increase in alcohol consumption, and 3.2% reported an increase in marijuana use. Persons who increased smoking of cigarettes and marijuana were more likely to experience posttraumatic stress disorder than were those who did not (24.2% vs. 5.6% posttraumatic stress disorder for cigarettes; 36.0% vs. 6.6% for marijuana). Depression was more common among those who increased than for those who did not increase cigarette smoking (22.1 vs. 8.2%), alcohol consumption (15.5 vs. 8.3%), and marijuana smoking (22.3 vs. 9.4%). The results of this study suggest a substantial increase in substance use in the acute postdisaster period after the September 11th attacks. Increase in use of different substances may be associated with the presence of different comorbid psychiatric conditions.-Increased use of cigarettes, alcohol, and marijuana among Manhattan, New York, residents after the September 11th terrorist attacks.",0
"The randomized controlled trial (RCT) is considered the gold standard study design to inform decisions about the effectiveness of interventions. However, a common limitation is inadequate reporting of the applicability of the intervention and trial results for people who are ""socially disadvantaged"" and this can affect policy-makers' decisions. We previously developed a framework for identifying health-equity-relevant trials, along with a reporting guideline for transparent reporting. In this study, we provide a descriptive assessment of health-equity considerations in 200 randomly sampled equity-relevant trials. We developed a search strategy to identify health-equity-relevant trials published between 2013 and 2015. We randomly sorted the 4316 records identified by the search and screened studies until 100 individually randomized (RCTs) and 100 cluster randomized controlled trials (CRTs) were identified. We developed and pilot-tested a data extraction form based on our initial work, to inform the development of our reporting guideline for equity-relevant randomized trials. In total, 39 trials (20%) were conducted in a low- and middle-income country and 157 trials (79%) in a high-income country focused on socially disadvantaged populations (78% CRTs, 79% RCTs). Seventy-four trials (37%) reported a subgroup analysis across a population characteristic associated with disadvantage (25% CRT, 49% RCTs), with 19% of included studies reporting subgroup analyses across sex, 9% across race/ethnicity/culture, and 4% across socioeconomic status. No subgroup analyses were reported for place of residence, occupation, religion, education, or social capital. One hundred and forty-one trials (71%) discussed the applicability of their results to one or more socially disadvantaged populations (68% of CRT, 73% of RCT). In this set of trials, selected for their relevance to health equity, data that were disaggregated for socially disadvantaged populations were rarely reported. We found that even when the data are available, opportunities to analyze health-equity considerations are frequently missed. The recently published equity extension of the Consolidated Reporting Standards for Randomized Trials (CONSORT-Equity) may help improve delineation of hypotheses related to socially disadvantaged populations, and transparency and completeness of reporting of health-equity considerations in RCTs. This study can serve as a baseline assessment of the reporting of equity considerations.-Reporting of health equity considerations in cluster and individually randomized trials",1
"Small-sample inference with clustered data has received increased attention recently in the methodological literature, with several simulation studies being presented on the small-sample behavior of many methods. However, nearly all previous studies focus on a single class of methods (e.g., only multilevel models, only corrections to sandwich estimators), and the differential performance of various methods that can be implemented to accommodate clustered data with very few clusters is largely unknown, potentially due to the rigid disciplinary preferences. Furthermore, a majority of these studies focus on scenarios with 15 or more clusters and feature unrealistically simple data-generation models with very few predictors. This article, motivated by an applied educational psychology cluster randomized trial, presents a simulation study that simultaneously addresses the extreme small sample and differential performance (estimation bias, Type I error rates, and relative power) of 12 methods to account for clustered data with a model that features a more realistic number of predictors. The motivating data are then modeled with each method, and results are compared. Results show that generalized estimating equations perform poorly; the choice of Bayesian prior distributions affects performance; and fixed effect models perform quite well. Limitations and implications for applications are also discussed.-Modeling Clustered Data with Very Few Clusters.",1
"The ability to accurately estimate the sample size required by a stepped-wedge (SW) cluster randomized trial (CRT) routinely depends upon the specification of several nuisance parameters. If these parameters are misspecified, the trial could be overpowered, leading to increased cost, or underpowered, enhancing the likelihood of a false negative. We address this issue here for cross-sectional SW-CRTs, analyzed with a particular linear-mixed model, by proposing methods for blinded and unblinded sample size reestimation (SSRE). First, blinded estimators for the variance parameters of a SW-CRT analyzed using the Hussey and Hughes model are derived. Following this, procedures for blinded and unblinded SSRE after any time period in a SW-CRT are detailed. The performance of these procedures is then examined and contrasted using two example trial design scenarios. We find that if the two key variance parameters were underspecified by 50%, the SSRE procedures were able to increase power over the conventional SW-CRT design by up to 41%, resulting in an empirical power above the desired level. Thus, though there are practical issues to consider, the performance of the procedures means researchers should consider incorporating SSRE in to future SW-CRTs.-Blinded and unblinded sample size reestimation procedures for stepped-wedge cluster randomized trials.",3
"The Doubles, funded by the National Institute on Drug Abuse, is a seven-episode series of media tools designed to teach third and fourth grade students about the science of drug addiction. The program's curriculum is delivered via a set of videos, interactive CD-ROMs, workbooks, or an Internet site. This article examines the process used to develop The Doubles and argues that its success stems from the careful balancing of the needs of the funding agency, state and national curricular guidelines, and the target audience.-The Doubles: a case study on developing a technology-based substance abuse education curriculum.",0
"Despite randomization, selection bias may occur in cluster randomized trials. Classical multivariable regression usually allows for adjusting treatment effect estimates with unbalanced covariates. However, for binary outcomes with low incidence, such a method may fail because of separation problems. This simulation study focused on the performance of propensity score (PS)-based methods to estimate relative risks from cluster randomized trials with binary outcomes with low incidence. The results suggested that among the different approaches used (multivariable regression, direct adjustment on PS, inverse weighting on PS, and stratification on PS), only direct adjustment on the PS fully corrected the bias and moreover had the best statistical properties.-Propensity score methods for estimating relative risks in cluster randomized trials with low-incidence binary outcomes and selection bias.",1
"A key step in the design of a randomised controlled trial (RCT) is the estimation of the number of participants needed. By far the most common approach is to specify a target difference and then estimate the corresponding sample size; this sample size is chosen to provide reassurance that the trial will have high statistical power to detect such a difference between the randomised groups (at the planned statistical significance level). The sample size has many implications for the conduct of the study, as well as carrying scientific and ethical aspects to its choice. Despite the critical role of the target difference for the primary outcome in the design of an RCT, the manner in which it is determined has received little attention. This article reports the protocol of the Difference ELicitation in TriAls (DELTA2) project, which will produce guidance on the specification and reporting of the target difference for the primary outcome in a sample size calculation for RCTs. The DELTA2 project has five components: systematic literature reviews of recent methodological developments (stage 1) and existing funder guidance (stage 2); a Delphi study (stage 3); a 2-day consensus meeting bringing together researchers, funders and patient representatives, as well as one-off engagement sessions at relevant stakeholder meetings (stage 4); and the preparation and dissemination of a guidance document (stage 5). Specification of the target difference for the primary outcome is a key component of the design of an RCT. There is a need for better guidance for researchers and funders regarding specification and reporting of this aspect of trial design. The aim of this project is to produce consensus based guidance for researchers and funders.-Choosing the target difference ('effect size') for a randomised controlled trial - DELTA2 guidance protocol.",1
"The POWERLIBSAS/IML software provides convenient power calculations for a wide range of multivariate linear models with Gaussian errors. The software includes the Box, Geisser-Greenhouse, Huynh-Feldt, and uncorrected tests in the ""univariate"" approach to repeated measures (UNIREP), the Hotelling Lawley Trace, Pillai-Bartlett Trace, and Wilks Lambda tests in ""multivariate"" approach (MULTIREP), as well as a limited but useful range of mixed models. The familiar univariate linear model with Gaussian errors is an important special case. For estimated covariance, the software provides confidence limits for the resulting estimated power. All power and confidence limits values can be output to a SAS dataset, which can be used to easily produce plots and tables for manuscripts.-POWERLIB: SAS/IML Software for Computing Power in Multivariate Linear Models.",1
"To assess the impact of direct to consumer advertising of prescription drugs in the United States on Canadian prescribing rates for three heavily marketed drugs-etanercept, mometasone, and tegaserod. Controlled quasi-experimental study using interrupted time series analysis. Representative sample of 2700 Canadian pharmacies and prescription data from 50 US Medicaid programmes. Differences in number of filled prescriptions per 10,000 population per month between English speaking and French speaking (control) Canadian provinces before and after the start of direct to consumer advertising in the United States. Spending on direct to consumer advertising for study drugs ranged from $194m to $314m ( pound104m- pound169m; euro131m-euro212m) over the study period. Prescription rates for etanercept and mometasone did not increase in English speaking provinces relative to French speaking controls after the start of direct to consumer advertising. In contrast, tegaserod prescriptions increased 42% (0.56 prescriptions/10,000 residents, 95% confidence interval 0.37 to 0.76) in English speaking provinces immediately after the start of US direct to consumer advertising. Uncontrolled analysis of US Medicaid data showed a larger 56% increase in tegaserod prescriptions. However, this increase did not persist over time in either country, despite continued advertising. Exposure to US direct to consumer advertising transiently influenced both Canadian and US prescribing rates for tegaserod, a drug later withdrawn owing to safety concerns. The impact of direct to consumer advertising on drug use seems to be highly variable and probably depends on the characteristics of the advertised drug, the level of exposure to direct to consumer advertising, and the cultural context.-Effect of illicit direct to consumer advertising on use of etanercept, mometasone, and tegaserod in Canada: controlled longitudinal study.",0
"Cancer immunotherapy trials have two special features: a delayed treatment effect and a cure rate. Both features violate the proportional hazard model assumption and ignoring either one of the two features in an immunotherapy trial design will result in substantial loss of statistical power. To properly design immunotherapy trials, we proposed a piecewise proportional hazard cure rate model to incorporate both delayed treatment effect and cure rate into the trial design consideration. A sample size formula is derived for a weighted log-rank test under a fixed alternative hypothesis. The accuracy of sample size calculation using the new formula is assessed and compared with the existing methods via simulation studies. A real immunotherapy trial is used to illustrate the study design along with practical consideration of balance between sample size and follow-up time.-Cancer immunotherapy trial design with cure rate and delayed treatment effect.",0
"The use of longitudinal data for predicting a subsequent binary event is often the focus of diagnostic studies. This is particularly important in obstetrics, where ultrasound measurements taken during fetal development may be useful for predicting various poor pregnancy outcomes. We propose a modeling framework for predicting a binary event from longitudinal measurements where a shared random effect links the two processes together. Under a Gaussian random effects assumption, the approach is simple to implement with standard statistical software. Using asymptotic and simulation results, we show that estimates of predictive accuracy under a Gaussian random effects distribution are robust to severe misspecification of this distribution. However, under some circumstances, estimates of individual risk may be sensitive to severe random effects misspecification. We illustrate the methodology with data from a longitudinal fetal growth study.-A linear mixed model for predicting a binary event from longitudinal data under random effects misspecification.",0
"The cause-specific cumulative incidence function quantifies the subject-specific disease risk with competing risk outcome. With longitudinally collected biomarker data, it is of interest to dynamically update the predicted cumulative incidence function by incorporating the most recent biomarker as well as the cumulating longitudinal history. Motivated by a longitudinal cohort study of chronic kidney disease, we propose a framework for dynamic prediction of end stage renal disease using multivariate longitudinal biomarkers, accounting for the competing risk of death. The proposed framework extends the local estimation-based landmark survival modeling to competing risks data, and implies that a distinct sub-distribution hazard regression model is defined at each biomarker measurement time. The model parameters, prediction horizon, longitudinal history and at-risk population are allowed to vary over the landmark time. When the measurement times of biomarkers are irregularly spaced, the predictor variable may not be observed at the time of prediction. Local polynomial is used to estimate the model parameters without explicitly imputing the predictor or modeling its longitudinal trajectory. The proposed model leads to simple interpretation of the regression coefficients and closed-form calculation of the predicted cumulative incidence function. The estimation and prediction can be implemented through standard statistical software with tractable computation. We conducted simulations to evaluate the performance of the estimation procedure and predictive accuracy. The methodology is illustrated with data from the African American Study of Kidney Disease and Hypertension.-Dynamic prediction of competing risk events using landmark sub-distribution hazard model with multiple longitudinal biomarkers.",0
"Biomarkers, increasingly used in biomedical studies for the diagnosis and prognosis of acute and chronic diseases, provide insight into the effectiveness of treatments and potential pathways that can be used to guide future treatment targets. The measurement of these markers is often limited by the sensitivity of the given assay, resulting in data that are censored either at the lower or at the upper limit of detection. For the Genetic and Inflammatory Markers of Sepsis (GenIMS) study, many different biomarkers were measured to examine the effect of different pathways on the development of sepsis. In this study, the left-censoring of several important inflammatory markers has led to the need for statistical methods that can incorporate this censoring into any analysis of the biomarker data. This paper focuses on the development of multiple imputation methods for the inclusion of multiple left-censored biomarkers in a logistic regression analysis. We assume a multivariate normal distribution to account for the correlations between biomarkers and use the Gibbs sampler for the estimation of the distributional parameters and the imputation of the censored markers. We evaluate and compare the proposed methods with some simple imputation methods through simulation. We use a data set of inflammatory and coagulation markers from the GenIMS study for illustration.-Multiple imputation for left-censored biomarker data based on Gibbs sampling method.",0
"We reanalyzed the data from a cluster-randomized controlled trial (C-RCT) of a quality improvement intervention for prescribing antihypertensive medication. Our objective was to estimate the effectiveness of the intervention using both interrupted time-series (ITS) and RCT methods, and to compare the findings. We first conducted an ITS analysis using data only from the intervention arm of the trial because our main objective was to compare the findings from an ITS analysis with the findings from the C-RCT. We used segmented regression methods to estimate changes in level or slope coincident with the intervention, controlling for baseline trend. We analyzed the C-RCT data using generalized estimating equations. Last, we estimated the intervention effect by including data from both study groups and by conducting a controlled ITS analysis of the difference between the slope and level changes in the intervention and control groups. The estimates of absolute change resulting from the intervention were ITS analysis, 11.5% (95% confidence interval [CI]: 9.5, 13.5); C-RCT, 9.0% (95% CI: 4.9, 13.1); and the controlled ITS analysis, 14.0% (95% CI: 8.6, 19.4). ITS analysis can provide an effect estimate that is concordant with the results of a cluster-randomized trial. A broader range of comparisons from other RCTs would help to determine whether these are generalizable results.-Interrupted time-series analysis yielded an effect estimate concordant with the cluster-randomized controlled trial result.",1
"In recent years health services researchers have conducted 'volume-outcome' studies to evaluate whether providers (hospitals or surgeons) who treat many patients for a specialized condition have better outcomes than those that treat few patients. These studies and the inherent clustering of events by provider present an unusual statistical problem. The volume-outcome setting is unique in that 'volume' reflects both the primary factor under study and also the cluster size. Consequently, the assumptions inherent in the use of available methods that correct for clustering might be violated in this setting. To address this issue, we investigate via simulation the properties of three estimation procedures for the analysis of cluster correlated data, specifically in the context of volume-outcome studies. We examine and compare the validity and efficiency of widely-available statistical techniques that have been used in the context of volume-outcome studies: generalized estimating equations (GEE) using both the independence and exchangeable correlation structures; random effects models; and the weighted GEE approach proposed by Williamson et al. (Biometrics 2003; 59:36-42) to account for informative clustering. Using data generated either from an underlying true random effects model or a cluster correlated model we show that both the random effects and the GEE with an exchangeable correlation structure have generally good properties, with relatively low bias for estimating the volume parameter and its variance. By contrast, the cluster weighted GEE method is inefficient.-Properties of analysis methods that account for clustering in volume-outcome studies when the primary predictor is cluster size.",1
"Informed consent procedures in cluster randomized trials (CRTs) are considerably more complicated than in individually randomized trials. In a CRT, the units of randomization, intervention, and observation may differ in a single trial; there can be multiple levels of participants (individual and cluster level); consent may be required separately for intervention and data collection; and there may be practical constraints to seeking informed consent, for example, due to cluster-level interventions or the sheer size of clusters. We aimed to document consent practices at individual and cluster levels, assess the adequacy of reporting consent in trial publications, and assess associations with two trial characteristics that may influence consent requirements in CRTs: presence or absence of study interventions and presence or absence of data collection procedures at individual and cluster levels. We reviewed a random sample of 300 CRTs published during 2000-2008. We sent survey questionnaires to 285 unique authors of these trials to gather detailed information about consent procedures used in each trial. In all, 182 authors (64%) responded. Overall, 93% (95% confidence interval (CI): 88.8%-96.6%) indicated that participant consent had been sought for some aspects of the study. Consent was less frequently sought for a study intervention (70% of respondents) than for data collection (88%). More than half of the respondents (52%) indicated that consent had been sought at both cluster and individual levels. There was strong evidence for under-reporting of consent in trial publications: only 63% of all trial publications reported that informed consent had been sought for some aspect of the study. The odds ratios (ORs) summarizing the association of the two trial characteristics with cluster-level participant consent were weak (OR = 1.17, p = 0.70 for presence of cluster-level study intervention and OR = 1.54, p = 0.29 for data collection); on the other hand, the ORs summarizing the associations with individual-level consent were strong (OR = 6.2, p &lt; 0.0001 for presence of individual-level intervention and OR = 14.7, p &lt; 0.0001 for data collection). In all, 36% of authors did not respond to the survey; to the extent that consent practices in their trials were different than in respondents' trials, our results may be biased. There is a need for improvements in research practices in CRTs as well as their reporting. There may be a lack of clarity about consent requirements at the cluster level in particular. With the publication of the Ottawa Statement on the Ethical Design and Conduct of Cluster Randomized Trials, researchers and research ethics committees now have access to comprehensive ethics guidelines specific to CRTs.-Survey of consent practices in cluster randomized trials: improvements are needed in ethical conduct and reporting.",1
"A popular method of using repeated measures data to compare treatment groups in a clinical trial is to summarize each individual's outcomes with a scalar summary statistic, and then to perform a two-group comparison of the resulting statistics using a rank or permutation test. Many different types of summary statistics are used in practice, including discrete and continuous functions of the underlying repeated measures data. When the repeated measures processes of the comparison groups differ by a location shift at each time point, the asymptotic relative efficiency of (continuous) summary statistics that are linear functions of the repeated measures has been determined and used to compare tests in this class. However, little is known about the non-null behaviour of discrete summary statistics, about continuous summary statistics when the groups differ in more complex ways than location shifts or where the summary statistics are not linear functions of the repeated measures. Indeed, even simple distributional structures on the repeated measures variables can lead to complex differences between the distribution of common summary statistics of the comparison groups. The presence of left censoring of the repeated measures, which can arise when these are laboratory markers with lower limits of detection, further complicates the distribution of, and hence the ability to compare, summary statistics. This paper uses recent theoretical results for the non-null behaviour of rank and permutation tests to examine the asymptotic relative efficiencies of several popular summary statistics, both discrete and continuous, under a variety of common settings. We assume a flexible linear growth curve model to describe the repeated measures responses and focus on the types of settings that commonly arise in HIV/AIDS and other diseases.-Efficiency comparisons of rank and permutation tests based on summary statistics computed from repeated measures data.",0
"Oppositional defiant disorder (ODD) is a major mental health concern and highly prevalent among children living in poverty-impacted communities. Despite that treatments for ODD are among the most effective, few children living in poverty receive these services due to substantial barriers to access, as well as difficulties in the uptake and sustained adoption of evidence-based practices (EBPs) in community settings. The purpose of this study is to examine implementation processes that impact uptake of an evidence-based practice for childhood ODD, and the impact of a Clinic Implementation Team (CIT)-driven structured adaptation to enhance its fit within the public mental health clinic setting. This study, a Hybrid Type II effectiveness-implementation research trial, blends clinical effectiveness and implementation research methods to examine the impact of the 4Rs and 2Ss Multiple Family Group (MFG) intervention, family level mediators of child outcomes, clinic/provider-level mediators of implementation, and the impact of CITs on uptake and long-term utilization of this model. All New York City public outpatient mental health clinics have been invited to participate. A sampling procedure that included randomization at the agency level and a sub-study to examine the impact of clinic choice upon outcomes yielded a distribution of clinics across three study conditions. Quantitative data measuring child outcomes, organizational factors and implementation fidelity will be collected from caregivers and providers at baseline, 8, and 16?weeks from baseline, and 6?months from treatment completion. The expected participation is 134 clinics, 268 providers, and 2688 caregiver/child dyads. We will use mediation analysis with a multi-level Structural Equation Modeling (SEM) (MSEM including family level variables, provider variables, and clinic variables), as well as mediation tests to examine study hypotheses. The aim of the study is to generate knowledge about effectiveness and mediating factors in the treatment of ODDs in children in the context of family functioning, and to propose an innovative approach to the adaptation and implementation of new treatment interventions within clinic settings. The proposed CIT adaptation and implementation model has the potential to enhance implementation and sustainability, and ultimately increase the extent to which effective interventions are available and can impact children and families in need of services for serious behavior problems. ClinicalTrials.gov, ID: NCT02715414 . Registered on 3 March 2016.-Testing the 4Rs and 2Ss Multiple Family Group intervention: study protocol for a randomized controlled trial.",0
"Cluster randomized trials (CRTs) and individually randomized trials (IRTs) are often pooled together in meta-analyses (MAs) of randomized trials. However, the potential systematic differences in intervention effect estimates between these two trial types has never been investigated. Therefore, we conducted a meta-epidemiological study comparing intervention effect estimates between CRTs and IRTs. All Cochrane MAs including at least one CRT and one IRT, published between 1 January 2010 and 31 December 2014, were included. For each MA, we estimated a ratio of odds ratios (ROR) for binary outcomes or a difference of standardized differences (DSMD) for continuous outcomes, where less than 1 (or 0, respectively) indicated a greater intervention effect estimate with CRTs. Among 1301 screened reviews, we selected 121 MAs, of which 76 had a binary outcome and 45 had a continuous outcome. For binary outcomes, intervention effect estimates did not differ between CRTs and IRTs [ROR 1.00, 95% confidence interval (0.93 to 1.08)]. Subgroup and adjusted analyses led to consistent results. For continuous outcomes, the DSMD was 0.13 (0.06 to 0.19). It was lower for MAs with a pharmacological intervention [-0.03, (-0.12 to 0.07)], an objective outcome [0.05, (-0.08 to 0.17)] or after adjusting for trial size [0.06, (-0.01 to 0.15)]. For binary outcomes, CRTs and IRTs can safely be pooled in MAs because of an absence of systematic differences between effect estimates. For continuous outcomes, the results were less clear although accounting for trial sample sizes led to a non-significant difference. More research is needed for continuous outcomes and, meanwhile, MAs should be completed with subgroup analyses (CRTs vs IRTs).-Intervention effect estimates in cluster randomized versus individually randomized trials: a meta-epidemiological study.",1
"The Lifestyle Improvement through Food and Exercise (LIFE) study is a community-based randomized-controlled trial to measure the effectiveness of a lifestyle intervention to improve glycemic control among African Americans with type 2 diabetes attending safety net clinics. The study enrolled African American adults with a diagnosis of type 2 diabetes and HbA1c ? 7.0 who had attended specific safety net community clinics in the prior year. 210 patients will be enrolled and randomized to either the LIFE intervention or a standard of care control group, which consists of two dietitian-led diabetes self-management classes. The LIFE intervention was delivered in 28 group sessions over 12 months and focused on improving diet through dietitian-led culturally-tailored nutrition education, increasing physical activity through self-monitoring using an accelerometer, increasing ability to manage blood sugar through modifications to lifestyle, and providing social support for behavior change. In addition to the group sessions, peer supporters made regular telephone calls to participants to monitor progress toward behavioral goals and provide social support. The 12-month intervention phase was followed by a six-month maintenance phase consisting of two group sessions. The primary outcome of the study is change in A1C from baseline to 12 months, and an additional follow-up will occur at 18 months. The hypothesis of the study is that the participants in the LIFE intervention will show a greater improvement in glycemic control over 12 months than participants in the control group.-Design of the Lifestyle Improvement through Food and Exercise (LIFE) study: a randomized controlled trial of self-management of type 2 diabetes among African American patients from safety net health centers.",0
"Age at ascertainment from prevalence case-control data identifies the age-specific odds of disease. When age at onset is available from the cases, the conditional distribution of age at onset, given that disease occurs, is identifiable. Combining both kinds of information by introducing a multiplicative intercept allows identification of the marginal distribution of age at onset. Here, the approach is extended to the two-sample setting through a generalization of the multiplicative intercept model. The efficiency of the approach is explored and a test statistic based on the integrated difference between distribution function estimates is proposed. An approach to regularization of the likelihood is discussed. The methods are illustrated through an application to data on colorectal polyps obtained from a case-control study of individuals undergoing colonoscopy.-Estimating and comparing the distribution of onset of disease from prevalence case-control data.",0
"Multilevel mediation analysis examines the indirect effect of an independent variable on an outcome achieved by targeting and changing an intervening variable in clustered data. We study analytically and through simulation the effects of an omitted variable at level 2 on a 1-1-1 mediation model for a randomized experiment conducted within clusters in which the treatment, mediator, and outcome are all measured at level 1. When the residuals in the equations for the mediator and the outcome variables are fully orthogonal, the two methods of calculating the indirect effect (ab, c - c') are equivalent at the between- and within-cluster levels. Omitting a variable at level 2 changes the interpretation of the indirect effect and will induce correlations between the random intercepts or random slopes. The equality of within-cluster ab and c - c' no longer holds. Correlation between random slopes implies that the within-cluster indirect effect is conditional, interpretable at the grand mean level of the omitted variable.-Multilevel mediation analysis: The effects of omitted variables in the 1-1-1 model.",1
"In the REMoxTB study of 4-month treatment-shortening regimens containing moxifloxacin compared to the standard 6-month regimen for tuberculosis, the proportion of unfavourable outcomes for women was similar in all study arms, but men had more frequent unfavourable outcomes (bacteriologically or clinically defined failure or relapse within 18?months after randomisation) on the shortened moxifloxacin-containing regimens. The reason for this gender disparity in treatment outcome is poorly understood. The gender differences in baseline variables were calculated, as was time to smear and culture conversion and Kaplan-Meier plots were constructed. In post hoc exploratory analyses, multivariable logistic regression modelling and an observed case analysis were used to explore factors associated with both gender and unfavourable treatment outcome. The per-protocol population included 472/1548 (30%) women. Women were younger and had lower rates of cavitation, smoking and weight (all p &lt; 0.05) and higher prevalence of HIV (10% vs 6%, p = 0.001). They received higher doses (mg/kg) than men of rifampicin, isoniazid, pyrazinamide and moxifloxacin (p ? 0.005). There was no difference in baseline smear grading or mycobacterial growth indicator tube (MGIT) time to positivity. Women converted to negative cultures more quickly than men on Lowenstein-Jensen (HR 1.14, p = 0.008) and MGIT media (HR 1.19, p &lt; 0.001). In men, the presence of cavitation, positive HIV status, higher age, lower BMI and 'ever smoked' were independently associated with unfavourable treatment outcome. In women, only 'ever smoked' was independently associated with unfavourable treatment outcome. Only for cavitation was there a gender difference in treatment outcomes by regimen; their outcome in the 4-month arms was significantly poorer compared to the 6-month treatment arm (p &lt; 0.001). Women, with or without cavities, and men without cavities had a similar outcome on all treatment arms (p = 0.218, 0.224 and 0.689 respectively). For all other covariate subgroups, there were no differences in treatment effects for men or women. Gender differences in TB treatment responses for the shorter regimens in the REMoxTB study may be explained by poor outcomes in men with cavitation on the moxifloxacin-containing regimens. We observed that women with cavities, or without, on the 4-month moxifloxacin regimens had similar outcomes to all patients on the standard 6-month treatment. The biological reasons for this difference are poorly understood and require further exploration.-Gender differences in tuberculosis treatment outcomes: a post hoc analysis of the REMoxTB study.",0
"This study used Monte Carlo simulations to evaluate the performance of alternative models for the analysis of group-randomized trials having more than two time intervals for data collection. The major distinction among the models tested was the sampling variance of the intervention effect. In the mixed-model ANOVA, the sampling variance of the intervention effect is based on the variance among group x time-interval means. In the random coefficients model, the sampling variance of the intervention effect is based on the variance among the group-specific slopes. These models are equivalent when the design includes only two time intervals, but not when there are more than two time intervals. The results indicate that the mixed-model ANOVA yields unbiased estimates of sampling variation and nominal type I error rates when the group-specific time trends are homogenous. However, when the group-specific time trends are heterogeneous, the mixed-model ANOVA yields downwardly biased estimates of sampling variance and inflated type I error rates. In contrast, the random coefficients model yields unbiased estimates of sampling variance and the nominal type I error rate regardless of the pattern among the groups. We discuss implications for the analysis of group-randomized trials with more than two time intervals.-Analysis of data from group-randomized trials with repeat observations on the same groups.",1
"A stepped wedge cluster randomised trial (SWCRT) is a multicentred study which allows an intervention to be rolled out at sites in a random order. Once the intervention is initiated at a site, all participants within that site remain exposed to the intervention for the remainder of the study. The time since the start of the study (""calendar time"") may affect outcome measures through underlying time trends or periodicity. The time since the intervention was introduced to a site (""exposure time"") may also affect outcomes cumulatively for successful interventions, possibly in addition to a step change when the intervention began. Motivated by a SWCRT of self-monitoring for bipolar disorder, we conducted a simulation study to compare model formulations to analyse data from a SWCRT under 36 different scenarios in which time was related to the outcome (improvement in mood score). The aim was to find a model specification that would produce reliable estimates of intervention effects under different scenarios. Nine different formulations of a linear mixed effects model were fitted to these datasets. These models varied in the specification of calendar and exposure times. Modelling the effects of the intervention was best accomplished by including terms for both calendar time and exposure time. Treating time as categorical (a separate parameter for each measurement time-step) achieved the best coverage probabilities and low bias, but at a cost of wider confidence intervals compared to simpler models for those scenarios which were sufficiently modelled by fewer parameters. Treating time as continuous and including a quadratic time term performed similarly well, with slightly larger variations in coverage probability, but narrower confidence intervals and in some cases lower bias. The impact of misspecifying the covariance structure was comparatively small. We recommend that unless there is a priori information to indicate the form of the relationship between time and outcomes, data from SWCRTs should be analysed with a linear mixed effects model that includes separate categorical terms for calendar time and exposure time. Prespecified sensitivity analyses should consider the different formulations of these time effects in the model, to assess their impact on estimates of intervention effects.-Mixed effects approach to the analysis of the stepped wedge cluster randomised trial-Investigating the confounding effect of time through simulation",3
"This paper discusses statistical models for multilevel ordinal data that may be more appropriate for prevention outcomes than models that assume continuous measurement and normality. Prevention outcomes often have distributions that make them inappropriate for many popular statistical models that assume normality and are more appropriately considered ordinal outcomes. Despite this, the modeling of ordinal outcomes is often not well understood. This article discusses ways to analyze multilevel ordinal outcomes that are clustered or longitudinal, including the proportional odds regression model for ordinal outcomes, which assumes that the covariate effects are the same across the levels of the ordinal outcome. The article will cover how to test this assumption and what to do if it is violated. It will also discuss application of these models using computer software programs.-Methods for Multilevel Ordinal Data in Prevention Research.",1
"Most studies that follow subjects over time are challenged by having some subjects who dropout. Double sampling is a design that selects and devotes resources to intensively pursue and find a subset of these dropouts, then uses data obtained from these to adjust na?ve estimates, which are potentially biased by the dropout. Existing methods to estimate survival from double sampling assume a random sample. In limited-resource settings, however, generating accurate estimates using a minimum of resources is important. We propose using double-sampling designs that oversample certain profiles of dropouts as more efficient alternatives to random designs. First, we develop a framework to estimate the survival function under these profile double-sampling designs. We then derive the precision of these designs as a function of the rule for selecting different profiles, in order to identify more efficient designs. We illustrate using data from the United States President's Emergency Plan for AIDS Relief-funded HIV care and treatment program in western Kenya. Our results show why and how more efficient designs should oversample patients with shorter dropout times. Further, our work suggests generalizable practice for more efficient double-sampling designs, which can help maximize efficiency in resource-limited settings.-Choosing profile double-sampling designs for survival estimation with application to President's Emergency Plan for AIDS Relief evaluation.",0
External validity is also an ethical consideration in cluster-randomised trials of policy changes.,1
Estimation of the Intraclass Correlation-Coefficient for the Analysis of Covariance Model,1
"Clinicians can miss up to half of patients' symptomatic toxicities in cancer clinical trials and routine practice. Although patient-reported outcome questionnaires have been developed to capture this information, it is unclear whether clinicians will make use of patient-reported outcomes to inform their own toxicity documentation, or to prompt symptom management activities. 44 lung cancer patients that participated in a phase 2 treatment trial self-reported 13 symptomatic toxicities derived from the National Cancer Institute's Common Terminology Criteria for Adverse Events and Karnofsky Performance Status via tablet computers in waiting areas immediately preceding scheduled visits. During visits, clinicians viewed patients' self-reported toxicity and performance status ratings on a computer interface and could agree or disagree/reassign grades (""shared"" reporting). Agreement of clinicians with patient-reported grades was tabulated, and compared using weighted kappa statistics. Clinical actions in response to patient-reported severe (grade 3/4) toxicities were measured (e.g. treatment discontinuation, dose reduction, supportive medications). For comparison, 45 non-trial patients with lung cancer being treated in the same clinic by the same physicians were simultaneously enrolled in a parallel cohort study in which patients also self-reported toxicity grades but reports were not shared with clinicians (""non-shared"" reporting). Toxicities and performance status were reported by patients and reviewed by clinicians at (780/782) 99.7% of study visits in the phase 2 trial which used ""shared"" reporting. Clinicians agreed with patients 93% of the time with kappas 0.82-0.92. Clinical actions were taken in response to 67% of severe patient-reported toxicities. In the ""non-shared"" reporting comparison group, clinicians agreed with patients 56% of the time with kappas 0.04-0.48 (significantly worse than shared reporting for all symptoms), and clinical actions were taken in response to 44% of severe patient-reported toxicities. Clinicians will frequently agree with patient-reported symptoms and performance status, and will use this information to guide documentation and symptom management. (ClinicalTrials.gov: NCT00807573).-Feasibility and clinical impact of sharing patient-reported symptom toxicities and performance status with clinical investigators during a phase 2 cancer treatment trial.",0
"Among the many applications of microarray technology, one of the most popular is the identification of genes that are differentially expressed in two conditions. A common statistical approach is to quantify the interest of each gene with a p-value, adjust these p-values for multiple comparisons, choose an appropriate cut-off, and create a list of candidate genes. This approach has been criticised for ignoring biological knowledge regarding how genes work together. Recently a series of methods, that do incorporate biological knowledge, have been proposed. However, the most popular method, gene set enrichment analysis (GSEA), seems overly complicated. Furthermore, GSEA is based on a statistical test known for its lack of sensitivity. In this article we compare the performance of a simple alternative to GSEA. We find that this simple solution clearly outperforms GSEA. We demonstrate this with eight different microarray datasets.-Gene set enrichment analysis made simple.",0
"(1) To systematically collect and organize into clinical categories all outcomes reported in trials for abnormal uterine bleeding (AUB); (2) to rank the importance of outcomes for patient decision making; and (3) to improve future comparisons of effects in trials of AUB interventions. Systematic review of English-language randomized controlled trials of AUB treatments in MEDLINE from 1950 to June 2008. All outcomes and definitions were extracted and organized into major outcome categories by an expert group. Each outcome was ranked ""critically important,"" ""important,"" or ""not important"" for informing patients' choices. One hundred thirteen articles from 79 trials met the criteria. One hundred fourteen different outcomes were identified, only 15 (13%) of which were ranked as critically important and 29 (25%) as important. Outcomes were grouped into eight categories: (1) bleeding; (2) quality of life; (3) pain; (4) sexual health; (5) patient satisfaction; (6) bulk-related complaints; (7) need for subsequent surgical treatment; and (8) adverse events. To improve the quality, consistency, and utility of future AUB trials, we recommend assessing a limited number of clinical outcomes for bleeding, disease-specific quality of life, pain, sexual health, and bulk-related symptoms both before and after treatment and reporting satisfaction and adverse events. Further development of validated patient-based outcome measures and the standardization of outcome reporting are needed.-Systematic review highlights difficulty interpreting diverse clinical outcomes in abnormal uterine bleeding trials.",0
Cluster Randomised Trials,1
"The Look AHEAD (Action for Health in Diabetes) Study is a long-term clinical trial that aims to determine the cardiovascular disease (CVD) benefits of an intensive lifestyle intervention (ILI) in obese adults with type 2 diabetes. The study was designed to have 90% statistical power to detect an 18% reduction in the CVD event rate in the ILI Group compared to the Diabetes Support and Education (DSE) Group over 10.5 years of follow-up. The original power calculations were based on an expected CVD rate of 3.125% per year in the DSE group; however, a much lower-than-expected rate in the first 2 years of follow-up prompted the Data and Safety Monitoring Board (DSMB) to recommend that the Steering Committee undertake a formal blinded evaluation of these design considerations. The Steering Committee created an Endpoint Working Group (EPWG) that consisted of individuals masked to study data to examine relevant issues. The EPWG considered two primary options: (1) expanding the definition of the primary endpoint and (2) extending follow-up of participants. Ultimately, the EPWG recommended that the Look AHEAD Steering Committee approve both strategies. The DSMB accepted these modifications, rather than recommending that the trial continue with inadequate statistical power. Trialists sometimes need to modify endpoints after launch. This decision should be well justified and should be made by individuals who are fully masked to interim results that could introduce bias. This article describes this process in the Look AHEAD study and places it in the context of recent articles on endpoint modification and recent trials that reported endpoint modification.-Midcourse correction to a clinical trial when the event rate is underestimated: the Look AHEAD (Action for Health in Diabetes) Study.",0
"Stepped-wedge cluster randomized trials (SW-CRTs) are increasingly popular in health-related research in both high- and low-resource settings. There may be specific ethical issues that researchers face when designing and conducting SW-CRTs in low-resource settings. Knowledge of these issues can help to improve the ethical conduct of SW-CRTs in a global health context. We performed an ethical analysis of two studies using SW-CRT designs in low-resource settings: the Que Vivan Las Madres study conducted from 2014 to 2017 in Guatemala and the Atmiyata study conducted from 2017 to 2018 in rural parts of India. For both case studies, we identified and evaluated the classification of the study as research or nonresearch and the ethical issues regarding the justification of the design, including the delayed rollout of an intervention that had a promising effect. In our case studies, some minor ethical issues surfaced about the registration and stakeholder pressure on the order of randomization, but both included good justification for the design and delayed rollout. Our analysis did, however, demonstrate that careful consideration of the role of randomization and registration of the trials is important. SW-CRTs can provide an opportunity for rigorous evaluation of interventions destined to be rolled out on the basis of limited evidence. Furthermore, in SW-CRTs, the underlying objective is often to provide a robust evaluation of the effectiveness for generalized dissemination, and this makes the SW-CRT no less a research study than any other form of cluster randomized trial. The design and conduct of stepped-wedge cluster randomized trials raises at least two ethical issues that need special consideration in both high- and low-resource settings: the justification for using the design, specifically the delayed rollout of the intervention to the control group, and the classification of the study as research or nonresearch. In our case studies, these issues did not seem to raise special ethical scrutiny in low-resource settings. Further ethical evaluation will hopefully result in specific ethical guidelines for the use of SW-CRTs in both high- and low-resource settings to contribute to responsible functioning of these trials and adequate protection of participants.-Ethical issues in the design and conduct of stepped-wedge cluster randomized trials in low-resource settings",3
"Correlated binary data occur very frequently in cluster sample surveys, dependent repeated cancer screening, teratological experiments, ophthalmologic and otolaryngologic studies, and other clinical trials. The standard methods to analyse these data include the use of beta-binomial models and generalized estimating equations with third and fourth moments specified by 'working matrices'. However, in many applications it is reasonable to assume that the data from the same cluster are exchangeable. When all sampled clusters have equal sizes, Bowman and George introduced maximum likelihood estimates (MLEs) of the population parameters such as the marginal means, moments, and correlations of order two and higher. They also extended their approach to sampled clusters with unequal sizes. It seems that their extension has a gap. This paper points out the source of this gap and shows that estimates introduced by Bowman and George are not the MLEs of the parameters which are used to identify the joint distribution of correlated binary data. We show that the MLEs of the population parameters have no closed form in general and should be calculated by numerical methods. We apply our results and a generalized estimating equation procedure to a data set from a double-blind randomized clinical trial comparing two antibiotics, cefaclor and amoxicillin, used for the treatment of acute otitis media. To see the performance of the MLEs with small or moderate sample sizes, several simulation studies are also conducted.-Modelling and analysing exchangeable binary data with random cluster sizes.",1
"Cost-effectiveness analyses (CEA) may be undertaken alongside cluster randomized trials (CRTs) where randomization is at the level of the cluster (for example, the hospital or primary care provider) rather than the individual. Costs (and outcomes) within clusters may be correlated so that the assumption made by standard bivariate regression models, that observations are independent, is incorrect. This study develops a flexible modeling framework to acknowledge the clustering in CEA that use CRTs. The authors extend previous Bayesian bivariate models for CEA of multicenter trials to recognize the specific form of clustering in CRTs. They develop new Bayesian hierarchical models (BHMs) that allow mean costs and outcomes, and also variances, to differ across clusters. They illustrate how each model can be applied using data from a large (1732 cases, 70 primary care providers) CRT evaluating alternative interventions for reducing postnatal depression. The analyses compare cost-effectiveness estimates from BHMs with standard bivariate regression models that ignore the data hierarchy. The BHMs show high levels of cost heterogeneity across clusters (intracluster correlation coefficient, 0.17). Compared with standard regression models, the BHMs yield substantially increased uncertainty surrounding the cost-effectiveness estimates, and altered point estimates. The authors conclude that ignoring clustering can lead to incorrect inferences. The BHMs that they present offer a flexible modeling framework that can be applied more generally to CEA that use CRTs.-Bayesian hierarchical models for cost-effectiveness analyses that use data from cluster randomized trials.",1
"Random effects regression imputation has been recommended for multiple imputation (MI) in cluster randomized trials (CRTs) because it is congenial to analyses that use random effects regression. This method relies heavily on model assumptions and may not be robust to misspecification of the imputation model. MI by predictive mean matching (PMM) is a semiparametric alternative, but current software for multilevel data relies on imputation models that ignore clustering or use fixed effects for clusters. When used directly for imputation, these two models result in underestimation (ignoring clustering) or overestimation (fixed effects for clusters) of variance estimates. We develop MI procedures based on PMM that leverage these opposing estimated biases in the variance estimates in one of three ways: weighting the distance metric (PMM-dist), weighting the average of the final imputed values from two PMM procedures (PMM-avg), or performing a weighted draw from the final imputed values from the two PMM procedures (PMM-draw). We use Monte-Carlo simulations to evaluate our newly proposed methods relative to established MI procedures, focusing on estimation of treatment group means and their variances after MI. The proposed PMM procedures reduce the bias in the MI variance estimator relative to established methods when the imputation model is correctly specified, and are generally more robust to model misspecification than even the random effects imputation methods. The PMM-draw procedure in particular is a promising method for multiply imputing missing data from CRTs that can be readily implemented in existing statistical software.-Multiple imputation by predictive mean matching in cluster-randomized trials",1
Design and analysis issues in community-based drug abuse prevention,1
"Bayesian approaches to inference in cluster randomized trials have been investigated for normally distributed and binary outcome measures. However, relatively little attention has been paid to outcome measures which are counts of events. We discuss an extension of previously published Bayesian hierarchical models to count data, which usually can be assumed to be distributed according to a Poisson distribution. We develop two models, one based on the traditional rate ratio, and one based on the rate difference which may often be more intuitively interpreted for clinical trials, and is needed for economic evaluation of interventions. We examine the relationship between the intracluster correlation coefficient (ICC) and the between-cluster variance for each of these two models. In practice, this allows one to use the previously published evidence on ICCs to derive an informative prior distribution which can then be used to increase the precision of the posterior distribution of the ICC. We demonstrate our models using a previously published trial assessing the effectiveness of an educational intervention and a prior distribution previously derived. We assess the robustness of the posterior distribution for effectiveness to departures from a normal distribution of the random effects.-Bayesian methods of analysis for cluster randomized trials with count outcome data.",1
"Cluster-randomized trials (CRTs) are being increasingly used to test a range of interventions, including medical interventions commonly used in clinical practice. Policies created by the NIH and the Food and Drug Administration (FDA) require the reporting of demographics and the examination of demographic heterogeneity of treatment effect (HTE) for individually randomized trials. Little is known about how frequent demographics are reported and HTE analyses are conducted in CRTs. We sought to understand the prevalence of HTE analyses and the statistical methods used to conduct them in CRTs focused on treating cardiovascular disease, cancer, and chronic lower respiratory diseases. Additionally, we also report on the proportion of CRTs that reported on baseline demographics of its populations and conducted demographic HTE analyses. We searched PubMed and Embase for CRTs published between 1/1/2010 and 3/29/2016 that focused on treating the top 3 Center for Disease Control causes of death (cardiovascular disease, chronic lower respiratory disease, and cancer). Evidence Screening And Review: Of 1,682 unique titles, 117 abstracts were screened. After excluding 53 articles, we included 64 CRT publications and abstracted information on study characteristics and demographic information, statistical analysis, HTE analysis, and study quality. Age and sex were reported in greater than 95.3% of CRTs, while race and ethnicity were reported in only 20.3% of CRTs. HTE analyses were conducted in 28.1% (n = 18) of included CRTs and 77.8% (n = 12) were prespecified analyses. Four CRTs conducted a demographic subgroup analysis. Only 6/18 CRTs used interaction testing to determine whether HTE existed. Baseline demographic reporting was high for age and sex in CRTs, but was uncommon for race and ethnicity. HTE analyses were uncommon and was rare for demographic subgroups, which limits the ability to examine the extent of benefits or risks for treatments tested with CRT designs.-Assessing heterogeneity of treatment effect analyses in health-related cluster randomized trials: A systematic review",1
"Hepatitis E virus (HEV) is the most common cause of acute viral hepatitis in the world. Most of South Asia is HEV endemic, with frequent seasonal epidemics of hepatitis E and continuous sporadic cases. This author group's epidemiologic work and clinical reports suggest that Bangladesh is HEV endemic, but there have been few population-based studies of this country's HEV burden. The authors calculated HEV infection rates, over an 18-month interval between 2003 and 2005, by following a randomly selected cohort of 1,134 subjects between the ages of 1 and 88 years, representative of rural communities in southern Bangladesh. Baseline prevalence of antibody to hepatitis E virus (anti-HEV) was 22.5%. Seroincidence was 60.3 per 1,000 person-years during the first 12 months and 72.4 per 1,000 person-years from &gt;12 to 18 months (during the monsoon season), peaking by age 50 years and with low rates during childhood. Few of the seroconverting subjects reported hepatitis-like illness. Overall incidence was calculated to be 64 per 1,000 person-years, with 1,172 person-years followed. No significant associations were found between anti-HEV incidence and demographic or socioeconomic factors for which data were available. This is the first study to document annual HEV infection rates among ""healthy"" and very young to elderly subjects in a rural Bangladeshi population.-Epidemiology and risk factors of incident hepatitis E virus infections in rural Bangladesh.",0
"Two-stage designs to develop and validate a panel of biomarkers present a natural setting for the inclusion of stopping rules for futility in the event of poor preliminary estimates of performance. We consider the design of a two-stage study to develop and validate a panel of biomarkers where a predictive model is developed using a subset of the samples in stage 1 and the model is validated using the remainder of the samples in stage 2. First, we illustrate how we can implement a stopping rule for futility in a standard, two-stage study for developing and validating a predictive model where samples are separated into a training sample and a validation sample. Simulation results indicate that our design has type I error rate and power similar to the fixed-sample design but with a substantially reduced sample size under the null hypothesis. We then illustrate how we can include additional interim analyses in stage 2 by applying existing group sequential methodology, which results in even greater savings in the number of samples required under both the null and the alternative hypotheses. Our simulation results also illustrate that the operating characteristics of our design are robust to changes in the underlying marker distribution.-Early termination of a two-stage study to develop and validate a panel of biomarkers.",0
"Cancer care delivery research (CCDR) is an emerging field that investigates ways to optimally provide care for patients within complex health-care systems. Novel research designs are essential to efficiently study CCDR research questions. A stepped-wedge trial (SWT) is one such pragmatic design and is similar to a parallel randomized controlled trial (RCT). An SWT design has several advantages. It can examine the clinical effectiveness of an intervention by using participants as the control group, address potential ethical issues, and extend time for trial implementation or policy changes with fewer resources than are used to conduct several RCTs. All participants eventually receive the intervention, which can make the trial more desirable for patient participation. This article aims to introduce and discuss the SWT study design and to encourage future application for CCDR and other oncology-related research.-The Stepped-Wedge Trial Design: Paving the Way for Cancer Care Delivery Research",3
"The major psychoactive cannabinoid compound of marijuana, delta-9 tetrahydrocannabinol (THC), has been shown to modulate immune responses and lymphocyte function. After primary infection the viral DNA genome of gamma herpesviruses persists in lymphoid cell nuclei in a latent episomal circular form. In response to extracellular signals, the latent virus can be activated, which leads to production of infectious virus progeny. Therefore, we evaluated the potential effects of THC on gamma herpesvirus replication. Tissue cultures infected with various gamma herpesviruses were cultured in the presence of increasing concentrations of THC and the amount of viral DNA or infectious virus yield was compared to those of control cultures. The effect of THC on Kaposi's Sarcoma Associated Herpesvirus (KSHV) and Epstein-Barr virus (EBV) replication was measured by the Gardella method and replication of herpesvirus saimiri (HVS) of monkeys, murine gamma herpesvirus 68 (MHV 68), and herpes simplex type 1 (HSV-1) was measured by yield reduction assays. Inhibition of the immediate early ORF 50 gene promoter activity was measured by the dual luciferase method. Micromolar concentrations of THC inhibit KSHV and EBV reactivation in virus infected/immortalized B cells. THC also strongly inhibits lytic replication of MHV 68 and HVS in vitro. Importantly, concentrations of THC that inhibit virus replication of gamma herpesviruses have no effect on cell growth or HSV-1 replication, indicating selectivity. THC was shown to selectively inhibit the immediate early ORF 50 gene promoter of KSHV and MHV 68. THC specifically targets viral and/or cellular mechanisms required for replication and possibly shared by these gamma herpesviruses, and the endocannabinoid system is possibly involved in regulating gamma herpesvirus latency and lytic replication. The immediate early gene ORF 50 promoter activity was specifically inhibited by THC. These studies may also provide the foundation for the development of antiviral strategies utilizing non-psychoactive derivatives of THC.-Delta-9 tetrahydrocannabinol (THC) inhibits lytic replication of gamma oncogenic herpesviruses in vitro.",0
"Generalized estimating equations are a common modeling approach used in cluster randomized trials to account for within-cluster correlation. It is well known that the sandwich variance estimator is biased when the number of clusters is small (?40), resulting in an inflated type I error rate. Various bias correction methods have been proposed in the statistical literature, but how adequately they are utilized in current practice for cluster randomized trials is not clear. The aim of this study is to evaluate the use of generalized estimating equation bias correction methods in recently published cluster randomized trials and demonstrate the necessity of such methods when the number of clusters is small. Review of cluster randomized trials published between August 2013 and July 2014 and using generalized estimating equations for their primary analyses. Two independent reviewers collected data from each study using a standardized, pre-piloted data extraction template. A two-arm cluster randomized trial was simulated under various scenarios to show the potential effect of a small number of clusters on type I error rate when estimating the treatment effect. The nominal level was set at 0.05 for the simulation study. Of the 51 included trials, 28 (54.9%) analyzed 40 or fewer clusters with a minimum of four total clusters. Of these 28 trials, only one trial used a bias correction method for generalized estimating equations. The simulation study showed that with four clusters, the type I error rate ranged between 0.43 and 0.47. Even though type I error rate moved closer to the nominal level as the number of clusters increases, it still ranged between 0.06 and 0.07 with 40 clusters. Our results showed that statistical issues arising from small number of clusters in generalized estimating equations is currently inadequately handled in cluster randomized trials. Potential for type I error inflation could be very high when the sandwich estimator is used without bias correction.-Generalized estimating equations in cluster randomized trials with a small number of clusters: Review of practice and simulation study.",1
"Motivational interviewing (MI) is a promising practice to increase motivation, treatment retention, and reducing recidivism among offender populations. Computer-delivered interventions have grown in popularity as a way to change behaviors associated with drug and alcohol use. Motivational Assistance Program to Initiate Treatment (MAPIT) is a three arm, multisite, randomized controlled trial, which examines the impact of Motivational interviewing (MI), a motivational computer program (MC), and supervision as usual (SAU) on addiction treatment initiation, engagement, and retention. Secondary outcomes include drug/alcohol use, probation progress, recidivism (i.e., criminal behavior) and HIV/AIDS testing and treatment among probationers. Participant characteristics are measured at baseline, 2, and 6 months after assignment. The entire study will include 600 offenders, with each site recruiting 300 offenders (Baltimore City, Maryland and Dallas, Texas). All participants will go through standard intake procedures for probation and participate in probation requirements as usual. After standard intake, participants will be recruited and screened for eligibility. The results of this clinical trial will fill a gap in knowledge about ways to motivate probationers to participate in addiction treatment and HIV care. This randomized clinical trial is innovative in the way it examines the use of in-person vs. technological approaches to improve probationer success. NCT01891656.-Motivational tools to improve probationer treatment outcomes.",0
"Informative birth size occurs when the average outcome depends on the number of infants per birth. Although analysis methods have been proposed for handling informative birth size, their performance is not well understood. Our aim was to evaluate the performance of these methods and to provide recommendations for their application in randomised trials including infants from single and multiple births. Three generalised estimating equation (GEE) approaches were considered for estimating the effect of treatment on a continuous or binary outcome: cluster weighted GEEs, which produce treatment effects with a mother-level interpretation when birth size is informative; standard GEEs with an independence working correlation structure, which produce treatment effects with an infant-level interpretation when birth size is informative; and standard GEEs with an exchangeable working correlation structure, which do not account for informative birth size. The methods were compared through simulation and analysis of an example dataset. Treatment effect estimates were affected by informative birth size in the simulation study when the effect of treatment in singletons differed from that in multiples (i.e. in the presence of a treatment group by multiple birth interaction). The strength of evidence supporting the effectiveness of treatment varied between methods in the example dataset. Informative birth size is always a possibility in randomised trials including infants from both single and multiple births, and analysis methods should be pre-specified with this in mind. We recommend estimating treatment effects using standard GEEs with an independence working correlation structure to give an infant-level interpretation.-Analysis of Randomised Trials Including Multiple Births When Birth Size Is Informative.",1
"Electronic cigarettes (ECIGs) use an electric heater to aerosolize a liquid that usually contains propylene glycol, vegetable glycerin, flavorants, and the dependence-producing drug nicotine. ECIG-induced nicotine dependence has become an important concern, as some ECIGs deliver very little nicotine while some may exceed the nicotine delivery profile of a tobacco cigarette. This variability is relevant to tobacco cigarette smokers who try to switch to ECIGs. Products with very low nicotine delivery may not substitute for tobacco cigarettes, so that ECIG use is accompanied by little reduced risk of cigarette-caused disease. Products with very high nicotine delivery may make quitting ECIGs particularly difficult should users decide to try. For non-smokers, the wide variability of ECIGs on the market is especially troublesome: low nicotine products may lead them to initiate nicotine self-administration and progress to higher dosing ECIGs or other products, and those that deliver more nicotine may produce nicotine dependence where it was not otherwise present. External regulatory action, guided by strong science, may be required to ensure that population-level nicotine dependence does not rise.-Electronic cigarettes and nicotine dependence: evolving products, evolving problems.",0
"The stepped wedge cluster randomized trial always requires fewer clusters but not always fewer measurements, that is, participants than a parallel cluster randomized trial in a cross-sectional design. In reply.",3
"Several cluster-randomized trials are underway to investigate the implementation and effectiveness of a universal test-and-treat strategy on the HIV epidemic in sub-Saharan Africa. We consider nesting studies of pre-exposure prophylaxis within these trials. Pre-exposure prophylaxis is a general strategy where high-risk HIV- persons take antiretrovirals daily to reduce their risk of infection from exposure to HIV. We address how to target pre-exposure prophylaxis to high-risk groups and how to maximize power to detect the individual and combined effects of universal test-and-treat and pre-exposure prophylaxis strategies. We simulated 1000 trials, each consisting of 32 villages with 200 individuals per village. At baseline, we randomized the universal test-and-treat strategy. Then, after 3 years of follow-up, we considered four strategies for targeting pre-exposure prophylaxis: (1) all HIV- individuals who self-identify as high risk, (2) all HIV- individuals who are identified by their HIV+ partner (serodiscordant couples), (3) highly connected HIV- individuals, and (4) the HIV- contacts of a newly diagnosed HIV+ individual (a ring-based strategy). We explored two possible trial designs, and all villages were followed for a total of 7 years. For each village in a trial, we used a stochastic block model to generate bipartite (male-female) networks and simulated an agent-based epidemic process on these networks. We estimated the individual and combined intervention effects with a novel targeted maximum likelihood estimator, which used cross-validation to data-adaptively select from a pre-specified library the candidate estimator that maximized the efficiency of the analysis. The universal test-and-treat strategy reduced the 3-year cumulative HIV incidence by 4.0% on average. The impact of each pre-exposure prophylaxis strategy on the 4-year cumulative HIV incidence varied by the coverage of the universal test-and-treat strategy with lower coverage resulting in a larger impact of pre-exposure prophylaxis. Offering pre-exposure prophylaxis to serodiscordant couples resulted in the largest reductions in HIV incidence (2% reduction), and the ring-based strategy had little impact (0% reduction). The joint effect was larger than either individual effect with reductions in the 7-year incidence ranging from 4.5% to 8.8%. Targeted maximum likelihood estimation, data-adaptively adjusting for baseline covariates, substantially improved power over the unadjusted analysis, while maintaining nominal confidence interval coverage. Our simulation study suggests that nesting a pre-exposure prophylaxis study within an ongoing trial can lead to combined intervention effects greater than those of universal test-and-treat alone and can provide information about the efficacy of pre-exposure prophylaxis in the presence of high coverage of treatment for HIV+ persons.-Using a network-based approach and targeted maximum likelihood estimation to evaluate the effect of adding pre-exposure prophylaxis to an ongoing test-and-treat trial.",0
"Cluster randomized trials evaluate the effect of a treatment on persons nested within clusters, where treatment is randomly assigned to clusters. Current equations for the optimal sample size at the cluster and person level assume that the outcome variances and/or the study costs are known and homogeneous between treatment arms. This paper presents efficient yet robust designs for cluster randomized trials with treatment-dependent costs and treatment-dependent unknown variances, and compares these with 2 practical designs. First, the maximin design (MMD) is derived, which maximizes the minimum efficiency (minimizes the maximum sampling variance) of the treatment effect estimator over a range of treatment-to-control variance ratios. The MMD is then compared with the optimal design for homogeneous variances and costs (balanced design), and with that for homogeneous variances and treatment-dependent costs (cost-considered design). The results show that the balanced design is the MMD if the treatment-to control cost ratio is the same at both design levels (cluster, person) and within the range for the treatment-to-control variance ratio. It still is highly efficient and better than the cost-considered design if the cost ratio is within the range for the squared variance ratio. Outside that range, the cost-considered design is better and highly efficient, but it is not the MMD. An example shows sample size calculation for the MMD, and the computer code (SPSS and R) is provided as supplementary material. The MMD is recommended for trial planning if the study costs are treatment-dependent and homogeneity of variances cannot be assumed.-Efficient design of cluster randomized trials with treatment-dependent costs and treatment-dependent unknown variances.",1
"Selection bias and non-participation bias are major methodological concerns which impact external validity. Cluster-randomized controlled trials are especially prone to selection bias as it is impractical to blind clusters to their allocation into intervention or control. This study assessed the impact of selection bias in a large cluster-randomized controlled trial. The Improved Cardiovascular Risk Reduction to Enhance Rural Primary Care (ICARE) study examined the impact of a remote pharmacist-led intervention in twelve medical offices. To assess eligibility, a standardized form containing patient demographics and medical information was completed for each screened patient. Eligible patients were approached by the study coordinator for recruitment. Both the study coordinator and the patient were aware of the site's allocation prior to consent. Patients who consented or declined to participate were compared across control and intervention arms for differing characteristics. Statistical significance was determined using a two-tailed, equal variance t-test and a chi-square test with adjusted Bonferroni p-values. Results were adjusted for random cluster variation. There were 2749 completed screening forms returned to research staff with 461 subjects who had either consented or declined participation. Patients with poorly controlled diabetes were found to be significantly more likely to decline participation in intervention sites compared to those in control sites. A higher mean diastolic blood pressure was seen in patients with uncontrolled hypertension who declined in the control sites compared to those who declined in the intervention sites. However, these findings were no longer significant after adjustment for random variation among the sites. After this adjustment, females were now found to be significantly more likely to consent than males (odds ratio?=?1.41; 95% confidence interval?=?1.03, 1.92). Though there appeared to be a higher consent rate for females than for males, the overall impact of potential selection bias and refusal to participate was minimal. Without rigorous methodology, selection bias may be a threat to external validity in cluster-randomized trials. NCT01983813 . Date of registration: Oct. 28, 2013.-Selection bias and subject refusal in a cluster-randomized controlled trial.",1
"We discuss methodological issues arising in a recent evaluation trial of a new antenatal care programme, as sponsored by the Special Programme of Research, Development and Research Training in Human Reproduction, and WHO's Division of Reproductive Health (Technical Support). The randomisation unit for the trial is the antenatal care clinic, with 53 clinics located in four countries randomly allocated to provide either the new programme or the traditional programme currently in use. Approximately 24,000 women presenting for antenatal care over an average period of 18 months will have been recruited.-Methodological considerations in the design of the WHO Antenatal Care Randomised Controlled Trial.",1
"For most chronic medical conditions, multiple medications are available and prescribers often have limited evidence about which therapy is likely to be the most effective and safe for an individual patient. As many patients are exposed every day to medicines that may be less effective than available alternatives, this is of public health importance. Cluster randomised trials of prescribing policy offer an opportunity to rapidly obtain evidence of comparative effectiveness and safety. These trials can pose a low risk to patients and cause minimal disruption to usual care. Despite the potential scientific value of this approach, there remain valid concerns about consent, medication switching and the use of routinely collected data in research. We discuss these concerns with reference to an ongoing pilot study (Evaluating Diuretics in Normal Care (EVIDENCE) - a cluster randomised evaluation of hypertension prescribing policy, ISRCTN 46635087, registered 11 August 2017).-Cluster randomised trials of prescribing policy: an ethical approach to generating drug safety evidence? A discussion of the ethical application of a new research method",1
"HER2 is an oncogene, expression of which leads to poor prognosis in 30% of breast cancer patients. Although trastuzumab is apparently an effective therapy against HER2-positive tumors, its systemic toxicity and resistance in the majority of patients restricts its applicability. In this study we evaluated the effects of phenethyl isothiocyanate (PEITC) in HER2-positive breast cancer cells. MDA-MB-231 and MCF-7 breast cancer cells stably transfected with HER2 (high HER2 (HH)) were used in this study. The effect of PEITC was evaluated using cytotoxicity and apoptosis assay in these syngeneic cells. Western blotting was used to delineate HER2 signaling. SCID/NOD mice were implanted with MDA-MB-231 (HH) xenografts. Our results show that treatment of MDA-MB-231 and MCF-7 cells with varying concentrations of PEITC for 24 h extensively reduced the survival of the cells with a 50% inhibitory concentration (IC50) of 8 ?M in MDA-MB-231 and 14 ?M in MCF-7 cells. PEITC treatment substantially decreased the expression of HER2, epidermal growth factor receptor (EGFR) and phosphorylation of signal transducer and activator of transcription 3 (STAT3) at Tyr-705. The expression of BCL-2-associated ? (BAX) and BIM proteins were increased, whereas the levels of B cell lymphoma-extra large (BCL-XL) and X-linked inhibitor of apoptosis protein (XIAP) were significantly decreased in both the cell lines in response to PEITC treatment. Substantial cleavage of caspase 3 and poly-ADP ribose polymerase (PARP) were associated with PEITC-mediated apoptosis in MDA-MB-231 and MCF-7 cells. Notably, transient silencing of HER2 decreased and overexpressing HER2 increased the effects of PEITC. Furthermore, reactive oxygen species (ROS) generation, mitochondrial depolarization and apoptosis by PEITC treatment were much higher in breast cancer cells expressing higher levels of HER2 (HH) as compared to parent cell lines. The IC50 of PEITC following 24 h of treatment was reduced remarkably to 5 ?M in MDA-MB-231 (HH) and 4 ?M in MCF-7 (HH) cells, stably overexpressing HER2. Oral administration of 12 ?M PEITC significantly suppressed the growth of breast tumor xenografts in SCID/NOD mice. In agreement with our in vitro results, tumors from PEITC-treated mice demonstrated reduced HER2, EGFR and STAT3 expression and increased apoptosis as revealed by cleavage of caspase 3 and PARP. In addition our results show that PEITC can enhance the efficacy of doxorubicin. Our results show a unique specificity of PEITC in inducing apoptosis in HER2-expressing tumor cells in vitro and in vivo and enhancing the effects of doxorubicin. This unique specificity of PEITC offers promise to a subset of breast cancer patients overexpressing HER2.-Antitumor activity of phenethyl isothiocyanate in HER2-positive breast cancer models.",0
"Protective vaccine efficacy, VEs, is measured as one minus the incidence rate ratio (IRR) or the relative risk (RR) in the vaccinated group compared with the unvaccinated group. In this paper, we systematically present Bayesian estimation of protective vaccine efficacy based on the Poisson and binomial distributions. We also propose a new tool, the vaccine efficacy acceptability curve, to represent the uncertainty for the estimate of the vaccine efficacy graphically. It is very useful, especially when there is no universal agreement on the acceptable vaccine efficacy. The vaccine efficacy acceptability curve is defined as the posterior probability that the measure of vaccine efficacy VEs &gt; or = k for each acceptable value k. When a vaccine is highly efficacious, the number of vaccinated susceptibles being infected is likely to be very small or even zero. Then the assumptions of normality and log-normality of IRR or RR usually do not hold well. Although frequentist exact methods provide good estimates of the confidence interval, they are overly conservative and are computationally difficult to extend to estimate the vaccine efficacy acceptability curve. In this paper, our focus is on Bayesian estimation of protective vaccine efficacy, its highest probability density credible set, and the vaccine efficacy acceptability curve through Markov chain Monte Carlo (MCMC) methods. We illustrate the methods using the data from two pertussis vaccine studies and the H. influenza Type B preventive trial.-Bayesian estimation of vaccine efficacy.",0
"Cluster randomized trials (CRTs) present unique methodological and ethical challenges. Researchers conducting systematic reviews of CRTs (e.g., addressing methodological or ethical issues) require efficient electronic search strategies (filters or hedges) to identify trials in electronic databases such as MEDLINE. According to the CONSORT statement extension to CRTs, the clustered design should be clearly identified in titles or abstracts; however, variability in terminology may make electronic identification challenging. Our objectives were to (a) evaluate sensitivity (""recall"") and precision of a well-known electronic search strategy (""randomized controlled trial"" as publication type) with respect to identifying CRTs, (b) evaluate the feasibility of new search strategies targeted specifically at CRTs, and (c) determine whether CRTs are appropriately identified in titles or abstracts of reports and whether there has been improvement over time. We manually examined a wide range of health journals to identify a gold standard set of CRTs. Search strategies were evaluated against the gold standard set, as well as an independent set of CRTs included in previous systematic reviews. The existing strategy (randomized controlled trial.pt) is sensitive (93.8%) for identifying CRTs, but has relatively low precision (9%, number needed to read 11); the number needed to read can be halved to 5 (precision 18.4%) by combining with cluster design-related terms using the Boolean operator AND; combining with the Boolean operator OR maximizes sensitivity (99.4%) but would require 28.6 citations read to identify one CRT. Only about 50% of CRTs are clearly identified as cluster randomized in titles or abstracts; approximately 25% can be identified based on the reported units of randomization but are not amenable to electronic searching; the remaining 25% cannot be identified except through manual inspection of the full-text article. The proportion of trials clearly identified has increased from 28% between the years 2000-2003, to 60% between 2004-2007 (absolute increase 32%, 95% CI 17 to 47%). CRTs should include the phrase ""cluster randomized trial"" in titles or abstracts; this will facilitate more accurate indexing of the publication type by reviewers at the National Library of Medicine, and efficient textword retrieval of the subset employing cluster randomization.-Electronic search strategies to identify reports of cluster randomized trials in MEDLINE: low precision will improve with adherence to reporting standards.",1
"The study aims to provide information about variance components of psychosocial outcomes: within and between-participant variance, within-participant correlation and for cluster randomised trials, the intra-cluster correlation (ICC) and, also, to demonstrate how estimates of these variance components and ICCs can be used to design randomised trials and cluster randomised trials. Data from 15 longitudinal multi-centre psycho-oncology studies were analysed, and variance components including ICCs were estimated. Studies with psychosocial outcomes that had at least one measurement post-baseline including individual randomised controlled trials, cluster randomised trials and observational studies were included. Variance components and ICCs from 87 outcome measures were estimated. The unadjusted, single timepoint (first post-baseline) ICCs ranged from 0 to 0.16, with a median value of 0.022 and inter-quartile range 0 to 0.0605. The longitudinal ICCs ranged from 0 to 0.09 with a median value of 0.0007 and inter-quartile range 0 to 0.018. Although the magnitude of variance components and ICCs used for sample-size calculation cannot be known in advance of the study, published estimates can help reduce the uncertainty in sample-size calculations. Psycho-oncology researchers should be conservative in their sample-size calculations and use approaches that improve efficiency in their design and analysis.-Designing psycho-oncology randomised trials and cluster randomised trials: variance components and intra-cluster correlation of commonly used psychosocial measures.",1
"In assessing the mechanism of treatment efficacy in randomized clinical trials, investigators often perform mediation analyses by analyzing if the significant intent-to-treat treatment effect on outcome occurs through or around a third intermediate or mediating variable: indirect and direct effects, respectively. Standard mediation analyses assume sequential ignorability, i.e. conditional on covariates the intermediate or mediating factor is randomly assigned, as is the treatment in a randomized clinical trial. This research focuses on the application of the principal stratification (PS) approach for estimating the direct effect of a randomized treatment but without the standard sequential ignorability assumption. This approach is used to estimate the direct effect of treatment as a difference between expectations of potential outcomes within latent subgroups of participants for whom the intermediate variable behavior would be constant, regardless of the randomized treatment assignment. Using a Bayesian estimation procedure, we also assess the sensitivity of results based on the PS approach to heterogeneity of the variances among these principal strata. We assess this approach with simulations and apply it to two psychiatric examples. Both examples and the simulations indicated robustness of our findings to the homogeneous variance assumption. However, simulations showed that the magnitude of treatment effects derived under the PS approach were sensitive to model mis-specification.-Mediation analysis with principal stratification.",0
Optimal stepped wedge designs,3
"Participants in trials may be randomized either individually or in groups and may receive their treatment either entirely individually, entirely in groups, or partially individually and partially in groups. This paper concerns cases in which participants receive their treatment either entirely or partially in groups, regardless of how they were randomized. Participants in group-randomized trials are randomized in groups, and participants in individually randomized group treatment trials are individually randomized, but participants in both types of trials receive part or all of their treatment in groups or through common change agents. Participants who receive part or all of their treatment in a group are expected to have positively correlated outcome measurements. This paper addresses a situation that occurs in group-randomized trials and individually randomized group treatment trials-participants receive treatment through more than one group. As motivation, we consider trials in The Childhood Obesity Prevention and Treatment Research Consortium, in which each child participant receives treatment in at least two groups. In simulation studies, we considered several possible analytic approaches over a variety of possible group structures. A mixed model with random effects for both groups provided the only consistent protection against inflated type I error rates and did so at the cost of only moderate loss of power when intraclass correlations were not large. We recommend constraining variance estimates to be positive and using the Kenward-Roger adjustment for degrees of freedom; this combination provided additional power but maintained type I error rates at the nominal level.-Analytic methods for individually randomized group treatment trials and group-randomized trials when subjects belong to multiple groups.",2
"Randomization by Cluster, But Analysis by Individual Without Accommodating Clustering in the Analysis Is Incorrect: Comment",1
The benefits of taxing cigarettes in middle income countries.,0
"This article investigates maximum likelihood estimation with saturated and unsaturated models for correlated exchangeable binary data, when a sample of independent clusters of varying sizes is available. We discuss various parameterizations of these models, and propose using the EM algorithm to obtain maximum likelihood estimates. The methodology is illustrated by applications to a study of familial disease aggregation and to the design of a proposed group randomized cancer prevention trial.-Likelihood inference for exchangeable binary data with varying cluster sizes.",1
"Many of the difficulties encountered in the design, organization and analysis of cluster randomized trials arise from the dual nature of such trials; that is, they focus on both the cluster and the individual. A trial now in progress to compare three methods of promoting secondary prevention of coronary heart disease in primary care includes only 21 general practices, but 2142 patients, and thus contains the problems of both small and large samples. With only seven practices in each arm, the trial demanded carefully restricted randomization, may be difficult to analyse, and risks loss of power if one practice should drop out. At the same time, the large number of patients makes for an expensive and administratively complex study. The simultaneous demands of clarity and thoroughness point to an analysis at both cluster and individual level. With two different approaches, however, there may be difficulties of presentation, even if the results agree, and additional problems of interpretation if they do not. Finally, practical considerations may conflict with theoretical demands. Since the trial contained a service element, all patients with heart disease had to be included, even though it would have been more efficient to take only a sample of patients from some practices.-Putting theory into practice: a cluster randomized trial with a small number of clusters.",1
"We consider the design of stepped wedge trials with continuous recruitment and continuous outcome measures. Suppose we recruit from a fixed number of clusters where eligible participants present continuously, and suppose we have fine control over when each cluster crosses to the intervention. Suppose also that we want to minimise the number of participants, leading us to consider ""incomplete"" designs (i.e. without full recruitment). How can we schedule recruitment and cross-over at different clusters to recruit efficiently while achieving good precision? The large number of possible designs can make exhaustive searches impractical. Instead we consider an algorithm using iterative improvements to hunt for an efficient design. At each iteration (starting from a complete design) a single participant - the one with the smallest impact on precision - is removed, and small changes preserving total sample size are made until no further improvement in precision can be found. Striking patterns emerge. Solutions typically focus recruitment and cross-over on the leading diagonal of the cluster-by-time diagram, but in some scenarios clusters form distinct phases resembling before-and-after designs. There is much to be learned about optimal design for incomplete stepped wedge trials. Algorithmic searches could offer a practical approach to trial design in complex settings generally.-The hunt for efficient, incomplete designs for stepped wedge trials with continuous recruitment and continuous outcome measures",3
"This methodological review aims to determine the extent to which design and analysis aspects of cluster randomization have been appropriately dealt with in reports of primary prevention trials. All reports of primary prevention trials using cluster randomization that were published from 1990 to 1993 in the American Journal of Public Health and Preventive Medicine were identified. Each article was examined to determine whether cluster randomization was taken into account in the design and statistical analysis. Of the 21 articles, only 4 (19%) included sample size calculations or discussions of power that allowed for clustering, while 12 (57%) took clustering into account in the statistical analysis. Design and analysis issues associated with cluster randomization are not recognized widely enough. Reports of cluster randomized trials should include sample size calculations and statistical analyses that take clustering into account, estimates of design effects to help others planning trials, and a table showing the baseline distribution of important characteristics by intervention group, including the number of clusters and average cluster size for each group.-Accounting for cluster randomization: a review of primary prevention trials, 1990 through 1993.",1
Reporting of stepped wedge cluster randomised trials: extension of the CONSORT 2010 statement with explanation and elaboration.,3
Extending CONSORT to include cluster trials.,1
"The prevalence of intermittent claudication (IC) in older adults by questionnaire is less than 5% while the prevalence of peripheral arterial disease (PAD) by non-invasive testing is 2-4-fold higher. Comorbid conditions may result in under-reporting intermittent claudication (IC) as assessed by the Rose Questionnaire. We examined characteristics of those who report leg pain in relationship to other comorbid conditions and disability in 5888 participants of the Cardiovascular Health Study (CHS). Older adults with exertional leg pain, not meeting criteria for IC, had a higher prevalence of PAD on non-invasive testing with the ankle-arm index than those without pain, as well as a higher prevalence of arthritis. The pattern of responses suggested that pain for both conditions was reported together. The Rose Questionnaire for IC is specific for PAD, but a negative questionnaire does not indicate a lack of symptoms, rather the presence of PAD along with other conditions that can cause pain.-The role of comorbidity in the assessment of intermittent claudication in older adults.",0
"Significant advances have been made in understanding the genetic basis of systemic sclerosis (scleroderma) in recent years. Can these discoveries lead to individualized monitoring and treatment? Besides robustly replicated genetic susceptibility loci, several genes have been recently linked to various systemic sclerosis disease manifestations. Furthermore, inclusion of genetic studies in design and analysis of drug trials could lead to development of genetic biomarkers that predict treatment response. Future genetic studies in well-characterized systemic sclerosis cohorts paired with advanced analytic approaches can lead to development of genetic biomarkers for targeted diagnostic and therapeutic interventions in systemic sclerosis.-Genetics of scleroderma: implications for personalized medicine?",0
"The choice of design between individual randomisation, cluster or pseudo-cluster randomisation is often made difficult. Clear methodological guidelines have been given for trials in general practice, but not for vaccine trials. This article proposes a decisional flow-chart to choose the most adapted design for evaluating the effectiveness of a vaccine in large-scale studies. Six criteria have been identified: importance of herd immunity or herd protection, ability to delimit epidemiological units, homogeneity of transmission probability across sub-populations, population's acceptability of randomisation, availability of logistical resources, and estimated sample size. This easy to use decisional method could help sponsors, trial steering committees and ethical committees adopt the most suitable design.-Designing phase III or IV trials for vaccines: choosing between individual or cluster randomised trial designs.",1
"The magnitude of the effect of an intervention on a quantitative outcome may be expressed as a standardized mean difference by dividing the difference in means by the standard deviation of the outcome. This is useful to compare outcomes measured using different scales, especially in meta-analysis. However, uncertainty about the standard deviation leads to complicated formulae to avoid bias and to compute the correct standard error. We review approximate and exact formulae and argue for the use of the exact formulae. We then extend the formulae to cluster-randomized trials, and show how the calculations may be implemented using published results. We also describe methods for estimating the standard deviation. Various pitfalls are identified which can lead to major errors especially in the cluster-randomized setting.-Standardized mean differences in individually-randomized and cluster-randomized trials, with applications to meta-analysis.",1
"The need for evidence about the effectiveness of therapeutics and other medical practices has triggered new interest in methods for comparative effectiveness research. Describe an approach to comparative effectiveness research involving cluster randomized trials in networks of hospitals, health plans, or medical practices with centralized administrative and informatics capabilities. We discuss the example of an ongoing cluster randomized trial to prevent methicillin-resistant Staphylococcus aureus (MRSA) infection in intensive care units (ICUs). The trial randomizes 45 hospitals to: (a) screening cultures of ICU admissions, followed by Contact Precautions if MRSA-positive, (b) screening cultures of ICU admissions followed by decolonization if MRSA-positive, or (c) universal decolonization of ICU admissions without screening. All admissions to adult ICUs. The primary outcome is MRSA-positive clinical cultures occurring &gt;or=2 days following ICU admission. Secondary outcomes include blood and urine infection caused by MRSA (and, separately, all pathogens), as well as the development of resistance to decolonizing agents. Recruitment of hospitals is complete. Data collection will end in Summer 2011. This trial takes advantage of existing personnel, procedures, infrastructure, and information systems in a large integrated hospital network to conduct a low-cost evaluation of prevention strategies under usual practice conditions. This approach is applicable to many comparative effectiveness topics in both inpatient and ambulatory settings.-Cluster randomized trials in comparative effectiveness research: randomizing hospitals to test methods for prevention of healthcare-associated infections.",1
"Drug self-administration experiments are a frequently used approach to assessing the abuse liability and reinforcing property of a compound. It has been used to assess the abuse liabilities of various substances such as psychomotor stimulants and hallucinogens, food, nicotine, and alcohol. The demand curve generated from a self-administration study describes how demand of a drug or non-drug reinforcer varies as a function of price. With the approval of the 2009 Family Smoking Prevention and Tobacco Control Act, demand curve analysis provides crucial evidence to inform the US Food and Drug Administration's policy on tobacco regulation, because it produces several important quantitative measurements to assess the reinforcing strength of nicotine. The conventional approach popularly used to analyze the demand curve data is individual-specific non-linear least square regression. The non-linear least square approach sets out to minimize the residual sum of squares for each subject in the dataset; however, this one-subject-at-a-time approach does not allow for the estimation of between- and within-subject variability in a unified model framework. In this paper, we review the existing approaches to analyze the demand curve data, non-linear least square regression, and the mixed effects regression and propose a new Bayesian hierarchical model. We conduct simulation analyses to compare the performance of these three approaches and illustrate the proposed approaches in a case study of nicotine self-administration in rats. We present simulation results and discuss the benefits of using the proposed approaches.-A Bayesian hierarchical model for demand curve analysis.",0
"Many different methods have been proposed for the analysis of cluster randomized trials (CRTs) over the last 30 years. However, the evaluation of methods on overdispersed count data has been based mostly on the comparison of results using empiric data; i.e. when the true model parameters are not known. In this study, we assess via simulation the performance of five methods for the analysis of counts in situations similar to real community-intervention trials. We used the negative binomial distribution to simulate overdispersed counts of CRTs with two study arms, allowing the period of time under observation to vary among individuals. We assessed different sample sizes, degrees of clustering and degrees of cluster-size imbalance. The compared methods are: (i) the two-sample t-test of cluster-level rates, (ii) generalized estimating equations (GEE) with empirical covariance estimators, (iii) GEE with model-based covariance estimators, (iv) generalized linear mixed models (GLMM) and (v) Bayesian hierarchical models (Bayes-HM). Variation in sample size and clustering led to differences between the methods in terms of coverage, significance, power and random-effects estimation. GLMM and Bayes-HM performed better in general with Bayes-HM producing less dispersed results for random-effects estimates although upward biased when clustering was low. GEE showed higher power but anticonservative coverage and elevated type I error rates. Imbalance affected the overall performance of the cluster-level t-test and the GEE's coverage in small samples. Important effects arising from accounting for overdispersion are illustrated through the analysis of a community-intervention trial on Solar Water Disinfection in rural Bolivia.-Performance of analytical methods for overdispersed counts in cluster randomized trials: sample size, degree of clustering and imbalance.",1
"In spite of recent contributions to the literature, informative cluster size settings are not well known and understood. In this paper, we give a formal definition of the problem and describe it from different viewpoints. Data generating mechanisms, parametric and nonparametric models are considered in light of examples. Our emphasis is on nonparametric and robust approaches to the inference on the marginal distribution. Descriptive statistics and parameters of interest are defined as functionals and they are accompanied with a generally applicable testing procedure. The theory is illustrated with an example on patients with incomplete spinal cord injuries.-Inference on the marginal distribution of clustered data with informative cluster size.",1
"This article presents the current state of patient-reported outcome measures and explains new opportunities for leveraging the recent adoption of electronic health records to expand the application of patient-reported outcomes in both clinical care and comparative effectiveness research. Historic developments of patient-reported outcome, electronic health record, and comparative effectiveness research are analyzed in two dimensions: patient centeredness and digitization. We pose the question, ""What needs to be standardized around the collection of patient-reported outcomes in electronic health records for comparative effectiveness research?"" We identified three converging trends: the progression of patient-reported outcomes toward greater patient centeredness and electronic adaptation; the evolution of electronic health records into personalized and fully digitized solutions; and the shift toward patient-oriented comparative effectiveness research. Related to this convergence, we propose an architecture for patient-reported outcome standardization that could serve as a first step toward a more comprehensive integration of patient-reported outcomes with electronic health record for both practice and research. The science of patient-reported outcome measurement has matured sufficiently to be integrated routinely into electronic health records and other electronic health solutions to collect data on an ongoing basis for clinical care and comparative effectiveness research. Further efforts and ideally coordinated efforts from various stakeholders are needed to refine the details of the proposed framework for standardization.-Measure once, cut twice--adding patient-reported outcome measures to the electronic health record for comparative effectiveness research.",0
"Marginal structural models were developed as a semiparametric alternative to the G-computation formula to estimate causal effects of exposures. In practice, these models are often specified using parametric regression models. As such, the usual conventions regarding regression model specification apply. This paper outlines strategies for marginal structural model specification and considerations for the functional form of the exposure metric in the final structural model. We propose a quasi-likelihood information criterion adapted from use in generalized estimating equations. We evaluate the properties of our proposed information criterion using a limited simulation study. We illustrate our approach using two empirical examples. In the first example, we use data from a randomized breastfeeding promotion trial to estimate the effect of breastfeeding duration on infant weight at 1 year. In the second example, we use data from two prospective cohorts studies to estimate the effect of highly active antiretroviral therapy on CD4 count in an observational cohort of HIV-infected men and women. The marginal structural model specified should reflect the scientific question being addressed but can also assist in exploration of other plausible and closely related questions. In marginal structural models, as in any regression setting, correct inference depends on correct model specification. Our proposed information criterion provides a formal method for comparing model fit for different specifications.-An information criterion for marginal structural models.",0
"Type I error probability spending functions are commonly used for designing sequential analysis of binomial data in clinical trials, but it is also quickly emerging for near-continuous sequential analysis of post-market drug and vaccine safety surveillance. It is well known that, for clinical trials, when the null hypothesis is not rejected, it is still important to minimize the sample size. Unlike in post-market drug and vaccine safety surveillance, that is not important. In post-market safety surveillance, specially when the surveillance involves identification of potential signals, the meaningful statistical performance measure to be minimized is the expected sample size when the null hypothesis is rejected. The present paper shows that, instead of the convex Type I error spending shape conventionally used in clinical trials, a concave shape is more indicated for post-market drug and vaccine safety surveillance. This is shown for both, continuous and group sequential analysis.-Type I error probability spending for post-market drug and vaccine safety surveillance with binomial data.",0
"We examined differences in preferences for the EQ-5D health states among blacks, Hispanics, and others living in the United States. A multi-stage probability sample was selected from the adult U.S. population. Each respondent valued a subset of the 243 EQ-5D health states. Regression analysis was used to analyze differences in mean valuations for 13 health states among the racial/ethnic groups. First, we compared unadjusted mean valuations among the three groups. Second, we evaluated differences in mean valuations among the groups after adjusting for other sociodemographic characteristics. Third, we evaluated the impact of race/ethnicity on estimates derived from the U.S. health state valuation (i.e., D1) model. Valuations differed among the groups for seven of the 13 health states, and these differences persisted after adjusting for other sociodemographic factors. Blacks appeared to perceive extreme health problems to be associated with less disutility than did members of the other two groups. Within the United States, there exist racial/ethnic differences in the perceived desirability of the EQ-5D health states that cannot be readily explained by socioeconomic disparities.-Racial/ethnic differences in preferences for the EQ-5D health states: results from the U.S. valuation study.",0
"There is an increased focus on randomized trials for proximal behavioral outcomes in early childhood research. However, planning sample sizes for such designs requires extant information on the size of effect, variance decomposition, and effectiveness of covariates. The purpose of this article is to employ a recent large representative sample of early childhood longitudinal study kindergartners to estimate design parameters for use in planning cluster randomized trials. A secondary objective is to compare the results of math and reading with the previous kindergartner cohort of 1999. For each measure, fall-spring gains in effect size units are calculated. In addition, multilevel models are fit to estimate variance components that are used to calculate intraclass correlations (ICCs) and R 2 statistics. The implications of the reported parameters are summarized in tables of required school sample sizes to detect small effects. The outcomes include information about student scores regarding learning behaviors, general behaviors, and academic abilities. Aside from math and reading, there were small gains in these measures from fall to spring, leading to effect sizes between about .1 and .2. In addition, the nonacademic ICCs are smaller than the academic ICCs but are still nontrivial. Use of a pretest covariate is generally effective in reducing the required sample size in power analyses. The ICCs for math and reading are smaller for the current sample compared with the 1999 sample.-Academic and Behavioral Design Parameters for Cluster Randomized Trials in Kindergarten: An Analysis of the Early Childhood Longitudinal Study 2011 Kindergarten Cohort (ECLS-K 2011).",1
"Identifying unusual growth-related measurements or longitudinal patterns in growth is often the focus in fetal and pediatric medicine. For example, the goal of the ongoing National Fetal Growth Study is to develop both cross-sectional and longitudinal reference curves for ultrasound fetal growth measurements that can be used for this purpose. Current methodology for estimating cross-sectional and longitudinal reference curves relies mainly on the linear mixed model. The focus of this paper is on examining the robustness of percentile estimation to the assumptions with respect to the Gaussian random-effect assumption implicitly made in the standard linear mixed model. We also examine a random-effects distribution based on mixtures of normals and compare the two approaches under both correct and misspecified random-effects distributions. In general, we find that the standard linear mixed model is relatively robust for cross-sectional percentile estimation but less robust for longitudinal or 'personalized' reference curves based on the conditional distribution given prior ultrasound measurements. The methodology is illustrated with data from a longitudinal fetal growth study.-The impact of random-effect misspecification on percentile estimation for longitudinal growth data.",0
"In the examination of the association between vaccines and rare adverse events after vaccination in postlicensure observational studies, it is challenging to define appropriate risk windows because prelicensure RCTs provide little insight on the timing of specific adverse events. Past vaccine safety studies have often used prespecified risk windows based on prior publications, biological understanding of the vaccine, and expert opinion. Recently, a data-driven approach was developed to identify appropriate risk windows for vaccine safety studies that use the self-controlled case series design. This approach employs both the maximum incidence rate ratio and the linear relation between the estimated incidence rate ratio and the inverse of average person time at risk, given a specified risk window. In this paper, we present a scan statistic that can identify appropriate risk windows in vaccine safety studies using the self-controlled case series design while taking into account the dependence of time intervals within an individual and while adjusting for time-varying covariates such as age and seasonality. This approach uses the maximum likelihood ratio test based on fixed-effects models, which has been used for analyzing data from self-controlled case series design in addition to conditional Poisson models.-A scan statistic for identifying optimal risk windows in vaccine safety studies using self-controlled case series design.",0
"Individual participant data (IPD) meta-analyses often analyze their IPD as if coming from a single study. We compare this approach with analyses that rather account for clustering of patients within studies. Comparison of effect estimates from logistic regression models in real and simulated examples. The estimated prognostic effect of age in patients with traumatic brain injury is similar, regardless of whether clustering is accounted for. However, a family history of thrombophilia is found to be a diagnostic marker of deep vein thrombosis [odds ratio, 1.30; 95% confidence interval (CI): 1.00, 1.70; P?=?0.05] when clustering is accounted for but not when it is ignored (odds ratio, 1.06; 95% CI: 0.83, 1.37; P?=?0.64). Similarly, the treatment effect of nicotine gum on smoking cessation is severely attenuated when clustering is ignored (odds ratio, 1.40; 95% CI: 1.02, 1.92) rather than accounted for (odds ratio, 1.80; 95% CI: 1.29, 2.52). Simulations show models accounting for clustering perform consistently well, but downwardly biased effect estimates and low coverage can occur when ignoring clustering. Researchers must routinely account for clustering in IPD meta-analyses; otherwise, misleading effect estimates and conclusions may arise.-Individual participant data meta-analyses should not ignore clustering.",1
A man with chest pain and acute ST elevations on electrocardiogram.,0
"The authors examined whether early ultrasound dating (?20 weeks) of gestational age (GA) in small-for-gestational-age (SGA) fetuses may underestimate gestational duration and therefore the incidence of SGA birth. Within a population-based case-control study (May 2002-June 2005) of Iowa SGA births and preterm deliveries identified from birth records (n = 2,709), the authors illustrate a novel methodological approach with which to assess and correct for systematic underestimation of GA by early ultrasound in women with suspected SGA fetuses. After restricting the analysis to subjects with first-trimester prenatal care, a nonmissing date of the last menstrual period (LMP), and early ultrasound (n = 1,135), SGA subjects' ultrasound GA was 5.5 days less than their LMP GA, on average. Multivariable linear regression was conducted to determine the extent to which ultrasound GA predicted LMP dating and to correct for systematic misclassification that results after applying standard guidelines to adjudicate differences in these measures. In the unadjusted model, SGA subjects required a correction of +1.5 weeks to the ultrasound estimate. With adjustment for maternal age, smoking, and first-trimester vaginal bleeding, standard guidelines for adjudicating differences in ultrasound and LMP dating underestimated SGA birth by 12.9% and overestimated preterm delivery by 8.7%. This methodological approach can be applied by researchers using different study populations in similar research contexts.-Correction of systematic bias in ultrasound dating in studies of small-for-gestational-age birth: an example from the Iowa Health in Pregnancy Study.",0
"Community intervention evaluations that measure changes over time may conduct repeated cross-sectional surveys, follow a cohort of residents over time, or (often) use both designs. Each survey design has implications for precision and cost. To explore these issues, we assume that two waves of surveys are conducted, and that the goal is to estimate change in behavior for people who reside in the community at both times. Cohort designs are shown to provide more accurate estimates (in the sense of lower mean squared error) than cross-sectional estimates if (1) there is strong correlation over time in an individual's behavior at time 0 and time 1, (2) relatively few subjects are lost to followup, (3) the bias is relatively small, and (4) the available sample size is not too large. Otherwise, a repeated cross-sectional design is more efficient. We developed methods for choosing between the two designs, and applied them to actual survey data. Owing to drop-outs and losses to followup, the cohort estimates were usually more biased than the cross-sectional estimates. The correlations over time for most of the variables studied were also high. In many instances the cohort estimate, although biased, is preferred to the relatively unbiased cross-sectional estimate because the mean squared error was smaller for the cohort than for the cross-sectional estimate. If these results are replicated in other data, they may result in guidelines for choosing a more efficient study design.-Optimal survey design for community intervention evaluations: cohort or cross-sectional?",1
"To investigate the role of human papillomavirus (HPV) in the development of cervical neoplasia in women with no previous cervical cytological abnormalities; whether the presence of virus DNA predicts development of squamous intraepithelial lesion; and whether the risk of incident squamous intraepithelial lesions differs with repeated detection of the same HPV type versus repeated detection of different types. Population based prospective cohort study. General population in Copenhagen, Denmark. 10 758 women aged 20-29 years followed up for development of cervical cytological abnormalities; 370 incident cases were detected (40 with atypical squamous cells of undetermined significance, 165 with low grade squamous intraepithelial lesions, 165 with high grade squamous intraepithelial lesions). RESULTS of cervical smear tests and cervical swabs at enrollment and at the second examination about two years later. Compared with women who were negative for human papillomavirus at enrollment, those with positive results had a significantly increased risk at follow up of having atypical cells (odds ratio 3.2, 95% confidence interval 1.3 to 7.9), low grade lesions (7.5, 4.8 to 11.7), or high grade lesions (25.8, 15.3 to 43.6). Similarly, women who were positive for HPV at the second examination had a strongly increased risk of low (34.3, 17.6 to 67.0) and high grade lesions (60.7, 25.5 to 144.0). For high grade lesions the risk was strongly increased if the same virus type was present at both examinations (813.0, 168.2 to 3229.2). Infection with human papillomavirus precedes the development of low and high grade squamous intraepithelial lesions. For high grade lesions the risk is greatest in women positive for the same type of HPV on repeated testing.-Type specific persistence of high risk human papillomavirus (HPV) as indicator of high grade cervical squamous intraepithelial lesions in young women: population based prospective follow up study.",0
"School-based drug-use prevention studies often apply interventions to entire schools. A major problem for these studies results from the intragroup dependence often seen when intact social groups are assigned to study conditions. Analysis of data from 2 such studies revealed intraclass correlation coefficients between 0.02 and 0.05 for common drug use measures. Because even such modest intragroup dependence can invalidate the traditional fixed-effects analyses, researchers should adopt alternative methods that acknowledge this dependence. These alternative methods are reviewed, and appropriate methods for computing sample size requirements are illustrated. Investigators should consider these analysis issues when planning future studies, because the number of schools required for an unbiased analysis may be substantially greater than for the traditional methods.-Planning for the appropriate analysis in school-based drug-use prevention studies.",1
"We examine the practicality of propensity score methods for estimating causal treatment effects conditional on intermediate posttreatment outcomes (principal effects) in the context of randomized experiments. In particular, we focus on the sensitivity of principal causal effect estimates to violation of principal ignorability, which is the primary assumption that underlies the use of propensity score methods to estimate principal effects. Under principal ignorability (PI), principal strata membership is conditionally independent of the potential outcome under control given the pre-treatment covariates; i.e. there are no differences in the potential outcomes under control across principal strata given the observed pretreatment covariates. Under this assumption, principal scores modeling principal strata membership can be estimated based solely on the observed covariates and used to predict strata membership and estimate principal effects. While this assumption underlies the use of propensity scores in this setting, sensitivity to violations of it has not been studied rigorously. In this paper, we explicitly define PI using the outcome model (although we do not actually use this outcome model in estimating principal scores) and systematically examine how deviations from the assumption affect estimates, including how the strength of association between principal stratum membership and covariates modifies the performance. We find that when PI is violated, very strong covariate predictors of stratum membership are needed to yield accurate estimates of principal effects.-On the use of propensity scores in principal causal effect estimation.",0
"In clinical trials with a small sample size, the characteristics (covariates) of patients assigned to different treatment arms may not be well balanced. This may lead to an inflated type I error rate. This problem can be more severe in trials that use response-adaptive randomization rather than equal randomization because the former may result in smaller sample sizes for some treatment arms. We have developed a patient allocation scheme for trials with binary outcomes to adjust the covariate imbalance during response-adaptive randomization. We used simulation studies to evaluate the performance of the proposed design. The proposed design keeps the important advantage of a standard response-adaptive design, that is to assign more patients to the better treatment arms, and thus it is ethically appealing. On the other hand, the proposed design improves over the standard response-adaptive design by controlling covariate imbalance between treatment arms, maintaining the nominal type I error rate, and offering greater power.-Response-adaptive randomization for clinical trials with adjustment for covariate imbalance.",0
"This article is part of a series of papers examining ethical issues in cluster randomized trials (CRTs) in health research. In the introductory paper in this series, we set out six areas of inquiry that must be addressed if the CRT is to be set on a firm ethical foundation. This paper addresses the sixth of the questions posed, namely, what is the role and authority of gatekeepers in CRTs in health research? 'Gatekeepers' are individuals or bodies that represent the interests of cluster members, clusters, or organizations. The need for gatekeepers arose in response to the difficulties in obtaining informed consent because of cluster randomization, cluster-level interventions, and cluster size. In this paper, we call for a more restrictive understanding of the role and authority of gatekeepers.Previous papers in this series have provided solutions to the challenges posed by informed consent in CRTs without the need to invoke gatekeepers. We considered that consent to randomization is not required when cluster members are approached for consent at the earliest opportunity and before any study interventions or data-collection procedures have started. Further, when cluster-level interventions or cluster size means that obtaining informed consent is not possible, a waiver of consent may be appropriate. In this paper, we suggest that the role of gatekeepers in protecting individual interests in CRTs should be limited. Generally, gatekeepers do not have the authority to provide proxy consent for cluster members. When a municipality or other community has a legitimate political authority that is empowered to make such decisions, cluster permission may be appropriate; however, gatekeepers may usefully protect cluster interests in other ways. Cluster consultation may ensure that the CRT addresses local health needs, and is conducted in accord with local values and customs. Gatekeepers may also play an important role in protecting the interests of organizations, such as hospitals, nursing homes, general practices, and schools. In these settings, permission to access the organization relies on resource implications and adherence to institutional policies.-What is the role and authority of gatekeepers in cluster randomized trials in health research?",1
"This paper reviews cluster randomized trials for HIV prevention, focusing on the unique aspects of such trials and describes key cluster randomized trials for HIV prevention from the past decade. Relevant methodological issues are also reviewed. Three cluster randomized trials evaluating sexually transmitted infection control as an HIV prevention tool have given mixed results. Recent modeling studies suggest that the differences are due to differences in the underlying populations rather than design differences. Two trials have shown reduced HIV incidence in intervention target groups without showing a community-wide effect. Areas of active methodological research include ensuring baseline comparability between arms in cluster randomized trials with relatively few clusters, and measuring incidence when the intervention may require time to be fully effective. Innovative approaches to measuring incidence based on so-called 'detuned assays' may be relevant to this problem. Cluster randomized trials are qualitatively different from individually randomized trials. As the intervention population may differ from the assessment population, cluster randomized trials of infectious diseases can estimate both direct and indirect intervention effects. Most cluster randomized trial interventions involve behavioral or social components, so the generalizability of results to other settings is often difficult. Modeling studies can aid in the interpretation of results.-Cluster randomized trials for HIV prevention.",1
"There is now widespread acknowledgement of the absence of a sound evidence base underpinning many of the decisions made in primary care. Randomised controlled trials represent the methodology of choicefor determining efficacy and effectiveness of interventions, yet researchers working in primary care have been reluctant to use intervention studies, favouring observational study designs. Unfamiliarity with the different trial designs now available, and the relative advantages and disadvantages conferred by each, may be one factor contributing to this paradox. In this paper, we consider the principal trial designs available to primary care researchers, discussing the contexts in which a particular design may prove most useful. This information will, we hope, also prove useful to primary care clinicians attempting to interpret trial findings.-Randomised controlled trials in primary care: scope and application.",1
"Cluster-randomized trials (CRTs) are able to address research questions that randomized controlled trials (RCTs) of individual patients cannot answer. Of great interest for infectious disease physicians and infection control practitioners are research questions relating to the impact of interventions on infectious disease dynamics at the whole-of-population level. However, there are important conceptual differences between CRTs and RCTs relating to design, analysis, and inference. These differences can be illustrated by the adage ""peas in a pod."" Does the question of interest relate to the ""peas"" (the individual patients) or the ""pods"" (the clusters)? Several examples of recent CRTs of community and intensive care unit infection prevention interventions are used to illustrate these key concepts. Examples of differences between the results of RCTs and CRTs on the same topic are given.-How the Cluster-randomized Trial ""Works""",1
"Studies involving clustering effects are common, but there is little consistency in their analysis. Various analytical methods were compared for a factorial cluster randomized trial (CRT) of two primary care-based interventions designed to increase breast screening attendance. Three cluster-level and five individual-level options were compared in respect of log odds ratios of attendance and their standard errors (SE), for the two intervention effects and their interaction. Cluster-level analyses comprised: (C1) unweighted regression of practice log odds; (C2) regression of log odds weighted by their inverse variance; (C3) random-effects meta-regression of log odds with practice as a random effect. Individual-level analyses comprised: (I1) standard logistic regression ignoring clustering; (I2) robust SE; (I3) generalized estimating equations; (I4) random-effects logistic regression; (I5) Bayesian random-effects logistic regression. Adjustments for stratification and baseline variables were investigated. As expected, method I1 was highly anti-conservative. The other, valid, methods exhibited considerable differences in parameter estimates and standard errors, even between the various random-effects methods based on the same statistical model. Method I4 was particularly sensitive to between-cluster variation and was computationally stable only after controlling for baseline uptake. Commonly used methods for the analysis of CRT can give divergent results. Simulation studies are needed to compare results from different methods in situations typical of cluster trials but when the true model parameters are known.-Comparison of methods for analysing cluster randomized trials: an example involving a factorial design.",1
"Cluster randomized control trials (cRCTs) have unique challenges compared to single site trials with regards to conduct of the trial, and it is important to understand these barriers. The aim of this scoping review was to describe the current literature surrounding the implementation of the cRCTs in hospitals. The search strategy was designed to identify literature relevant to conduct of cRCTs, with hospitals as the unit of randomization. Data was extracted and was mapped using the Consolidated Framework for Implementation Research (CFIR) as a codebook, which contains 39 constructs organized into five domains. Twenty-two articles met inclusion criteria and were included. 18 of 39 constructs of the CFIR were identified in coding, spanning four of the five domains. Barriers to the conduct of the trial were rarely reported as the main outcome of the study, and few details were included in the identified literature. The review can provide guidance to future researchers planning cRCTs in hospitals. It also identified a large gap in reporting of conduct of these trials, demonstrating the need for a research agenda that further explores the barriers and facilitators, with the aim of garnering knowledge for improved guidance in the implementation.-Barriers and enablers to conducting cluster randomized control trials in hospitals: A theory-informed scoping review",1
"We are motivated by a randomized clinical trial evaluating the efficacy of amitriptyline for the treatment of interstitial cystitis and painful bladder syndrome in treatment-na?ve patients. In the trial, both the non-adherence rate and the rate of loss to follow-up are fairly high. To estimate the effect of the treatment received on the outcome, we use the generalized structural mean model (GSMM), originally proposed to deal with non-adherence, to adjust for both non-adherence and loss to follow-up. In the model, loss to follow-up is handled by weighting the estimation equations for GSMM with one over the probability of not being lost to follow-up, estimated using a logistic regression model. We re-analyzed the data from the trial and found a possible benefit of amitriptyline when administered at a high-dose level.-Estimating the efficacy of an interstitial cystitis/painful bladder syndrome medication in a randomized trial with both non-adherence and loss to follow-up.",0
Design and Analysis of Cluster Randomization Trials,1
"There is increasing recognition of the critical role of intracluster correlations of health behavior outcomes in cluster intervention trials. This study examines the estimation, reporting, and use of intracluster correlations in planning cluster trials. We use an estimating equations approach to estimate the intracluster correlations corresponding to the multiple-time-point nested cross-sectional design. Sample size formulae incorporating 2 types of intracluster correlations are examined for the purpose of planning future trials. The traditional intracluster correlation is the correlation among individuals within the same community at a specific time point. A second type is the correlation among individuals within the same community at different time points. For a ""time x condition"" analysis of a pretest-posttest nested cross-sectional trial design, we show that statistical power considerations based upon a posttest-only design generally are not an adequate substitute for sample size calculations that incorporate both types of intracluster correlations. Estimation, reporting, and use of intracluster correlations are illustrated for several dichotomous measures related to underage drinking collected as part of a large nonrandomized trial to enforce underage drinking laws in the United States from 1998 to 2004.-The importance and role of intracluster correlations in planning cluster trials.",1
"Transdiagnostic processes confer risk for multiple types of psychopathology and explain the co-occurrence of different disorders. For this reason, transdiagnostic processes provide ideal targets for early intervention and treatment. Childhood trauma exposure is associated with elevated risk for virtually all commonly occurring forms of psychopathology. We articulate a transdiagnostic model of the developmental mechanisms that explain the strong links between childhood trauma and psychopathology as well as protective factors that promote resilience against multiple forms of psychopathology. We present a model of transdiagnostic mechanisms spanning three broad domains: social information processing, emotional processing, and accelerated biological aging. Changes in social information processing that prioritize threat-related information-such as heightened perceptual sensitivity to threat, misclassification of negative and neutral emotions as anger, and attention biases towards threat-related cues-have been consistently observed in children who have experienced trauma. Patterns of emotional processing common in children exposed to trauma include elevated emotional reactivity to threat-related stimuli, low emotional awareness, and difficulties with emotional learning and emotion regulation. More recently, a pattern of accelerated aging across multiple biological metrics, including pubertal development and cellular aging, has been found in trauma-exposed children. Although these changes in social information processing, emotional responding, and the pace of biological aging reflect developmental adaptations that may promote safety and provide other benefits for children raised in dangerous environments, they have been consistently associated with the emergence of multiple forms of internalizing and externalizing psychopathology and explain the link between childhood trauma exposure and transdiagnostic psychopathology. Children with higher levels of social support, particularly from caregivers, are less likely to develop psychopathology following trauma exposure. Caregiver buffering of threat-related processing may be one mechanism explaining this protective effect. Childhood trauma exposure is a powerful transdiagnostic risk factor associated with elevated risk for multiple forms of psychopathology across development. Changes in threat-related social and emotional processing and accelerated biological aging serve as transdiagnostic mechanisms linking childhood trauma with psychopathology. These transdiagnostic mechanisms represent critical targets for early interventions aimed at preventing the emergence of psychopathology in children who have experienced trauma.-Mechanisms linking childhood trauma exposure and psychopathology: a transdiagnostic model of risk and resilience.",0
"Multilevel modeling (MLM) is frequently used to detect cluster-level group differences in cluster randomized trial and observational studies. Group differences on the outcomes (posttest scores) are detected by controlling for the covariate (pretest scores) as a proxy variable for unobserved factors that predict future attributes. The pretest and posttest scores that are most often used in MLM are total scores. In prior research, there have been concerns regarding measurement error in the use of total scores in using MLM. In this article, using ordinary least squares and an attenuation formula, we derive the measurement error correction formula for cluster-level group difference estimates from MLM in the presence of measurement error in the outcome, the covariate, or both. Examples are provided to illustrate the correction formula in cluster randomized and observational studies using between-cluster reliability coefficients recently developed.-Measurement Error Correction Formula for Cluster-Level Group Differences in Cluster Randomized and Observational Studies.",1
"There is increasing interest in estimating the causal effects of treatments using observational data. Propensity-score matching methods are frequently used to adjust for differences in observed characteristics between treated and control individuals in observational studies. Survival or time-to-event outcomes occur frequently in the medical literature, but the use of propensity score methods in survival analysis has not been thoroughly investigated. This paper compares two approaches for estimating the Average Treatment Effect (ATE) on survival outcomes: Inverse Probability of Treatment Weighting (IPTW) and full matching. The performance of these methods was compared in an extensive set of simulations that varied the extent of confounding and the amount of misspecification of the propensity score model. We found that both IPTW and full matching resulted in estimation of marginal hazard ratios with negligible bias when the ATE was the target estimand and the treatment-selection process was weak to moderate. However, when the treatment-selection process was strong, both methods resulted in biased estimation of the true marginal hazard ratio, even when the propensity score model was correctly specified. When the propensity score model was correctly specified, bias tended to be lower for full matching than for IPTW. The reasons for these biases and for the differences between the two methods appeared to be due to some extreme weights generated for each method. Both methods tended to produce more extreme weights as the magnitude of the effects of covariates on treatment selection increased. Furthermore, more extreme weights were observed for IPTW than for full matching. However, the poorer performance of both methods in the presence of a strong treatment-selection process was mitigated by the use of IPTW with restriction and full matching with a caliper restriction when the propensity score model was correctly specified.-The performance of inverse probability of treatment weighting and full matching on the propensity score in the presence of model misspecification when estimating the effect of treatment on survival outcomes.",0
"Cluster randomization trials in which intact social units are randomly assigned to different intervention groups have become very popular in recent years, particularly for the evaluation of innovations in the delivery of health care. An extensive literature dealing with the associated methodological challenges has also appeared. Although the monitoring of such trials using formal stopping rules is clearly indicated when the outcomes are irreversible and individual-level data are available sequentially, simple and reliable statistical methods that may be used for this purpose are currently not available. To investigate the validity of standard group sequential methods when applied to cluster randomization trials having binary outcomes. The large sample distributions for each of five test statistics computed from sequentially accumulated data are derived. A simulation study is performed to evaluate the finite sample properties of these statistics when applied to the interim analysis of cluster randomization trials. Data from the World Health Organization antenatal care trial are used to illustrate the methods. Each of the joint distributions is shown to be characterized by a covariance structure that asymptotically satisfies an independent increments structure, a foundation that simplifies group sequential methods. The simulation study reveals that four of the five test statistics evaluated provide satisfactory performance with as few as 10 clusters allocated to each of two interventions. The applicability of our results to effect estimation following a group sequential cluster randomization trial is not investigated, although a theoretical foundation which may be used for this purpose is presented. Standard group sequential methods can be applied to cluster randomization trials when interim analyses are warranted.-Group sequential methods for cluster randomization trials with binary outcomes.",1
"Colorectal cancer is the second leading cause of death from cancer in the United States. To facilitate the efficiency of colorectal cancer screening, there is a need to stratify risk for colorectal cancer among the 90% of US residents who are considered ""average risk."" In this article, we investigate such risk stratification rules for advanced colorectal neoplasia (colorectal cancer and advanced, precancerous polyps). We use a recently completed large cohort study of subjects who underwent a first screening colonoscopy. Logistic regression models have been used in the literature to estimate the risk of advanced colorectal neoplasia based on quantifiable risk factors. However, logistic regression may be prone to overfitting and instability in variable selection. Since most of the risk factors in our study have several categories, it was tempting to collapse these categories into fewer risk groups. We propose a penalized logistic regression method that automatically and simultaneously selects variables, groups categories, and estimates their coefficients by penalizing the [Formula: see text]-norm of both the coefficients and their differences. Hence, it encourages sparsity in the categories, i.e. grouping of the categories, and sparsity in the variables, i.e. variable selection. We apply the penalized logistic regression method to our data. The important variables are selected, with close categories simultaneously grouped, by penalized regression models with and without the interactions terms. The models are validated with 10-fold cross-validation. The receiver operating characteristic curves of the penalized regression models dominate the receiver operating characteristic curve of naive logistic regressions, indicating a superior discriminative performance.-Advanced colorectal neoplasia risk stratification by penalized logistic regression.",0
"Numerous publications have now addressed the principles of designing, analyzing, and reporting the results of stepped-wedge cluster randomized trials. In contrast, there is little research available pertaining to the design and analysis of multiarm stepped-wedge cluster randomized trials, utilized to evaluate the effectiveness of multiple experimental interventions. In this paper, we address this by explaining how the required sample size in these multiarm trials can be ascertained when data are to be analyzed using a linear mixed model. We then go on to describe how the design of such trials can be optimized to balance between minimizing the cost of the trial and minimizing some function of the covariance matrix of the treatment effect estimates. Using a recently commenced trial that will evaluate the effectiveness of sensor monitoring in an occupational therapy rehabilitation program for older persons after hip fracture as an example, we demonstrate that our designs could reduce the number of observations required for a fixed power level by up to 58%. Consequently, when logistical constraints permit the utilization of any one of a range of possible multiarm stepped-wedge cluster randomized trial designs, researchers should consider employing our approach to optimize their trials efficiency.-Admissible multiarm stepped-wedge cluster randomized trial designs.",3
"Outcome-dependent sampling (ODS) scheme is a cost-effective way to conduct a study. For a study with continuous primary outcome, an ODS scheme can be implemented where the expensive exposure is only measured on a simple random sample and supplemental samples selected from 2 tails of the primary outcome variable. With the tremendous cost invested in collecting the primary exposure information, investigators often would like to use the available data to study the relationship between a secondary outcome and the obtained exposure variable. This is referred as secondary analysis. Secondary analysis in ODS designs can be tricky, as the ODS sample is not a random sample from the general population. In this article, we use the inverse probability weighted and augmented inverse probability weighted estimating equations to analyze the secondary outcome for data obtained from the ODS design. We do not make any parametric assumptions on the primary and secondary outcome and only specify the form of the regression mean models, thus allow an arbitrary error distribution. Our approach is robust to second- and higher-order moment misspecification. It also leads to more precise estimates of the parameters by effectively using all the available participants. Through simulation studies, we show that the proposed estimator is consistent and asymptotically normal. Data from the Collaborative Perinatal Project are analyzed to illustrate our method.-Secondary outcome analysis for data from an outcome-dependent sampling design.",0
"Receiver operating characteristic analysis provides an important methodology for assessing traditional (e.g., imaging technologies and clinical practices) and new (e.g., genomic studies, biomarker development) diagnostic problems. The area under the clinically/practically relevant part of the receiver operating characteristic curve (partial area or partial area under the receiver operating characteristic curve) is an important performance index summarizing diagnostic accuracy at multiple operating points (decision thresholds) that are relevant to actual clinical practice. A robust estimate of the partial area under the receiver operating characteristic curve is provided by the area under the corresponding part of the empirical receiver operating characteristic curve. We derive a closed-form expression for the jackknife variance of the partial area under the empirical receiver operating characteristic curve. Using the derived analytical expression, we investigate the differences between the jackknife variance and a conventional variance estimator. The relative properties in finite samples are demonstrated in a simulation study. The developed formula enables an easy way to estimate the variance of the empirical partial area under the receiver operating characteristic curve, thereby substantially reducing the computation burden, and provides important insight into the structure of the variability. We demonstrate that when compared with the conventional approach, the jackknife variance has substantially smaller bias, and leads to a more appropriate type I error rate of the Wald-type test. The use of the jackknife variance is illustrated in the analysis of a data set from a diagnostic imaging study.-Jackknife variance of the partial area under the empirical receiver operating characteristic curve.",0
"To provide a critical overview of gene expression profiling methodology and discuss areas of future development. Gene expression profiling has been used extensively in biological research and has resulted in significant advances in the understanding of the molecular mechanisms of complex disorders, including cancer, heart disease, and metabolic disorders. However, translating this technology into genomic medicine for use in diagnosis and prognosis faces many challenges. In addition, gene expression profile analysis is frequently controversial, because its conclusions often lack reproducibility and claims of effective dissemination into translational medicine have, in some cases, been remarkably unjustified. In the last decade, a large number of methodological and technical solutions have been offered to overcome the challenges. We consider the strengths, limitations, and appropriate applications of gene expression profiling techniques, with particular reference to the clinical relevance. Some studies have demonstrated the ability and clinical utility of gene expression profiling for use as diagnostic, prognostic, and predictive molecular markers. The challenges of gene expression profiling lie with the standardization of analytic approaches and the evaluation of the clinical merit in broader heterogeneous populations by prospective clinical trials.-Expectations, validity, and reality in gene expression profiling.",0
"Balance of prognostic factors between treatment groups is desirable because it improves the accuracy, precision, and credibility of the results. In cluster-controlled trials, imbalance can easily occur by chance when the number of cluster is small. If all clusters are known at the start of the study, the ""best balance"" allocation method (BB) can be used to obtain optimal balance. This method will be compared with other allocation methods. We carried out a simulation study to compare the balance obtained with BB, minimization, unrestricted randomization, and matching for four to 20 clusters and one to five categorical prognostic factors at cluster level. BB resulted in a better balance than randomization in 13-100% of the situations, in 0-61% for minimization, and in 0-88% for matching. The superior performance of BB increased as the number of clusters and/or the number of factors increased. BB results in a better balance of prognostic factors than randomization, minimization, stratification, and matching in most situations. Furthermore, BB cannot result in a worse balance of prognostic factors than the other methods.-The ""best balance"" allocation led to optimal balance in cluster-controlled trials.",1
"Two Monte Carlo simulations were performed to compare methods for estimating and testing hypotheses of quadratic effects in latent variable regression models. The methods considered in the current study were (a) a 2-stage moderated regression approach using latent variable scores, (b) an unconstrained product indicator approach, (c) a latent moderated structural equation method, (d) a fully Bayesian approach, and (e) marginal maximum likelihood estimation. Of the 5 estimation methods, it was found that overall the methods based on maximum likelihood estimation and the Bayesian approach performed best in terms of bias, root-mean-square error, standard error ratios, power, and Type I error control, although key differences were observed. Similarities as well as disparities among methods are highlight and general recommendations articulated. As a point of comparison, all 5 approaches were fit to a reparameterized version of the latent quadratic model to educational reading data.-A comparison of methods for estimating quadratic effects in nonlinear structural equation models.",0
Benefits of Cluster Randomization for Surgical Trials and Quality Improvement,1
IS THIS A PORTRAIT OF JOHN GRAUNT? AN ART HISTORY MYSTERY.,0
"In some clinical trials, treatment allocation on a patient level is not feasible, and whole groups or clusters of patients are allocated to the same treatment. If, for example, a clinical trial is investigating the efficacy of various patient coaching methods and randomization is done on a patient level, then patients who are receiving different methods may come into contact with each other and influence each other. This would create contamination of the treatment effects. Such bias might be prevented by randomization on the coaches level. The patients of a coach constitute a cluster and all the subjects in that cluster receive the same treatment. Disadvantages of this approach may be reduced statistical efficiency and recruitment bias, as the treatment that a subject will receive is known in advance. Pseudo cluster randomization avoids this, because in pseudo cluster randomization, not everybody in a certain cluster receives the same treatment, just the majority. There are two groups of clusters: in one group the majority of subjects receive treatment A, while a limited number receive treatment B. In the other group of clusters the proportions are reversed. The statistical properties of this method are described. When contamination is present, the method appears to be more efficient than randomization on a patient level or on a cluster level.-Pseudo cluster randomization: a treatment allocation method to minimize contamination and selection bias.",1
"The current methodology for sample size calculations for stepped-wedge cluster randomised trials (SW-CRTs) is based on the assumption of equal cluster sizes. However, as is often the case in cluster randomised trials (CRTs), the clusters in SW-CRTs are likely to vary in size, which in other designs of CRT leads to a reduction in power. The effect of an imbalance in cluster size on the power of SW-CRTs has not previously been reported, nor what an appropriate adjustment to the sample size calculation should be to allow for any imbalance. We aimed to assess the impact of an imbalance in cluster size on the power of a cross-sectional SW-CRT and recommend a method for calculating the sample size of a SW-CRT when there is an imbalance in cluster size. The effect of varying degrees of imbalance in cluster size on the power of SW-CRTs was investigated using simulations. The sample size was calculated using both the standard method and two proposed adjusted design effects (DEs), based on those suggested for CRTs with unequal cluster sizes. The data were analysed using generalised estimating equations with an exchangeable correlation matrix and robust standard errors. An imbalance in cluster size was not found to have a notable effect on the power of SW-CRTs. The two proposed adjusted DEs resulted in trials that were generally considerably over-powered. We recommend that the standard method of sample size calculation for SW-CRTs be used, provided that the assumptions of the method hold. However, it would be beneficial to investigate, through simulation, what effect the maximum likely amount of inequality in cluster sizes would be on the power of the trial and whether any inflation of the sample size would be required.-An imbalance in cluster sizes does not lead to notable loss of power in cross-sectional, stepped-wedge cluster randomised trials with a continuous outcome.",3
"We propose a new approach to fitting marginal models to clustered data when cluster size is informative. This approach uses a generalized estimating equation (GEE) that is weighted inversely with the cluster size. We show that our approach is asymptotically equivalent to within-cluster resampling (Hoffman, Sen, and Weinberg, 2001, Biometrika 73, 13-22), a computationally intensive approach in which replicate data sets containing a randomly selected observation from each cluster are analyzed, and the resulting estimates averaged. Using simulated data and an example involving dental health, we show the superior performance of our approach compared to unweighted GEE, the equivalence of our approach with WCR for large sample sizes, and the superior performance of our approach compared with WCR when sample sizes are small.-Marginal analyses of clustered data when cluster size is informative.",1
"In this paper we propose a sample size calculation method for testing on a binomial proportion when binary observations are dependent within clusters. In estimating the binomial proportion in clustered binary data, two weighting systems have been popular: equal weights to clusters and equal weights to units within clusters. When the number of units varies cluster by cluster, performance of these two weighting systems depends on the extent of correlation among units within each cluster. In addition to them, we will also use an optimal weighting method that minimizes the variance of the estimator. A sample size formula is derived for each of the estimators with different weighting schemes. We apply these methods to the sample size calculation for the sensitivity of a periodontal diagnostic test. Simulation studies are conducted to evaluate a finite sample performance of the three estimators. We also assess the influence of misspecified input parameter values on the calculated sample size. The optimal estimator requires equal or smaller sample sizes and is more robust to the misspecification of an input parameter than those assigning equal weights to units or clusters.-Sample size calculations for clustered binary data.",1
"PEDS-C is the first multicenter placebo-controlled trial for the treatment of chronic hepatitis C (HCV) in childhood that has ever been conducted in the United States (USA). Establishment of the research team, protocol, administrative infrastructure, and ancillary contributors for the pediatric trial took years of planning. To study the safety and efficacy of pegylated-interferon alpha (PEG-2a) plus ribavirin (RV) with PEG-2a monotherapy in children aged 5 years through 18 years. To assess the health-related quality of life and growth and body composition in children with chronic hepatitis C infection, before, during, and after treatment. Eleven centers of pediatric hepatobiliary clinical research were united in a National Institutes of Diabetes and Digestive and Kidney Diseases (NIDDK) funded grant with financial support from the Food and Drug Administration (FDA) and a corporate sponsor to conduct the treatment trial. The most important initial limitation in the design of this complex study was securing the financial support and infrastructural organization, a process that took several years. Challenges faced by the study group included identifying the optimal study design given the limited study population, and determining what ancillary studies could be incorporated into the treatment trial. In this article the process taken to design the study and administrative infrastructure, the lessons learned, and the controversial issues deliberated during the planning process are discussed. The evolution of the study and the considerations taken in the development of the protocol are valuable tools which can be applied to pediatric clinical trials in general.-Design of the PEDS-C trial: pegylated interferon +/- ribavirin for children with chronic hepatitis C viral infection.",0
How to design and analyse cluster randomized trials with a small number of clusters? Comment on Leyrat et al.,1
"It is often anticipated in a longitudinal cluster randomized clinical trial (cluster-RCT) that the course of outcome over time will diverge between intervention arms. In these situations, testing the significance of a local intervention effect at the end of the trial may be more clinically relevant than evaluating overall mean differences between treatment groups. In this paper, we present a closed-form power function for detecting this local intervention effect based on maximum likelihood estimates from a mixed-effects linear regression model for three-level continuous data. Sample size requirements for the number of units at each data level are derived from the power function. The power function and the corresponding sample size requirements are verified by a simulation study. Importantly, it is shown that sample size requirements computed with the proposed power function are smaller than that required when testing group mean difference using data only at the end of trial and ignoring the course of outcome over the entire study period.-Sample size requirement to detect an intervention effect at the end of follow-up in a longitudinal cluster randomized trial.",1
"Many jurisdictions have enacted community treatment order (CTO) legislation that requires a person, who suffers from a severe mental disorder, to follow a treatment plan when living in the community. CTOs have been a source of debate because of controversies on whether evidence of effectiveness should only be considered from randomized controlled trials (RCTs). RCTs are considered the ""gold standard"" method to evaluate effectiveness of simple therapeutic interventions such as medication, but they are problematic for evaluation of complex interventions because valid attribution of causation in complex interventions is not guaranteed with RCTs. CTOs are complex interventions that require the interaction of many individuals and organizations to achieve their effects and effectiveness research must measure these complexities of delivery and outcomes. This paper examines conceptual, methodological and analytical challenges of CTO research within the context of RCTs and other research designs. It also discusses the current state of knowledge on effectiveness of CTOs. Finally, we suggest a way forward by presenting alternative causal inference approaches and potential models for evaluation complex interventions, such as CTOs. We propose that these approaches should be used alongside other research designs in a nuanced approach that may involve using findings from initial studies to refine the intervention and/or its implementation.-Are Randomized Control Trials the Best Method to Assess the Effectiveness of Community Treatment Orders?",1
"In this nested case-control study, lipoprotein (a) [Lp(a)] concentrations and apo(a) isoform size were measured in serum samples obtained from men participating in the prospective Multiple Risk Factor Intervention Trial (MRFIT). Serum from men aged 35 to 57 years and stored for up to 20 years were analyzed for Lp(a) levels (n=736) and isoform size (n=487), respectively. Cases involved nonfatal myocardial infarctions (MI; n=98), documented during the active phase of the study that ended on February 28, 1982 and coronary heart disease (CHD) deaths (n=148) monitored through 1990. Median Lp(a) levels did not differ between cases and controls and mean apo(a) size did not vary between cases and controls in the entire study population. When adjusted for age and Lp(a) concentration, logistic regression analysis indicated that small apo(a) isoforms were associated with CHD deaths among smokers (OR 3.31; 95% CI 1.07-10.28).-Prospective association of lipoprotein(a) concentrations and apo(a) size with coronary heart disease among men in the Multiple Risk Factor Intervention Trial.",0
"To evaluate the effect of intensive care unit (ICU) admission on mortality among patients with ST elevation myocardial infarction (STEMI). Retrospective cohort study. 1727 acute care hospitals in the United States. Medicare beneficiaries (aged 65 years or older) admitted with STEMI to either an ICU or a non-ICU unit (general/telemetry ward or intermediate care) between January 2014 and October 2015. 30 day mortality. An instrumental variable analysis was done to account for confounding, using as an instrument the additional distance that a patient with STEMI would need to travel beyond the closest hospital to arrive at a hospital in the top quarter of ICU admission rates for STEMI. The analysis included 109 375 patients admitted to hospital with STEMI. Hospitals in the top quarter of ICU admission rates admitted 85% or more of STEMI patients to an ICU. Among patients who received ICU care dependent on their proximity to a hospital in the top quarter of ICU admission rates, ICU admission was associated with lower 30 day mortality than non-ICU admission (absolute decrease 6.1 (95% confidence interval -11.9 to -0.3) percentage points). In a separate analysis among patients with non-STEMI, a group for whom evidence suggests that routine ICU care does not improve outcomes, ICU admission was not associated with differences in mortality (absolute increase 1.3 (-0.9 to 3.4) percentage points). ICU care for STEMI is associated with improved mortality among patients who could be treated in an ICU or non-ICU unit. An urgent need exists to identify which patients with STEMI benefit from ICU admission and what about ICU care is beneficial.-Intensive care use and mortality among patients with ST elevation myocardial infarction: retrospective cohort study.",0
"School neighborhood food environment is recognized as an important contributor to childhood obesity; however, large-scale and longitudinal studies remain limited. This study aimed to examine this association and its variation across gender and urbanicity at multiple geographic scales. We used the US nationally representative Early Childhood Longitudinal Study-Kindergarten cohort data and included 7530 kindergarteners followed up from 1998 to 2007. The Census, road network, and Dun and Bradstreet commercial datasets were used to construct time-varying measurements of 11 types of food outlet within 800-m straight-line and road-network buffer zones of schools and school ZIP codes, including supermarket, convenience store, full-service restaurant, fast-food restaurant, retail bakery, dairy product store, health/dietetic food store, candy store, fruit/vegetable market, meat/fish market, and beverage store. Two-level mixed-effect and cluster-robust logistic regression models were performed to examine the association. A higher body mass index (BMI) in 2007 was observed among children experiencing an increase of convenience stores in school neighborhoods during 1998-2007 (? = 0.39, p &lt; 0.05), especially among girls (? = 0.50) and urban schoolchildren (? = 0.41), as well as among children with a decrease of dairy product stores (? = 0.39, p &lt; 0.05), especially among boys (? = 1.86) and urban schoolchildren (? = 0.92). The higher obesity risk was associated with the increase of fast-food restaurants in urban schoolchildren (OR = 1.27 [95% CI = 1.02-1.59]) and of convenience stores in girls (OR = 1.41 [95% CI = 1.09-1.82]) and non-urban schoolchildren (OR = 1.60 [95% CI = 1.10-2.33]). The increase of full-service restaurants was related to lower obesity risk in boys (OR = 0.74 [95% CI = 0.57-0.95]). The decrease of dairy product stores was associated with the higher obesity risk (OR = 1.68 [95% CI = 1.07-2.65]), especially boys (OR = 2.92 [95% CI = 1.58-5.40]) and urban schoolchildren (OR = 1.67 [95% CI = 1.07-2.61]). The schoolchildren exposed to the decrease of meat/fish markets showed the lower obesity risk (OR = 0.57 [95% CI = 0.35-0.91]), especially urban schoolchildren (OR = 0.53 [95% CI = 0.32-0.87]). Results from analyses within 800-m straight-line buffer zones of schools were more consistent with our theory-based hypotheses than those from analyses within 800-m road-network buffer zones of schools and school ZIP codes. National data in the USA suggest that long-term exposure to the food environment around schools could affect childhood obesity risk; this association varied across gender and urbanicity. This study has important public health implications for future school-based dietary intervention design and urban planning.-Effects of school neighborhood food environments on childhood obesity at multiple scales: a longitudinal kindergarten cohort study in the USA.",0
"For censored survival outcomes, it can be of great interest to evaluate the predictive power of individual markers or their functions. Compared with alternative evaluation approaches, approaches based on the time-dependent receiver operating characteristics (ROC) rely on much weaker assumptions, can be more robust, and hence are preferred. In this article, we examine evaluation of markers' predictive power using the time-dependent ROC curve and a concordance measure that can be viewed as a weighted area under the time-dependent area under the ROC curve profile. This study significantly advances from existing time-dependent ROC studies by developing nonparametric estimators of the summary indexes and, more importantly, rigorously establishing their asymptotic properties. It reinforces the statistical foundation of the time-dependent ROC-based evaluation approaches for censored survival outcomes. Numerical studies, including simulations and application to an HIV clinical trial, demonstrate the satisfactory finite-sample performance of the proposed approaches.-Nonparametric receiver operating characteristic-based evaluation for survival outcomes.",0
"Cluster randomized trials are trials that randomize clusters of people, rather than individuals. They are becoming increasingly common. A number of innovations have been developed recently, particularly in the calculation of the required size of a cluster trial, the handling of missing data, designs to minimize recruitment bias, the ethics of cluster randomized trials and the stepped wedge design. This article will highlight and illustrate these developments. It will also discuss issues with regards to the reporting of cluster randomized trials.-Challenges of cluster randomized trials.",1
"Nonparametric regression models are proposed in the framework of ecological inference for exploratory modeling of disease prevalence rates adjusted for variables, such as age, ethnicity/race, and socio-economic status. Ecological inference is needed when a response variable and covariate are not available at the subject level because only summary statistics are available for the reporting unit, for example, in the form of R x C tables. In this article, only the marginal counts are assumed available in the sample of R x C contingency tables for modeling the joint distribution of counts. A general form for the ecological regression model is proposed, whereby certain covariates are included as a varying coefficient regression model, whereas others are included as a functional linear model. The nonparametric regression curves are modeled as splines fit by penalized weighted least squares. A data-driven selection of the smoothing parameter is proposed using the pointwise maximum squared bias computed from averaging kernels (explained by O'Sullivan, 1986, Statistical Science 1, 502-517). Analytic expressions for bias and variance are provided that could be used to study the rates of convergence of the estimators. Instead, this article focuses on demonstrating the utility of the estimators in a study of disparity in health outcomes by ethnicity/race.-Incorporating marginal covariate information in a nonparametric regression model for a sample of R x C tables.",0
"Stepped-wedge cluster randomised trials (SW-CRTs) are a pragmatic trial design, providing an unprecedented opportunity to increase the robustness of evidence underpinning implementation and quality improvement interventions. Given the complexity of the SW-CRT, the likelihood of trials not delivering on their objectives will be mitigated if a feasibility study precedes the definitive trial. It is not currently known if feasibility studies are being conducted for SW-CRTs nor what the objectives of these studies are. Searches were conducted of several databases to identify published feasibility studies which were designed to inform a future SW-CRT. For each eligible study, data were extracted on the characteristics of and rationale for the feasibility study; the process for determining progression to the main trial; how the feasibility study informed the main trial; and whether the main trial went ahead. A narrative synthesis and descriptive analysis are presented. Eleven feasibility studies were identified, which included eight completed study reports and three protocols. Three studies used a stepped-wedge design and these were the only studies to be randomised. Studies were predominantly of a mixed-methods design. Only one study assessed specific features related to the feasibility of using a SW-CRT and one investigated the time taken to complete the study procedures. The other studies were mostly assessing the feasibility and acceptability of the intervention. Published feasibility studies for SW-CRTs are scarce and those that are being reported do not investigate issues specific to the complexities of the trial design. When conducting feasibility studies in advance of a definitive SW-CRT, researchers should consider assessing the feasibility of study procedures, particularly those specific to the SW-CRT design, and ensure that the findings are published for the benefit of other researchers.-The current use of feasibility studies in the assessment of feasibility for stepped-wedge cluster randomised trials: a systematic review",3
"Controlled implementation trials often randomize the intervention at the site level, enrolling relatively few sites (e.g., 6-20) compared to trials that randomize by subject. Trials with few sites carry a substantial risk of an imbalance between intervened (cases) and non-intervened (control) sites in important site characteristics, thereby threatening the internal validity of the primary comparison. A stepped wedge design (SWD) staggers the intervention at sites over a sequence of times or time waves until all sites eventually receive the intervention. We propose a new randomization method, sequential balance, to control time trend in site allocation by minimizing sequential imbalance across multiple characteristics. We illustrate the new method by applying it to a SWD implementation trial. The trial investigated the impact of blended internal-external facilitation on the establishment of evidence-based teams in general mental health clinics in nine US Department of Veterans Affairs medical centers. Prior to randomization to start time, an expert panel of implementation researchers and health system program leaders identified by consensus a series of eight facility-level characteristics judged relevant to the success of implementation. We characterized each of the nine sites according to these consensus features. Using a weighted sum of these characteristics, we calculated imbalance scores for each of 1680 possible site assignments to identify the most sequentially balanced assignment schemes. From 1680 possible site assignments, we identified 34 assignments with minimal imbalance scores, and then randomly selected one assignment by which to randomize start time. Initially, the mean imbalance score was 3.10, but restricted to the 34 assignments, it declined to 0.99. Sequential balancing of site characteristics across groups of sites in the time waves of a SWD strengthens the internal validity of study conclusions by minimizing potential confounding. Registered at ClinicalTrials.gov as clinical trials # NCT02543840 ; entered 9/4/2015.-A method to reduce imbalance for site-level randomized stepped wedge implementation trial designs",3
"Several statistical methods for meta-analysis of diagnostic accuracy studies have been discussed in the presence of a gold standard. However, in practice, the selected reference test may be imperfect due to measurement error, non-existence, invasive nature, or expensive cost of a gold standard. It has been suggested that treating an imperfect reference test as a gold standard can lead to substantial bias in the estimation of diagnostic test accuracy. Recently, two models have been proposed to account for imperfect reference test, namely, a multivariate generalized linear mixed model (MGLMM) and a hierarchical summary receiver operating characteristic (HSROC) model. Both models are very flexible in accounting for heterogeneity in accuracies of tests across studies as well as the dependence between tests. In this article, we show that these two models, although with different formulations, are closely related and are equivalent in the absence of study-level covariates. Furthermore, we provide the exact relations between the parameters of these two models and assumptions under which two models can be reduced to equivalent submodels. On the other hand, we show that some submodels of the MGLMM do not have corresponding equivalent submodels of the HSROC model, and vice versa. With three real examples, we illustrate the cases when fitting the MGLMM and HSROC models leads to equivalent submodels and hence identical inference, and the cases when the inferences from two models are slightly different. Our results generalize the important relations between the bivariate generalized linear mixed model and HSROC model when the reference test is a gold standard.-A unification of models for meta-analysis of diagnostic accuracy studies without a gold standard.",0
"Maternal cigarette smoking is a well-established risk factor for oral clefts. Evidence is less clear for passive (secondhand) smoke exposure. We combined individual-level data from 4 population-based studies (the Norway Facial Clefts Study, 1996-2001; the Utah Child and Family Health Study, 1995-2004; the Norwegian Mother and Child Cohort Study, 1999-2009; and the National Birth Defects Prevention Study (United States), 1999-2007) to obtain 4,508 cleft cases and 9,626 controls. We categorized first-trimester passive and active smoke exposure. Multivariable logistic models adjusted for possible confounders (maternal alcohol consumption, use of folic acid supplements, age, body size, education, and employment, plus study fixed effects). Children whose mothers actively smoked had an increased risk of oral clefts (odds ratio (OR) = 1.27, 95% confidence interval (CI): 1.11, 1.46). Children of passively exposed nonsmoking mothers also had an increased risk (OR = 1.14, 95% CI: 1.02, 1.27). Cleft risk was further elevated among babies of smoking mothers who were exposed to passive smoke (OR = 1.51, 95% CI: 1.35, 1.70). Using a large pooled data set, we found a modest association between first-trimester passive smoking and oral clefts that was consistent across populations, diverse study designs, and cleft subtypes. While this association may reflect subtle confounding or bias, we cannot rule out the possibility that passive smoke exposure during pregnancy is teratogenic.-Passive Smoke Exposure as a Risk Factor for Oral Clefts-A Large International Population-Based Study.",0
"For a potentially lethal chronic disease like cancer, it is often infeasible to compare treatments on the basis of overall survival, so a combined outcome such as progression-free survival (which is the time from randomization to progression or death) has become an acceptable primary endpoint. The rationale of using an efficacy measure that is dominated by the time to progression is that an effective treatment will delay progression and when treatment is stopped at progression, the effect of treatment after this time is small. However, often trials that show a significant benefit for delaying progression but not on overall survival are not universally viewed as providing convincing evidence that the drug should become the standard of care. We propose that when there is a significant treatment effect of delaying progression, a Bayesian analysis of overall survival should be undertaken. We suggest using a joint piecewise exponential model, where the treatment effect on the hazard for progression and for death after progression is captured through two distinct parameters. We develop a plot of the overall survival advantage of the new therapy versus the prior distribution of the relative hazard for death after progression. This plot can augment the discussion about whether the new treatment is beneficial on survival. In the example of an early breast cancer trial for which a new treatment significantly delayed disease recurrence, our Bayesian analysis showed that with very reasonable assumptions on the effects of treatment after recurrence, there is a high probability that the new treatment improves overall survival. For a clinical trial for which treatment delays progression, the proposed method can improve the interpretability of the survival comparison using data from the study.-Assessing survival benefit when treatment delays disease progression.",0
"Nonequivalent controlled pretest-posttest designs are central to evaluation science, yet no practical and unified approach for estimating power in the two most widely used analytic approaches to these designs exists. This article fills the gap by presenting and comparing useful, unified power formulas for ANCOVA and change-score analyses, indicating the implications of each on sample-size requirements. The authors close with practical recommendations for evaluators. Mathematical details and a simple spreadsheet approach are included in appendices.-Statistical power for nonequivalent pretest-posttest designs. The impact of change-score versus ANCOVA models.",0
"Prospective randomized clinical trials addressing biomarkers are time consuming and costly, but are necessary for regulatory agencies to approve new therapies with predictive biomarkers. For this reason, recently, there have been many discussions and proposals of various trial designs and comparisons of their efficiency in the literature. We compare statistical efficiencies between the marker-stratified design and the marker-based precision medicine design regarding testing/estimating 4 hypotheses/parameters of clinical interest, namely, treatment effects in each marker-positive and marker-negative cohorts, marker-by-treatment interaction, and the marker's clinical utility. As may be expected, the stratified design is more efficient than the precision medicine design. However, it is perhaps surprising to find out how low the relative efficiency can be for the precision medicine design. We quantify the relative efficiency as a function of design factors including the marker-positive prevalence rate, marker assay and classification sensitivity and specificity, and the treatment randomization ratio. It is interesting to examine the trends of the relative efficiency with these design parameters in testing different hypotheses. We advocate to use the stratified design over the precision medicine design in clinical trials with predictive biomarkers.-Relative efficiency of precision medicine designs for clinical trials with predictive biomarkers.",0
"To evaluate the extent to which structural variation between English general practices is accounted for at higher organisational levels in the National Health Service (NHS). We analysed data for 11 structural characteristics of all general practices in England. These included characteristics of general practitioners (GPs), the practice list and the services provided by practices. A four-level random effects model was used for analysis and components of variance were estimated at the levels of practice, primary care group (PCG), health authority and region. The proportion of single-handed practices ranged from 0% to 74% at PCG level and from 14% to 43% in different regions. The proportion of practices providing diabetes services ranged from 0% to 100% at PCG level and from 71% to 96% in different regions. The list size per GP ranged from 1314 to 2704 patients per GP at PCG level and from 1721 to 2225 at regional level. Across the 11 variables analysed, components of variance at general practice level accounted for between 43% and 95% of the total variance. The PCG level accounted for between 1% and 29%, the health authority level for between 2% and 15% and the regional level for between 0% and 13% of the total variance. Adjusting for an index of deprivation and the supply of GPs gave a median 8% decrease in the sum of variance components. Geographical and organisational variation in the structure of primary care services should be considered in designing studies in health systems such as the English NHS. Stratified designs may be used to increase study efficiency, but variation between areas may sometimes compromise generalisability.-Geographical and organisational variation in the structure of primary care services: implications for study design.",1
"The design of family studies to estimate the value of an intraclass correlation coefficient p is considered when ni individuals are to be selected from each of k families, i = 1, 2, ..., k. In particular, the accuracy of a balance design (ni = n, i = 1, 2, ..., k) for estimating p is compared with the accuracy of an unbalanced ""natural"" design, in which the ni are sampled at random from family size distributions that tend to occur in practice. It is found for two different estimators of p that the balanced design is usually preferable, but only to a small degree if the number of families sampled is greater than 50.-Design considerations in the estimation of intraclass correlation.",1
"This study assessed the effects of unified family and drug treatment courts (DTCs) on the resolution of cases involving foster care children and the resulting effects on school performance. The first analytic step was to assess the impacts of presence of unified and DTCs in North Carolina counties on time children spent in foster care and the type of placement at exit from foster care. In the second step, the same data on foster care placements were merged with school records for youth in Grades 3-8 in public schools. The effect of children's time in foster care and placement outcomes on school performance as measured by math and reading tests, grade retention, and attendance was assessed using child fixed-effects regression. Children in counties with unified family courts experienced shorter foster care spells and higher rates of reunification with parents or primary caregivers. Shorter foster care spells translated into improved school performance measured by end-of-grade reading and math test scores. Adult DTCs were associated with lower probability of reunification with parents/primary caregivers. The shortened time in foster care implies an efficiency gain attributable to unified family courts, which translate into savings for the court system through the use of fewer resources. Children also benefit through shortened stays in temporary placements, which are related to some improved educational outcomes.-Do specialty courts achieve better outcomes for children in foster care than general courts?",0
"An important step in designing, executing, and evaluating cluster-randomized trials (CRTs) is understanding the correlation and thus nonindependence that exists among individuals in a cluster. In hospital epidemiology, there is a shortage of CRTs that have published their intraclass correlation coefficient or coefficient of variation (CV), making prospective sample size calculations difficult for investigators. To estimate the number of hospitals needed to power parallel CRTs of interventions to reduce health care-associated infection outcomes and to demonstrate how different parameters such as CV and expected effect size are associated with the sample size estimates in practice. This longitudinal cohort study estimated parameters for sample size calculations using national rates developed by the Centers for Disease Control and Prevention for methicillin-resistant Staphylococcus aureus (MRSA) bacteremia, central-line-associated bloodstream infections (CLABSI), catheter-associated urinary tract infections (CAUTI), and Clostridium difficile infections (CDI) from 2016. For MRSA and vancomycin-resistant enterococci (VRE) acquisition, outcomes were estimated using data from 2012 from the Benefits of Universal Glove and Gown study. Data were collected from June 2017 through September 2018 and analyzed from September 2018 through January 2019. Calculated number of clusters needed for adequate power to detect an intervention effect using a 2-group parallel CRT. To study an intervention with a 30% decrease in daily rates, 73 total clusters were needed (37 in the intervention group and 36 in the control group) for MRSA bacteremia, 82 for CAUTI, 60 for CLABSI, and 31 for CDI. If a 10% decrease in rates was expected, 768 clusters were needed for MRSA bacteremia, 875 for CAUTI, 631 for CLABSI, and 329 for CDI. For MRSA or VRE acquisition, 50 or 40 total clusters, respectively, were required to observe a 30% decrease, whereas 540 or 426 clusters, respectively, were required to detect a 10% decrease. This study suggests that large sample sizes are needed to appropriately power parallel CRTs targeting infection prevention outcomes. Sample sizes are most associated with expected effect size and CV of hospital rates.-Sample Size Estimates for Cluster-Randomized Trials in Hospital Infection Control and Antimicrobial Stewardship",1
"Cluster randomised trial (CRT) investigators face challenges in seeking informed consent from individual patients (cluster members). This study examined associations between reporting of patient consent in healthcare CRTs and characteristics of these trials. Consent practices and study characteristics were abstracted from a random sample of 160 CRTs performed in primary or hospital care settings that were published from 2000 to 2008. Multivariable logistic regression was used to examine associations between reporting of patient consent and methodological characteristics, as well as publication features such as date and journal of publication. 82 (53.8%) of 160 studies reported obtaining informed consent from individual patients. Reporting of patient consent was independently and positively associated with: smaller cluster size, the evaluation of experimental interventions targeted at patients, data collection from individual patients, publication later than 2004 and publication in higher-impact journals. Reporting of consent practices in published CRTs should be improved. Consent practices in published CRTs appear to be related to the type of interventions under study, as well as journal impact and trends in research ethics practices. These findings will inform best practices in trial conduct and ethics review, remediation of errors in consent practices and ethics review and the development of regulatory guidance for CRTs.-Reporting of patient consent in healthcare cluster randomised trials is associated with the type of study interventions and publication characteristics.",1
"The evaluation of cure fractions in oncology research under the well known cure rate model has attracted considerable attention in the literature, but most of the existing testing procedures have relied on restrictive assumptions. A common assumption has been to restrict the cure fraction to a constant under alternatives to homogeneity, thereby neglecting any information from covariates. This article extends the literature by developing a score-based statistic that incorporates covariate information to detect cure fractions, with the existing testing procedure serving as a special case. A complication of this extension, however, is that the implied hypotheses are not typical and standard regularity conditions to conduct the test may not even hold. Using empirical processes arguments, we construct a sup-score test statistic for cure fractions and establish its limiting null distribution as a functional of mixtures of chi-square processes. In practice, we suggest a simple resampling procedure to approximate this limiting distribution. Our simulation results show that the proposed test can greatly improve efficiency over tests that neglect the heterogeneity of the cure fraction under the alternative. The practical utility of the methodology is illustrated using ovarian cancer survival data with long-term follow-up from the surveillance, epidemiology, and end results registry.-A sup-score test for the cure fraction in mixture models for long-term survivors.",0
"The purpose of this study is to determine the effect of three common approaches to handling missing data on the results of a predictive model. Monte Carlo simulation study using simulated data was used. A baseline logistic regression using complete data was performed to predict hospital admission, based on the white blood cell count (WBC) (dichotomized as normal or high), presence of fever, or procedures performed (PROC). A series of simulations was then performed in which WBC data were deleted for varying proportions (15-85%) of patients under various patterns of missingness. Three analytic approaches were used: analysis restricted to cases with complete data, missing data assumed to be normal (MAN), and use of imputed values. In the baseline analysis, all three predictors were all significantly associated with admission. Using either the MAN approach or imputation, the odds ratio (OR) for WBC was substantially over- or underestimated depending on the missingness pattern, and there was considerable bias toward the null in the OR estimates for fever. In the CC analyses, OR for WBC was consistently biased toward the null, OR for PROC was biased away from the null, and the OR for fever was biased toward or away from the null. Estimates for overall model discrimination were substantially biased using all analytic approaches. All three methods of handling large amounts of missing data can lead to biased estimates of the OR and of model performance in predictive models. Predictor variables that are measured inconsistently can affect the validity of such models.-Bias arising from missing data in predictive models.",0
Capitalizing on Natural Experiments to Improve Our Understanding of Population Health.,0
"Methods based on the propensity score comprise one set of valuable tools for comparative effectiveness research and for estimating causal effects more generally. These methods typically consist of two distinct stages: (1) a propensity score stage where a model is fit to predict the propensity to receive treatment (the propensity score), and (2) an outcome stage where responses are compared in treated and untreated units having similar values of the estimated propensity score. Traditional techniques conduct estimation in these two stages separately; estimates from the first stage are treated as fixed and known for use in the second stage. Bayesian methods have natural appeal in these settings because separate likelihoods for the two stages can be combined into a single joint likelihood, with estimation of the two stages carried out simultaneously. One key feature of joint estimation in this context is ""feedback"" between the outcome stage and the propensity score stage, meaning that quantities in a model for the outcome contribute information to posterior distributions of quantities in the model for the propensity score. We provide a rigorous assessment of Bayesian propensity score estimation to show that model feedback can produce poor estimates of causal effects absent strategies that augment propensity score adjustment with adjustment for individual covariates. We illustrate this phenomenon with a simulation study and with a comparative effectiveness investigation of carotid artery stenting versus carotid endarterectomy among 123,286 Medicare beneficiaries hospitlized for stroke in 2006 and 2007.-Model feedback in Bayesian propensity score estimation.",0
"In some general practice intervention trials, patients must be randomized in practices rather than individually, and this must be taken into account in the analysis. In this article we aim to show how failure to do this may lead to spurious statistical significance and CIs which are narrower than they should be, and to describe the use of summary measures for each practice as a simple method of analysis. The statistical issues are demonstrated by an example of a trial in general practice. The choice of unit of analysis will be most important where there are large numbers of patients recruited from each practice or a high degree of variability between practices.-Trials which randomize practices I: how should they be analysed?",1
"We describe a Bayesian methodology for estimating the cost-effectiveness of a new treatment compared to a standard in a clinical trial, when censoring of survival, the effectiveness variable, induces censoring of total cost. The statistical model assumes that survival follows a Weibull distribution and that total health care cost follows a gamma distribution whose mean has a linear regression on survival time. We summarize the posterior distributions of key parameters by importance sampling. We illustrate the method with an analysis of data from a randomized clinical trial of a treatment for cardiovascular disease.-Bayesian estimation of cost-effectiveness from censored data.",0
"The statistical methodology of health research experiments published in Lancet, the New England Journal of Medicine, and Medical Care between 1975 and 1980 for the presence or absence of an error of experimental design and analysis was examined. The error is the result of inappropriately using patient-related observations as the unit of analysis to form conclusions about provider behavior or outcomes determined jointly by patients and providers. The error was present in 20 of 28 (71%) health care experiments addressing an issue of health provider professional performance. Its usual effect is to increase erroneously the power of an experiment to detect differences between experimental and control groups. It is likely that this type of error could be avoided by the explicit and prospective definition of hypotheses and the populations to which they are intended to pertain.-Choosing the correct unit of analysis in medical care experiments",2
"Group-randomized designs are well suited for studies of professional development because they can accommodate programs that are delivered to intact groups (e.g., schools), the collaborative nature of professional development, and extant teacher/school assignments. Though group designs may be theoretically favorable, prior evidence has suggested that they may be challenging to conduct in professional development studies because well-powered designs will typically require large sample sizes or expect large effect sizes. Using teacher knowledge outcomes in mathematics, we investigated when and the extent to which there is evidence that covariance adjustment on a pretest, teacher certification, or demographic covariates can reduce the sample size necessary to achieve reasonable power. Our analyses drew on multilevel models and outcomes in five different content areas for over 4,000 teachers and 2,000 schools. Using these estimates, we assessed the minimum detectable effect sizes for several school-randomized designs with and without covariance adjustment. The analyses suggested that teachers' knowledge is substantially clustered within schools in each of the five content areas and that covariance adjustment for a pretest or, to a lesser extent, teacher certification, has the potential to transform designs that are unreasonably large for professional development studies into viable studies.-Strategies for Improving Power in School-Randomized Studies of Professional Development.",1
"Functional regression allows for a scalar response to be dependent on a functional predictor; however, not much work has been done when a scalar exposure that interacts with the functional covariate is introduced. In this paper, we present 2 functional regression models that account for this interaction and propose 2 novel estimation procedures for the parameters in these models. These estimation methods allow for a noisy and/or sparsely observed functional covariate and are easily extended to generalized exponential family responses. We compute standard errors of our estimators, which allows for further statistical inference and hypothesis testing. We compare the performance of the proposed estimators to each other and to one found in the literature via simulation and demonstrate our methods using a real data example.-Functional interaction-based nonlinear models with application to multiplatform genomics data.",0
"In many cluster randomization studies, cluster sizes are not fixed and may be highly variable. For those studies, sample size estimation assuming a constant cluster size may lead to under-powered studies. Sample size formulas have been developed to incorporate the variability in cluster size for clinical trials with continuous and binary outcomes. Count outcomes frequently occur in cluster randomized studies. In this paper, we derive a closed-form sample size formula for count outcomes accounting for the variability in cluster size. We compare the performance of the proposed method with the average cluster size method through simulation. The simulation study shows that the proposed method has a better performance with empirical powers and type I errors closer to the nominal levels.-Sample Size Calculation for Count Outcomes in Cluster Randomization Trials with Varying Cluster Sizes",1
"Several reviews of published cluster randomised trials have reported that about half did not take clustering into account in the analysis, which was thus incorrect and potentially misleading. In this paper I ask whether cluster randomised trials are increasing in both number and quality of reporting. Computer search for papers on cluster randomised trials since 1980, hand search of trial reports published in selected volumes of the British Medical Journal over 20 years. There has been a large increase in the numbers of methodological papers and of trial reports using the term 'cluster random' in recent years, with about equal numbers of each type of paper. The British Medical Journal contained more such reports than any other journal. In this journal there was a corresponding increase over time in the number of trials where subjects were randomised in clusters. In 2003 all reports showed awareness of the need to allow for clustering in the analysis. In 1993 and before clustering was ignored in most such trials. Cluster trials are becoming more frequent and reporting is of higher quality. Perhaps statistician pressure works.-Cluster randomised trials in the medical literature: two bibliometric surveys.",1
"Recently a great deal of attention has been given to binary regression models for clustered or correlated observations. The data of interest are of the form of a binary dependent or response variable, together with independent variables X1,...., Xk, where sets of observations are grouped together into clusters. A number of models and methods of analysis have been suggested to study such data. Many of these are extensions in some way of the familiar logistic regression model for binary data that are not grouped (i.e., each cluster is of size 1). In general, the analyses of these clustered data models proceed by assuming that the observed clusters are a simple random sample of clusters selected from a population of clusters. In this paper, we consider the application of these procedures to the case where the clusters are selected randomly in a manner that depends on the pattern of responses in the cluster. For example, we show that ignoring the retrospective nature of the sample design, by fitting standard logistic regression models for clustered binary data, may result in misleading estimates of the effects of covariates and the precision of estimated regression coefficients.-The effect of retrospective sampling on binary regression models for clustered data.",1
"Cluster randomised controlled trials for health promotion, education, public health or organisational change interventions are becoming increasingly common to inform evidence-based policy. However, there is little published methodological evidence on recruitment strategies for primary care population clusters. In this paper, we discuss how choosing which population cluster to randomise can impact on the practicalities of recruitment in primary care. We describe strategies developed through our experiences of recruiting primary care organisations to participate in a national randomised controlled trial of a policy to provide community breastfeeding groups for pregnant and breastfeeding mothers, the BIG (Breastfeeding in Groups) trial. We propose an iterative qualitative approach to recruitment; collecting data generated through the recruitment process, identifying themes and using the constant comparative method of analysis. This can assist in developing successful recruitment strategies and contrasts with the standardised approach commonly used when recruiting individuals to participate in randomised controlled trials. Recruiting primary care population clusters to participate in trials is currently an uphill battle in Britain. It is a complex process, which can benefit from applying qualitative methods to inform trial design and recruitment strategy. Recruitment could be facilitated if health service managers were committed to supporting peer reviewed, funded and ethics committee approved research at national level.-Recruitment issues when primary care population clusters are used in randomised controlled clinical trials: climbing mountains or pushing boulders uphill?",1
"Trial monitoring protects participant safety and study integrity. While monitors commonly go on-site to verify source data, there is little evidence that this practice is efficient or effective. An ongoing international HIV treatment trial (START) provides an opportunity to explore the usefulness of different monitoring approaches. All START sites are centrally monitored and required to follow a local monitoring plan requiring specific quality assurance activities. Additionally, sites were randomized (1:1) to receive, or not receive, annual on-site monitoring. The study will determine if on-site monitoring increases the identification of major protocol deviations (eligibility or consent violations, improper study drug use, primary or serious event underreporting, data alteration or fraud). The START study completed enrollment in December 2013, with planned follow-up through December 2016. The monitoring study is ongoing at 196 sites in 34 countries. Results are expected when the START study concludes in December 2016.-INVESTIGATING THE EFFICACY OF CLINICAL TRIAL MONITORING STRATEGIES: Design and Implementation of the Cluster Randomized START Monitoring Substudy.",1
"In a multicenter trial, responses for subjects belonging to a common center are correlated. Such a clustering is usually assessed through the design effect, defined as a ratio of two variances. The aim of this work was to describe and understand situations where the design effect involves a gain or a loss of power. We developed a design effect formula for a multicenter study aimed at testing the effect of a binary factor (which thus defines two groups) on a continuous outcome, and explored this design effect for several designs (from individually stratified randomized trials to cluster randomized trials, and for other designs such as matched pair designs or observational multicenter studies). The design effect depends on the intraclass correlation coefficient (ICC) (which assesses the correlation between data for two subjects from the same center) but also on a statistic S, which quantifies the heterogeneity of the group distributions among centers (thus the level of association between the binary factor and the center) and on the degree of global imbalance (the number of subjects are then different) between the two groups. This design effect may induce either a loss or a gain in power, depending on whether the S statistic is respectively higher or lower than 1. We provided a global design effect formula applying for any multicenter study and allowing identifying factors - the ICC and the distribution of the group proportions among centers - that are associated with a gain or a loss of power in such studies.-Design effect in multicenter studies: gain or loss of power?",1
"In many biomedical studies, it is of interest to assess dependence between bivariate failure time data. We focus here on a special type of such data, referred to as semi-competing risks data. In this article, we develop methods for making inferences regarding dependence of semi-competing risks data across strata of a discrete covariate Z. A class of rank statistics for testing constancy of association across strata are proposed; its asymptotic properties are also derived. We develop a novel re-sampling-based technique for calculating the variances of the proposed test statistics. In addition, we develop methods for combining test statistics for assessing marginal effects of Z on the dependent censoring variable as well as its effects on association. The finite-sample properties of the proposed methodology are assessed using simulation studies, and they are applied to data from a leukaemia transplantation study.-Semi-parametric inferences for association with semi-competing risks data.",0
"This article presents the first estimates of school-level intraclass correlation for dietary measures based on data from the Teens Eating for Energy and Nutrition at School study. This study involves 3,878 seventh graders from 16 middle schools from Minneapolis-St. Paul, Minnesota. The sample was 66.8% White, 11.2% Black, and 7.0% Asian; 48.8% of the sample was female. Typical fruit and vegetable intake was assessed with a modified version of the Behavior Risk Factor Surveillance System questionnaire. Twenty-four-hour dietary recalls were conducted by nutritionists using the Minnesota Nutrition Data System. Mixed-model regression methods were used to estimate variance components for school and residual error, both before and after adjustment for demographic factors. School-level intraclass correlations were large enough, if ignored, to substantially inflate the Type I error rate in an analysis of treatment effects. The authors show how to use the estimates to determine sample size requirements for future studies.-Intraclass correlation for measures from a middle school nutrition intervention study: estimates, correlates, and applications.",1
Efficiency and robustness of alternative estimators for two- and three-level models: The case of NAEP,1
"In school-based research, usually the nature of the intervention or other practical factors indicate that assignment of treatment be done by school or classroom rather than by individual student. In this situation, randomization of schools (or classrooms) and analysis by school means (or classroom means) provide a firm statistical basis for internal validity of the study. When the number of schools available is small, this approach is not practicable, and therefore the investigator must be both more creative in developing solutions and more cautious in interpreting the results. This article provides a number of suggestions which the authors hope will assist the field in dealing with such circumstances. The authors stress that the best approach to assessing treatment effect is a well-designed, properly analyzed randomized experiment. The suggestions in this article attempt to indicate how one might make the most that one can from more limited data.-Analysis issues in school-based health promotion studies.",1
"Typically, clusters and individuals in cluster randomized trials are allocated across treatment conditions in a balanced fashion. This is optimal under homogeneous costs and outcome variances. However, both the costs and the variances may be heterogeneous. Then, an unbalanced allocation is more efficient but impractical as the outcome variance is unknown in the design stage of a study. A practical alternative to the balanced design could be a design optimal for known and possibly heterogeneous costs and homogeneous variances. However, when costs and variances are heterogeneous, both designs suffer from loss of efficiency, compared with the optimal design. Focusing on cluster randomized trials with a 2 ? 2 design, the relative efficiency of the balanced design and of the design optimal for heterogeneous costs and homogeneous variances is evaluated, relative to the optimal design. We consider two heterogeneous scenarios (two treatment arms with small, and two with large, costs or variances, or one small, two intermediate, and one large costs or variances) at each design level (cluster, individual, and both). Within these scenarios, we compute the relative efficiency of the two designs as a function of the extents of heterogeneity of the costs and variances, and the congruence (the cheapest treatment has the smallest variance) and incongruence (the cheapest treatment has the largest variance) between costs and variances. We find that the design optimal for heterogeneous costs and homogeneous variances is generally more efficient than the balanced design and we illustrate this theory on a trial that examines methods to reduce radiological referrals from general practices. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Efficient treatment allocation in 2???2 cluster randomized trials, when costs and variances are heterogeneous.",1
"To determine whether China's New Rural Cooperative Medical Scheme (NCMS), which aims to provide health insurance to 800 million rural citizens and to correct distortions in rural primary care, and the individual policy attributes have affected the operation and use of village health clinics. We performed a difference-in-difference analysis using multivariate linear regressions, controlling for clinic and individual attributes as well as village and year effects. 100 villages within 25 rural counties across five Chinese provinces in 2004 and 2007. Participants 160 village primary care clinics and 8339 individuals. Clinic outcomes were log average weekly patient flow, log average monthly gross income, log total annual net income, and the proportion of monthly gross income from medicine sales. Individual outcomes were probability of seeking medical care, log annual ""out of pocket"" health expenditure, and two measures of exposure to financial risk (probability of incurring out of pocket health expenditure above the 90th percentile of spending among the uninsured and probability of financing medical care by borrowing or selling assets). For village clinics, we found that NCMS was associated with a 26% increase in weekly patient flow and a 29% increase in monthly gross income, but no change in annual net revenue or the proportion of monthly income from drug revenue. For individuals, participation in NCMS was associated with a 5% increase in village clinic use, but no change in overall medical care use. Also, out of pocket medical spending fell by 19% and the two measures of exposure to financial risk declined by 24-63%. These changes occurred across heterogeneous county programmes, even in those with minimal benefit packages. NCMS provides some financial risk protection for individuals in rural China and has partly corrected distortions in Chinese rural healthcare (reducing the oversupply of specialty services and prescription drugs). However, the scheme may have also shifted uncompensated new responsibilities to village clinics. Given renewed interest among Chinese policy makers in strengthening primary care, the effect of NCMS deserves greater attention.-New evidence on the impact of China's New Rural Cooperative Medical Scheme and its implications for rural primary healthcare: multivariate difference-in-difference analysis.",0
"To describe population mean, variance, and correlation of cycle length across the life span and by age at menopause and age at menarche using a new statistical approach. Data from the Tremin Trust (n=997), a prospective menstrual diary study, was analyzed. Marginal models with generalized estimating equations were used to describe changes in menstrual parameters across the reproductive life span. During the menopausal transition, the increase in standard deviation preceded that in mean by 2 to 6 years. Although beginning earlier in women with earlier menopause, increases in mean and variance for women with different ages at menopause were parallel. Women with later menopause had longer cycles throughout life and longer, more variable cycles during the transition. The transition from late reproductive life to early menopausal transition appears to begin in the late thirties when variability of cycle length increases. Patterns of change in menstrual function during the menopausal transition do not differ by age at menopause; thus, differences in age at menopause are likely to reflect changes in the timing and not changes in the process of ovarian senescence, at least for the normative ages of menopause.-A new statistical approach demonstrated menstrual patterns during the menopausal transition did not vary by age at menopause.",0
"To construct a confidence interval for the mean of a log-normal distribution in small samples, we propose likelihood-based approaches - the signed log-likelihood ratio and modified signed log-likelihood ratio methods. Extensive Monte Carlo simulation results show the advantages of the modified signed log-likelihood ratio method over the signed log-likelihood ratio method and other methods. In particular, the modified signed log-likelihood ratio method produces a confidence interval with a nearly exact coverage probability and highly accurate and symmetric error probabilities even for extremely small sample sizes. We then apply the methods to two sets of real-life data.-Likelihood-based confidence intervals for a log-normal mean.",0
"This report describes the performance of a surveillance system and computerized algorithm for the assignment of definite or probable hospitalized cardiac events for large epidemiologic studies. The algorithm, developed by the Coordinating Committee for Community Demonstration Studies (CCCDS), evolved from the Gillum criteria, and included selected ICD-9-CM codes including codes 410 through 414 for discharge record screening, plus creatine kinase. For the small percentage of cases in which enzyme analysis was inconclusive (8%), presence of pain and/or Minnesota-coded electrocardiograms were included to define the outcome. All data items were easily obtained from medical records by trained lay record abstractors and required no interpretation. From January 1980 through December 1991, 21,183 medical records were screened for ICD-9-CM codes 410 through 414. Of all 410 to 411 ICD-9-CM codes (n = 9026), 36.9% (n = 3220) were classified as definite cardiac events and 10.6% (n = 1057) as probable events. Of all 412 through 414 codes (n = 9070), only 1.8% (n = 227) were classified as definite cardiac events and 5.4% (n = 716) as probable events. The epidemiologic diagnostic algorithm presented in this article used computerized data to assign diagnoses in a standard, objective manner, and was a lower cost alternative to classification of cardiac events on the basis of clinical review and/or more complex record abstraction approaches.-Coronary heart disease surveillance: field application of an epidemiologic algorithm.",0
"Cluster randomised trials, like individually randomised trials, may benefit from a baseline period of data collection. We consider trials in which clusters prospectively recruit or identify participants as a continuous process over a given calendar period, and ask whether and for how long investigators should collect baseline data as part of the trial, in order to maximise precision. We show how to calculate and plot the variance of the treatment effect estimator for different lengths of baseline period in a range of scenarios, and offer general advice. In some circumstances it is optimal not to include a baseline, while in others there is an optimal duration for the baseline. All other things being equal, the circumstances where it is preferable not to include a baseline period are those with a smaller recruitment rate, smaller intracluster correlation, greater decay in the intracluster correlation over time, or wider transition period between recruitment under control and intervention conditions. The variance of the treatment effect estimator can be calculated numerically, and plotted against the duration of baseline to inform design. It would be of interest to extend these investigations to cluster randomised trial designs with more than two randomised sequences of control and intervention condition, including stepped wedge designs.-Optimal design of cluster randomised trials with continuous recruitment and prospective baseline period.",1
"For rare outcomes, meta-analysis of randomized trials may be the only way to obtain reliable evidence of the effects of healthcare interventions. However, many methods of meta-analysis are based on large sample approximations, and may be unsuitable when events are rare. Through simulation, we evaluated the performance of 12 methods for pooling rare events, considering estimability, bias, coverage and statistical power. Simulations were based on data sets from three case studies with between five and 19 trials, using baseline event rates between 0.1 and 10 per cent and risk ratios of 1, 0.75, 0.5 and 0.2. We found that most of the commonly used meta-analytical methods were biased when data were sparse. The bias was greatest in inverse variance and DerSimonian and Laird odds ratio and risk difference methods, and the Mantel-Haenszel (MH) odds ratio method using a 0.5 zero-cell correction. Risk difference meta-analytical methods tended to show conservative confidence interval coverage and low statistical power at low event rates. At event rates below 1 per cent the Peto one-step odds ratio method was the least biased and most powerful method, and provided the best confidence interval coverage, provided there was no substantial imbalance between treatment and control group sizes within trials, and treatment effects were not exceptionally large. In other circumstances the MH OR without zero-cell corrections, logistic regression and the exact method performed similarly to each other, and were less biased than the Peto method.-Much ado about nothing: a comparison of the performance of meta-analytical methods with rare events.",0
"and Three network laboratories measured antibodies to islet autoantigens. Antibodies to glutamic acid decarboxylase (GAD65 [GADA]) and the intracellular portion of protein tyrosine phosphatase (IA-2(ic) [IA-2A]) were measured by similar, but not identical, methods in samples from participants in the Type 1 Diabetes Genetics Consortium (T1DGC). All laboratories used radiobinding assays to detect antibodies to in vitro transcribed and translated antigen, but with different local standards, calibrated against the World Health Organization (WHO) reference reagent. Using a common method to calculate WHO units/mL, we compared results reported on samples included in the Diabetes Autoantibody Standardization Program (DASP), and developed standard methods for reporting in WHO units/mL. We evaluated intra-assay and inter-assay coefficient of variation (CV) in blind duplicate samples and assay comparability in four DASP workshops. Values were linearly related in the three laboratories for both GADA and IA-2A, and intra-assay technical errors for values within the standard curve were below 13% for GADA and below 8.5% for IA-2A. Correlations in samples tested 1-2 years apart were &gt;97%. Over the course of the study, internal CVs were 10-20% with one exception, and the laboratories concordantly called samples GADA or IA-2A positive or negative in 96.7% and 99.6% of duplicates within the standard curve. Despite acceptable CVs and general concordance in ranking samples, the laboratories differed markedly in absolute values for GADA and IA-2A reported in WHO units/mL in DASP over a large range of values. With three laboratories using different assay methods (including calibrators), consistent values among them could not be attained. Modifications in the assays are needed to improve comparability of results expressed as WHO units/mL across laboratories. It will be essential to retain high intra- and inter-assay precision, sensitivity and specificity and to confirm the accuracy of harmonized methods.-Measurement of islet cell antibodies in the Type 1 Diabetes Genetics Consortium: efforts to harmonize procedures among the laboratories.",0
"The Women's Health Initiative trial found a modestly increased risk of invasive breast cancer with daily 0.625-mg conjugated equine estrogens plus 2.5-mg medroxyprogesterone acetate, with most evidence among women who had previously received postmenopausal hormone therapy. In comparison, observational studies mostly report a larger risk increase. To explain these patterns, the authors examined the effects of this regimen in relation to both prior hormone therapy and time from menopause to first use of postmenopausal hormone therapy (""gap time"") in the Women's Health Initiative trial and in a corresponding subset of the Women's Health Initiative observational study. Postmenopausal women with a uterus enrolled at 40 US clinical centers during 1993-1998. The authors found that hazard ratios agreed between the two cohorts at a specified gap time and time from hormone therapy initiation. Combined trial and observational study data support an adverse effect on breast cancer risk. Women who initiate use soon after menopause, and continue for many years, appear to be at particularly high risk. For example, for a woman who starts soon after menopause and adheres to this regimen, estimated hazard ratios are 1.64 (95% confidence interval: 1.00, 2.68) over a 5-year period of use and 2.19 (95% confidence interval: 1.56, 3.08) over a 10-year period of use.-Estrogen plus progestin therapy and breast cancer in recently postmenopausal women.",0
"There is growing interest in conducting clinical and cluster randomized trials through electronic health records. This paper reports on the methodological issues identified during the implementation of two cluster randomized trials using the electronic health records of the Clinical Practice Research Datalink (CPRD). Two trials were completed in primary care: one aimed to reduce inappropriate antibiotic prescribing for acute respiratory infection; the other aimed to increase physician adherence with secondary prevention interventions after first stroke. The paper draws on documentary records and trial datasets to report on the methodological experience with respect to research ethics and research governance approval, general practice recruitment and allocation, sample size calculation and power, intervention implementation, and trial analysis. We obtained research governance approvals from more than 150 primary care organizations in England, Wales, and Scotland. There were 104 CPRD general practices recruited to the antibiotic trial and 106 to the stroke trial, with the target number of practices being recruited within six months. Interventions were installed into practice information systems remotely over the internet. The mean number of participants per practice was 5,588 in the antibiotic trial and 110 in the stroke trial, with the coefficient of variation of practice sizes being 0.53 and 0.56 respectively. Outcome measures showed substantial correlations between the 12?months before, and after intervention, with coefficients ranging from 0.42 for diastolic blood pressure to 0.91 for proportion of consultations with antibiotics prescribed, defining practice and participant eligibility for analysis requires careful consideration. Cluster randomized trials may be performed efficiently in large samples from UK general practices using the electronic health records of a primary care database. The geographical dispersal of trial sites presents a difficulty for research governance approval and intervention implementation. Pretrial data analyses should inform trial design and analysis plans. Current Controlled Trials ISRCTN 47558792 and ISRCTN 35701810 (both registered on 17 March 2010).-Cluster randomized trials utilizing primary care electronic health records: methodological issues in design, conduct, and analysis (eCRT Study).",1
"Low birthweight (BW) and childhood growth have been hypothesized to be associated with an increased risk of hypertension in later life. We analysed data among 13,467 women with a recalled BW from the Shanghai Women's Health Study. Cases included those with a self-reported hypertension with ('confirmed cases') or without ('possible cases') antihypertensive medication(s) use. Logistic regression was used to derive adjusted odds ratios (OR) and 95% CI. Birthweight was inversely associated with the odds of early onset (at age 20-40 years) hypertension in a dose response manner (P for trend = 0.01). This association is stronger for 'confirmed' hypertension (only OR for 'confirmed' hypertension are referred to subsequently). Being heavier or taller than average at 15 years of age were both related to elevated odds of early onset hypertension. Women who had a low BW but were heavier than average at age 15 were more than four times (OR = 4.63, 95% CI: 2.40-8.94) more likely to have an early onset hypertension, and those who had a low BW and became taller at 15 years of age had an OR of 1.87 (95% CI: 1.05-3.31). A significant interaction between BW and weight at age 15 was observed (P = 0.04). Our study suggests that low BW, particularly if accompanied by accelerated childhood growth, may increase the risk of early onset hypertension in adulthood.-Birthweight, childhood growth and hypertension in adulthood.",0
"Cluster randomised trials are an increasingly important methodological tool in health research but they present challenges to the informed consent requirement. In the relatively limited literature on the ethics of cluster research there is not much clarity about the reasons for which seeking informed consent in cluster randomised trials may be morally challenging. In this paper, I distinguish between the cases where informed consent in cluster trials may be problematic due to the distinct features of 'population-based' interventions, which have not been adequately discussed in the research ethics literature, and the cases where informed consent may be problematic for reasons that investigators also encounter in other research designs. I claim that informed consent requirements in cluster trials should be adjusted to the level of risk involved, arguing for a more comprehensive notion of research risk than that currently found in the research ethics guidelines, and the amount of freedom to be sacrificed in relation to a particular research aim. I conclude that these two factors are the most important to consider when assessing whether a cluster study should proceed when informed consent is infeasible or difficult to obtain.-Informed consent in cluster randomised trials: new and common ethical challenges.",1
"The stepped wedge design is increasingly being used in cluster randomized trials (CRTs). However, there is not much information available about the design and analysis strategies for these kinds of trials. Approaches to sample size and power calculations have been provided, but a simple sample size formula is lacking. Therefore, our aim is to provide a sample size formula for cluster randomized stepped wedge designs. We derived a design effect (sample size correction factor) that can be used to estimate the required sample size for stepped wedge designs. Furthermore, we compared the required sample size for the stepped wedge design with a parallel group and analysis of covariance (ANCOVA) design. Our formula corrects for clustering as well as for the design. Apart from the cluster size and intracluster correlation, the design effect depends on choices of the number of steps, the number of baseline measurements, and the number of measurements between steps. The stepped wedge design requires a substantial smaller sample size than a parallel group and ANCOVA design. For CRTs, the stepped wedge design is far more efficient than the parallel group and ANCOVA design in terms of sample size.-Stepped wedge designs could reduce the required sample size in cluster randomized trials.",3
"Meta-analyses involving the synthesis of evidence from cluster randomization trials are being increasingly reported. These analyses raise challenging methodologic issues beyond those raised by meta-analyses which include only individually randomized trials. In this paper we review and comment on a selected number of these issues, including problems of study heterogeneity, difficulties in estimating design effects from individual trials and the choice of statistical methods.-Issues in the meta-analysis of cluster randomized trials.",1
"In March 2008, the D:A:D study published results demonstrating an increased risk of myocardial infarction (MI) for patients on abacavir (ABC). We describe changes to the use of ABC since this date, and investigate changes to the association between ABC and MI with subsequent follow-up. A total of 49,717 D:A:D participants were followed from study entry until the first of an MI, death, 1 February 2013 or 6 months after last visit. Associations between a person's 10-year cardiovascular disease (CVD) risk and the likelihood of initiating or discontinuing ABC were assessed using multivariable logistic/Poisson regression. Poisson regression was used to assess the association between current ABC use and MI risk, adjusting for potential confounders, and a test of interaction was performed to assess whether the association had changed in the post-March 2008 period. Use of ABC increased from 10 % of the cohort in 2000 to 20 % in 2008, before stabilising at 18-19 %. Increases in use pre-March 2008, and subsequent decreases, were greatest in those at moderate and high CVD risk. Post-March 2008, those on ABC at moderate/high CVD risk were more likely to discontinue ABC than those at low/unknown CVD risk, regardless of viral load (?1,000 copies/ml: relative rate 1.49 [95 % confidence interval 1.34-1.65]; &gt;1,000 copies/ml: 1.23 [1.02-1.48]); no such associations were seen pre-March 2008. There was some evidence that antiretroviral therapy (ART)-na?ve persons at moderate/high CVD risk post-March 2008 were less likely to initiate ABC than those at low/unknown CVD risk (odds ratio 0.74 [0.48-1.13]). By 1 February 2013, 941 MI events had occurred in 367,559 person-years. Current ABC use was associated with a 98 % increase in MI rate (RR 1.98 [1.72-2.29]) with no difference in the pre- (1.97 [1.68-2.33]) or post- (1.97 [1.43-2.72]) March 2008 periods (interaction P = 0.74). Despite a reduction in the channelling of ABC for patients at higher CVD risk since 2008, we continue to observe an association between ABC use and MI risk. Whilst confounding cannot be fully ruled out, this further diminishes channelling bias as an explanation for our findings.-Is there continued evidence for an association between abacavir usage and myocardial infarction risk in individuals with HIV? A cohort collaboration.",0
"Stepped wedge cluster randomised trials introduce interventions to groups of clusters in a random order and have been used to evaluate interventions for health and wellbeing. Standardised guidance for reporting stepped wedge trials is currently absent, and a range of potential analytic approaches have been described. We systematically identified and reviewed recently published (2010 to 2014) analyses of stepped wedge trials. We extracted data and described the range of reporting and analysis approaches taken across all studies. We critically appraised the strategy described by three trials chosen to reflect a range of design characteristics. Ten reports of completed analyses were identified. Reporting varied: seven of the studies included a CONSORT diagram, and only five also included a diagram of the intervention rollout. Seven assessed the balance achieved by randomisation, and there was considerable heterogeneity among the approaches used. Only six reported the trend in the outcome over time. All used both 'horizontal' and 'vertical' information to estimate the intervention effect: eight adjusted for time with a fixed effect, one used time as a condition using a Cox proportional hazards model, and one did not account for time trends. The majority used simple random effects to account for clustering and repeat measures, assuming a common intervention effect across clusters. Outcome data from before and after the rollout period were often included in the primary analysis. Potential lags in the outcome response to the intervention were rarely investigated. We use three case studies to illustrate different approaches to analysis and reporting. There is considerable heterogeneity in the reporting of stepped wedge cluster randomised trials. Correct specification of the time-trend underlies the validity of the analytical approaches. The possibility that intervention effects vary by cluster or over time should be considered. Further work should be done to standardise the reporting of the design, attrition, balance, and time-trends in stepped wedge trials.-Analysis and reporting of stepped wedge randomised controlled trials: synthesis and critical appraisal of published studies, 2010 to 2014.",3
"To systematically review the quality of reporting of pilot and feasibility of cluster randomised trials (CRTs). In particular, to assess (1) the number of pilot CRTs conducted between 1 January 2011 and 31 December 2014, (2) whether objectives and methods are appropriate and (3) reporting quality. We searched PubMed (2011-2014) for CRTs with 'pilot' or 'feasibility' in the title or abstract; that were assessing some element of feasibility and showing evidence the study was in preparation for a main effectiveness/efficacy trial. Quality assessment criteria were based on the Consolidated Standards of Reporting Trials (CONSORT) extensions for pilot trials and CRTs. Eighteen pilot CRTs were identified. Forty-four per cent did not have feasibility as their primary objective, and many (50%) performed formal hypothesis testing for effectiveness/efficacy despite being underpowered. Most (83%) included 'pilot' or 'feasibility' in the title, and discussed implications for progression from the pilot to the future definitive trial (89%), but fewer reported reasons for the randomised pilot trial (39%), sample size rationale (44%) or progression criteria (17%). Most defined the cluster (100%), and number of clusters randomised (94%), but few reported how the cluster design affected sample size (17%), whether consent was sought from clusters (11%), or who enrolled clusters (17%). That only 18 pilot CRTs were identified necessitates increased awareness of the importance of conducting and publishing pilot CRTs and improved reporting. Pilot CRTs should primarily be assessing feasibility, avoiding formal hypothesis testing for effectiveness/efficacy and reporting reasons for the pilot, sample size rationale and progression criteria, as well as enrolment of clusters, and how the cluster design affects design aspects. We recommend adherence to the CONSORT extensions for pilot trials and CRTs.-Quality of reporting of pilot and feasibility cluster randomised trials: a systematic review.",1
"Optimism bias refers to unwarranted belief in the efficacy of new therapies. We assessed the impact of optimism bias on a proportion of trials that did not answer their research question successfully and explored whether poor accrual or optimism bias is responsible for inconclusive results. Systematic review. Retrospective analysis of a consecutive-series phase III randomized controlled trials (RCTs) performed under the aegis of National Cancer Institute Cooperative groups. Three hundred fifty-nine trials (374 comparisons) enrolling 150,232 patients were analyzed. Seventy percent (262 of 374) of the trials generated conclusive results according to the statistical criteria. Investigators made definitive statements related to the treatment preference in 73% (273 of 374) of studies. Investigators' judgments and statistical inferences were concordant in 75% (279 of 374) of trials. Investigators consistently overestimated their expected treatment effects but to a significantly larger extent for inconclusive trials. The median ratio of expected and observed hazard ratio or odds ratio was 1.34 (range: 0.19-15.40) in conclusive trials compared with 1.86 (range: 1.09-12.00) in inconclusive studies (P&lt;0.0001). Only 17% of the trials had treatment effects that matched original researchers' expectations. Formal statistical inference is sufficient to answer the research question in 75% of RCTs. The answers to the other 25% depend mostly on subjective judgments, which at times are in conflict with statistical inference. Optimism bias significantly contributes to inconclusive results.-Optimism bias leads to inconclusive results-an empirical study.",0
"This study proposes a time-varying effect model that can be used to characterize gender-specific trajectories of health behaviors and conduct hypothesis testing for gender differences. The motivating examples demonstrate that the proposed model is applicable to not only multi-wave longitudinal studies but also short-term studies that involve intensive data collection. The simulation study shows that the accuracy of estimation of trajectory functions improves as the sample size and the number of time points increase. In terms of the performance of the hypothesis testing, the type I error rates are close to their corresponding significance levels under all combinations of sample size and number of time points. Furthermore, the power increases as the alternative hypothesis deviates more from the null hypothesis, and the rate of this increasing trend is higher when the sample size and the number of time points are larger.-A time-varying effect model for studying gender differences in health behavior.",0
"Process evaluations are recommended to open the 'black box' of complex interventions evaluated in trials, but there is limited guidance to help researchers design process evaluations. Much current literature on process evaluations of complex interventions focuses on qualitative methods, with less attention paid to quantitative methods. This discrepancy led us to develop our own framework for designing process evaluations of cluster-randomised controlled trials. We reviewed recent theoretical and methodological literature and selected published process evaluations; these publications identified a need for structure to help design process evaluations. We drew upon this literature to develop a framework through iterative exchanges, and tested this against published evaluations. The developed framework presents a range of candidate approaches to understanding trial delivery, intervention implementation and the responses of targeted participants. We believe this framework will be useful to others designing process evaluations of complex intervention trials. We also propose key information that process evaluations could report to facilitate their identification and enhance their usefulness. There is no single best way to design and carry out a process evaluation. Researchers will be faced with choices about what questions to focus on and which methods to use. The most appropriate design depends on the purpose of the process evaluation; the framework aims to help researchers make explicit their choices of research questions and methods. Clinicaltrials.gov NCT01425502.-Process evaluations for cluster-randomised trials of complex interventions: a proposed framework for design and reporting.",1
"This paper considers a marginal approach for the analysis of the effect of covariates on multivariate interval-censored survival data.Interval censoring of multivariate events can occur when the events are not directly observable but are detected by periodically performing clinical examinations or laboratory tests. The method assumes the marginal distribution for each event is based on a discrete analogue of the proportional hazards model for interval-censored data. A robust estimator for the covariance matrix is developed that accounts for the correlation between events. A simulation study comparing the performance of this method and a midpoint imputation approach indicates the parameter estimates from the proposed method are less biased. Furthermore, even when the events are only modestly correlated, ignoring the correlation can result in erroneous variance estimators. The method is illustrated using data from an ongoing clinical trial involving subjects with systemic lupus erythematosus.-The analysis of multivariate interval-censored survival data.",0
"Recent methodological advances in covariate adjustment in randomized clinical trials have used semiparametric theory to improve efficiency of inferences by incorporating baseline covariates; these methods have focused on independent outcomes. We modify one of these approaches, augmentation of standard estimators, for use within cluster randomized trials in which treatments are assigned to groups of individuals, thereby inducing correlation. We demonstrate the potential for imbalance correction and efficiency improvement through consideration of both cluster-level covariates and individual-level covariates. To improve small-sample estimation, we consider several variance adjustments. We evaluate this approach for continuous and binary outcomes through simulation and apply it to data from a cluster randomized trial of a community behavioral intervention related to HIV prevention in Tanzania.-Augmented generalized estimating equations for improving efficiency and validity of estimation in cluster randomized trials by leveraging cluster-level and individual-level covariates.",1
"In June 2013, a 1-day workshop on Dynamic Treatment Strategies (DTSs) and Sequential Multiple Assignment Randomized Trials (SMARTs) was held at the University of Pennsylvania in Philadelphia, Pennsylvania. These two linked topics have generated a great deal of interest as researchers have recognized the importance of comparing entire strategies for managing chronic disease. A number of articles emerged from that workshop. The purpose of this survey of the DTS/SMART methodology (which is taken from the introductory talk in the workshop) is to provide the reader the collected articles presented in this volume with sufficient background to appreciate the more detailed discussions in the articles. The way that the DTS arises naturally in clinical practice is described, along with its connection to the well-known difficulties of interpreting the analysis by intention-to-treat. The SMART methodology for comparing DTS is described, and the basics of estimation and inference presented. The DTS/SMART methodology can be a flexible and practical way to optimize ongoing clinical decision making, providing evidence (based on randomization) for comparative effectiveness. The DTS/SMART methodology is not a solution for unstandardized study protocols. The DTS/SMART methodology has growing relevance to comparative effectiveness research and the needs of the learning healthcare system.-Introduction to dynamic treatment strategies and sequential multiple assignment randomization.",0
The Design of Cluster Randomized Crossover Trials,1
"Infectious disease interventions are increasingly tested using cluster-randomized trials (CRTs). These trial settings tend to involve a set of sampling units, such as villages, whose geographic arrangement may present a contamination risk in treatment exposure. The most widely used approach for reducing contamination in these settings is the so-called fried-egg design, which excludes the outer portion of all available clusters from the primary trial analysis. However, the fried-egg design ignores potential intra-cluster spatial heterogeneity and makes the outcome measure inherently less precise. Whereas the fried-egg design may be appropriate in specific settings, alternative methods to optimize the design of CRTs in other settings are lacking. We present a novel approach for CRT design that either fully includes or fully excludes available clusters in a defined study region, recognizing the potential for intra-cluster spatial heterogeneity. The approach includes an algorithm that allows investigators to identify the maximum number of clusters that could be included for a defined study region and maintain randomness in both the selection of included clusters and the allocation of clusters to either the treatment group or control group. The approach was applied to the design of a CRT testing the effectiveness of malaria vector-control interventions in southern Malawi. Those planning CRTs to evaluate interventions should consider the approach presented here during trial design. The approach provides a novel framework for reducing the risk of contamination among the CRT randomization units in settings where investigators determine the reduction of contamination risk as a high priority and where intra-cluster spatial heterogeneity is likely. By maintaining randomness in the allocation of clusters to either the treatment group or control group, the approach also permits a randomization-valid test of the primary trial hypothesis.-Reducing contamination risk in cluster-randomized infectious disease-intervention trials",1
"The statistical analysis of community-based trials and of other cluster-randomised trials, requires specific statistical methods. We show the consequences of the application of these models for study results, using data of the German Cardiovascular Prevention Study (GCP) as an example. Data of 30,285 subjects were analysed, which were collected at the beginning and at the end of the study period. These data had been collected in 7 intervention regions and by national surveys. We grouped data of the national surveys in 7 control clusters to mimick a design typical for cluster-randomised trials. We applied the following statistical models to estimate the effect of the intervention on total cholesterol as well as on systolic blood pressure and the respective confidence intervals: a linear model, a mixed model, and fixed and random effects meta-analyses. While the estimates and confidence intervals for the intervention effect were similar in mixed model analysis and random effects meta-analysis, results from models incorporating fixed effects only were anti-conservative. The underestimation of variance in models incorporating fixed effects only was especially large in the analysis of systolic blood pressure data, where great heterogeneity between intervention communities was observed. Despite seemingly low intraclass correlation coefficients of 0.0019 for total cholesterol and 0.0166 for systolic blood pressure, respectively, the variance of the intervention effect was increased in the mixed model 2.8fold or 17.1fold, respectively, in comparison to the variance estimated in the linear model. Due to this variance inflation the intervention effect on systolic blood pressure lost statistical significance. Our results emphasise the importance to account for correlations in community-based trials. Besides the mixed model random effects meta-analysis can be applied as an alternative method.-[Statistical analysis of community-based studies -- presentation and comparison of possible solutions with reference to statistical meta-analytic methods].",1
"Longitudinal studies aimed at assessing the impact of interventions on disease risk factors often confront several statistical problems. These problems include 1) dependent variables measured by ordered categories, 2) numerous potentially relevant patterns of transition between outcome levels, 3) mixed units of analysis (e.g., assignment by social unit while theorizing in terms of individuals), 4) incomplete randomization, and 5) correlated estimates for successive occasions of longitudinal measurement. Longitudinal data on use of cigarettes, alcohol, and marijuana among adolescents (n = 1,244, complete data) from the Midwestern Prevention Project are used to demonstrate solutions to each of these problems: 1) a proportional odds regression model, 2) conditional logistic models of transitions with interactions between baseline level and intervention effect, 3) a logistic model estimated with linear regression methods on measures aggregated by social unit, 4) conditional and unconditional models of effect magnitude, and 5) a repeated measures logistic regression technique. Panel data fit to the various models yielded the following conclusions concerning intervention effects in the Midwestern Prevention Project: reduction in the prevalence of cigarette users in treatment schools compared with control schools (8% vs. 18% smoked in the last week at one year follow-up), mixed evidence of an effect on marijuana use, and no evidence of an effect on alcohol use.-Estimating intervention effects in longitudinal studies.",1
"There is growing interest in pooling specimens across subjects in epidemiologic studies, especially those involving biomarkers. This paper is concerned with regression analysis of epidemiologic data where a binary exposure is subject to pooling and the pooled measurement is dichotomized to indicate either that no subjects in the pool are exposed or that some are exposed, without revealing further information about the exposed subjects in the latter case. The pooling process may be stratified on the disease status (a binary outcome) and possibly other variables but is otherwise assumed random. We propose methods for estimating parameters in a prospective logistic regression model and illustrate these with data from a population-based case-control study of colorectal cancer. Simulation results show that the proposed methods perform reasonably well in realistic settings and that pooling can lead to sizable gains in cost efficiency. We make recommendations with regard to the choice of design for pooled epidemiologic studies.-Logistic regression analysis of biomarker data subject to pooling and dichotomization.",0
"For cluster randomized and multicentre trials evaluating the effect of a treatment on persons nested within clusters, equations have been published to compute the optimal sample sizes at the cluster and person level as a function of sampling costs and intraclass correlation (ICC). Here, optimal means maximum power and precision for a given sampling budget, or minimum sampling costs for a given power and precision. However, the ICC is usually unknown, and the optimal sample sizes depend strongly on this ICC. To overcome this local optimality problem, this study presents Maximin designs (MMDs) based on relative efficiency (RE) and efficiency. These designs perform well over a range of possible ICC values either in terms of RE compared with the locally optimal designs, or in terms of minimum efficiency (maximum variance) of the treatment effect estimator. The use of MMDs is illustrated using information from many cluster randomized trials in primary care. It is concluded that MMDs and the optimal design for an ICC halfway its assumed range are efficient for a range of ICC values and recommendable for practical use. This requires that trial reports mention the study cost per cluster and person.-Efficient design of cluster randomized and multicentre trials with unknown intraclass correlation.",1
"In this work, we develop a Bayesian approach to perform selection of predictors that are linked within a network. We achieve this by combining a sparse regression model relating the predictors to a response variable with a graphical model describing conditional dependencies among the predictors. The proposed method is well-suited for genomic applications because it allows the identification of pathways of functionally related genes or proteins that impact an outcome of interest. In contrast to previous approaches for network-guided variable selection, we infer the network among predictors using a Gaussian graphical model and do not assume that network information is available a priori. We demonstrate that our method outperforms existing methods in identifying network-structured predictors in simulation settings and illustrate our proposed model with an application to inference of proteins relevant to glioblastoma survival.-Joint Bayesian variable and graph selection for regression models with network-structured predictors.",0
"In clustered survival data, subunits within each cluster share similar characteristics, so that observations made from them tend to be positively correlated. In clinical trials, the correlated subunits from the same cluster are often randomized to different treatment groups. In this case, the variance formulas of the standard rank tests such as the logrank, Gehan-Wilcoxon or Prentice-Wilcoxon, proposed for independent samples, need to be adjusted for intracluster correlations both within and between treatment groups for testing equality of marginal survival distributions. In this paper we derive a general form of simple variance formulas of the rank tests when subunits from the same cluster are randomized into different treatment groups. Extensive simulation studies are conducted to investigate small sample performance of the variance formulas. We compare our non-parametric rank tests based on the adjusted variances with one from a shared frailty model, which is an optimal semi-parametric testing procedure when the intracluster correlations within and between groups are the same.-Rank tests for clustered survival data when dependent subunits are randomized.",1
Use of the stepped wedge design cannot be recommended: a critical appraisal and comparison with the classic cluster randomized controlled trial design.,3
"Monthly counts of medical visits across several years for persons identified to have alcoholism problems are modeled using two-state hidden Markov models (HMM) in order to describe the effect of alcoholism treatment on the likelihood of persons to be in a 'healthy' or 'unhealthy' state. The medical visits can be classified into different types leading to multivariate counts of medical visits each month. A multiple indicator HMM is introduced, which simultaneously fits the multivariate Poisson counts by assuming a shared hidden state underlying all of them. The multiple indicator HMM borrows information across different types of medical encounters. A univariate HMM based on the total count across types of medical visits each month is also considered. Comparisons between the multiple indicator HMM and the total count HMM are made, as well as comparisons with more traditional longitudinal models that directly model the counts. A Bayesian framework is used for the estimation of the HMM and implementation is in Winbugs.-Multiple indicator hidden Markov model with an application to medical utilization data.",0
"Cluster randomized trials (CRTs) are increasingly used to assess the effectiveness of interventions to improve health outcomes or prevent diseases. However, the efficiency and consistency of using different analytical methods in the analysis of binary outcome have received little attention. We described and compared various statistical approaches in the analysis of CRTs using the Community Hypertension Assessment Trial (CHAT) as an example. The CHAT study was a cluster randomized controlled trial aimed at investigating the effectiveness of pharmacy-based blood pressure clinics led by peer health educators, with feedback to family physicians (CHAT intervention) against Usual Practice model (Control), on the monitoring and management of BP among older adults. We compared three cluster-level and six individual-level statistical analysis methods in the analysis of binary outcomes from the CHAT study. The three cluster-level analysis methods were: i) un-weighted linear regression, ii) weighted linear regression, and iii) random-effects meta-regression. The six individual level analysis methods were: i) standard logistic regression, ii) robust standard errors approach, iii) generalized estimating equations, iv) random-effects meta-analytic approach, v) random-effects logistic regression, and vi) Bayesian random-effects regression. We also investigated the robustness of the estimates after the adjustment for the cluster and individual level covariates. Among all the statistical methods assessed, the Bayesian random-effects logistic regression method yielded the widest 95% interval estimate for the odds ratio and consequently led to the most conservative conclusion. However, the results remained robust under all methods - showing sufficient evidence in support of the hypothesis of no effect for the CHAT intervention against Usual Practice control model for management of blood pressure among seniors in primary care. The individual-level standard logistic regression is the least appropriate method in the analysis of CRTs because it ignores the correlation of the outcomes for the individuals within the same cluster. We used data from the CHAT trial to compare different methods for analysing data from CRTs. Using different methods to analyse CRTs provides a good approach to assess the sensitivity of the results to enhance interpretation.-Comparison of Bayesian and classical methods in the analysis of cluster randomized controlled trials with a binary outcome: the Community Hypertension Assessment Trial (CHAT).",1
"Adaptive designs (ADs) allow pre-planned changes to an ongoing trial without compromising the validity of conclusions and it is essential to distinguish pre-planned from unplanned changes that may also occur. The reporting of ADs in randomised trials is inconsistent and needs improving. Incompletely reported AD randomised trials are difficult to reproduce and are hard to interpret and synthesise. This consequently hampers their ability to inform practice as well as future research and contributes to research waste. Better transparency and adequate reporting will enable the potential benefits of ADs to be realised.This extension to the Consolidated Standards Of Reporting Trials (CONSORT) 2010 statement was developed to enhance the reporting of randomised AD clinical trials. We developed an Adaptive designs CONSORT Extension (ACE) guideline through a two-stage Delphi process with input from multidisciplinary key stakeholders in clinical trials research in the public and private sectors from 21 countries, followed by a consensus meeting. Members of the CONSORT Group were involved during the development process.The paper presents the ACE checklists for AD randomised trial reports and abstracts, as well as an explanation with examples to aid the application of the guideline. The ACE checklist comprises seven new items, nine modified items, six unchanged items for which additional explanatory text clarifies further considerations for ADs, and 20 unchanged items not requiring further explanatory text. The ACE abstract checklist has one new item, one modified item, one unchanged item with additional explanatory text for ADs, and 15 unchanged items not requiring further explanatory text.The intention is to enhance transparency and improve reporting of AD randomised trials to improve the interpretability of their results and reproducibility of their methods, results and inference. We also hope indirectly to facilitate the much-needed knowledge transfer of innovative trial designs to maximise their potential benefits.-The Adaptive designs CONSORT Extension (ACE) statement: a checklist with explanation and elaboration guideline for reporting randomised trials that use an adaptive design",1
"Quantitative procedures for evaluating added values from new markers over a conventional risk scoring system for predicting event rates at specific time points have been extensively studied. However, a single summary statistic, for example, the area under the receiver operating characteristic curve or its derivatives, may not provide a clear picture about the relationship between the conventional and the new risk scoring systems. When there are no censored event time observations in the data, two simple scatterplots with individual conventional and new scores for ""cases"" and ""controls"" provide valuable information regarding the overall and the subject-specific level incremental values from the new markers. Unfortunately, in the presence of censoring, it is not clear how to construct such plots. In this article, we propose a nonparametric estimation procedure for the distributions of the differences between two risk scores conditional on the conventional score. The resulting quantile curves of these differences over the subject-specific conventional score provide extra information about the overall added value from the new marker. They also help us to identify a subgroup of future subjects who need the new predictors, especially when there is no unified utility function available for cost-risk-benefit decision making. The procedure is illustrated with two data sets. The first is from a well-known Mayo Clinic primary biliary cirrhosis liver study. The second is from a recent breast cancer study on evaluating the added value from a gene score, which is relatively expensive to measure compared with the routinely used clinical biomarkers for predicting the patient's survival after surgery.-Graphical procedures for evaluating overall and subject-specific incremental values from new predictors with censored event time data.",0
"Measures for assessing patient-reported outcomes (PROs) that may have initially been developed for research are increasingly being recommended for use in clinical practice as well. Although psychometric rigor is essential, this article focuses on pragmatic characteristics of PROs that may enhance uptake into clinical practice. Three sources were drawn on in identifying pragmatic criteria for PROs: (1) selected literature review including recommendations by other expert groups; (2) key features of several model public domain PROs; and (3) the authors' experience in developing practical PROs. Eight characteristics of a practical PRO include: (1) actionability (i.e., scores guide diagnostic or therapeutic actions/decision making); (2) appropriateness for the relevant clinical setting; (3) universality (i.e., for screening, severity assessment, and monitoring across multiple conditions); (4) self-administration; (5) item features (number of items and bundling issues); (6) response options (option number and dimensions, uniform vs. varying options, time frame, intervals between options); (7) scoring (simplicity and interpretability); and (8) accessibility (nonproprietary, downloadable, available in different languages and for vulnerable groups, and incorporated into electronic health records). Balancing psychometric and pragmatic factors in the development of PROs is important for accelerating the incorporation of PROs into clinical practice.-Pragmatic characteristics of patient-reported outcome measures are important for use in clinical practice.",0
"Cluster randomization trials are widely used to test the effect of an intervention when individuals are naturally found in groups such as communities. For several separate studies of a similar intervention, it may be of interest to combine their results using meta-analysis procedures. However, this task requires consideration of both the likely dependencies among cluster members (intracluster correlation) and stratification based on the studies considered. In this article, several possible approaches for meta-analysis are considered for cluster randomization trials having a binary outcome. It is first noted that the standard Mantel-Haenszel test is invalid in this context since it ignores dependencies among cluster members. Two modifications are therefore considered as well as a general inverse variance approach and a procedure based on the Woolf statistic which does not require the availability of trial-specific design effects. Empirical Type I errors and powers for the different procedures considered are evaluated using Monte Carlo simulation. To illustrate the techniques, data are used from trials performed in four countries to compare two antenatal care programs with respect to their effects on the risk of hypertension during pregnancy. For the simulation scenarios considered, an adjusted Mantel-Haenszel procedure provides a valid test with the greatest power slightly outperforming the general inverse variance approach. The potential need to adjust for possible confounding was not considered. However, more detailed information on confounders would not likely be available for most meta-analyses. Two procedures performed well. However, the choice of analysis approach also inevitably depends on the nature and extent of the available data.-Meta-analysis of community-based cluster randomization trials with binary outcomes.",1
"Evaluation studies frequently draw on fallible outcomes that contain significant measurement error. Ignoring outcome measurement error in the planning stages can undermine the sufficiency and efficiency of an otherwise well-designed study and can further constrain the evidence studies bring to bear on the effectiveness of programs. We develop simple formulas to adjust statistical power, minimum detectable effect (MDE), and optimal sample allocation formulas for two-level cluster- and multisite-randomized designs when the outcome is subject to measurement error. The resulting adjusted formulas suggest that outcome measurement error typically amplifies treatment effect uncertainty, reduces power, increases the MDE, and undermines the efficiency of conventional optimal sampling schemes. Therefore, achieving adequate power for a given effect size will typically demand increased sample sizes when considering fallible outcomes, while maintaining design efficiency will require increasing portions of a budget be applied toward sampling a larger number of individuals within clusters. We illustrate evaluation planning with the new formulas while comparing them to conventional formulas using hypothetical examples based on recent empirical studies. To encourage adoption of the new formulas, we implement them in the R package PowerUpR and in the PowerUp software.-Optimal Design of Cluster- and Multisite-Randomized Studies Using Fallible Outcome Measures",1
"Methods to unravel the genetic determinants of non-Mendelian diseases lie at the next frontier of statistical approaches for human genetics. It is generally agreed that, before proceeding with segregation or linkage analysis, the trait under study ought to be shown to exhibit familial correlation. By coding dichotomous traits as binary variables, a single robust approach in the estimation of pedigree correlations, rather than two distinct approaches, can be used to assess the potential heritability of a trait, and, latterly, to examine the mode of inheritance. The asymptotic theory to conduct hypothesis tests and confidence intervals for correlations among different members of nuclear families is well established but is applicable only if the nuclear families are independent. As a further contribution to the literature, we derive the asymptotic sampling distribution of correlations between random variables among arbitrary pairs of members in extended families for the Pearson product-moment estimator with generalized weights. This derivation is done without assuming normality of the traits. The sampling distribution is shown to be asymptotically normal to first order, and hence large-sample hypothesis tests and confidence intervals with estimates of the variances and correlation coefficients are proposed. Discussion concludes with an example and a suggestion for future research.-Robust asymptotic sampling theory for correlations in pedigrees.",0
"Vietnam has been experiencing an epidemiologic transition to that of a lower-middle income country with an increasing prevalence of non-communicable diseases. The key risk factors for cardiovascular disease (CVD) are either on the rise or at alarming levels in Vietnam, particularly hypertension (HTN). Inasmuch, the burden of CVD will continue to increase in the Vietnamese population unless effective prevention and control measures are put in place. The objectives of the proposed project are to evaluate the implementation and effectiveness of two multi-faceted community and clinic-based strategies on the control of elevated blood pressure (BP) among adults in Vietnam via a cluster randomized trial design. Sixteen communities will be randomized to either an intervention (8 communities) or a comparison group (8 communities). Eligible and consenting adult study participants with HTN (n = 680) will be assigned to intervention/comparison status based on the community in which they reside. Both comparison and intervention groups will receive a multi-level intervention modeled after the Vietnam National Hypertension Program including education and practice change modules for health care providers, accessible reading materials for patients, and a multi-media community awareness program. In addition, the intervention group only will receive three carefully selected enhancements integrated into routine clinical care: (1) expanded community health worker services, (2) home BP self-monitoring, and (3) a ""storytelling intervention,"" which consists of interactive, literacy-appropriate, and culturally sensitive multi-media storytelling modules for motivating behavior change through the power of patients speaking in their own voices. The storytelling intervention will be delivered by DVDs with serial installments at baseline and at 3, 6, and 9 months after trial enrollment. Changes in BP will be assessed in both groups at several follow-up time points. Implementation outcomes will be assessed as well. Results from this full-scale trial will provide health policymakers with practical evidence on how to combat a key risk factor for CVD using a feasible, sustainable, and cost-effective intervention that could be used as a national program for controlling HTN in Vietnam. ClinicalTrials.gov NCT03590691 . Registered on July 17, 2018. Protocol version: 6. Date: August 15, 2019.-Conquering hypertension in Vietnam-solutions at grassroots level: study protocol of a cluster randomized controlled trial.",0
"The use of multi-level logistic regression models was explored for the analysis of data from a cluster randomized trial investigating whether a training programme for general practitioners' reception staff could improve women's attendance at breast screening. Twenty-six general practices were randomized with women nested within them, requiring a two-level model which allowed for between-practice variability. Comparisons were made with fixed effect (FE) and random effects (RE) cluster summary statistic methods, ordinary logistic regression and a marginal model based on generalized estimating equations with robust variance estimates. An FE summary statistic method and ordinary logistic regression considerably understated the variance of the intervention effect, thus overstating its statistical significance. The marginal model produced a higher statistical significance for the intervention effect compared to that obtained from the RE summary statistic method and the multi-level model. Because there was only a moderate number of practices and these had unbalanced cluster sizes, reliable asymptotic properties for the robust standard errors used in the marginal model may not have been achieved. While the RE summary statistic method cannot handle multiple covariates easily, marginal and multi-level models can do so. In contrast to multi-level models however, marginal models do not provide direct estimates of variance components, but treat these as nuisance parameters. Estimates of the variance components were of particular interest in this example. Additionally, parametric bootstrap methods within the multi-level model framework provide confidence intervals for these variance components, as well as a confidence interval for the effect of intervention which allows for the imprecision in the estimated variance components. The assumption of normality of the random effects can be checked, and the models extended to investigate multiple sources of variability.-Analysis of a cluster randomized trial with binary outcome data using a multi-level model.",1
"In a five-arm randomized clinical trial (RCT) with stratified randomization across 54 sites, we encountered low primary outcome event proportions, resulting in multiple sites with zero events either overall or in one or more study arms. In this paper, we systematically evaluated different statistical methods of accounting for center in settings with low outcome event proportions. We conducted a simulation study and a reanalysis of a completed RCT to compare five popular methods of estimating an odds ratio for multicenter trials with stratified randomization by center: (i) no center adjustment, (ii) random intercept model, (iii) Mantel-Haenszel model, (iv) generalized estimating equation (GEE) with an exchangeable correlation structure, and (v) GEE with small sample correction (GEE-small sample correction). We varied the number of total participants (200, 500, 1000, 5000), number of centers (5, 50, 100), control group outcome percentage (2%, 5%, 10%), true odds ratio (1, &gt; 1), intra-class correlation coefficient (ICC) (0.025, 0.075), and distribution of participants across the centers (balanced, skewed). Mantel-Haenszel methods generally performed poorly in terms of power and bias and led to the exclusion of participants from the analysis because some centers had no events. Failure to account for center in the analysis generally led to lower power and type I error rates than other methods, particularly with ICC = 0.075. GEE had an inflated type I error rate except in some settings with a large number of centers. GEE-small sample correction maintained the type I error rate at the nominal level but suffered from reduced power and convergence issues in some settings when the number of centers was small. Random intercept models generally performed well in most scenarios, except with a low event rate (i.e., 2% scenario) and small total sample size (n ? 500), when all methods had issues. Random intercept models generally performed best across most scenarios. GEE-small sample correction performed well when the number of centers was large. We do not recommend the use of Mantel-Haenszel, GEE, or models that do not account for center. When the expected event rate is low, we suggest that the statistical analysis plan specify an alternative method in the case of non-convergence of the primary method.-Analysis of multicenter clinical trials with very low event rates.",0
Statistical analysis and optimal design for cluster randomized trials,1
"Clinical trials often assess efficacy by comparing treatments on the basis of two or more event-time outcomes. In the case of cancer clinical trials, progression-free survival (PFS), which is the minimum of the time from randomization to progression or to death, summarizes the comparison of treatments on the hazards for disease progression and mortality. However, the analysis of PFS does not utilize all the information we have on patients in the trial. First, if both progression and death times are recorded, then information on death time is ignored in the PFS analysis. Second, disease progression is monitored at regular clinic visits, and progression time is recorded as the first visit at which evidence of progression is detected. However, many patients miss or have irregular visits (resulting in interval-censored data) and sometimes die of the cancer before progression was recorded. In this case, the previous progression-free time could provide additional information on the treatment efficacy. The aim of this paper is to propose a method for comparing treatments that could more fully utilize the data on progression and death. We develop a test for treatment effect based on of the joint distribution of progression and survival. The issue of interval censoring is handled using the very simple and intuitive approach of the Conditional Expected Score Test (CEST). We focus on the application of these methods in cancer research.-A joint test for progression and survival with interval-censored data from a cancer clinical trial.",0
"Most phase I clinical trials are designed to determine a maximum-tolerated dose (MTD) for one initial administration or treatment course of a cytotoxic experimental agent. Toxicity usually is defined as the indicator of whether one or more particular adverse events occur within a short time period from the start of therapy. However, physicians often administer an agent to the patient repeatedly and monitor long-term toxicity due to cumulative effects. We propose a new method for such settings. It is based on the time to toxicity rather than a binary outcome, and the goal is to determine a maximum-tolerated schedule (MTS) rather than a conventional MTD. The model and method account for a patient's entire sequence of administrations, with the overall hazard of toxicity modeled as the sum of a sequence of hazards, each associated with one administration. Data monitoring and decision making are done continuously throughout the trial. We illustrate the method with an allogeneic bone marrow transplantation (BMT) trial to determine how long a recombinant human growth factor can be administered as prophylaxis for acute graft-versus-host disease (aGVHD), and we present a simulation study in the context of this trial.-Determining a maximum-tolerated schedule of a cytotoxic agent.",0
"Observational studies may provide suggestive evidence for the results of behavior change and lifestyle modification, but they do not replace randomized trials for comparing interventions. To obtain a valid comparison of competing intervention strategies, randomized trials of adequate size are the recommended approach. Randomization avoids bias, achieves balance (on average) of both known and unknown predictive factors between intervention and comparison groups, and provides the basis of statistical tests. The value of randomization is as relevant when investigating community interventions as it is for studies that are directed at individuals. Randomization by group is less efficient statistically than randomization by individual, but there are reasons why randomization by group (such as community) may be chosen, including feasibility of delivery of the intervention, political and administrative considerations, avoiding contamination between individuals allocated to competing interventions, and the very nature of the intervention. One example is the Community Intervention Trial for Smoking Cessation (COMMIT), which involved 11 matched pairs of communities and randomized within these pairs to active community-level intervention versus comparison. For analysis of results, community-level permutation tests (and corresponding test-based confidence intervals) can be designed based on the randomization distribution. The advantages of this approach are that it is robust, and the unit of randomization is the unit of analysis, yet it can incorporate individual-level covariates. Such covariates can play a role in imputation for missing values, adjustment for imbalances, and separate analyses in demographic subsets (with appropriate tests for interaction). A community-randomized trial can investigate a multichannel community-based approach to lifestyle modification, thus providing generalizability coupled with a rigorous evaluation of the intervention.-The advantages of community-randomized trials for evaluating lifestyle modification.",1
"To examine the validity of deterministic compared to probabilistic record linkage in the ascertainment of hospitalizations in two linked cohorts. HIV-negative (HIV-ve) (n?=?1,325) and HIV-positive (HIV+ve) gay and bisexual men (n?=?557) recruited in Sydney, Australia, were probabilistically and deterministically linked to a statewide hospital registry (July 2000-June 2012). Using probabilistic linkage as the reference standard, deterministic linkage had higher specificity but much lower sensitivity [34.67% (95% confidence interval: 33.44, 35.92)]. A disproportionate number of links missed were individuals with poorer socioeconomic and health indicators, including HIV status. Risk of hospitalization compared to the general male population [HIV+ve standardized incidence ratio (SIR)?=?1.45 (1.33-1.59); HIV-ve SIR?=?0.72 (0.67-0.78)] was significantly underestimated when deterministic linkage was used [HIV+ve SIR?=?0.46 (0.37-0.58); HIV-ve SIR?=?0.29 (0.24-0.35)]. The impact of linkage strategy on the calculation of incidence rate ratios (IRRs) was less, but a greater discrepancy in IRRs was seen for diagnostic categories where event rates were low or where the sensitivity of the deterministic linkage was differential between the two cohorts. Linkage without proven high sensitivity and specificity should be carefully considered. In circumstances of undetermined sensitivity, SIRs should not be calculated as the extent of underestimation is unknown. The comparison of linked events within or between cohorts is more robust to linkage misclassification; however, selection bias does affect estimates and should be considered before linkage.-Poor record linkage sensitivity biased outcomes in a linked cohort analysis.",0
"Inference for randomized clinical trials is generally based on the assumption that outcomes are independently and identically distributed under the null hypothesis. In some trials, particularly in infectious disease, outcomes may be correlated. This may be known in advance (e.g. allowing randomization of family members) or completely unplanned (e.g. sexual sharing among randomized participants). There is particular concern when the form of the correlation is essentially unknown, in which case we cannot take advantage of the correlation to construct a more efficient test. Instead, we can only investigate the impact of potential correlation on the independent-samples test statistic. Randomization tends to balance out treatment and control assignments within clusters, so it is logical that performance of tests averaged over all possible randomization assignments would be essentially unaffected by arbitrary correlation. We confirm this intuition by showing that a permutation test controls the type 1 error rate in a certain average sense whenever the clustering is independent of treatment assignment. It is nonetheless possible to obtain a 'bad' randomization such that members of a cluster tend to be assigned to the same treatment. Conditioned on such a bad randomization, the type 1 error rate is increased.-Cluster without fluster: The effect of correlated outcomes on inference in randomized clinical trials.",1
"In longitudinal studies, matched designs are often employed to control the potential confounding effects in the field of biomedical research and public health. Because of clinical interest, recurrent time-to-event data are captured during the follow-up. Meanwhile, the terminal event of death is always encountered, which should be taken into account for valid inference because of informative censoring. In some scenarios, a certain large portion of subjects may not have any recurrent events during the study period due to nonsusceptibility to events or censoring; thus, the zero-inflated nature of data should be considered in analysis. In this paper, a joint frailty model with recurrent events and death is proposed to adjust for zero inflation and matched designs. We incorporate 2 frailties to measure the dependency between subjects within a matched pair and that among recurrent events within each individual. By sharing the random effects, 2 event processes of recurrent events and death are dependent with each other. The maximum likelihood based approach is applied for parameter estimation, where the Monte Carlo expectation-maximization algorithm is adopted, and the corresponding R program is developed and available for public usage. In addition, alternative estimation methods such as Gaussian quadrature (PROC NLMIXED) and a Bayesian approach (PROC MCMC) are also considered for comparison to show our method's superiority. Extensive simulations are conducted, and a real data application on acute ischemic studies is provided in the end.-Joint modeling of recurrent events and a terminal event adjusted for zero inflation and a matched design.",0
"Power and sample size calculations for cluster randomized trials require prediction of the degree of correlation that will be realized among outcomes of participants in the same cluster. This correlation is typically quantified as the intraclass correlation coefficient (ICC), defined as the Pearson correlation between two members of the same cluster or proportion of the total variance attributable to variance between clusters. It is widely known but perhaps not fully appreciated that for binary outcomes, the ICC is a function of outcome prevalence. Hence, the ICC and the outcome prevalence are intrinsically related, making the ICC poorly generalizable across study conditions and between studies with different outcome prevalences. We use a simple parametrization of the ICC that aims to isolate that part of the ICC that measures dependence among responses within a cluster from the outcome prevalence. We incorporate this parametrization into sample size calculations for cluster randomized trials and compare our method to the traditional approach using the ICC. Our dependence parameter, R, may be less influenced by outcome prevalence and has an intuitive meaning that facilitates interpretation. Estimates of R from previous studies can be obtained using simple statistics. Comparison of methods showed that the traditional ICC approach to sample size determination tends to overpower studies under many scenarios, calling for more clusters than truly required. The methods are developed for equal-sized clusters, whereas cluster size may vary in practice. The dependence parameter R is an alternative measure of dependence among binary outcomes in cluster randomized trials that has a number of advantages over the ICC.-A new dependence parameter approach to improve the design of cluster randomized trials with binary outcomes.",1
"Trauma is the leading cause of morbidity and mortality in children in the United States. The antifibrinolytic drug tranexamic acid (TXA) improves survival in adults with traumatic hemorrhage, however, the drug has not been evaluated in a clinical trial in severely injured children. We designed the Traumatic Injury Clinical Trial Evaluating Tranexamic Acid in Children (TIC-TOC) trial to evaluate the feasibility of conducting a confirmatory clinical trial that evaluates the effects of TXA in children with severe trauma and hemorrhagic injuries. Children with severe trauma and evidence of hemorrhagic torso or brain injuries will be randomized to one of three arms: (1) TXA dose A (15?mg/kg bolus dose over 20?min, followed by 2?mg/kg/hr infusion over 8?h), (2) TXA dose B (30?mg/kg bolus dose over 20?min, followed by 4?mg/kg/hr infusion over 8?h), or (3) placebo. We will use permuted-block randomization by injury type: hemorrhagic brain injury, hemorrhagic torso injury, and combined hemorrhagic brain and torso injury. The trial will be conducted at four pediatric Level I trauma centers. We will collect the following outcome measures: global functioning as measured by the Pediatric Quality of Life (PedsQL) and Pediatric Glasgow Outcome Scale Extended (GOS-E Peds), working memory (digit span test), total amount of blood products transfused in the initial 48?h, intracranial hemorrhage progression at 24?h, coagulation biomarkers, and adverse events (specifically thromboembolic events and seizures). This multicenter trial will provide important preliminary data and assess the feasibility of conducting a confirmatory clinical trial that evaluates the benefits of TXA in children with severe trauma and hemorrhagic injuries to the torso and/or brain. ClinicalTrials.gov registration number: NCT02840097 . Registered on 14 July 2016.-Traumatic injury clinical trial evaluating tranexamic acid in children (TIC-TOC): study protocol for a pilot randomized controlled trial.",0
"Observational studies can be used to evaluate treatment effectiveness among patients with a broader range of illness severity than typically seen in randomized controlled clinical trials. However, there are several difficulties with observational evaluations including non-equivalent comparison groups, treatment doses and durations that vary widely, and, in longitudinal studies, multiple courses of treatment per subject. A mixed-effects approach to the propensity adjustment for non-equivalent comparison groups is described that can account for each of these perturbations. The strategy involves two stages. First, characteristics that distinguish among subjects who receive various levels of treatment are examined in a model of propensity for treatment intensity using mixed-effects ordinal logistic regression. Second, the propensity-stratified effectiveness of ordered categorical doses is compared in a mixed-effects grouped time survival model of time until recovery. The model is applied in a longitudinal, observational study of antidepressant effectiveness. Then a Monte Carlo simulation study indicates that the strategy has acceptable type I error rates and minimal bias in the estimates of treatment effectiveness. Statistical power exceeds 0.90 for an odds ratio of 1.5 with N = 250 and 500, and is acceptable for an odds ratio of 2.0 with N = 100. Nevertheless, with N = 100, the models that had high intraclass correlation coefficients had greater tendency towards non-convergence. This approach is a useful strategy for observational studies of treatment effectiveness. It is capable of adjusting for selection bias, incorporating multiple observations per subject, and comparing effectiveness of ordinal doses.-A mixed-effects quintile-stratified propensity adjustment for effectiveness analyses of ordered categorical doses.",0
Hospital length of stay (LOS) is an important measure of healthcare utilization and is generally positively skewed and heterogeneous. We fit a Coxian phase-type distribution to LOS and identify the hidden states of the underlying latent homogeneous Markov model. We demonstrate that selecting an appropriate number of phases and a regression model for hazard rates can account for some heterogeneity in LOS. Reversible jump MCMC method enables us to dynamically uncover the hidden stochastic Markov structure. A classification method is used to assign patients to different LOS groups. The methodology is illustrated with application to hospital admissions for acute myocardial infarction in the 2003 Nationwide Inpatient Sample from the Healthcare Utilization Project.-Modeling hospital length of stay by Coxian phase-type regression with heterogeneity.,0
"Familiarity with experimental design and statistical analysis techniques is necessary for investigators to conduct state-of-the-art research in clinical medicine. A wide range of options for biostatistical training is available. These include formal academic courses for credit, one-on-one education offered under the guise of consultation, self-paced textbook or computer learning, intensive short courses, portions of extended courses spanning an assortment of other subjects, and a continuing series of seminars or lectures covering selected topics in statistics. The latter structure is described in this article. We offered an eight-week lecture series focusing on a new topic in biostatistics each week. The course targeted clinical faculty, postdoctoral fellows, residents, and other interested research staff at a large medical research institution. The series focused on interpretation of medical journal articles with regard to statistical issues. Another objective was to provide information to increase a participant's expertise in the design of research studies. Guidance was offered for effective communication with collaborating biostatisticians. The format and contents of the seminar series are described. Aspects of the course that proved to be successful are discussed.-A seminar series in applied biostatistics for clinical research fellows, faculty and staff.",0
Estimators for Clustered Education RCTs Using the Neyman Model for Causal Inference,1
