{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326700ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:07:15.517645: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-30 10:07:16.188139: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-30 10:07:16.188176: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-30 10:07:16.293789: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-30 10:07:17.693705: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-30 10:07:17.693899: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-30 10:07:17.693918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/elahehaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             classification_report, confusion_matrix)\n",
    "import transformers\n",
    "from transformers import (AutoTokenizer , AutoModelForSequenceClassification, TrainingArguments, \n",
    "                          Trainer, pipeline, DataCollatorWithPadding, AutoModelForSeq2SeqLM, \n",
    "                          EarlyStoppingCallback, IntervalStrategy)\n",
    "from datasets import load_dataset, Dataset, load_metric\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import wandb\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import collections\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c17684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 11% | 13% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 24% | 14% |\n"
     ]
    }
   ],
   "source": [
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38c2093",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "def load_data(train_file_path, valid_file_path):\n",
    "    if train_file_path and valid_file_path:\n",
    "        dataset = load_dataset(\"csv\", data_files = {'train': train_file_path, 'validation': valid_file_path})\n",
    "    elif not valid_file_path:\n",
    "        dataset = load_dataset(\"csv\", data_files = {'train': train_file_path})\n",
    "    return dataset\n",
    "model_checkpoint = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
    "def tokenize_data(example):\n",
    "    return tokenizer(preprocess(example['text']),  padding = 'max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "#training and validation - 60/40\n",
    "#dataset = load_data(['RT_train.csv'],  ['RT_test.csv'])\n",
    "#train_tokenized = dataset['train'].map(tokenize_data, batched = True) \n",
    "#valid_tokenized = dataset['validation'].map(tokenize_data, batched = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35908511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def model_init():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=4,\n",
    "        #id2label={index: label for index, label in enumerate(labels.names)},\n",
    "        #label2id={label: index for index, label in enumerate(labels.names)}\n",
    "    )\n",
    "    return model\n",
    "model = model_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a285d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "}\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "parameters_dict = {\n",
    "    'epochs': {\n",
    "        'values': [1,2,4]\n",
    "        },\n",
    "    'batch_size': {\n",
    "        'values': [4, 8, 16]\n",
    "        },\n",
    "    'learning_rate': {\n",
    "        'distribution': 'log_uniform_values',\n",
    "        'min': 2e-7,\n",
    "        'max': 2e-4\n",
    "    },\n",
    "    'weight_decay': {\n",
    "        'values': [0.0, 0.01, 0.005, 0.001]\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56040335",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='ft_all+PK_PubMedBERT3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc2d78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize/process title and abstract separately\n",
    "def customized_data_collator(examples):\n",
    "    titles = [example['Title'] for example in examples]\n",
    "    abstracts = [example['Abstract'] for example in examples]\n",
    "    \n",
    "    tokenized_title = tokenizer(titles , padding = 'max_length', truncation=True, max_length=30)\n",
    "    tokenized_abstract = tokenizer(abstracts, padding = 'max_length', truncation=True, max_length=256)\n",
    "    \n",
    "    #inputs = {\n",
    "        #'input_ids' : torch.cat([tokenized_title['input_ids'] , tokenized_abstract['input_ids']], dim =1),\n",
    "        #'attention_mask': torch.cat([tokenized_title['attention_mask'] , tokenized_abstract['attention_mask']], dim =1),\n",
    "        #'token_type_ids' : torch.cat([tokenized_title['token_type_ids'] , tokenized_abstract['token_type_ids']], dim =1)\n",
    "    #}\n",
    "    \n",
    "    inputs = {\n",
    "        'input_ids' : [a+b for a,b in zip(tokenized_title['input_ids'],tokenized_abstract['input_ids'])],\n",
    "        'attention_mask': [a+b for a,b in zip(tokenized_title['attention_mask'],tokenized_abstract['attention_mask'])],\n",
    "        'token_type_ids' : [a+b for a,b in zip(tokenized_title['token_type_ids'],tokenized_abstract['token_type_ids'])]\n",
    "    }\n",
    "    \n",
    "    label = [example['labels'] for example in examples]\n",
    "    batch = {\n",
    "        'input_ids': torch.tensor(inputs['input_ids']),\n",
    "        'attention_mask': torch.tensor(inputs['attention_mask']),\n",
    "        'token_type_ids': torch.tensor(inputs['token_type_ids']),\n",
    "        'labels': torch.tensor(label)\n",
    "    }\n",
    "    \n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "233311f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to compute metrics\n",
    "def compute_metrics_fn(eval_preds):\n",
    "    \n",
    "    res = dict()\n",
    "\n",
    "    accuracy_metric = load_metric('accuracy')\n",
    "    precision_metric = load_metric('precision')\n",
    "    recall_metric = load_metric('recall')\n",
    "    f1_metric = load_metric('f1')\n",
    "    \n",
    "    logits = eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "    preds = np.argmax(logits, axis=-1)  \n",
    "    \n",
    "    report = classification_report(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    print(report)\n",
    "    print('confusion matrix: ', cm)\n",
    "    \n",
    "    res.update(accuracy_metric.compute(predictions=preds, references=labels))\n",
    "    res.update(precision_metric.compute(predictions=preds, references=labels, average='macro'))\n",
    "    res.update(recall_metric.compute(predictions=preds, references=labels, average='macro'))\n",
    "    res.update(f1_metric.compute(predictions=preds, references=labels, average='macro'))\n",
    "    \n",
    "    return res \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e96b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiments using 60/40 - hyperparam tuning\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='ft_all+PK_PubMedBERT3',\n",
    "            report_to='wandb', \n",
    "            num_train_epochs=config.epochs,\n",
    "            learning_rate=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            per_device_train_batch_size=config.batch_size,\n",
    "            per_device_eval_batch_size=16,\n",
    "            save_strategy='epoch',\n",
    "            evaluation_strategy='epoch',\n",
    "            logging_strategy='epoch',\n",
    "            load_best_model_at_end=True,\n",
    "            remove_unused_columns=False,\n",
    "            fp16=True,\n",
    "            save_total_limit = 1,\n",
    "            run_name= 'ft_all+PK_PubMedBERT3',\n",
    "        )\n",
    "        \n",
    "\n",
    "        trainer = Trainer(\n",
    "            model_init=model_init,\n",
    "            args=training_args,\n",
    "            train_dataset=train_tokenized,\n",
    "            eval_dataset=valid_tokenized,\n",
    "            compute_metrics=compute_metrics_fn\n",
    "        )\n",
    "\n",
    "\n",
    "        trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df3fcbba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train final model with all data - test with early stopping , return to normal if did not work, add class weights\n",
    "torch.manual_seed(42)\n",
    "class CustomTrainer(Trainer):\n",
    "    def calculate_class_weights(training_set):\n",
    "        print(\"hi\")\n",
    "        labels = [set(training_set)]\n",
    "        class_distribution = [0]*len(labels)\n",
    "        for i in labels:\n",
    "            class_distribution[i] = training_set.count(i)\n",
    "        weights = []\n",
    "        class_distribution = np.array(class_distribution)\n",
    "        num_classes = len(labels)\n",
    "        weight = np.sum(class_distribution)/(num_classes * class_distribution)\n",
    "        return weight\n",
    "    def compute_custom_loss(model, inputs, return_outputs=False):\n",
    "        target = inputs.get('labels')\n",
    "        weights = calculate_class_weights(target)\n",
    "        print(weights)\n",
    "        print(\"hi2\")\n",
    "        ce_loss = nn.CrossEntropyLoss(weight=torch.tensor(weights))\n",
    "        outputs = model(*inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        probs = nn.functional.softmax(logits, dim = -1)\n",
    "        \n",
    "        loss = ce_loss(probs.view(-1, model.config.num_labels), target.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "def train(train_set, validation_set, learning_rate, weight_decay, run_name, epochs, batch_size):\n",
    "    mlflow.start_run()\n",
    "\n",
    "    # Log training parameters\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", epochs)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= run_name,\n",
    "        num_train_epochs= epochs,\n",
    "        learning_rate= learning_rate,\n",
    "        weight_decay= weight_decay,\n",
    "        per_device_train_batch_size= batch_size,\n",
    "        save_strategy= 'epoch' ,#IntervalStrategy.STEPS,\n",
    "        logging_strategy= 'epoch', #IntervalStrategy.STEPS,\n",
    "        remove_unused_columns=False,\n",
    "        save_total_limit = 1,\n",
    "        run_name= run_name,\n",
    "        fp16 = True,\n",
    "        load_best_model_at_end=True,\n",
    "        evaluation_strategy = 'epoch', #IntervalStrategy.STEPS,\n",
    "        eval_steps = 1,\n",
    "        metric_for_best_model = 'eval_f1',\n",
    "        seed = 42\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model_init= model_init,\n",
    "        args= training_args,\n",
    "        #train_tokenized for joined title and abstract, dataset['train'] for seperate title and abstract\n",
    "        train_dataset= train_set,\n",
    "        #use to create seperate title and abstract\n",
    "        data_collator = customized_data_collator,\n",
    "        compute_metrics= compute_metrics_fn,\n",
    "        #valid_tokenized for joined title and abstract, dataset['validation'] for seperate title and abstract\n",
    "        eval_dataset = validation_set,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        \n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    mlflow.end_run()\n",
    "\n",
    "    return eval_result\n",
    "\n",
    "\n",
    "#final model training with all data\n",
    "#train(1.098e-4, 0.01, \"final_PubMedBERT_longer_longerepoch\", 5, 8)\n",
    "#final model training with all data + 50% coded 2021 data, joined abstract and title\n",
    "#train(1.098e-4, 0.01, \"final_PubMedBERT_2021\",4, 8)\n",
    "#final model training with all data + 50% coded 2021 data , separete title and abstract\n",
    "#train(train_set, test_set, 1.098e-4, 0.01, \"final_PubMedBERT_2021_separateInput\", 5, 8)\n",
    "\n",
    "#final model training with all data + 50% coded 2021 data , separate title and abstract\n",
    "#final = train(train_set, test_set, 1.098e-4, 0.01, \"final_PubMedBERT_2021_separateInput_v3\", 6, 8)\n",
    "#final = train(train_set, test_set, 1.098e-4, 0.01, \"final_PubMedBERT_2021_separateInput_test2\", 20, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ab50992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratified sampling to make sure enough sample from each class exists in the classification\n",
    "mlflow.end_run()\n",
    "# Set the experiment path\n",
    "experiment_path = \"PubClassifier\"\n",
    "\n",
    "# Set the experiment\n",
    "mlflow.set_experiment(experiment_path)\n",
    "#stratified k_fold cross validation for imbalanced dataset\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#heldout_set\n",
    "k_fold , test = next(skf.split(dataset['train'], dataset['train']['labels']))\n",
    "#kfold training and validation\n",
    "eval_result = []\n",
    "i = 0\n",
    "for train_idx , valid_idx in skf.split(dataset['train'][k_fold]['Title'], dataset['train'][k_fold]['labels']):\n",
    "    train_set = [dataset['train'][int(i)] for i in train_idx]\n",
    "    validation_set = [dataset['train'][int(i)] for i in valid_idx]\n",
    "    eval_result.append(train(train_set, validation_set, 3.098e-5, 0.001, \"PubMedBERT_2021_lr35\"+str(i), 10, 16))\n",
    "    i += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e98e8dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28899/2999843985.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#final model training with all data + 50% coded 2021 data , separate title and abstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.098e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"final_PubMedBERT_2021_separateInput_test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "#adding class weights to imptove IRGT class performance\n",
    "class CustomTrainer(Trainer):\n",
    "    def calculate_class_weights(training_set):\n",
    "        print(\"hi\")\n",
    "        labels = [set(training_set)]\n",
    "        class_distribution = [0]*len(labels)\n",
    "        for i in labels:\n",
    "            class_distribution[i] = training_set.count(i)\n",
    "        weights = []\n",
    "        class_distribution = np.array(class_distribution)\n",
    "        num_classes = len(labels)\n",
    "        weight = np.sum(class_distribution)/(num_classes * class_distribution)\n",
    "        return weight\n",
    "    def compute_custom_loss(model, inputs, return_outputs=False):\n",
    "        target = inputs.get('labels')\n",
    "        weights = calculate_class_weights(target)\n",
    "        print(weights)\n",
    "        print(\"hi2\")\n",
    "        ce_loss = nn.CrossEntropyLoss(weight=torch.tensor(weights))\n",
    "        outputs = model(*inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        probs = nn.functional.softmax(logits, dim = -1)\n",
    "        \n",
    "        loss = ce_loss(probs.view(-1, model.config.num_labels), target.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "def train(train_set, validation_set, learning_rate, weight_decay, run_name, epochs, batch_size):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= run_name,\n",
    "        num_train_epochs= epochs,\n",
    "        learning_rate= learning_rate,\n",
    "        weight_decay= weight_decay,\n",
    "        per_device_train_batch_size= batch_size,\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy='epoch',\n",
    "        remove_unused_columns=False,\n",
    "        save_total_limit = 1,\n",
    "        run_name= run_name,\n",
    "        fp16 = True\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model_init= model_init,\n",
    "        args= training_args,\n",
    "        #train_tokenized for joined title and abstract, dataset['train'] for seperate title and abstract\n",
    "        train_dataset= train_set,\n",
    "        #use to create seperate title and abstract\n",
    "        data_collator = customized_data_collator,\n",
    "        compute_metrics= compute_metrics_fn,\n",
    "        #valid_tokenized for joined title and abstract, dataset['validation'] for seperate title and abstract\n",
    "        eval_dataset = validation_set,\n",
    "        \n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    return eval_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14c7ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratified train and test set\n",
    "test_set = [dataset['train'][int(i)] for i in test]\n",
    "train_set = [dataset['train'][int(i)] for i in k_fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "417f1643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/resolve/main/config.json from cache at /home/elahehaa/.cache/huggingface/transformers/76e7b0967140f134278c3209cffe98f69eb013b9de505a434b3359c057aedaa3.2411d0fafcf181e9b95d9cb7972d93b27c57a2cb75819924f8fc7ec848b708f2\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/resolve/main/pytorch_model.bin from cache at /home/elahehaa/.cache/huggingface/transformers/41964abe9a7c4ccbc21929b6d256f1b79a5a39566e329b380fae9d0bf622f7b7.ca2d7d719ab41e712011b6e8381af6b4be73841c1e580600c522386c5ed9b6f0\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "loading configuration file https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/resolve/main/config.json from cache at /home/elahehaa/.cache/huggingface/transformers/76e7b0967140f134278c3209cffe98f69eb013b9de505a434b3359c057aedaa3.2411d0fafcf181e9b95d9cb7972d93b27c57a2cb75819924f8fc7ec848b708f2\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/resolve/main/pytorch_model.bin from cache at /home/elahehaa/.cache/huggingface/transformers/41964abe9a7c4ccbc21929b6d256f1b79a5a39566e329b380fae9d0bf622f7b7.ca2d7d719ab41e712011b6e8381af6b4be73841c1e580600c522386c5ed9b6f0\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/elahehaa/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2925\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 732\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='732' max='732' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [732/732 12:09, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.448500</td>\n",
       "      <td>0.257617</td>\n",
       "      <td>0.905738</td>\n",
       "      <td>0.686920</td>\n",
       "      <td>0.709270</td>\n",
       "      <td>0.696922</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.180900</td>\n",
       "      <td>0.283669</td>\n",
       "      <td>0.919399</td>\n",
       "      <td>0.833679</td>\n",
       "      <td>0.856207</td>\n",
       "      <td>0.844052</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>0.311875</td>\n",
       "      <td>0.938525</td>\n",
       "      <td>0.883923</td>\n",
       "      <td>0.874187</td>\n",
       "      <td>0.878921</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.313196</td>\n",
       "      <td>0.935792</td>\n",
       "      <td>0.889207</td>\n",
       "      <td>0.875452</td>\n",
       "      <td>0.881781</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 732\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       439\n",
      "           1       0.82      0.95      0.88       238\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.97      0.97      0.97        35\n",
      "\n",
      "    accuracy                           0.91       732\n",
      "   macro avg       0.69      0.71      0.70       732\n",
      "weighted avg       0.89      0.91      0.89       732\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elahehaa/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/elahehaa/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/elahehaa/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/elahehaa/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to PubMedBERT_2021_test2/checkpoint-183\n",
      "Configuration saved in PubMedBERT_2021_test2/checkpoint-183/config.json\n",
      "Model weights saved in PubMedBERT_2021_test2/checkpoint-183/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 732\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       439\n",
      "           1       0.88      0.93      0.91       238\n",
      "           2       0.52      0.60      0.56        20\n",
      "           3       0.97      0.97      0.97        35\n",
      "\n",
      "    accuracy                           0.92       732\n",
      "   macro avg       0.83      0.86      0.84       732\n",
      "weighted avg       0.92      0.92      0.92       732\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to PubMedBERT_2021_test2/checkpoint-366\n",
      "Configuration saved in PubMedBERT_2021_test2/checkpoint-366/config.json\n",
      "Model weights saved in PubMedBERT_2021_test2/checkpoint-366/pytorch_model.bin\n",
      "Deleting older checkpoint [PubMedBERT_2021_test2/checkpoint-183] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 732\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       439\n",
      "           1       0.93      0.91      0.92       238\n",
      "           2       0.68      0.65      0.67        20\n",
      "           3       0.97      0.97      0.97        35\n",
      "\n",
      "    accuracy                           0.94       732\n",
      "   macro avg       0.88      0.87      0.88       732\n",
      "weighted avg       0.94      0.94      0.94       732\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to PubMedBERT_2021_test2/checkpoint-549\n",
      "Configuration saved in PubMedBERT_2021_test2/checkpoint-549/config.json\n",
      "Model weights saved in PubMedBERT_2021_test2/checkpoint-549/pytorch_model.bin\n",
      "Deleting older checkpoint [PubMedBERT_2021_test2/checkpoint-366] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 732\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       439\n",
      "           1       0.90      0.93      0.92       238\n",
      "           2       0.72      0.65      0.68        20\n",
      "           3       0.97      0.97      0.97        35\n",
      "\n",
      "    accuracy                           0.94       732\n",
      "   macro avg       0.89      0.88      0.88       732\n",
      "weighted avg       0.94      0.94      0.94       732\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to PubMedBERT_2021_test2/checkpoint-732\n",
      "Configuration saved in PubMedBERT_2021_test2/checkpoint-732/config.json\n",
      "Model weights saved in PubMedBERT_2021_test2/checkpoint-732/pytorch_model.bin\n",
      "Deleting older checkpoint [PubMedBERT_2021_test2/checkpoint-549] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from PubMedBERT_2021_test2/checkpoint-732 (score: 0.8817807275030399).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 732\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [92/92 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       439\n",
      "           1       0.90      0.93      0.92       238\n",
      "           2       0.72      0.65      0.68        20\n",
      "           3       0.97      0.97      0.97        35\n",
      "\n",
      "    accuracy                           0.94       732\n",
      "   macro avg       0.89      0.88      0.88       732\n",
      "weighted avg       0.94      0.94      0.94       732\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.31319618225097656,\n",
       " 'eval_accuracy': 0.9357923497267759,\n",
       " 'eval_precision': 0.8892072120160331,\n",
       " 'eval_recall': 0.8754524702819625,\n",
       " 'eval_f1': 0.8817807275030399,\n",
       " 'eval_0': 0.575,\n",
       " 'eval_1': 0.4,\n",
       " 'eval_2': 0.5833333333333334,\n",
       " 'eval_3': 0.5,\n",
       " 'eval_runtime': 14.6419,\n",
       " 'eval_samples_per_second': 49.993,\n",
       " 'eval_steps_per_second': 6.283,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate candidate model on test set\n",
    "train(train_set, test_set, 3.098e-5, 0.001, \"candidate_PubMedBERT_2021_test2\", 4, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "142b028b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/resolve/main/config.json from cache at /home/elahehaa/.cache/huggingface/transformers/76e7b0967140f134278c3209cffe98f69eb013b9de505a434b3359c057aedaa3.2411d0fafcf181e9b95d9cb7972d93b27c57a2cb75819924f8fc7ec848b708f2\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/resolve/main/pytorch_model.bin from cache at /home/elahehaa/.cache/huggingface/transformers/41964abe9a7c4ccbc21929b6d256f1b79a5a39566e329b380fae9d0bf622f7b7.ca2d7d719ab41e712011b6e8381af6b4be73841c1e580600c522386c5ed9b6f0\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "loading configuration file https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/resolve/main/config.json from cache at /home/elahehaa/.cache/huggingface/transformers/76e7b0967140f134278c3209cffe98f69eb013b9de505a434b3359c057aedaa3.2411d0fafcf181e9b95d9cb7972d93b27c57a2cb75819924f8fc7ec848b708f2\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/resolve/main/pytorch_model.bin from cache at /home/elahehaa/.cache/huggingface/transformers/41964abe9a7c4ccbc21929b6d256f1b79a5a39566e329b380fae9d0bf622f7b7.ca2d7d719ab41e712011b6e8381af6b4be73841c1e580600c522386c5ed9b6f0\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/elahehaa/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3657\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 916\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melaheh-aghaarabi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/elahehaa/my_project_dir/wandb/run-20230527_141011-3m509wo5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/elaheh-aghaarabi/huggingface/runs/3m509wo5\" target=\"_blank\">candidate_PubMedBERT_2021_5/27/23</a></strong> to <a href=\"https://wandb.ai/elaheh-aghaarabi/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='916' max='916' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [916/916 14:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>0.169700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>0.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>916</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to candidate_PubMedBERT_2021_5/27/23/checkpoint-229\n",
      "Configuration saved in candidate_PubMedBERT_2021_5/27/23/checkpoint-229/config.json\n",
      "Model weights saved in candidate_PubMedBERT_2021_5/27/23/checkpoint-229/pytorch_model.bin\n",
      "Saving model checkpoint to candidate_PubMedBERT_2021_5/27/23/checkpoint-458\n",
      "Configuration saved in candidate_PubMedBERT_2021_5/27/23/checkpoint-458/config.json\n",
      "Model weights saved in candidate_PubMedBERT_2021_5/27/23/checkpoint-458/pytorch_model.bin\n",
      "Deleting older checkpoint [candidate_PubMedBERT_2021_5/27/23/checkpoint-229] due to args.save_total_limit\n",
      "Saving model checkpoint to candidate_PubMedBERT_2021_5/27/23/checkpoint-687\n",
      "Configuration saved in candidate_PubMedBERT_2021_5/27/23/checkpoint-687/config.json\n",
      "Model weights saved in candidate_PubMedBERT_2021_5/27/23/checkpoint-687/pytorch_model.bin\n",
      "Deleting older checkpoint [candidate_PubMedBERT_2021_5/27/23/checkpoint-458] due to args.save_total_limit\n",
      "Saving model checkpoint to candidate_PubMedBERT_2021_5/27/23/checkpoint-916\n",
      "Configuration saved in candidate_PubMedBERT_2021_5/27/23/checkpoint-916/config.json\n",
      "Model weights saved in candidate_PubMedBERT_2021_5/27/23/checkpoint-916/pytorch_model.bin\n",
      "Deleting older checkpoint [candidate_PubMedBERT_2021_5/27/23/checkpoint-687] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#deploy\n",
    "torch.manual_seed(42)\n",
    "class CustomTrainer(Trainer):\n",
    "    def calculate_class_weights(training_set):\n",
    "        labels = [set(training_set)]\n",
    "        class_distribution = [0]*len(labels)\n",
    "        for i in labels:\n",
    "            class_distribution[i] = training_set.count(i)\n",
    "        weights = []\n",
    "        class_distribution = np.array(class_distribution)\n",
    "        num_classes = len(labels)\n",
    "        weight = np.sum(class_distribution)/(num_classes * class_distribution)\n",
    "        return weight\n",
    "    def compute_custom_loss(model, inputs, return_outputs=False):\n",
    "        target = inputs.get('labels')\n",
    "        weights = calculate_class_weights(target)\n",
    "        ce_loss = nn.CrossEntropyLoss(weight=torch.tensor(weights))\n",
    "        outputs = model(*inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        probs = nn.functional.softmax(logits, dim = -1)\n",
    "        \n",
    "        loss = ce_loss(probs.view(-1, model.config.num_labels), target.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "def train(train_set, learning_rate, weight_decay, run_name, epochs, batch_size):\n",
    "    mlflow.start_run()\n",
    "\n",
    "    # Log training parameters\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", epochs)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= run_name,\n",
    "        num_train_epochs= epochs,\n",
    "        learning_rate= learning_rate,\n",
    "        weight_decay= weight_decay,\n",
    "        per_device_train_batch_size= batch_size,\n",
    "        save_strategy= 'epoch' ,#IntervalStrategy.STEPS,\n",
    "        logging_strategy= 'epoch', #IntervalStrategy.STEPS,\n",
    "        remove_unused_columns=False,\n",
    "        save_total_limit = 1,\n",
    "        run_name= run_name,\n",
    "        fp16 = True,\n",
    "        #load_best_model_at_end=True,\n",
    "        #evaluation_strategy = 'epoch', #IntervalStrategy.STEPS,\n",
    "        metric_for_best_model = 'eval_f1',\n",
    "        seed = 42\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model_init= model_init,\n",
    "        args= training_args,\n",
    "        #train_tokenized for joined title and abstract, dataset['train'] for seperate title and abstract\n",
    "        train_dataset= train_set,\n",
    "        #use to create seperate title and abstract\n",
    "        data_collator = customized_data_collator,\n",
    "        compute_metrics= compute_metrics_fn,\n",
    "        #valid_tokenized for joined title and abstract, dataset['validation'] for seperate title and abstract\n",
    "        \n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    mlflow.end_run()\n",
    "\n",
    "mlflow.end_run()\n",
    "# Set the experiment path\n",
    "experiment_path = \"PubClassifier\"\n",
    "# Set the experiment\n",
    "mlflow.set_experiment(experiment_path)\n",
    "\n",
    "#train model on all available data to classify unseen 2022 data\n",
    "train(dataset['train'], 3.098e-5, 0.001, \"candidate_PubMedBERT_2021_5/27/23\", 4, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df092df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine-tuning for final classification with whole data\n",
    "run_name = \"ftw_PubMedBert_GRT_256_lr77_13_2\"\n",
    "def fine_tune(model_checkpoint, output_dir, lr, batch_size, epochs, weight_decay):\n",
    "    seed = 123\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels = 2)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = output_dir,\n",
    "        learning_rate = lr,\n",
    "        per_device_train_batch_size = batch_size,\n",
    "        per_device_eval_batch_size = batch_size,\n",
    "        num_train_epochs= epochs,\n",
    "        weight_decay = weight_decay,\n",
    "        #report_to='wandb',\n",
    "        #logging_steps = 'epoch',\n",
    "        #load_best_model_at_end = True,\n",
    "        #evaluation_strategy = 'epoch',\n",
    "        save_strategy= 'epoch',\n",
    "        save_total_limit = 1,\n",
    "        #run_name= run_name,\n",
    "        \n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    #eval_dataset=valid_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #callbacks = [early_stopping],\n",
    "     )\n",
    "    a = trainer.train()\n",
    "    #wandb.finish()\n",
    "    return a\n",
    "\n",
    "fine_tune(model_checkpoint, \"ftw_PubMedBert_GRT_256_lr77_13_2\", 7e-7, 4, 12, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8951f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiments\n",
    "ft_model = AutoModelForSequenceClassification.from_pretrained(\"ft_all_PubMedBERT_selected32/checkpoint-465\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "clf = pipeline(\"text-classification\", model = ft_model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiments\n",
    "def res(example):\n",
    "    return clf(example['text'],  truncation = True)\n",
    "#predictions = dataset['test'].map(res, batched = True)\n",
    "#print(predictions)\n",
    "predictions = {}\n",
    "for i in range(len(dataset['validation'])):\n",
    "    predictions[i] = clf(dataset['validation']['text'][i], truncation = 'longest_first', max_length = max_length)[0]['label']\n",
    "#for i in range(12, len(dataset['validation'])):\n",
    "    #print(i)\n",
    "    #predictions[i] = clf(dataset['validation']['text'][i], truncation = 'longest_first', max_length = 20)[0]['label']\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a148d41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96583144 0.90756303 0.6        0.97142857] [0.92491468 0.96963563 0.98876404 0.99856528] [0.95067265 0.93506494 0.6        0.97142857]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def multi_class_performance(y_true, y_pred):\n",
    "\n",
    "    mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    tps = mcm[:, 1, 1]\n",
    "    tns = mcm[:, 0, 0]\n",
    "\n",
    "    recall      = tps / (tps + mcm[:, 1, 0])         # Sensitivity\n",
    "    specificity = tns / (tns + mcm[:, 0, 1])         # Specificity\n",
    "    precision   = tps / (tps + mcm[:, 0, 1])         # PPV\n",
    "    return recall, specificity, precision\n",
    "\n",
    "recall , specificity, precision = multi_class_performance(y_true, y_pred)\n",
    "print(recall, specificity, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation of joined abstract and title\n",
    "max_length = 256\n",
    "ft_model = \"PubMedBERT_2021_2epoch/checkpoint-714\"\n",
    "#ft_model = 'final_PubMedBERT/checkpoint-771'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "clf = pipeline(\"text-classification\", model = ft_model, tokenizer = tokenizer)\n",
    "\n",
    "y_pred_true = collections.defaultdict(list)\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for i in range(len(dataset['validation'])):\n",
    "    p = clf(dataset['validation']['text'][i], truncation = True, max_length = max_length)[0]['label']\n",
    "    if p == 'LABEL_0':\n",
    "        y_pred_true[dataset['validation'][i]['PMID']].append(0)\n",
    "        y_pred.append(0)\n",
    "    elif p == 'LABEL_1':\n",
    "        y_pred_true[dataset['validation'][i]['PMID']].append(1)\n",
    "        y_pred.append(1)\n",
    "    elif p == 'LABEL_2':\n",
    "        print('hi')\n",
    "        y_pred_true[dataset['validation'][i]['PMID']].append(2)\n",
    "        y_pred.append(2)\n",
    "    else:\n",
    "        print('hi')\n",
    "        y_pred_true[dataset['validation'][i]['PMID']].append(3)\n",
    "        y_pred.append(3)\n",
    "    y_pred_true[dataset['validation'][i]['PMID']].append(dataset['validation'][i]['labels'])\n",
    "    y_true.append(dataset['validation'][i]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe854a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deployment\n",
    "y_pred = collections.defaultdict(list)\n",
    "for i in range(len(dataset['unknown'])):\n",
    "    p = clf(dataset['unknown']['text'][i], truncation = True, max_length = max_length)[0]\n",
    "    \n",
    "    if p['label'] == 'LABEL_0':\n",
    "        y_pred[dataset['unknown'][i]['PMID']].append(0)\n",
    "    elif p['label'] == 'LABEL_1':\n",
    "        y_pred[dataset['unknown'][i]['PMID']].append(1)\n",
    "    elif p['label'] == 'LABEL_2':\n",
    "        y_pred[dataset['unknown'][i]['PMID']].append(2)\n",
    "    else:\n",
    "        y_pred[dataset['unknown'][i]['PMID']].append(3)\n",
    "    y_pred[dataset['unknown'][i]['PMID']].append(p['score'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
