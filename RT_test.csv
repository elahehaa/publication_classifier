text,labels
"To facilitate management of acute sinusitis, we conducted a meta-analysis of published studies comparing diagnostic tests for this disorder. Thirteen studies were identified through literature search. Based on sinus puncture/aspiration (considered most accurate), 49-83% of symptomatic patients had acute sinusitis. Compared with puncture/aspiration, radiography offered moderate ability to diagnose sinusitis (summary receiver operator curve [SROC] area, 0.83). Using sinus opacity or fluid as the criterion for sinusitis, radiography had sensitivity of 0.73 and specificity of 0.80. Studies evaluating ultrasonography revealed substantial variation in test performance. The clinical evaluation, particularly risk scores formally incorporating history and physical examination findings, had moderate ability to identify patients with positive radiographs (SROC area, 0.74). Many studies were of poor quality, with inadequately described test methods and unblinded test interpretation. In conclusion, acute sinusitis is common among symptomatic patients. Radiography and clinical evaluation (especially risk scores) appear to provide useful information for diagnosis of sinusitis.-Meta-analysis of diagnostic tests for acute sinusitis.",0
"GLIMMPSE is a free, web-based software tool that calculates power and sample size for the general linear multivariate model with Gaussian errors (http://glimmpse.SampleSizeShop.org/). GLIMMPSE provides a user-friendly interface for the computation of power and sample size. We consider models with fixed predictors, and models with fixed predictors and a single Gaussian covariate. Validation experiments demonstrate that GLIMMPSE matches the accuracy of previously published results, and performs well against simulations. We provide several online tutorials based on research in head and neck cancer. The tutorials demonstrate the use of GLIMMPSE to calculate power and sample size.-GLIMMPSE: Online Power Computation for Linear Models with and without a Baseline Covariate",1
"Case-cohort design has been advocated in many epidemiologic studies when studying rare diseases or events. In this design, with a rare event, all the events are selected for risk-factor assessment. When the event is not rare, it is desirable to consider a generalized case-cohort design, where only a fraction of events are sampled. We provide a valid test statistic to compare hazards functions between two samples for this generalized design and give a method for calculating power. Our result generalizes the result in Cai and Zeng (2004, Biometrics60, 1015-1024), and it shows numerically that efficiency loss due to sampling only part of the events is very low under nonrare-events situation.-Power calculation for case-cohort studies with nonrare events.",0
Multiple Imputation of Multilevel Missing Data?Rigor Versus Simplicity,1
"Positron emission tomography (PET) imaging is a useful tool for quantifying various aspects of the distribution of neuroreceptors throughout the human brain in vivo. A typical analysis consists of applying a pharmacokinetic model to the data, estimating the parameters of the model using non-linear least squares methods, then taking the appropriate function of estimated model parameters as a final estimate of the parameter(s) of interest. As an alternative for fitting these models, it has been shown previously that taking a particular transformation of the data results in two variables that have a linear relationship, and that the slope of this linear relationship is the parameter of primary interest. However, estimating the slope using ordinary least squares (OLS) regression results in a large negative bias. By rearranging the terms in the relationship, the problem may be reformed to allow direct application of standard estimation principles. Estimators resulting from this approach are shown via simulation to have better estimation properties as compared to the OLS estimators.-Estimation of kinetic parameters in graphical analysis of PET imaging data.",0
"We propose a site random-cluster model by introducing an additional cluster weight in the partition function of the traditional site percolation. To simulate the model on a square lattice, we combine the color-assignation and the Swendsen-Wang methods to design a highly efficient cluster algorithm with a small critical slowing-down phenomenon. To verify whether or not it is consistent with the bond random-cluster model, we measure several quantities, such as the wrapping probability Re, the percolating cluster density P?, and the magnetic susceptibility per site ?p, as well as two exponents, such as the thermal exponent yt and the fractal dimension yh of the percolating cluster. We find that for different exponents of cluster weight q=1.5, 2, 2.5, 3, 3.5, and 4, the numerical estimation of the exponents yt and yh are consistent with the theoretical values. The universalities of the site random-cluster model and the bond random-cluster model are completely identical. For larger values of q, we find obvious signatures of the first-order percolation transition by the histograms and the hysteresis loops of percolating cluster density and the energy per site. Our results are helpful for the understanding of the percolation of traditional statistical models.-Percolation of the site random-cluster model by Monte Carlo method.",1
"Attrition, which leads to missing data, is a common problem in cluster randomized trials (CRTs), where groups of patients rather than individuals are randomized. Standard multiple imputation (MI) strategies may not be appropriate to impute missing data from CRTs since they assume independent data. In this paper, under the assumption of missing completely at random and covariate dependent missing, we compared six MI strategies which account for the intra-cluster correlation for missing binary outcomes in CRTs with the standard imputation strategies and complete case analysis approach using a simulation study. We considered three within-cluster and three across-cluster MI strategies for missing binary outcomes in CRTs. The three within-cluster MI strategies are logistic regression method, propensity score method, and Markov chain Monte Carlo (MCMC) method, which apply standard MI strategies within each cluster. The three across-cluster MI strategies are propensity score method, random-effects (RE) logistic regression approach, and logistic regression with cluster as a fixed effect. Based on the community hypertension assessment trial (CHAT) which has complete data, we designed a simulation study to investigate the performance of above MI strategies. The estimated treatment effect and its 95% confidence interval (CI) from generalized estimating equations (GEE) model based on the CHAT complete dataset are 1.14 (0.76 1.70). When 30% of binary outcome are missing completely at random, a simulation study shows that the estimated treatment effects and the corresponding 95% CIs from GEE model are 1.15 (0.76 1.75) if complete case analysis is used, 1.12 (0.72 1.73) if within-cluster MCMC method is used, 1.21 (0.80 1.81) if across-cluster RE logistic regression is used, and 1.16 (0.82 1.64) if standard logistic regression which does not account for clustering is used. When the percentage of missing data is low or intra-cluster correlation coefficient is small, different approaches for handling missing binary outcome data generate quite similar results. When the percentage of missing data is large, standard MI strategies, which do not take into account the intra-cluster correlation, underestimate the variance of the treatment effect. Within-cluster and across-cluster MI strategies (except for random-effects logistic regression MI strategy), which take the intra-cluster correlation into account, seem to be more appropriate to handle the missing outcome from CRTs. Under the same imputation strategy and percentage of missingness, the estimates of the treatment effect from GEE and RE logistic regression models are similar.-Imputation strategies for missing binary outcomes in cluster randomized trials.",1
"Peer support intervention trials are typically conducted in community-based settings and provide generalizable results. The logistic challenges of community-based trials often result in unplanned temporal imbalances in recruitment and follow-up. When imbalances are present, as in the ENCOURAGE trial, appropriate statistical methods must be used to account for these imbalances. We present the design, conduct, and analysis of the ENCOURAGE trial as a case study of a cluster-randomized, community-based, peer-coaching intervention. Preliminary data analysis included examination of study data for imbalances in participant characteristics at baseline, the presence of both secular and seasonal trends in outcome measures, and imbalances in time from baseline to follow-up. Additional examination suggested the presence of nonlinear trends in the intervention effect. The final analyses adjusted for all identified imbalances with accounting for community clustering by supplementing linear mixed effect models with generalized additive mixed models (GAMM) to examine nonlinear trends. Largely due to the location of participants across a considerable geographic area, temporal imbalances were discovered in recruitment, baseline, and follow-up data collection, along with evidence for both secular and seasonal trends in study outcome measures. Using the standard analytical approach, ENCOURAGE appeared to be a null trial. After incorporating adjustment for these temporal imbalances, linear regression analyses still showed no intervention effect. Upon further analyses using GAMM to consider nonlinear intervention trends, we observed intervention effects that were both significant (P &lt;.05) and nonlinear. In community-based trials, recruitment and follow-up may not occur as planned, and complex temporal imbalance may greatly influence the analysis. Real-world trials should use careful logistic planning and monitoring to avoid temporal imbalance. If imbalance is unavoidable, sophisticated statistical methods may nevertheless extract useful information, although the potential problem of residual confounding due to other unmeasured imbalances must be considered.-Challenges of Prolonged Follow-up and Temporal Imbalance in Pragmatic Trials: Analysis of the ENCOURAGE Trial.",1
"Latent class analysis (LCA) and latent class regression (LCR) are widely used for modeling multivariate categorical outcomes in social science and biomedical studies. Standard analyses assume data of different respondents to be mutually independent, excluding application of the methods to familial and other designs in which participants are clustered. In this article, we consider multilevel latent class models, in which subpopulation mixing probabilities are treated as random effects that vary among clusters according to a common Dirichlet distribution. We apply the expectation-maximization (EM) algorithm for model fitting by maximum likelihood (ML). This approach works well, but is computationally intensive when either the number of classes or the cluster size is large. We propose a maximum pairwise likelihood (MPL) approach via a modified EM algorithm for this case. We also show that a simple latent class analysis, combined with robust standard errors, provides another consistent, robust, but less-efficient inferential procedure. Simulation studies suggest that the three methods work well in finite samples, and that the MPL estimates often enjoy comparable precision as the ML estimates. We apply our methods to the analysis of comorbid symptoms in the obsessive compulsive disorder study. Our models' random effects structure has more straightforward interpretation than those of competing methods, thus should usefully augment tools available for LCA of multilevel data.-Multilevel latent class models with dirichlet mixing distribution.",0
"Having substantial missing data is a common problem in administrative and cancer registry data. We propose a sensitivity analysis to evaluate the impact of a covariate that is potentially missing not at random in survival analyses using Weibull proportional hazards regressions. We apply the method to an investigation of the impact of missing grade on post-surgical mortality outcomes in individuals with metastatic kidney cancer. Data came from the Surveillance Epidemiology and End Results (SEER) registry which provides population-based information on those undergoing cytoreductive nephrectomy. Tumor grade is an important component of risk stratification for patients with both localized and metastatic kidney cancer. Many individuals in SEER with metastatic kidney cancer are missing tumor grade information. We found that surgery was protective, but that the magnitude of the effect depended on assumptions about the relationship of grade with missingness.-Sensitivity analysis to investigate the impact of a missing covariate on survival analyses using cancer registry data.",0
Investigating multilevel mediation with fully or partially nested data,2
"Important evidence about the mental health effects of unemployment exist; however, little is known about the possible protective effects of various social interventions or about their long-term impact. This study examines the long-term consequences that different types of social programmes, i.e. entitlement and means-tested benefits, might have as regards ameliorating a negative mental health impact of unemployment among women and men. Multiple regression models were used to analyse panel data collected in the National Survey of Families and Households in 1987 and 1992. In all 8029 individuals interviewed in both 1987 and 1992 were included in the analysis. A depression index was created from the responses to 15 items from the Center for Epidemiological Studies' Depression Scale-D (CES-D) which were included in the survey. The receipt of government entitlement benefits by unemployed women is associated with a reduction of depression symptoms in the long term. Men and women not working and receiving means-tested or welfare benefits are more likely to report depression in both the short and long term. The study underscores the need for monitoring the impact of welfare reform on mental health.-Do social programmes contribute to mental well-being? The long-term impact of unemployment on depression in the United States.",0
Equity and evidence during vaccine rollout: stepped wedge cluster randomised trials could help.,3
At high risk for early withdrawal: using a cumulative risk model to increase retention in the first year of the TEDDY study.,0
"This paper describes research designs and statistical analyses to investigate how tobacco prevention programs achieve their effects on tobacco use. A theoretical approach to program development and evaluation useful for any prevention program guides the analysis. The theoretical approach focuses on action theory for how the program affects mediating variables and on conceptual theory for how mediating variables are related to tobacco use. Information on the mediating mechanisms by which tobacco prevention programs achieve effects is useful for the development of efficient programs and provides a test of the theoretical basis of prevention efforts. Examples of these potential mediating mechanisms are described including mediated effects through attitudes, social norms, beliefs about positive consequences, and accessibility to tobacco. Prior research provides evidence that changes in social norms are a critical mediating mechanism for successful tobacco prevention. Analysis of mediating variables in single group designs with multiple mediators are described as well as multiple group randomized designs which are the most likely to accurately uncover important mediating mechanisms. More complicated dismantling and constructive designs are described and illustrated based on current findings from tobacco research. Mediation analysis for categorical outcomes and more complicated statistical methods are outlined.-Mediation designs for tobacco prevention research.",1
"To evaluate whether clustering effects, often quantified by the intracluster correlation coefficient (ICC), were appropriately accounted for in design and analysis of school-based trials. We searched PubMed and extracted variables concerning study characteristics, power analysis, ICC use for power analysis, applied statistical models, and the report of the ICC estimated from the observed data. N=263 papers were identified, and N=121 papers were included for evaluation. Overall, only a minority (21.5%) of studies incorporated ICC values for power analysis, fewer studies (8.3%) reported the estimated ICC, and 68.6% of studies applied appropriate multilevel models. A greater proportion of studies applied the appropriate models during the past five years (2013-2017) compared to the prior years (74.1% versus 63.5%, p=0.176). Significantly associated with application of appropriate models were a larger number of schools (p=0.030), a larger sample size (p=0.002), longer follow-up (p=0.014), and randomization at a cluster level (p &lt; 0.001) and so were studies that incorporated the ICC into power analysis (p=0.016) and reported the estimated ICC (p=0.030). Although application of appropriate models has increased over the years, consideration of clustering effects in power analysis has been inadequate, as has report of estimated ICC. To increase rigor, future school-based trials should address these issues at both the design and analysis stages.-Trial Characteristics and Appropriateness of Statistical Methods Applied for Design and Analysis of Randomized School-Based Studies Addressing Weight-Related Issues: A Literature Review.",1
"We consider in this article testing rare variants by environment interactions in sequencing association studies. Current methods for studying the association of rare variants with traits cannot be readily applied for testing for rare variants by environment interactions, as these methods do not effectively control for the main effects of rare variants, leading to unstable results and/or inflated Type 1 error rates. We will first analytically study the bias of the use of conventional burden-based tests for rare variants by environment interactions, and show the tests can often be invalid and result in inflated Type 1 error rates. To overcome these difficulties, we develop the interaction sequence kernel association test (iSKAT) for assessing rare variants by environment interactions. The proposed test iSKAT is optimal in a class of variance component tests and is powerful and robust to the proportion of variants in a gene that interact with environment and the signs of the effects. This test properly controls for the main effects of the rare variants using weighted ridge regression while adjusting for covariates. We demonstrate the performance of iSKAT using simulation studies and illustrate its application by analysis of a candidate gene sequencing study of plasma adiponectin levels.-Test for rare variants by environment interactions in sequencing association studies.",0
"Women who give birth at younger ages (e.g. teenage mothers) are more likely to have children who exhibit behaviour problems, such as attention-deficit/hyperactivity disorder (ADHD). However, it is not clear whether young maternal age is causally associated with poor offspring outcomes or confounded by familial factors. The association between early maternal age at childbirth and offspring ADHD was studied using data from Swedish national registers. The sample included all children born in Sweden between 1988 and 2003 (N = 1 495 543), including 30 674 children with ADHD. We used sibling- and cousin-comparisons to control for unmeasured genetic and environmental confounding. Further, we used a children-of-siblings model to quantify the genetic and environmental contribution to the association between maternal age and offspring ADHD. Maternal age at first birth (MAFB) was associated with offspring ADHD. Teenage childbirth (&lt;20 years) was associated with 78% increased risk of ADHD. The association attenuated in cousin-comparison, suggesting unmeasured familial confounding. The children-of-siblings model indicated that the association between MAFB and ADHD was mainly explained by genetic confounding. All children born to mothers who bore their first child early in their reproductive lives were at increased risk of ADHD. The association was mainly explained by genetic factors transmitted from mothers to their offspring that contribute to both age at childbirth and ADHD in offspring. Our results highlight the importance of using family-based designs to understand how early life circumstances affect child development.-Maternal age at childbirth and risk for ADHD in offspring: a population-based cohort study.",0
"This study proposes a generalized time-varying effect model that can be used to characterize a discrete longitudinal covariate process and its time-varying effect on a later outcome that may be discrete. The proposed method can be applied to examine two important research questions for daily process data: measurement reactivity and predictive validity. We demonstrate these applications using health risk behavior data collected from alcoholic couples through an interactive voice response system. The statistical analysis results show that the effect of measurement reactivity may only be evident in the first week of interactive voice response assessment. Moreover, the level of urge to drink before measurement reactivity takes effect may be more predictive of a later depression outcome. Our simulation study shows that the performance of the proposed method improves with larger sample sizes, more time points, and smaller proportions of zeros in the binary longitudinal covariate.-Two-stage model for time-varying effects of discrete longitudinal covariates with applications in analysis of daily process data.",0
"The stepped-wedge (SW) cluster randomized controlled trial, in which clusters cross over in a randomized sequence from control to intervention, is ideal for the implementation and testing of complex health service interventions. In certain cases however, implementation of the intervention may pose logistical challenges, and variations in SW design may be required. We examine the logistical and statistical implications of variations in SW design using the optimization of the Patient-Centered Care Transitions in Heart Failure trial for illustration. We review the following complete SW design variations: a typical SW design; an SW design with multiple clusters crossing over per period to achieve balanced cluster sizes at each step; hierarchical randomization to account for higher-level clustering effects; nested substudies to measure outcomes requiring a smaller sample size than the primary outcomes; and hybrid SW design, which combines parallel cluster with SW design to improve efficiency. We also reviewed 3 incomplete SW design variations in which data are collected in some but not all steps to ease measurement burden. These include designs with a learning period that improve fidelity to the intervention, designs with reduced measurements to minimize collection burden, and designs with early and late blocks to accommodate cluster readiness. Variations in SW design offer pragmatic solutions to logistical challenges but have implications to statistical power. Advantages and disadvantages of each variation should be considered before finalizing the design of an SW randomized controlled trial.-Variations in stepped-wedge cluster randomized trial design: Insights from the Patient-Centered Care Transitions in Heart Failure trial",3
"Medical cost data are often skewed to the right and heteroscedastic, having a nonlinear relation with covariates. To tackle these issues, we consider an extension to generalized linear models by assuming nonlinear associations of covariates in the mean function and allowing the variance to be an unknown but smooth function of the mean. We make no further assumption on the distributional form. The unknown functions are described by penalized splines, and the estimation is carried out using nonparametric quasi-likelihood. Simulation studies show the flexibility and advantages of our approach. We apply the model to the annual medical costs of heart failure patients in the clinical data repository at the University of Virginia Hospital System.-A flexible model for the mean and variance functions, with application to medical cost data.",0
"The health consequences of anxiety in late life have not been adequately investigated. We sought to examine the association between anxiety and death in an older tri-ethnic population. A longitudinal population-based study of 506 older noninstitutionalized non-Hispanic Whites, non-Hispanic Blacks, and Hispanics aged 75 years or older from Galveston County. Average age was 80.8 (SD 4.4) and 50.8% were women. Older non-Hispanic Whites (21.6%) reported the highest prevalence of anxiety, followed by Hispanics (12.4%) and non-Hispanic blacks (11.3%) (P=.0001). High anxiety was significantly associated with an increased hazard of all cause death (HR 1.52; 95% CI 1.02, 2.28) and cardiovascular death (HR 1.90; 95% CI 1.06, 3.36); and was associated with an increased hazard of cancer death (HR 2.38; 95% CI 0.88, 6.45) during 5-years of follow-up. There is a high prevalence of anxiety in late life. Our results indicate an association between anxiety and increased risk of death in persons aged 75 and older.-High anxiety is associated with an increased risk of death in an older tri-ethnic population.",0
"When comparing two different kinds of group therapy or two individual treatments where patients within each arm are nested within care providers, clustering of observations may occur in both arms. The arms may differ in terms of (a) the intraclass correlation, (b) the outcome variance, (c) the cluster size, and (d) the number of clusters, and there may be some ideal group size or ideal caseload in case of care providers, fixing the cluster size. For this case, optimal cluster numbers are derived for a linear mixed model analysis of the treatment effect under cost constraints as well as under power constraints. To account for uncertain prior knowledge on relevant model parameters, also maximin sample sizes are given. Formulas for sample size calculation are derived, based on the standard normal as the asymptotic distribution of the test statistic. For small sample sizes, an extensive numerical evaluation shows that in a two-tailed test employing restricted maximum likelihood estimation, a safe correction for both 80% and 90% power, is to add three clusters to each arm for a 5% type I error rate and four clusters to each arm for a 1% type I error rate.-Sample size calculation for treatment effects in randomized trials with fixed cluster sizes and heterogeneous intraclass correlations and variances.",1
"The design of randomized controlled trials entails decisions that have economic as well as statistical implications. In particular, the choice of an individual or cluster randomization design may affect the cost of achieving the desired level of power, other things being equal. Furthermore, if cluster randomization is chosen, the researcher must decide how to balance the number of clusters, or ""sites,"" and the size of each site. This article investigates these interrelated statistical and economic issues. Its principal purpose is to elucidate the statistical and economic trade-offs to assist researchers to employ randomized controlled trials that have desired economic, as well as statistical, properties.-Balancing the number and size of sites: an economic approach to the optimal design of cluster samples.",1
"The multisite trial, widely used in mental health research and education, enables experimenters to assess the average impact of a treatment across sites, the variance of treatment impact across sites, and the moderating effect of site characteristics on treatment efficacy. Key design decisions include the sample size per site and the number of sites. To consider power implications, this article proposes a standardized hierarchical linear model and uses rules of thumb similar to those proposed by J. Cohen (1988) for small, medium, and large effect sizes and for small, medium, and large treatment-by-site variance. Optimal allocation of resources within and between sites as a function of variance components and costs at each level are also considered. The approach generalizes to quasiexperiments with a similar structure. These ideas are illustrated with newly developed software.-Statistical power and optimal design for multisite randomized trials.",1
"A complication of cardiovascular disease (CVD) and the metabolic syndrome (MetS) among older adults is loss of mobility. The American Heart Association has identified weight management as a core component of secondary prevention programs for CVD and is an important risk factor for physical disability. The American Society for Nutrition and the Obesity Society have highlighted the need for long-term randomized clinical trials to evaluate the independent and additive effects of diet-induced weight loss (WL) and physical activity in older persons on outcomes such as mobility, muscle function, and obesity related diseases. Here we describe the rationale, design, and methods of a translational study, the Cooperative Lifestyle Intervention Program-II (CLIP-II). CLIP-II will randomize 252 obese, older adults with CVD or MetS to a weight loss only treatment (WL), aerobic exercise training (AT)+WL, or resistance exercise training (RT)+WL for 18 months. The dual primary outcomes are mobility and knee extensor strength. The interventions will be delivered by YMCA community partners with our staff as trainers and advisers. This study will provide the first large scale trial to evaluate the effects of diet-induced WL on mobility in obese, older adults with CVD or MetS as compared to WL combined with two different modes of physical activity (AT and RT). Because uncertainty exists about the best approach for promoting WL in older adults due to concerns with the loss of lean mass, the design also permits a contrast between AT+WL and RT+WL on muscle strength.-The Cooperative Lifestyle Intervention Program-II (CLIP-II): design and methods.",0
"Clustered data commonly arise in epidemiology. We assume each cluster member has an outcome Y and covariates X. When there are missing data in Y, the distribution of Y given X in all cluster members (""complete clusters"") may be different from the distribution just in members with observed Y (""observed clusters""). Often the former is of interest, but when data are missing because in a fundamental sense Y does not exist (e.g., quality of life for a person who has died), the latter may be more meaningful (quality of life conditional on being alive). Weighted and doubly weighted generalized estimating equations and shared random-effects models have been proposed for observed-cluster inference when cluster size is informative, that is, the distribution of Y given X in observed clusters depends on observed cluster size. We show these methods can be seen as actually giving inference for complete clusters and may not also give observed-cluster inference. This is true even if observed clusters are complete in themselves rather than being the observed part of larger complete clusters: here methods may describe imaginary complete clusters rather than the observed clusters. We show under which conditions shared random-effects models proposed for observed-cluster inference do actually describe members with observed Y. A psoriatic arthritis dataset is used to illustrate the danger of misinterpreting estimates from shared random-effects models.-Methods for observed-cluster inference when cluster size is informative: a review and clarifications.",1
"In meta-analyses, where a continuous outcome is measured with different scales or standards, the summary statistic is the mean difference standardised to a common metric with a common variance. Where trial treatment is delivered by a person, nesting of patients within care providers leads to clustering that may interact with, or be limited to, one or more of the arms. Assuming a common standardising variance is less tenable and options for scaling the mean difference become numerous. Metrics suggested for cluster-randomised trials are within, between and total variances and for unequal variances, the control arm or pooled variances. We consider summary measures and individual-patient-data methods for meta-analysing standardised mean differences from trials with two-level nested clustering, relaxing independence and common variance assumptions, allowing sample sizes to differ across arms. A general metric is proposed with comparable interpretation across designs. The relationship between the method of standardisation and choice of model is explored, allowing for bias in the estimator and imprecision in the standardising metric. A meta-analysis of trials of counselling in primary care motivated this work. Assuming equal clustering effects across trials, the proposed random-effects meta-analysis model gave a pooled standardised mean difference of -0.27 (95% CI -0.45 to -0.08) using summary measures and -0.26 (95% CI -0.45 to -0.09) with the individual-patient-data. While treatment-related clustering has rarely been taken into account in trials, it is now recommended that it is considered in trials and meta-analyses. This paper contributes to the uptake of this guidance. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Meta-analysis of standardised mean differences from randomised trials with treatment-related clustering associated with care providers.",2
"In a crossover design in the absence of any carryover effect, including period-specific baselines as covariates in an analysis of covariance, is known to increase the precision of the estimated treatment effect. The extent of the efficiency gain is a function of the joint covariance structure of the baselines and post-treatment responses, as well as the metric used to incorporate the baselines into the analysis. Here, we show how the underlying covariance structure can be leveraged to find an optimal linear combination of baselines so as to minimize the theoretical variance of the analysis of covariance-based estimated treatment effect. Our work is relevant to complete designs with up to four periods, specifically the 2 ? 2, 3 ? 3, and 4 ? 4. Given that the optimal linear combination of baselines is a function of the covariance structure, which in practice is unknown, we propose an adaptive method. Here, the covariance structure is chosen using information criterion to guide the choice of the linear combination of baselines. Evaluation of the proposed approach suggests that the type I error rate is maintained. Moreover, relative to previously published methods, sizeable gains in power are possible with this method. Results from a 2 ? 2 trial exploring renal function, and a 3 ? 3 trial with heart rate as the outcome, are used to illustrate the methods. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Improved power in crossover designs through linear combinations of baselines.",0
"The Patient Navigation Research Program (PNRP) is a cooperative effort of nine research projects, with similar clinical criteria but with different study designs. To evaluate projects such as PNRP, it is desirable to perform a pooled analysis to increase power relative to the individual projects. There is no agreed-upon prospective methodology, however, for analyzing combined data arising from different study designs. Expert opinions were thus solicited from the members of the PNRP Design and Analysis Committee. To review possible methodologies for analyzing combined data arising from heterogeneous study designs. The Design and Analysis Committee critically reviewed the pros and cons of five potential methods for analyzing combined PNRP project data. The conclusions were based on simple consensus. The five approaches reviewed included the following: (1) analyzing and reporting each project separately, (2) combining data from all projects and performing an individual-level analysis, (3) pooling data from projects having similar study designs, (4) analyzing pooled data using a prospective meta-analytic technique, and (5) analyzing pooled data utilizing a novel simulated group-randomized design. Methodologies varied in their ability to incorporate data from all PNRP projects, to appropriately account for differing study designs, and to accommodate differing project sample sizes. The conclusions reached were based on expert opinion and not derived from actual analyses performed. The ability to analyze pooled data arising from differing study designs may provide pertinent information to inform programmatic, budgetary, and policy perspectives. Multisite community-based research may not lend itself well to the more stringent explanatory and pragmatic standards of a randomized controlled trial design. Given our growing interest in community-based population research, the challenges inherent in the analysis of heterogeneous study design are likely to become more salient. Discussion of the analytic issues faced by the PNRP and the methodological approaches we considered may be of value to other prospective community-based research programs.-Analysis of combined data from heterogeneous study designs: an applied example from the patient navigation research program.",1
"An amendment to this paper has been published and can be accessed via the original article.-Correction to: Cluster identification, selection, and description in Cluster randomized crossover trials: the PREP-IT trials",1
"We review design and analytic methods available for multilevel interventions in cancer research with particular attention to study design, sample size requirements, and potential to provide statistical evidence for causal inference. The most appropriate methods will depend on the stage of development of the research and whether randomization is possible. Early on, fractional factorial designs may be used to screen intervention components, particularly when randomization of individuals is possible. Quasi-experimental designs, including time-series and multiple baseline designs, can be useful once the intervention is designed because they require few sites and can provide the preliminary evidence to plan efficacy studies. In efficacy and effectiveness studies, group-randomized trials are preferred when randomization is possible and regression discontinuity designs are preferred otherwise if assignment based on a quantitative score is possible. Quasi-experimental designs may be used, especially when combined with recent developments in analytic methods to reduce bias in effect estimates.-Designing studies that would address the multilayered nature of health care.",1
"Several approaches have been proposed to analyze clustered binary data, which arise in fields such as teratology and ophthalmology. These methods include mixed-effects and quasi-likelihood approaches, as well as models that use cluster responses as covariates. The three approaches measure different effects of covariates on binary responses, but simple approximations relate the magnitudes of their parameters. In this article, we present approximations to relate the standard errors of model parameters and Wald tests for covariate effects obtained from the different approaches. These approximations show that Wald tests involving cluster-level covariates will be approximately equivalent using the different approaches. However, approaches that model intracluster correlation, such as the mixed-effects model, provide more powerful tests of within-cluster covariates than those that do not model the correlation. Simulations and example data illustrate these findings.-Estimation efficiency and tests of covariate effects with clustered binary data.",1
"In many longitudinal follow-up studies, we observe more than one longitudinal outcome. Impaired renal and liver functions are indicators of poor clinical outcomes for patients who are on mechanical circulatory support and awaiting heart transplant. Hence, monitoring organ functions while waiting for heart transplant is an integral part of patient management. Longitudinal measurements of bilirubin can be used as a marker for liver function and glomerular filtration rate for renal function. We derive an approximation to evolution of association between these two organ functions using a bivariate nonlinear mixed effects model for continuous longitudinal measurements, where the two submodels are linked by a common distribution of time-dependent latent variables and a common distribution of measurement errors.-Evolution of association between renal and liver functions while awaiting heart transplant: An application using a bivariate multiphase nonlinear mixed effects model.",0
"The linear mixed effects model with normal errors is a popular model for the analysis of repeated measures and longitudinal data. The generalized linear model is useful for data that have non-normal errors but where the errors are uncorrelated. A descendant of these two models generates a model for correlated data with non-normal errors, called the generalized linear mixed model (GLMM). Frequentist attempts to fit these models generally rely on approximate results and inference relies on asymptotic assumptions. Recent advances in computing technology have made Bayesian approaches to this class of models computationally feasible. Markov chain Monte Carlo methods can be used to obtain 'exact' inference for these models, as demonstrated by Zeger and Karim. In the linear or generalized linear mixed model, the random effects are typically taken to have a fully parametric distribution, such as the normal distribution. In this paper, we extend the GLMM by allowing the random effects to have a non-parametric prior distribution. We do this using a Dirichlet process prior for the general distribution of the random effects. The approach easily extends to more general population models. We perform computations for the models using the Gibbs sampler.-A semi-parametric Bayesian approach to generalized linear mixed models.",1
"In this article, we review and evaluate a number of methods used in the design and analysis of small three-arm parallel cluster randomized trials. We conduct a simulation-based study to evaluate restricted randomization methods including covariate-constrained randomization and a novel method for matched-group cluster randomization. We also evaluate the appropriate modelling of the data and small sample inferential methods for a variety of treatment effects relevant to three-arm trials. Our results indicate that small-sample corrections are required for high (0.05) but not low (0.001) values of the intraclass correlation coefficient and their performance can depend on trial design, number of clusters, and the nature of the hypothesis being tested. The Satterthwaite correction generally performed best at an ICC of 0.05 with a nominal type I error rate for single-period trials, and in trials with repeated measures type I error rates were between 0.04 and 0.06. Restricted randomization methods produce little benefit in trials with repeated measures but in trials with single post-intervention design can provide relatively large gains in power when compared to the most unbalanced possible allocations. Matched-group randomization improves power but is not as effective as covariate-constrained randomization. For model-based analysis, adjusting for fewer covariates than were used in a restricted randomization process under any design can produce non-nominal type I error rates and reductions in power. Where comparisons to two-arm cluster trials are possible, the performance of the methods is qualitatively very similar.-Design and analysis of three-arm parallel cluster randomized trials with small numbers of clusters",1
"We propose a new weighted hurdle regression method for modeling count data, with particular interest in modeling cardiovascular events in patients on dialysis. Cardiovascular disease remains one of the leading causes of hospitalization and death in this population. Our aim is to jointly model the relationship/association between covariates and (i) the probability of cardiovascular events, a binary process, and (ii) the rate of events once the realization is positive-when the 'hurdle' is crossed-using a zero-truncated Poisson distribution. When the observation period or follow-up time, from the start of dialysis, varies among individuals, the estimated probability of positive cardiovascular events during the study period will be biased. Furthermore, when the model contains covariates, then the estimated relationship between the covariates and the probability of cardiovascular events will also be biased. These challenges are addressed with the proposed weighted hurdle regression method. Estimation for the weighted hurdle regression model is a weighted likelihood approach, where standard maximum likelihood estimation can be utilized. The method is illustrated with data from the United States Renal Data System. Simulation studies show the ability of proposed method to successfully adjust for differential follow-up times and incorporate the effects of covariates in the weighting.-Weighted hurdle regression method for joint modeling of cardiovascular events likelihood and rate in the US dialysis population.",0
Cluster randomized trials for health care quality improvement research.,1
"The polymorphic nature of many malaria vaccine candidates presents major challenges to achieving highly efficacious vaccines. Presently, there is very little knowledge on the prevalence and patterns of functional immune responses to polymorphic vaccine candidates in populations to guide vaccine design. A leading polymorphic vaccine candidate against blood-stage Plasmodium falciparum is apical membrane antigen 1 (AMA1), which is essential for erythrocyte invasion. The importance of AMA1 as a target of acquired human inhibitory antibodies, their allele specificity and prevalence in populations is unknown, but crucial for vaccine design. P. falciparum lines expressing different AMA1 alleles were genetically engineered and used to quantify functional antibodies from two malaria-exposed populations of adults and children. The acquisition of AMA1 antibodies was also detected using enzyme-linked immunosorbent assay (ELISA) and competition ELISA (using different AMA1 alleles) from the same populations. We found that AMA1 was a major target of naturally acquired invasion-inhibitory antibodies that were highly prevalent in malaria-endemic populations and showed a high degree of allele specificity. Significantly, the prevalence of inhibitory antibodies to different alleles varied substantially within populations and between geographic locations. Inhibitory antibodies to three specific alleles were highly prevalent (FVO and W2mef in Papua New Guinea; FVO and XIE in Kenya), identifying them for potential vaccine inclusion. Measurement of antibodies by standard or competition ELISA was not strongly predictive of allele-specific inhibitory antibodies. The patterns of allele-specific functional antibody responses detected with our novel assays may?indicate that acquired immunity is elicited towards serotypes that are prevalent in each geographic location. These findings provide new insights into the nature and acquisition of functional immunity to a polymorphic vaccine candidate and strategies to quantify functional?immunity in populations to guide rational vaccine design.-A novel approach to identifying patterns of human invasion-inhibitory antibodies guides the design of malaria vaccines incorporating polymorphic antigens.",0
"The relation between exposure to low levels of polychlorinated biphenyls (PCBs), a class of persistent organic pollutants, and cognitive and motor development in young children has been examined in several studies, and results have varied. The authors evaluated the association between prenatal exposure to PCBs and children's neurodevelopment using data from the Collaborative Perinatal Project. Pregnant women were enrolled from 1959 to 1965 from 12 sites across the United States. PCBs were measured in maternal serum taken during pregnancy. To measure children's mental and psychomotor development at 8 months of age, the authors administered the Bayley Scales of Infant Development (means, 87 (standard deviation, 15) and 88 (standard deviation, 18), respectively). Overall, they did not observe a relation between prenatal PCB exposure and children's mental or psychomotor scores (n = 1,207; multivariate adjusted beta = 0.1 point per micro g/liter increase of PCB, p = 0.71, and beta = 0.5, p = 0.14, respectively). The PCB-psychomotor score relation varied by study center (p &lt; 0.05): The association was direct in some centers, inverse in others. This could not be attributed to variation in the timing or measurement of the child's neurodevelopment or analysis of PCBs because these were standardized across centers. The reasons for variation in results within this study and across other studies remain unclear.-Prenatal exposure to low-level polychlorinated biphenyls in relation to mental and motor development at 8 months.",0
"To assess recruitment bias and the techniques employed to counter this problem in a recent selection of published cluster randomized trials. Review of 24 cluster trials published in 2008 in four leading medical journals. Studies were assessed by four reviewers to identify if an alternative design could have been employed using individual randomization. Data were also extracted on the randomization procedure and the likelihood of this introducing bias to the selection of participants into the study. Of the 24 trials, eight could have used individual randomization as an alternative to cluster allocation. Seven studies could have recruited participants prior to cluster randomization but did not. In eight studies where recruitment bias was possible, more than half (five) demonstrated some evidence of differential recruitment rates. Many cluster trials published in leading medical journals are not clear in their justification for the design. We also found significant proportions of cluster trials used suboptimal designs that increase their risk of introducing selection bias. Better design of cluster trials is possible and should be adopted.-Bias in recruitment to cluster randomized trials: a review of recent publications",1
"Characteristics of the built environment, including access to unhealthy food outlets, are hypothesized to contribute to type 2 diabetes mellitus (T2D). Swedish nationwide registry data on 4,718,583 adults aged 35-80 years living in 9,353 neighborhoods, each with at least 1 food outlet, were geocoded and linked to commercial registers (e.g., restaurants and grocery stores). Multilevel logistic regression was used to examine the prospective relationship between characteristics of the food environment and T2D from 2005 to 2010. Relative access to health-harming food outlets was associated with greater likelihood of both prevalent and incident T2D in a curvilinear manner, with the highest risk being observed for environments in which one-third of outlets were health-harming. Relative to individuals whose food environment did not change, those who moved into areas with more health-harming food outlets had higher odds of developing T2D (odds ratio = 3.67, 95% confidence interval: 2.14, 6.30). Among those who did not move, living in an area that gained relative access to health-harming food outlets was also associated with higher odds of T2D (odds ratio = 1.72, 95% confidence interval: 1.27, 2.33). These results suggest that local food environment, including changes that result in greater access to unhealthy food outlets, is associated with T2D.-Beyond Access: Characteristics of the Food Environment and Risk of Diabetes.",0
"MIXREG is a program that provides estimates for a mixed-effects regression model (MRM) for normally-distributed response data including autocorrelated errors. This model can be used for analysis of unbalanced longitudinal data, where individuals may be measured at a different number of timepoints, or even at different timepoints. Autocorrelated errors of a general form or following an AR(1), MA(1), or ARMA(1,1) form are allowable. This model can also be used for analysis of clustered data, where the mixed-effects model assumes data within clusters are dependent. The degree of dependency is estimated jointly with estimates of the usual model parameters, thus adjusting for clustering. MIXREG uses maximum marginal likelihood estimation, utilizing both the EM algorithm and a Fisher-scoring solution. For the scoring solution, the covariance matrix of the random effects is expressed in its Gaussian decomposition, and the diagonal matrix reparameterized using the exponential transformation. Estimation of the individual random effects is accomplished using an empirical Bayes approach. Examples illustrating usage and features of MIXREG are provided.-MIXREG: a computer program for mixed-effects regression analysis with autocorrelated errors.",1
"When correlated observations are obtained in a randomized controlled trial, the assumption of independence among observations within cluster likely will not hold because the observations share the same cluster (e.g. clinic, physician, or subject). Further, the outcome measurements of interest are often binary. The objective of this paper is to compare the performance of four statistical methods for analysis of clustered binary observations: namely (1) full likelihood method; (2) penalized quasi-likelihood method; (3) generalized estimating equation method; (4) fixed-effects logistic regression method. The first three methods take correlations into account in inferential processes whereas the last method does not. Type I error rate, power, bias, and standard error are compared across the four statistical methods through computer simulations under varying effect sizes, intraclass correlation coefficients, number of clusters, and number of observations per cluster, including large numbers 20 and 100 of observations per cluster. The results show that the performance of the full likelihood and the penalized quasi-likelihood methods is superior for analysis of clustered binary observations, and is not necessarily inferior to that of the fixed-effects logistic regression fit even when within-cluster correlations are zero.-Comparison of statistical methods for analysis of clustered binary observations.",1
"Misclassification of outcomes or event types is common in health sciences research and can lead to serious bias when estimating the cumulative incidence functions in settings with competing risks. Recent work has shown how to estimate nonparametric cumulative incidence functions in the presence of nondifferential outcome misclassification when the misclassification probabilities are known. Here, we extend this approach to account for misclassification that is differential with respect to important predictors of the outcome using misclassification probabilities estimated from external validation data. Moreover, we propose a bootstrap approach in which the observations from both the main study data and the external validation study are resampled to allow the uncertainty in the misclassification probabilities to propagate through the analysis into the final confidence intervals, ensuring appropriate confidence interval coverage probabilities. The proposed estimator is shown to be uniformly consistent and simulation studies indicate that both the estimator and the standard error estimation approach perform well in finite samples. The methodology is applied to estimate the cumulative incidence of death and disengagement from HIV care in a large cohort of HIV infected individuals in sub-Saharan Africa, where a significant death underreporting issue leads to outcome misclassification. This analysis uses external validation data from a separate study conducted in the same country.-Nonparametric estimation of the cumulative incidence function under outcome misclassification using external validation data.",0
"Dropout is a common problem in longitudinal cohort studies and clinical trials, often raising concerns of nonignorable dropout. Selection, frailty, and mixture models have been proposed to account for potentially nonignorable missingness by relating the longitudinal outcome to time of dropout. In addition, many longitudinal studies encounter multiple types of missing data or reasons for dropout, such as loss to follow-up, disease progression, treatment modifications and death. When clinically distinct dropout reasons are present, it may be preferable to control for both dropout reason and time to gain additional clinical insights. This may be especially interesting when the dropout reason and dropout times differ by the primary exposure variable. We extend a semi-parametric varying-coefficient method for nonignorable dropout to accommodate dropout reason. We apply our method to untreated HIV-infected subjects recruited to the Acute Infection and Early Disease Research Program HIV cohort and compare longitudinal CD4+ T cell count in injection drug users to nonusers with two dropout reasons: anti-retroviral treatment initiation and loss to follow-up.-Accounting for dropout reason in longitudinal studies with nonignorable dropout.",0
"In many phase II clinical trials, interim monitoring is based on the probability of a binary event, response, defined in terms of one or more time-to-event variables within a time period of fixed length. Such outcome-adaptive methods may require repeated interim suspension of accrual in order to follow each patient for the time period required to evaluate response. This may increase trial duration, and eligible patients arriving during such delays either must wait for accrual to reopen or be treated outside the trial. Alternatively, monitoring may be done continuously by ignoring censored data each time the stopping rule is applied, which wastes information. We propose an adaptive Bayesian method that eliminates these problems. At each patient's accrual time, an approximate posterior for the response probability based on all of the event-time data is used to compute an early stopping criterion. Application to a leukemia trial with a composite event shows that the method can reduce trial duration substantially while maintaining the reliability of interim decisions.-Monitoring the rates of composite events with censored data in phase II clinical trials.",0
"Researchers who conduct cluster-randomized studies must account for clustering during study planning; failure to do so can result in insufficient study power. To plan adequately, investigators need accurate estimates of clustering in the form of intraclass correlation coefficients (ICCs). We used data for 5,042 patients, from 61 practices in 8 practice-based research networks, obtained from the Prescription for Health program, sponsored by the Robert Wood Johnson Fund, to estimate ICCs for demographic and behavioral variables and for physician and practice characteristics. We used an approach similar to analysis of variance to calculate ICCs for binary variables and mixed models that directly estimated between- and within-cluster variances to calculate ICCs for continuous variables. ICCs indicating substantial within-practice clustering were calculated for age (ICC = 0.151), race (ICC = 0.265), and such behaviors as smoking (ICC = 0.118) and unhealthy diet (ICC = 0.206). Patients' intent-to-change behaviors related to smoking, diet, or exercise were less clustered (ICCs ?0.007). Within-network ICCs were generally smaller, reflecting heterogeneity among practices within the same network. ICCs for practice-level measures indicated that practices within networks were relatively homogenous with respect to practice type (ICC = 0.29) and the use of electronic medical records (ICC = 0.23), but less homogenous with respect to size and rates of physician and staff turnover. ICCs for patient behaviors and intent to change those behaviors were generally less than 0.1. Though small, such ICCs are not trivial; if cluster sizes are large, even small levels of clustering that is unaccounted for reduces the statistical power of a cluster-randomized study.-Intraclass correlation coefficients typical of cluster-randomized studies: estimates from the Robert Wood Johnson Prescription for Health projects.",1
"Estimation of sample size and power for stepped wedge cluster randomised trials can be determined by one of a number of related methods. These include exact analytical approaches, design effects or simulation. A recent paper compared the design effect to the analytical method. There were some differences between the two approaches. We show here that these differences occur because the design effect approach is only technically correct when there is an equal number of clusters crossing over at each step. The design effect for the stepped wedge cluster randomised trial is only appropriate when there is an equal number of clusters switching at each step.-Sample size calculations for stepped wedge trials using design effects are only approximate in some circumstances.",3
"The Wilcoxon rank sum test is widely used for two-group comparisons of nonnormal data. An assumption of this test is independence of sampling units both within and between groups, which will be violated in the clustered data setting such as in ophthalmological clinical trials, where the unit of randomization is the subject, but the unit of analysis is the individual eye. For this purpose, we have proposed the clustered Wilcoxon test to account for clustering among multiple subunits within the same cluster (Rosner, Glynn, and Lee, 2003, Biometrics 59, 1089-1098; 2006, Biometrics 62, 1251-1259). However, power estimation is needed to plan studies that use this analytic approach. We have recently published methods for estimating power and sample size for the ordinary Wilcoxon rank sum test (Rosner and Glynn, 2009, Biometrics 65, 188-197). In this article we present extensions of this approach to estimate power for the clustered Wilcoxon test. Simulation studies show a good agreement between estimated and empirical power. These methods are illustrated with examples from randomized trials in ophthalmology. Enhanced power is achieved with use of the subunit as the unit of analysis instead of the cluster using the ordinary Wilcoxon rank sum test.-Power and sample size estimation for the clustered wilcoxon test.",1
"The assessment of the sensitivity of statistical methods has received little attention in cluster randomized trials (CRTs), especially for stratified CRT when the outcome of interest is continuous. We empirically examined the sensitivity of five methods for analyzing the continuous outcome from a stratified CRT - aimed to investigate the efficacy of the Classroom Communication Resource (CCR) compared to usual care to improve the peer attitude towards children who stutter among grade 7 students. Schools - the clusters, were divided into quintile based on their socio-political resources, and then stratified by quintile. The schools were then randomized to CCR and usual care groups in each stratum. The primary outcome was Stuttering Resource Outcomes Measure. Five methods, including the primary method, were used in this study to examine the effect of CCR. The individual-level methods were: (i) linear regression; (ii) mixed-effects method; (iii) GEE with exchangeable correlation structure (primary method of analysis). And the cluster-level methods were: (iv) cluster-level linear regression; and (v) meta-regression. These methods were also compared with or without adjustment for stratification. Ten schools were stratified by quintile, and then randomized to CCR (223 students) and usual care (231 students) groups. The direction of the estimated differences was same for all the methods except meta-regression. The widths of the 95% confidence intervals were narrower when adjusted for stratification. The overall conclusion from all the methods was similar but slightly differed in terms of effect estimate and widths of confidence intervals. Clinicaltrials.gov, NCT03111524. Registered on 9 March 2017.-Sensitivity of methods for analyzing continuous outcome from stratified cluster randomized trials - an empirical comparison study",1
"This article revisits an article published in Evaluation Review in 2005 on sample size estimation and power analysis for group-randomized trials. With help from a careful reader, we learned of an important error in the spreadsheet used to perform the calculations and generate the results presented in that article. As we studied the spreadsheet, we discovered other minor errors. When we corrected the errors, we found that the results were substantially different and that the conclusions reported in the original article were not always appropriate. This article corrects the errors and reports the results as they should have been reported originally. Using a random-effects meta-analytic model, estimates of intraclass correlation were combined from two studies to guide sample size calculations for a new study. The df * method can result in improved power or smaller studies when used a priori to plan future group-randomized trials, though the improvements will be modest in larger studies and will likely be insufficient to provide adequate power to small studies. Smaller group-randomized trials are often desirable, for example, as pilot studies to help plan for a full-scale efficacy trial, as replication studies, or in situations in which resource constraints prohibit a larger trial. We discuss the circumstances under which the df * method will be most helpful and the risks associated with conducting smaller studies.-Increasing the degrees of freedom in future group randomized trials: the df * method revisited.",1
"Intervention trials that employ a group-randomized trial design require an adaptation of the usual analytic methods to account for the randomization of intact economic/social groups to study conditions and the positive ICC that is implied by such a design. In the absence of valid estimates of the ICCs for the outcomes of interest, investigators designing trials could only guess at how large a problem they faced and how much they would need to increase sample size to compensate. Aside from this paper, we are aware of only one other publication that provides such estimates, and that study provides estimates for only a handful of outcomes. Our purpose here has been to provide a replication and extension of those findings to a broader array of outcomes. The results presented here suggest that worksite-level ICCs for a variety of smoking and health-related outcomes are generally small and that these ICCs can generally be reduced by adjustment for individual-level characteristics. We have demonstrated how information about these ICCs can be incorporated in sample size calculations to avoid designing ""underpowered"" studies. Our results should assist investigators in planning studies to evaluate the effectiveness of worksite-based health promotion efforts.-Intraclass correlation for measures from a worksite health promotion study: estimates, correlates, and applications.",1
"The analysis of data from epidemiologic and environmental studies presents challenges such as skewness of distribution, rounding and multiple measurements over time. To model trends over time based on repeated measurements, we propose a general latent model suitable for highly skewed data. The model assumes that the observed outcome is determined by an unobservable outcome that follows a Weibull distribution. To accommodate correlations among repeated responses over time, we introduce a general random effect from the power variance function (PVF) family of distributions, including the gamma distribution often employed in the literature. The resulting marginal likelihood has a closed form without resorting to numerical or approximation methods. We study estimation and hypothesis testing under these models, with different choices of random effect distributions. Simulation studies are conducted to evaluate their performance. Finally, we apply the proposed method to exposure data collected from the Michigan polybrominated biphenyl (MIPBB) study.-Flexible modeling of longitudinal highly skewed outcomes.",0
"Study designs involving clustering in some study arms, but not all study arms, are common in clinical treatment-outcome and educational settings. For instance, in a treatment arm, persons may be nested in therapy groups, whereas in a control arm there are no groups. Methodological approaches for handling such partially nested designs have recently been developed in a multilevel modeling framework (MLM-PN) and have proved very useful. We introduce two alternative structural equation modeling (SEM) approaches for analyzing partially nested data: a multivariate single-level SEM (SSEM-PN) and a multiple-arm multilevel SEM (MSEM-PN). We show how SSEM-PN and MSEM-PN can produce results equivalent to existing MLM-PNs and can be extended to flexibly accommodate several modeling features that are difficult or impossible to handle in MLM-PNs. For instance, using an SSEM-PN or MSEM-PN, it is possible to specify complex structural models involving cluster-level outcomes, obtain absolute model fit, decompose person-level predictor effects in the treatment arm using latent cluster means, and include traditional factors as predictors/outcomes. Importantly, implementation of such features for partially nested designs differs from that for fully nested designs. An empirical example involving a partially nested depression intervention combines several of these features in an analysis of interest for treatment-outcome studies.-Structural Equation Modeling Approaches for Analyzing Partially Nested Data",2
"Different methods of obtaining confidence intervals for the intraclass correlation coefficient rho in the unbalanced one-way random-effects model are investigated, focusing on applications to family studies. Methods based on simple modifications of formulas for the case of equal group sizes are found to provide adequate coverage at small to moderate values of rho. A method based on the large-sample standard error of the sample intraclass correlation, as derived by Smith (1956, Annals of Human Genetics 21, 363-373), is shown to provide consistently good coverage at all values of rho. A method proposed by Thomas and Hultquist (1978, Annals of Statistics 6, 582-587) also provides consistently good coverage, but generates mean interval widths substantially greater than those generated by Smith's method at values of rho likely to arise in practice.-A comparison of confidence interval methods for the intraclass correlation coefficient.",1
"In many biomedical studies, disease progress is monitored by a biomarker over time, eg, repeated measures of CD4 in AIDS and hemoglobin in end-stage renal disease patients. The endpoint of interest, eg, death or diagnosis of a specific disease, is correlated with the longitudinal biomarker. In this paper, we examine and compare different models of longitudinal and survival data to investigate causal mechanisms, specifically, those related to the role of random effects. We illustrate the methods by data from two clinical trials: an AIDS study and a liver cirrhosis study.-Exploring causality mechanism in the joint analysis of longitudinal and survival data.",0
"To assess injection practices and to test interventions aimed at reducing unsafe injections in developing countries, cluster surveys and cluster randomized trials are needed. The design of cluster-based studies requires estimates of intraclass correlation coefficients that have to be obtained from previous studies. This study presents such estimates. Data were derived from a cross-sectional study of injection use and health seeking in Pakistan that used 34 clusters to select 1150 study subjects aged &gt; or =3 months. We analysed variance to separate its components. Most of intraclass correlation coefficients were in the range of 0.01-0.05. For proportion of injections received during last 3 months, mean number of injections received and health seeking during the past 3 months the intraclass correlation coefficients were 0.02, 0.04 and 0.02, respectively. These estimates can be useful in designing cluster surveys and cluster randomized trials for injection safety in Pakistan and other developing countries.-Estimates of intraclass correlation coefficient and design effect for surveys and cluster randomized trials on injection use in Pakistan and developing countries.",1
"The matched case-control design is frequently used in the study of complex disorders and can result in significant gains in efficiency, especially in the context of measuring biomarkers; however, risk prediction in this setting is not straightforward. We propose an inverse-probability weighting approach to estimate the predictive ability associated with a set of covariates. In particular, we propose an algorithm for estimating the summary index, area under the curve corresponding to the Receiver Operating Characteristic curve associated with a set of pre-defined covariates for predicting a binary outcome. By combining data from the parent cohort with that generated in a matched case control study, we describe methods for estimation of the population parameters of interest and the corresponding area under the curve. We evaluate the bias associated with the proposed methods in simulations by considering a range of parameter settings. We illustrate the methods in two data applications: (1) a prospective cohort study of cardiovascular disease in women, the Women's Health Study, and (2)?a matched case-control study nested within the Nurses' Health Study aimed at risk prediction of invasive breast cancer.-Estimating the receiver operating characteristic curve in matched case control studies.",0
"To determine rates of major bleeding by level of kidney function for older adults with atrial fibrillation starting warfarin. Retrospective cohort study. Community based, using province wide laboratory and administrative data in Alberta, Canada. 12,403 adults aged 66 years or more, with atrial fibrillation who started warfarin treatment between 1 May 2003 and 31 March 2010 and had a measure of kidney function at baseline. Kidney function was estimated using the Chronic Kidney Disease Epidemiology Collaboration equation and participants were categorised based on estimated glomerular filtration rate (eGFR): ? 90, 60-89, 45-59, 30-44, 15-29, &lt;15 mL/min/1.73 m(2). We excluded participants with end stage renal disease (dialysis or renal transplant) at baseline. Admission to hospital or visit to an emergency department for major bleeding (intracranial, upper and lower gastrointestinal, or other). Of 12,403 participants, 45% had an eGFR &lt;60 mL/min/1.73 m(2). Overall, 1443 (11.6%) experienced a major bleeding episode over a median follow-up of 2.1 (interquartile range: 1.0-3.8) years. During the first 30 days of warfarin treatment, unadjusted and adjusted rates of major bleeding were higher at lower eGFR (P for trend &lt;0.001 and 0.001, respectively). Adjusted bleeding rates per 100 person years were 63.4 (95% confidence interval 24.9 to 161.6) in participants with eGFR &lt;15 mL/min/1.73 m(2) compared with 6.1 (1.9 to 19.4) among those with eGFR &gt;90 mL/min/1.73 m(2) (adjusted incidence rate ratio 10.3, 95% confidence interval 2.3 to 45.5). Similar associations were observed at more than 30 days after starting warfarin, although the magnitude of the increase in rates across eGFR categories was attenuated. Across all eGFR categories, adjusted rates of major bleeding were consistently higher during the first 30 days of warfarin treatment compared with the remainder of follow-up. Increases in major bleeding rates were largely due to gastrointestinal bleeding (3.5-fold greater in eGFR &lt;15 mL/min/1.73 m(2) compared with ? 90 mL/min/1.73 m(2)). Intracranial bleeding was not increased with worsening kidney function. Reduced kidney function was associated with an increased risk of major bleeding among older adults with atrial fibrillation starting warfarin; excess risks from reduced eGFR were most pronounced during the first 30 days of treatment. Our results support the need for careful consideration of the bleeding risk relative to kidney function when assessing the risk-benefit ratio of warfarin treatment in people with chronic kidney disease and atrial fibrillation, particularly in the first 30 days of treatment.-The association between kidney function and major bleeding in older adults with atrial fibrillation starting warfarin treatment: population based observational study.",0
Symposium on Community Intervention Trials,1
"The intraclass correlation coefficient (ICC) is a fundamental parameter of interest in cluster randomized trials as it can greatly affect statistical power. We compare common methods of estimating the ICC in cluster randomized trials with binary outcomes, with a specific focus on their application to community-based cancer prevention trials with primary outcome of self-reported cancer screening. Using three real data sets from cancer screening intervention trials with different numbers and types of clusters and cluster sizes, we obtained point estimates and 95% confidence intervals for the ICC using five methods: the analysis of variance estimator, the Fleiss-Cuzick estimator, the Pearson estimator, an estimator based on generalized estimating equations and an estimator from a random intercept logistic regression model. We compared estimates of the ICC for the overall sample and by study condition. Our results show that ICC estimates from different methods can be quite different, although confidence intervals generally overlap. The ICC varied substantially by study condition in two studies, suggesting that the common practice of assuming a common ICC across all clusters in the trial is questionable. A simulation study confirmed pitfalls of erroneously assuming a common ICC. Investigators should consider using sample size and analysis methods that allow the ICC to vary by study condition.-Comparison of methods for estimating the intraclass correlation coefficient for binary responses in cancer prevention cluster randomized trials.",1
"The purpose of the study is to examine variation in adolescent drug-use patterns by using latent class regression analysis and evaluate the properties of an estimating-equations approach under different cluster-unit trial designs. A set of second-order estimating equations for latent class models under the cluster-unit trial design are proposed. This approach models the correlation within subclusters (drug-use behaviors), but ignores the correlation within clusters (communities). A robust covariance estimator is proposed that accounts for within-cluster correlation. Performance of this approach is addressed through a Monte Carlo simulation study, and practical implications are illustrated by using data from the National Evaluation of the Enforcing Underage Drinking Laws Randomized Community Trial. The example shows that the proposed method provides useful information about the heterogeneous nature of drug use by identifying two subtypes of adolescent problem drinkers. A Monte Carlo simulation study supports the proposed estimation method by suggesting that the latent class model parameters were unbiased for 30 or more clusters. Consistent with other studies of generalized estimating equation (GEE) estimators, the robust covariance estimator tended to underestimate the true variance of regression parameters, but the degree of inflation in the test size was relatively small for 70 clusters and only slightly inflated for 30 clusters. The proposed model for studying adolescent drug use provides an alternative to standard diagnostic criteria, focusing on the nature of the drug-use profile, rather than relying on univariate symptom counts. The second-order GEE-type estimation procedure provided a computationally feasible approach that performed well for a moderate number of clusters and was consistent with prior studies of GEE under the generalized linear model framework.-Modeling adolescent drug-use patterns in cluster-unit trials with multiple sources of correlation using robust latent class regressions.",1
"In this paper, the optimal sample sizes at the cluster and person levels for each of two treatment arms are obtained for cluster randomized trials where the cost-effectiveness of treatments on a continuous scale is studied. The optimal sample sizes maximize the efficiency or power for a given budget or minimize the budget for a given efficiency or power. Optimal sample sizes require information on the intra-cluster correlations (ICCs) for effects and costs, the correlations between costs and effects at individual and cluster levels, the ratio of the variance of effects translated into costs to the variance of the costs (the variance ratio), sampling and measuring costs, and the budget. When planning, a study information on the model parameters usually is not available. To overcome this local optimality problem, the current paper also presents maximin sample sizes. The maximin sample sizes turn out to be rather robust against misspecifying the correlation between costs and effects at the cluster and individual levels but may lose much efficiency when misspecifying the variance ratio. The robustness of the maximin sample sizes against misspecifying the ICCs depends on the variance ratio. The maximin sample sizes are robust under misspecification of the ICC for costs for realistic values of the variance ratio greater than one but not robust under misspecification of the ICC for effects. Finally, we show how to calculate optimal or maximin sample sizes that yield sufficient power for a test on the cost-effectiveness of an intervention.-Sample size calculation in   cost-effectiveness cluster randomized   trials: optimal and maximin approaches.",1
"Within cluster randomized trials no algorithms exist to generate a full enumeration of a block randomization, balancing for covariates across treatment arms. Furthermore, often for practical reasons multiple blocks are required to fully randomize a study, which may not have been well balanced within blocks. We present a convenient and easy to use randomization tool to undertake allocation concealed block randomization. Our algorithm highlights allocations that minimize imbalance between treatment groups across multiple baseline covariates. We demonstrate the algorithm using a cluster randomized trial in primary care (the PRE-EMPT Study) and show that the software incorporates a trade off between independent random allocations that were likely to be imbalanced, and predictable deterministic approaches that would minimise imbalance. We extend the methodology of single block randomization to allocate to multiple blocks conditioning on previous allocations. The algorithm is included as Additional file 1 and we advocate its use for robust randomization within cluster randomized trials.-Balance algorithm for cluster randomized trials.",1
"Type 2 diabetes, which is highly prevalent in older Mexican Americans, may influence cognitive functioning. We examined the association of diabetes with decline in global cognitive function and memory function over a 2-year period. Study subjects were derived from an existing cohort of Latinos aged 60 and over in the SALSA project (n=1,789). Statistical analysis was conducted using logistic regression and a generalized estimating equation (GEE). Logistic regression analysis indicated that baseline diabetes was a significant predictor of major cognitive impairment in Modified Mini Mental State Exam (3MSE) (OR=1.68, 95% CI=1.21, 2.34) and word-list test (OR=1.31, 95% CI=0.99, 1.75). GEE analysis showed that there was no significant difference between diabetic and nondiabetic subjects in change of cognitive scores over 2 years (3MSE, mean=-0.58, 95% CI=-1.48, 0.32; word-list test, mean=-0.10, 95% CI=-0.32, 0.11). More diabetic complications were associated with major cognitive decline among diabetic subjects. Research on long-term impact of treatment for type 2 diabetes is warranted.-Impact of diabetes on cognitive function among older Latinos: a population-based cohort study.",0
Standardized Mean Differences in Two-Level Cross-Classified Random Effects Models,2
"Community intervention trials are becoming increasingly popular as a tool for evaluating the effectiveness of health education and intervention strategies. Typically, units such as households, schools, towns, counties, are randomized to receive either intervention or control, then outcomes are measured on individuals within each of the units of randomization. It is well recognized that the design and analysis of such studies must account for the clustering of subjects within the units of randomization. Furthermore, there are usually both subject level and cluster level covariates that must be considered in the modelling process. While suitable methods are available for continuous outcomes, data analysis is more complicated when dichotomous outcomes are measured on each subject. This paper will compare and contrast several of the available methods that can be applied in such settings, including random effects models, generalized estimating equations and methods based on the calculation of 'design effects', as implemented in the computer package SUDAAN. For completeness, the paper will also compare these methods of analysis with more simplistic approaches based on the summary statistics. All the methods will be applied to a case study based on an adolescent anti-smoking intervention in Australia. The paper concludes with some general discussion and recommendations for routine design and analysis.-Analysis of dichotomous outcome data for community intervention studies.",1
In reply: When and how should we cluster and cross over: methodological and ethical issues (letters 1 and 2).,1
"Successful recruitment in clinical trials for chronic pain conditions is challenging, especially in women with provoked vulvodynia due to reluctance in discussing pain associated with sexual intercourse. The most successful recruitment methods and the characteristics of women reached with these methods are unknown. To compare the effectiveness and efficiency of four recruitment methods and to determine socioeconomic predictors for successful enrollment in a National Institutes of Health-sponsored multicenter clinical trial evaluating a gabapentin intervention in women with provoked vulvodynia. Recruitment methods utilized mass mailing, media, clinician referrals and community outreach. Effectiveness (number of participants enrolled) and efficiency (proportion screened who enrolled) were determined. Socioeconomic variables including race, educational level, annual household income, relationship status, age, menopausal status and employment status were also evaluated regarding which recruitment strategies were best at targeting specific cohorts. Of 868 potential study participants, 219 were enrolled. The most effective recruitment method in enrolling participants was mass mailing ( p &lt; 0.001). There were no statistically significant differences in efficiency between recruitment methods ( p = 0.11). Relative to clinician referral, black women were 13 times as likely to be enrolled through mass mailing (adjusted odds ratio 12.5, 95% confidence interval, 3.6-43.1) as white women. There were no differences in enrollment according to educational level, annual income, relationship status, age, menopausal status, or employment status and recruitment method. In this clinical trial, mass mailing was the most effective recruitment method. Race of participants enrolled in a provoked vulvodynia trial was related to the recruitment method.-Recruitment methods in a clinical trial of provoked vulvodynia: Predictors of enrollment.",0
"Studies in health research are commonly carried out in clustered settings, where the individual response data are correlated within clusters. Estimation and modelling of the extent of between-cluster variation contributes to understanding of the current study and to design of future studies. It is common to express between-cluster variation as an intracluster correlation coefficient (ICC), since this measure is directly comparable across outcomes. ICCs are generally reported unaccompanied by confidence intervals. In this paper, we describe a Bayesian modelling approach to interval estimation of the ICC. The flexibility of this framework allows useful extensions which are not easily available in existing methods, for example assumptions other than Normality for continuous outcome data, adjustment for individual-level covariates and simultaneous interval estimation of several ICCs. There is also the opportunity to incorporate prior beliefs on likely values of the ICC. The methods are exemplified using data from a cluster randomized trial.-Constructing intervals for the intracluster correlation coefficient using Bayesian modelling, and application in cluster randomized trials.",1
"In cluster randomized trials, the intraclass correlation coefficient (ICC) is classically used to measure clustering. When the outcome is binary, the ICC is known to be associated with the prevalence of the outcome. This association challenges its interpretation and can be problematic for sample size calculation. To overcome these situations, Crespi et?al. extended a coefficient named R, initially proposed by Rosner for ophthalmologic data, to cluster randomized trials. Crespi et?al. asserted that R may be less influenced by the outcome prevalence than is the ICC, although the authors provided only empirical data to support their assertion. They also asserted that ""the traditional ICC approach to sample size determination tends to overpower studies under many scenarios, calling for more clusters than truly required"", although they did not consider empirical power. The aim of this study was to investigate whether R could indeed be considered independent of the outcome prevalence. We also considered whether sample size calculation should be better based on the R coefficient or the ICC. Considering the particular case of 2 individuals per cluster, we theoretically demonstrated that R is not symmetrical around the 0.5 prevalence value. This in itself demonstrates the dependence of R on prevalence. We also conducted a simulation study to explore the case of both fixed and variable cluster sizes greater than 2. This simulation study demonstrated that R decreases when prevalence increases from 0 to 1. Both the analytical and simulation results demonstrate that R depends on the outcome prevalence. In terms of sample size calculation, we showed that an approach based on the ICC is preferable to an approach based on the R coefficient because with the former, the empirical power is closer to the nominal one. Hence, the R coefficient does not outperform the ICC for binary outcomes because it does not offer any advantage over the ICC.-Is the R coefficient of interest in cluster randomized trials with a binary outcome?",1
"Large comparative clinical trials usually target a wide-range of patients population in which subgroups exist according to certain patients' characteristics. Often, scientific knowledge or existing empirical data support the assumption that patients' improvement is larger among certain subgroups than others. Such information can be used to design a more cost-effective clinical trial. The goal of the article is to use such information to design a more cost-effective clinical trial. A two-stage sample-enrichment design strategy is proposed that begins with enrollment from certain subgroup of patients and allows the trial to be terminated for futility in that subgroup. Simulation studies show that the two-stage sample-enrichment strategy is cost-effective if indeed the null hypothesis of no treatment improvement is true, as also so illustrated with data from a completed trial of calcium to prevent preeclampsia. Feasibility of the proposed enrichment design relies on the knowledge prior to the start of the trial that certain patients can benefit more than others from the treatment. Prolonged accrual and longer-waited outcomes may hinder utilization of the proposed design. The two-stage sample-enrichment approach borrows strength from treatment heterogeneity among target patients in a large-scale comparative clinical trial, and is more cost-effective if the treatment arms are indeed of no difference.-A threshold sample-enrichment approach in a clinical trial with heterogeneous subpopulations.",0
"To summarise evidence on the association between white rice consumption and risk of type 2 diabetes and to quantify the potential dose-response relation. Meta-analysis of prospective cohort studies. Searches of Medline and Embase databases for articles published up to January 2012 using keywords that included both rice intake and diabetes; further searches of references of included original studies. Included studies were prospective cohort studies that reported risk estimates for type 2 diabetes by rice intake levels. Relative risks were pooled using a random effects model; dose-response relations were evaluated using data from all rice intake categories in each study. Four articles were identified that included seven distinct prospective cohort analyses in Asian and Western populations for this study. A total of 13,284 incident cases of type 2 diabetes were ascertained among 352,384 participants with follow-up periods ranging from 4 to 22 years. Asian (Chinese and Japanese) populations had much higher white rice consumption levels than did Western populations (average intake levels were three to four servings/day versus one to two servings/week). The pooled relative risk was 1.55 (95% confidence interval 1.20 to 2.01) comparing the highest with the lowest category of white rice intake in Asian populations, whereas the corresponding relative risk was 1.12 (0.94 to 1.33) in Western populations (P for interaction=0.038). In the total population, the dose-response meta-analysis indicated that for each serving per day increment of white rice intake, the relative risk of type 2 diabetes was 1.11 (1.08 to 1.14) (P for linear trend&lt;0.001). Higher consumption of white rice is associated with a significantly increased risk of type 2 diabetes, especially in Asian (Chinese and Japanese) populations.-White rice consumption and risk of type 2 diabetes: meta-analysis and systematic review.",0
"Alcohol intervention studies that allocate intact social groups to study conditions require adjustment to the usual analytic methods to account for the positive intraclass correlation that exists in such groups. This article presents intraclass correlations for measures related to alcohol use among young adults and discusses the use of those estimates to plan new studies. Young adults aged 18-20 were selected at random from driver's license lists in each of the 15 communities participating in the Communities Mobilizing for Change on Alcohol project. Respondents were surveyed by telephone to assess their drinking habits and other factors related to alcohol use. Community-level intraclass correlations were computed for those measures, both prior to and after adjustment for person- and community-level covariates. The community-level intraclass correlations tend to be small, with larger values for belief and attitude items than for self-reported behaviors. Even so, correlations of this magnitude can have important deleterious effects on the power to detect important treatment effects in an otherwise well-designed and well-executed study. Adjustment for person-level covariates often reduced those correlations, and adjustment for community-level covariates often reduced them substantially. There is measurable variation in measures related to alcohol use among young adults that is attributable to their community of residence. With adjustment for selected person- and community-level covariates, the magnitude of these correlations can be sharply reduced allowing the investigator to plan a more efficient community trial.-Intraclass correlation among measures related to alcohol use by young adults: estimates, correlates and applications in intervention studies.",1
"Many studies have investigated the relation between magnesium and iron intake and diabetes and, separately, between diabetes and pancreatic cancer. However, no known study has examined the direct association of magnesium and iron intake with pancreatic cancer risk. The authors obtained magnesium and iron intake data using food frequency questionnaires from the US male Health Professionals Follow-up Study, which began in 1986. During 851,476 person-years and 20 years of follow-up, 300 pancreatic cancer cases were documented. Cox proportional hazards models were used to estimate relative risks, adjusting for age, smoking, and body mass index. No associations were observed between magnesium or iron intake and pancreatic cancer (highest vs. lowest quintile: relative risk (RR) = 0.94, 95% confidence interval (CI): 0.66, 1.32 and RR = 0.93, 95% CI: 0.65, 1.34, respectively). Similarly, iron or magnesium supplement use was not related to pancreatic cancer. A statistically significant inverse relation was noted between magnesium and pancreatic cancer for subjects with a body mass index of &gt; or =25 kg/m(2) (RR = 0.67, 95% CI: 0.46, 0.99; P-trend = 0.04). Although, overall, no relation between magnesium or iron intake and pancreatic cancer was observed in this cohort of men, an inverse association with magnesium was suggested among overweight individuals, which should be examined in other studies.-A prospective study of magnesium and iron intake and pancreatic cancer in men.",0
"This article offers an applied review of key issues and methods for the analysis of longitudinal panel data in the presence of missing values. The authors consider the unique challenges associated with attrition (survey dropout), incomplete repeated measures, and unknown observations of time. Using simulated data based on 4 waves of the Marital Instability Over the Life Course Study (n = 2,034), they applied a fixed effect regression model and an event-history analysis with time-varying covariates. They then compared results for analyses with nonimputed missing data and with imputed data both in long and in wide structures. Imputation produced improved estimates in the event-history analysis but only modest improvements in the estimates and standard errors of the fixed effects analysis. Factors responsible for differences in the value of imputation are examined, and recommendations for handling missing values in panel data are presented.-Handling Missing Values in Longitudinal Panel Data With Multiple Imputation.",1
"Vitamin D measurements are influenced by seasonal variation and specific assay used. Motivated by multicenter studies of associations of vitamin D with cancer, we formulated an analytic framework for matched case-control data that accounts for seasonal variation and calibrates to a reference assay. Calibration data were obtained from controls sampled within decile strata of the uncalibrated vitamin D values. Seasonal sine-cosine series were fit to control data. Practical findings included the following: (1) failure to adjust for season and calibrate increased variance, bias, and mean square error and (2) analysis of continuous vitamin D requires a variance adjustment for variation in the calibration estimate. An advantage of the continuous linear risk model is that results are independent of the reference date for seasonal adjustment. (3) For categorical risk models, procedures based on categorizing the seasonally adjusted and calibrated vitamin D have near nominal operating characteristics; estimates of log odds ratios are not robust to choice of seasonal reference date, however. Thus, public health recommendations based on categories of vitamin D should also define the time of year to which they refer. This work supports the use of simple methods for calibration and seasonal adjustment and is informing analytic approaches for the multicenter Vitamin D Pooling Project for Breast and Colorectal Cancer. Published 2016. This article has been contributed to by US Government employees and their work is in the public domain in the USA.-Calibration and seasonal adjustment for matched case-control studies of vitamin D and cancer.",0
"Fecal microbiota transplantation (FMT) has been shown to be safe and effective in treating refractory or relapsing C. difficile infection (CDI), but its use has been limited by practical barriers. We recently reported a small preliminary feasibility study using orally administered frozen fecal capsules. Following these early results, we now report our clinical experience in a large cohort with structured follow-up. We prospectively followed a cohort of patients with recurrent or refractory CDI who were treated with frozen, encapsulated FMT at our institution. The primary endpoint was defined as clinical resolution whilst off antibiotics for CDI at 8?weeks after last capsule ingestion. Safety was defined as any FMT-related adverse event grade 2 or above. Overall, 180 patients aged 7-95 years with a minimal follow-up of 8?weeks were included in the analysis. CDI resolved in 82?% of patients after a single treatment, rising to a 91?% cure rate with two treatments. Three adverse events Grade 2 or above, deemed related or possibly related to FMT, were observed. We confirm the effectiveness and safety of oral administration of frozen encapsulated fecal material, prepared from unrelated donors, in treating recurrent CDI. Randomized studies and FMT registries are still needed to ascertain long-term safety.-Oral, frozen fecal microbiota transplant (FMT) capsules for recurrent Clostridium difficile infection.",0
"A mixed-effects propensity adjustment is described that can reduce bias in longitudinal studies involving non-equivalent comparison groups. There are two stages in this data analytic strategy. First, a model of propensity for treatment intensity examines variables that distinguish among subjects who receive various ordered doses of treatment across time using mixed-effects ordinal logistic regression. Second, the effectiveness model examines multiple times until recurrence to compare the ordered doses using a mixed-effects grouped-time survival model. Effectiveness analyses are initially stratified by propensity quintile. Then the quintile-specific results are pooled, assuming that there is not a propensity x treatment interaction. A Monte Carlo simulation study compares bias reduction in fully specified propensity model relative to misspecified models. In addition, type I error rate and statistical power are examined. The approach is illustrated by applying it to a longitudinal, observational study of maintenance treatment of major depression.-Bias reduction in effectiveness analyses of longitudinal ordinal doses with a mixed-effects propensity adjustment.",0
"The Trial for Activity in Adolescent Girls (TAAG) is a multi-center group-randomized trial to reduce the usual decline in moderate to vigorous physical activity (MVPA) among middle-school girls. In group-randomized trials, the group-level intraclass correlation (ICC) has a strong inverse relationship to power and a good estimate of ICC is needed to determine sample size. As a result, we conducted a substudy to estimate the school-level ICC for intensity-weighted minutes of MVPA measured using an accelerometer. To estimate the ICC, each of six sites recruited two schools and randomly selected 45 eighth grade girls from each school; 80.7% participated. Each girl wore an Actigraph accelerometer for 7 d. Readings above 1500 counts per half minute were counted as MVPA. These counts were converted into metabolic equivalents (MET) and summed over 6 a.m. to midnight to provide MET-minutes per 18-h day of MVPA. Minutes of MVPA per 18-h day also were calculated ignoring the MET value. The unadjusted school-level ICC for minutes of MVPA was 0.0205 (95%CI: -0.0079, 0.1727) and for MET-minutes of MVPA was 0.0045 (95% CI: -0.0147, 0.1145). Adjustment for age and BMI had no measurable effect, whereas adjustment for ethnicity reduced both ICC; adjusted values were 0.0175 (95% CI: -0.0092, 0.1622) for minutes of MVPA and 0.0000 (95% CI: -0.0166, 0.0968) for MET-minutes of MVPA. This information was used to calculate the number of schools and girls needed for TAAG to have 90% power to detect a 50% reduction in the decline of MET-minutes of MVPA between sixth and eighth grade. The results called for 36 schools in TAAG, with 120 girls invited for measurements at each school, and a minimum participation rate of 80%.-School-level intraclass correlation for physical activity in adolescent girls.",1
"Telomeres are repeating regions of DNA that cap chromosomes. They shorten over the mammalian life span, especially in the presence of oxidative stress and inflammation. Telomeres may play a direct role in cell senescence, serving as markers of premature vascular aging. Leukocyte telomere length (LTL) may be associated with premature vascular brain injury and cerebral atrophy. However, reports have been inconsistent, especially among minority populations with a heavy burden of illness related to vascular aging. We examined associations between LTL and magnetic resonance imaging in 363 American Indians aged 64-93 years from the Strong Heart Study (1989-1991) and its ancillary study, Cerebrovascular Disease and Its Consequences in American Indians (2010-2013). Our results showed significant associations of LTL with ventricular enlargement and the presence of white matter hyperintensities. Secondary models indicated that renal function may mediate these associations, although small case numbers limited inference. Hypertension and diabetes showed little evidence of effect modification. Results were most extreme among participants who evinced the largest decline in LTL. Although this study was limited to cross-sectional comparisons, it represents (to our knowledge) the first consideration of associations between telomere length and brain aging in American Indians. Findings suggest a relationship between vascular aging by cell senescence and severity of brain disease.-Telomere Length and Magnetic Resonance Imaging Findings of Vascular Brain Injury and Central Brain Atrophy: The Strong Heart Study.",0
"Adequately powered sample size calculations for cluster randomized trials primarily depend on the event rate variability, effect size, average cluster size, and intracluster correlation (ICC). Furthermore, an ICC estimate depends on event rate variability among clusters, cluster size, and number of clusters. We evaluated the impact on ICC estimates of event rates, event rate variations, cluster size, and cluster size variations for different numbers of clusters. We also evaluated how the event rate changes at the end of the trial affect ICC estimates. We created one simulation exercise to investigate how different event rates, event rate variations, cluster size, and cluster size variations impact ICC estimates and 95% confidence intervals. A separate simulation exercise in four different trial scenarios examined the impact of an intervention or drug effect in the intervention group on ICC estimates, 95% confidence intervals, and on sample size. The first simulation results suggest that the ICC value depends upon the event rate and event rate variations in addition to the cluster size, cluster size variations, and number of clusters. The second simulation exercise suggested that adjusting the sample size will help to preserve the appropriate power at the end of the trial.-Intracluster correlation adjustments to maintain power in cluster trials for binary outcomes.",1
"Previous research has compared methods of estimation for fitting multilevel models to binary data, but there are reasons to believe that the results will not always generalize to the ordinal case. This article thus evaluates (a) whether and when fitting multilevel linear models to ordinal outcome data is justified and (b) which estimator to employ when instead fitting multilevel cumulative logit models to ordinal data, maximum likelihood (ML), or penalized quasi-likelihood (PQL). ML and PQL are compared across variations in sample size, magnitude of variance components, number of outcome categories, and distribution shape. Fitting a multilevel linear model to ordinal outcomes is shown to be inferior in virtually all circumstances. PQL performance improves markedly with the number of ordinal categories, regardless of distribution shape. In contrast to binary data, PQL often performs as well as ML when used with ordinal data. Further, the performance of PQL is typically superior to ML when the data include a small to moderate number of clusters (i.e., ? 50 clusters).-Fitting multilevel models with ordinal outcomes: performance of alternative specifications and methods of estimation.",1
Cluster-randomized controlled trials: part 1.,1
"The Selenium and Vitamin E Cancer Prevention Trial (SELECT) was a randomized, double-blind, placebo-controlled prostate cancer prevention study funded by the National Cancer Institute (NCI) and conducted by the Southwest Oncology Group (SWOG). A total of 35,533 men were assigned randomly to one of the four treatment groups (vitamin E + placebo, selenium + placebo, vitamin E + selenium, and placebo + placebo). The independent Data and Safety Monitoring Committee (DSMC) recommended the discontinuation of study supplements because of the lack of efficacy for risk reduction and because futility analyses demonstrated no possibility of benefit of the supplements to the anticipated degree (25% reduction in prostate cancer incidence) with additional follow-up. Study leadership agreed that the randomized trial should be terminated but believed that the cohort should be maintained and followed as the additional follow-up would contribute important information to the understanding of the biologic consequences of the intervention. Since the participants no longer needed to be seen in person to assess acute toxicities or to be given study supplements, it was determined that the most efficient and cost-effective way to follow them was via a central coordinated effort. A number of changes were necessary at the local Study Sites and SELECT Statistical Center to transition to following participants via a Central Coordinating Center. We describe the transition process from a randomized clinical trial to the observational Centralized Follow-Up (CFU) study. The process of transitioning SELECT, implemented at more than 400 Study Sites across the United States, Canada, and Puerto Rico, entailed many critical decisions and actions including updates to online documents such as the SELECT Workbench and Study Manual, a protocol amendment, reorganization of the Statistical Center, creation of a Transition Committee, development of materials for SELECT Study Sites, development of procedures to close Study Sites, and revision of data collection procedures and the process by which to contact participants. At the time of the publication of the primary SELECT results in December 2008, there were 32,569 men alive and currently active in the trial. As of 31 December 2011, 17,761 participants had been registered to the CFU study. This number is less than had been anticipated due to unforeseen difficulties with local Study Site institutional review boards (IRBs). However, from this cohort, we estimate that an additional 580 prostate cancer cases and 215 Gleason 7 or higher grade cancers will be identified. Over 109,000 individual items have been mailed to participants. Active SELECT ancillary studies have continued. The substantial SELECT biorepository is available to researchers; requests to use the specimens are reviewed for feasibility and scientific merit. As of April 2012, 12 proposals had been approved. The accrual goal of the follow-up study was not met, limiting our power to address the study objectives satisfactorily. The CFU study is also dependent on a number of factors including continued funding, continued interest of investigators in the biorepository, and the continued contribution of the participants. Our experience may be less pertinent to investigators who wish to follow participants in a treatment trial or participants in prevention trials in other medical areas. Extended follow-up of participants in prevention research is important to study the long-term effects of the interventions, such as those used in SELECT. The approach taken by SELECT investigators was to continue to follow participants centrally via an annual questionnaire and with a web-based option. The participants enrolled in the CFU study represent a large, well-characterized, generally healthy cohort. The CFU has enabled us to collect additional prostate and other cancer endpoints and longer follow-up on the almost 18,000 participants enrolled. The utility of the extensive biorepository that was developed during the course of the SELECT is enhanced by longer follow-up.-Moving a randomized clinical trial into an observational cohort.",0
"We consider efficient study designs to estimate sensitivity and specificity of a candidate diagnostic or screening test. Our focus is the setting in which the candidate test is inexpensive to administer compared to evaluation of disease status, and the test results, available in a large cohort, can be used as a basis for sampling subjects for verification of disease status. We examine designs in which disease status is verified in a sample chosen so as to optimize estimation of either sensitivity or specificity. We then propose a sequential design in which the first step of sampling is conducted to efficiently estimate specificity. If the candidate test is determined to be of sufficient specificity, then step two of sampling is conducted to estimate sensitivity. We propose estimators based on this sequential sampling scheme, and show that the performance of these estimators is excellent. We develop sample size calculations for the sequential design, and show that this design, in most situations, compares favourably in terms of expected sample size to a fixed size design.-A sequential design to estimate sensitivity and specificity of a diagnostic or screening test.",0
"We highlight common problems in the application of random treatment assignment in large-scale program evaluation. Random assignment is the defining feature of modern experimental design, yet errors in design, implementation, and analysis often result in real-world applications not benefiting from its advantages. The errors discussed here cover the control of variability, levels of randomization, size of treatment arms, and power to detect causal effects, as well as the many problems that commonly lead to post-treatment bias. We illustrate these issues by identifying numerous serious errors in the Medicare Health Support evaluation and offering recommendations to improve the design and analysis of this and other large-scale randomized experiments.-Avoiding randomization failure in program evaluation, with application to the Medicare Health Support program.",1
"Stepped wedge randomised trial designs involve sequential roll-out of an intervention to participants (individuals or clusters) over a number of time periods. By the end of the study, all participants will have received the intervention, although the order in which participants receive the intervention is determined at random. The design is particularly relevant where it is predicted that the intervention will do more good than harm (making a parallel design, in which certain participants do not receive the intervention unethical) and/or where, for logistical, practical or financial reasons, it is impossible to deliver the intervention simultaneously to all participants. Stepped wedge designs offer a number of opportunities for data analysis, particularly for modelling the effect of time on the effectiveness of an intervention. This paper presents a review of 12 studies (or protocols) that use (or plan to use) a stepped wedge design. One aim of the review is to highlight the potential for the stepped wedge design, given its infrequent use to date. Comprehensive literature review of studies or protocols using a stepped wedge design. Data were extracted from the studies in three categories for subsequent consideration: study information (epidemiology, intervention, number of participants), reasons for using a stepped wedge design and methods of data analysis. The 12 studies included in this review describe evaluations of a wide range of interventions, across different diseases in different settings. However the stepped wedge design appears to have found a niche for evaluating interventions in developing countries, specifically those concerned with HIV. There were few consistent motivations for employing a stepped wedge design or methods of data analysis across studies. The methodological descriptions of stepped wedge studies, including methods of randomisation, sample size calculations and methods of analysis, are not always complete. While the stepped wedge design offers a number of opportunities for use in future evaluations, a more consistent approach to reporting and data analysis is required.-The stepped wedge trial design: a systematic review.",3
"The problem of confidence interval construction for the odds ratio of two independent binomial samples is considered. Two methods of eliminating the nuisance parameter from the exact likelihood, conditioning and maximization, are described. A conditionally exact tail method exists by putting together upper and lower bounds. A shorter interval can be obtained by simultaneous consideration of both tails. We present here new methods that extend the tail and simultaneous approaches to the maximized likelihood. The methods are unbiased and applicable to case-control data, for which the odds ratio is important. The confidence interval procedures are compared unconditionally for small sample sizes in terms of their expected length and coverage probability. A Bayesian confidence interval method and a large-sample chi2 procedure are included in the comparisons.-Unbiased confidence intervals for the odds ratio of two independent binomial samples with application to case-control data.",0
"With the emergence of rich information on biomarkers after treatments, new types of prognostic tools are being developed: dynamic prognostic tools that can be updated at each new biomarker measurement. Such predictions are of interest in oncology where after an initial treatment, patients are monitored with repeated biomarker data. However, in such setting, patients may receive second treatments to slow down the progression of the disease. This paper aims to develop and validate dynamic individual predictions that allow the possibility of a new treatment in order to help understand the benefit of initiating new treatments during the monitoring period. The prediction of the event in the next x years is done under two scenarios: (1) the patient initiates immediately a second treatment, (2) the patient does not initiate any treatment in the next x years. Predictions are derived from shared random-effect models. Applied to prostate cancer data, different specifications for the dependence between the prostate-specific antigen repeated measures, the initiation of a second treatment (hormonal therapy), and the risk of clinical recurrence are investigated and compared. The predictive accuracy of the dynamic predictions is evaluated with two measures (Brier score and prognostic cross-entropy) for which approximated cross-validated estimators are proposed.-Individualized dynamic prediction of prostate cancer recurrence with and without the initiation of a second treatment: Development and validation.",0
"Recreational physical activity has been both positively and inversely associated with cancer risk for postmenopausal women, acting presumably through hormonal mechanisms. Relatively little is known about the effects of exercise on postmenopausal steroid hormone levels. The authors evaluated the association between recreational activity and plasma steroid hormones among 623 US healthy, postmenopausal women in the Nurses' Health Study not using exogenous hormones at the time of blood draw (1989-1990). Participants self-reported recreational physical activity by questionnaire in 1986, 1988, and 1992. Plasma samples were assayed for estrogens, androgens, and sex hormone-binding globulin. Geometric mean hormone levels adjusted and not adjusted for body mass index were calculated. In general, estrogen and androgen levels were lower in the most- and the least-active women compared with those reporting moderate activity, suggesting a U-shaped relation. For example, estrone sulfate levels in quintiles 1-5 of metabolic equivalent task-hours were 197, 209, 222, 214, and 195 pg/mL, respectively. Tests for nonlinearity using polynomial regression were significant for several estrogens, androgens, and sex hormone-binding globulin (2-sided P &lt;or= 0.01). These results suggest the possibility of a nonlinear relation between recreational physical activity and hormone levels in postmenopausal women.-Recreational physical activity and steroid hormone levels in postmenopausal women.",0
"Interventional researchers face many design challenges when assessing intervention implementation in real-world settings. Intervention implementation requires holding fast on internal validity needs while incorporating external validity considerations (such as uptake by diverse subpopulations, acceptability, cost, and sustainability). Quasi-experimental designs (QEDs) are increasingly employed to achieve a balance between internal and external validity. Although these designs are often referred to and summarized in terms of logistical benefits, there is still uncertainty about (a) selecting from among various QEDs and (b) developing strategies to strengthen the internal and external validity of QEDs. We focus here on commonly used QEDs (prepost designs with nonequivalent control groups, interrupted time series, and stepped-wedge designs) and discuss several variants that maximize internal and external validity at the design, execution and implementation, and analysis stages.-Selecting and Improving Quasi-Experimental Designs in Effectiveness and Implementation Research.",1
"To assess whether an association exists between financial links to the indoor tanning industry and conclusions of indoor tanning literature. Systematic review. PubMed, Embase, and Web of Science, up to 15 February 2019. Articles discussing indoor tanning and health were eligible for inclusion, with no article type restrictions (original research, systematic reviews, review articles, case reports, editorials, commentaries, and letters were all eligible). Basic science studies, articles describing only indoor tanning prevalence, non-English articles, and articles without full text available were excluded. 691 articles were included in analysis, including empiric articles (eg, original articles or systematic reviews) (357/691; 51.7%) and non-empiric articles letters (eg, commentaries, letters, or editorials) (334/691; 48.3%). Overall, 7.2% (50/691) of articles had financial links to the indoor tanning industry; 10.7% (74/691) articles favored indoor tanning, 3.9% (27/691) were neutral, and 85.4% (590/691) were critical of indoor tanning. Among the articles without industry funding, 4.4% (27/620) favored indoor tanning, 3.5% (22/620) were neutral, and 92.1% (571/620) were critical of indoor tanning. Among the articles with financial links to the indoor tanning industry, 78% (39/50) favored indoor tanning, 10% (5/50) were neutral, and 12% (6/50) were critical of indoor tanning. Support from the indoor tanning industry was significantly associated with favoring indoor tanning (risk ratio 14.3, 95% confidence interval 10.0 to 20.4). Although most articles in the indoor tanning literature are independent of industry funding, articles with financial links to the indoor tanning industry are more likely to favor indoor tanning. Public health practitioners and researchers need to be aware of and account for industry funding when interpreting the evidence related to indoor tanning. PROSPERO CRD42019123617.-Association between financial links to indoor tanning industry and conclusions of published studies on indoor tanning: systematic review.",0
"The intraclass correlation coefficient rho plays a key role in the design of cluster randomized trials. Estimates of rho obtained from previous cluster trials and used to inform sample size calculation in planned trials may be imprecise due to the typically small numbers of clusters in such studies. It may be useful to quantify this imprecision. This study used simulation to compare different methods for assigning bootstrap confidence intervals to rho for continuous outcomes from a balanced design. Data were simulated for combinations of numbers of clusters (10, 30, 50), intraclass correlation coefficients (0.001, 0.01, 0.05, 0.3) and outcome distributions (normal, non-normal continuous). The basic, bootstrap-t, percentile, bias corrected and bias corrected accelerated bootstrap intervals were compared with new methods using the basic and bootstrap-t intervals applied to a variance stabilizing transformation of rho. The standard bootstrap methods provided coverage levels for 95 per cent intervals that were markedly lower than the nominal level for data sets with only 10 clusters, and only provided close to 95 per cent coverage when there were 50 clusters. Application of the bootstrap-t method to the variance stabilizing transformation of rho improved upon the performance of the standard bootstrap methods, providing close to nominal coverage.-Non-parametric bootstrap confidence intervals for the intraclass correlation coefficient.",1
"Background The prevalence of low testosterone levels in men increases with age, as does the prevalence of decreased mobility, sexual function, self-perceived vitality, cognitive abilities, bone mineral density, and glucose tolerance, and of increased anemia and coronary artery disease. Similar changes occur in men who have low serum testosterone concentrations due to known pituitary or testicular disease, and testosterone treatment improves the abnormalities. Prior studies of the effect of testosterone treatment in elderly men, however, have produced equivocal results. Purpose To describe a coordinated set of clinical trials designed to avoid the pitfalls of prior studies and to determine definitively whether testosterone treatment of elderly men with low testosterone is efficacious in improving symptoms and objective measures of age-associated conditions. Methods We present the scientific and clinical rationale for the decisions made in the design of this set of trials. Results We designed The Testosterone Trials as a coordinated set of seven trials to determine if testosterone treatment of elderly men with low serum testosterone concentrations and symptoms and objective evidence of impaired mobility and/or diminished libido and/or reduced vitality would be efficacious in improving mobility (Physical Function Trial), sexual function (Sexual Function Trial), fatigue (Vitality Trial), cognitive function (Cognitive Function Trial), hemoglobin (Anemia Trial), bone density (Bone Trial), and coronary artery plaque volume (Cardiovascular Trial). The scientific advantages of this coordination were common eligibility criteria, common approaches to treatment and monitoring, and the ability to pool safety data. The logistical advantages were a single steering committee, data coordinating center and data and safety monitoring board, the same clinical trial sites, and the possibility of men participating in multiple trials. The major consideration in participant selection was setting the eligibility criterion for serum testosterone low enough to ensure that the men were unequivocally testosterone deficient, but not so low as to preclude sufficient enrollment or eventual generalizability of the results. The major considerations in choosing primary outcomes for each trial were identifying those of the highest clinical importance and identifying the minimum clinically important differences between treatment arms for sample size estimation. Potential limitations Setting the serum testosterone concentration sufficiently low to ensure that most men would be unequivocally testosterone deficient, as well as many other entry criteria, resulted in screening approximately 30 men in person to randomize one participant. Conclusion Designing The Testosterone Trials as a coordinated set of seven trials afforded many important scientific and logistical advantages but required an intensive recruitment and screening effort.-The Testosterone Trials: Seven coordinated trials of testosterone treatment in elderly men.",0
A Statistical Model for Misreported Binary Outcomes in Clustered RCTs of Education Interventions,1
"Polygenic risk scores (PRS) for breast cancer can be used to stratify the population into groups at substantially different levels of risk. Combining PRS and environmental risk factors will improve risk prediction; however, integrating PRS into risk prediction models requires evaluation of their joint association with known environmental risk factors. Analyses were based on data from 20 studies; datasets analysed ranged from 3453 to 23 104 invasive breast cancer cases and similar numbers of controls, depending on the analysed environmental risk factor. We evaluated joint associations of a 77-single nucleotide polymorphism (SNP) PRS with reproductive history, alcohol consumption, menopausal hormone therapy (MHT), height and body mass index (BMI). We tested the null hypothesis of multiplicative joint associations for PRS and each of the environmental factors, and performed global and tail-based goodness-of-fit tests in logistic regression models. The outcomes were breast cancer overall and by estrogen receptor (ER) status. The strongest evidence for a non-multiplicative joint associations with the 77-SNP PRS was for alcohol consumption (P-interaction = 0.009), adult height (P-interaction = 0.025) and current use of combined MHT (P-interaction = 0.038) in ER-positive disease. Risk associations for these factors by percentiles of PRS did not follow a clear dose-response. In addition, global and tail-based goodness of fit tests showed little evidence for departures from a multiplicative risk model, with alcohol consumption showing the strongest evidence for ER-positive disease (P = 0.013 for global and 0.18 for tail-based tests). The combined effects of the 77-SNP PRS and environmental risk factors for breast cancer are generally well described by a multiplicative model. Larger studies are required to confirm possible departures from the multiplicative model for individual risk factors, and assess models specific for ER-negative disease.-Joint associations of a polygenic risk score and environmental risk factors for breast cancer in the Breast Cancer Association Consortium.",0
"Gaussian graphical models have been widely used as an effective method for studying the conditional independency structure among genes and for constructing genetic networks. However, gene expression data typically have heavier tails or more outlying observations than the standard Gaussian distribution. Such outliers in gene expression data can lead to wrong inference on the dependency structure among the genes. We propose a l(1) penalized estimation procedure for the sparse Gaussian graphical models that is robustified against possible outliers. The likelihood function is weighted according to how the observation is deviated, where the deviation of the observation is measured based on its own likelihood. An efficient computational algorithm based on the coordinate gradient descent method is developed to obtain the minimizer of the negative penalized robustified-likelihood, where nonzero elements of the concentration matrix represents the graphical links among the genes. After the graphical structure is obtained, we re-estimate the positive definite concentration matrix using an iterative proportional fitting algorithm. Through simulations, we demonstrate that the proposed robust method performs much better than the graphical Lasso for the Gaussian graphical models in terms of both graph structure selection and estimation when outliers are present. We apply the robust estimation procedure to an analysis of yeast gene expression data and show that the resulting graph has better biological interpretation than that obtained from the graphical Lasso.-Robust Gaussian graphical modeling via l1 penalization.",0
"The design and analysis of cluster randomized trials can require more sophistication than individually randomized trials. However, the need for statistical methods that account for the clustered design has not always been appreciated, and past reviews have found widespread deficiencies in methodology and reporting. We reviewed cluster randomized trials of cancer screening interventions published in 1995-2010 to determine whether the use of appropriate statistical methods had increased over time. Literature searches yielded 50 articles reporting outcome analyses of cluster randomized trials of breast, cervix and colorectal cancer screening interventions. Of studies published in 1995-1999, 2000-2002, 2003-2006 and 2007-2010, 55% (6/11), 82% (9/11), 92% (12/13) and 60% (9/15) used appropriate analytic methods, respectively. Results were suggestive of a peak in 2003-2006 (p =.06) followed by a decline in 2007-2010 (p =.08). While the sample of studies was small, these results indicate that many cluster randomized trials of cancer screening interventions have had deficiencies in the application of correct statistical procedures for the outcome analysis, and that increased adoption of appropriate methods in the early and mid-2000's may not have been sustained.-Cluster randomized trials of cancer screening interventions: are appropriate statistical methods being used?",1
"Within the rich literature on generalized linear models, substantial efforts have been devoted to models for categorical responses that are either completely ordered or completely unordered. Few studies have focused on the analysis of partially ordered outcomes, which arise in practically every area of study, including medicine, the social sciences, and education. To fill this gap, we propose a new class of generalized linear models--the partitioned conditional model--that includes models for both ordinal and unordered categorical data as special cases. We discuss the specification of the partitioned conditional model and its estimation. We use an application of the method to a sample of the National Longitudinal Study of Youth to illustrate how the new method is able to extract from partially ordered data useful information about smoking youths that is not possible using traditional methods.-Generalized linear model for partially ordered data.",0
"While the mixed model approach to cluster randomization trials is relatively well developed, there has been less attention given to the design and analysis of population-averaged models for randomized and non-randomized cluster trials. We provide novel implementations of familiar methods to meet these needs. A design strategy that selects matching control communities based upon propensity scores, a statistical analysis plan for dichotomous outcomes based upon generalized estimating equations (GEE) with a design-based working correlation matrix, and new sample size formulae are applied to a large non-randomized study to reduce underage drinking. The statistical power calculations, based upon Wald tests for summary statistics, are special cases of a general power method for GEE.-An integrated population-averaged approach to the design, analysis and sample size determination of cluster-unit trials.",1
"In randomized trials, adjustment for measured covariates during the analysis can reduce variance and increase power. To avoid misleading inference, the analysis plan must be pre-specified. However, it is often unclear a priori which baseline covariates (if any) should be adjusted for in the analysis. Consider, for example, the Sustainable East Africa Research in Community Health (SEARCH) trial for HIV prevention and treatment. There are 16 matched pairs of communities and many potential adjustment variables, including region, HIV prevalence, male circumcision coverage, and measures of community-level viral load. In this paper, we propose a rigorous procedure to data-adaptively select the adjustment set, which maximizes the efficiency of the analysis. Specifically, we use cross-validation to select from a pre-specified library the candidate targeted maximum likelihood estimator (TMLE) that minimizes the estimated variance. For further gains in precision, we also propose a collaborative procedure for estimating the known exposure mechanism. Our small sample simulations demonstrate the promise of the methodology to maximize study power, while maintaining nominal confidence interval coverage. We show how our procedure can be tailored to the scientific question (intervention effect for the study sample vs. for the target population) and study design (pair-matched or not). Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Adaptive pre-specification in randomized trials with and without pair-matching.",1
"Emergency departments (EDs) are important for preventing suicide. Historically, many patients with suicide risk are not detected during routine clinical care, and those who are often do not receive suicide-specific intervention. The original Emergency Department Safety Assessment and Follow-up Evaluation (ED-SAFE 1) study examined the implementation of universal suicide risk screening and a multi-component ED-initiated suicide prevention intervention. The ED-SAFE 2 aims to study the impact of using a continuous quality improvement approach (CQI) to improve suicide related care, with a focus on improving universal suicide risk screening in adult ED patients and evaluating implementation of a new brief intervention called the Safety Planning Intervention (SPI) into routine clinical practice. CQI is a quality management process that uses data and collaboration to drive incremental, iterative improvements. The SPI is a personalized approach that focuses on early identification of warning signs and execution of systematic steps to manage suicidal thoughts. ED-SAFE 2 will provide data on the effectiveness of CQI procedures in improving suicide-related care processes, as well as the impact of these improvements on reducing suicide-related outcomes. Using a stepped wedge design, eight EDs collected data cross three study phases: Baseline (retrospective), Implementation (12?months), and Maintenance (12?months). Lean methods, a specific approach to pursuing CQI which focuses on increasing value and eliminating waste, were used to evaluate and improve suicide-related care. The results will build upon the success of the ED-SAFE 1 and will have a broad public health impact through promoting better suicide-related care processes and improved suicide prevention.-Emergency department safety assessment and follow-up evaluation 2: An implementation trial to improve suicide prevention.",0
"Parent/caregivers' inability to recognize the importance of baby teeth has been associated with inadequate self-management of children's oral health (i.e. lower likelihood of preventive dental visits) which may result in dental caries and the need for more expensive caries-related restorative treatment under general anesthesia. Health behavior theories aid researchers in understanding the impact and effectiveness of interventions on changing health behaviors and health outcomes. One example is the Common-Sense Model of Self-Regulation (CSM) which focuses on understanding an individual's illness perception (i.e. illness and treatment representations), and subsequently has been used to develop behavioral interventions to change inaccurate perceptions and describe the processes involved in behavior change. We present two examples of randomized clinical trials that are currently testing oral health behavioral interventions to change parental illness perception and increase dental utilization for young children disproportionately impacted by dental caries in elementary schools and pediatric primary care settings. Additionally, we compared empiric data regarding parent/caregiver perception of the chronic nature of dental caries (captured by the illness perception questionnaire revised for dental: IPQ-RD constructs: identity, consequences, control, timeline, illness coherence, emotional representations) between parent/caregivers who did and did not believe baby teeth were important. Caregivers who believed that baby teeth don't matter had significantly (P&lt;0.05) less accurate perception in the majority of the IPQ-RD constructs (except timeline construct) compared to caregivers who believed baby teeth do matter. These findings support our CSM-based behavioral interventions to modify caregiver caries perception, and improve dental utilization for young children.-Do baby teeth really matter? Changing parental perception and increasing dental care utilization for young children.",0
"To raise awareness among clinicians and epidemiologists that single-patient (n-of-1) trials are potentially useful for informing personalized treatment decisions for patients with chronic conditions. We reviewed the clinical and statistical literature on methods and applications of single-patient trials and then critically evaluated the needs for further methodological developments. Existing literature reports application of 2,154 single-patient trials in 108 studies for diverse clinical conditions; various recent commentaries advocate for wider application of such trials in clinical decision making. Preliminary evidence from several recent pilot acceptability studies suggests that single-patient trials have the potential for widespread acceptance by patients and clinicians as an effective modality for increasing the therapeutic precision. Bayesian and adaptive statistical methods hold promise for increasing the informational yield of single-patient trials while reducing participant burden, but are not widely used. Personalized applications of single-patient trials can be enhanced through further development and application of methodologies on adaptive trial design, stopping rules, network meta-analysis, washout methods, and methods for communicating trial findings to patients and clinicians. Single-patient trials may be poised to emerge as an important part of the methodological armamentarium for comparative effectiveness research and patient-centered outcomes research. By permitting direct estimation of individual treatment effects, they can facilitate finely graded individualized care, enhance therapeutic precision, improve patient outcomes, and reduce costs.-Single-patient (n-of-1) trials: a pragmatic clinical decision methodology for patient-centered comparative effectiveness research.",0
"Approximately 14% of military personnel and veterans who have deployed to the combat theater are at risk for combat-related posttraumatic stress disorder (PTSD). The treatment of combat-related PTSD in active duty service members and veterans is challenging. Combat trauma may involve multiple high levels of exposure to different types of traumatic events (e.g., human carnage after explosive blasts, life threat/injuries to self/others, etc.). Many service members and veterans are unable or unwilling to receive treatment in government facilities due to avoidance, scheduling difficulties, transportation or parking problems, concerns about career advancement, or stigma associated with seeking treatment. Innovative treatment-delivery approaches are needed to help overcome these barriers. The present study is a randomized clinical trial to evaluate three versions of Cognitive Processing Therapy (CPT; [54]) for the treatment of combat-related PTSD in active duty military service members and veterans: (1) standard In-Office CPT, (2) In-Home Telebehavioral Health CPT from the provider's office to the participant's home, and (3) In-Home CPT in which the provider delivers treatment in the participant's home. Use of an equipoise-stratified randomization design allows participants to decline one of the treatment arms. This research design partly overcomes the problems active duty military and veterans face when receiving PTSD treatment by allowing them to opt out of one inappropriate or unacceptable treatment modality and still permitting randomization to the two remaining treatment modalities. This manuscript provides an overview of the research design and methods for the study.-Design of a clinical effectiveness trial of in-home cognitive processing therapy for combat-related PTSD.",0
"We consider evaluation and comparison of the diagnostic accuracy of biomarkers with continuous test outcomes, possibly correlated due to repeated measurements. We develop nonparametric group sequential testing procedures to evaluate and compare the area of biomarkers under their receiver operating characteristic curves, with either independent or paired test outcomes. These procedures rely on the construction of a two-dimensional statistic of Whitehead (Statist. Med. 1999; 18:2271-2286) so that design methods based on Brownian motion can be applied.-Nonparametric sequential evaluation of diagnostic biomarkers.",0
"A commonly cited purpose for conducting a meta-analysis of randomized trials is to increase the statistical power for detecting the effect of an intervention on a specified set of endpoints. At the same time, it also has been noted by several authors that many large-scale cluster randomization trials have not had the power to detect small or even moderate effect sizes. The loss of efficiency associated with cluster randomization relative to individual randomization, and the frequent failure of investigators to take this loss of efficiency into account at the planning stage of a trial, undoubtedly contributes to this problem. In this article, the authors present an approach that may be used to estimate the power of a planned meta-analysis that includes trials that are cluster randomized. Two examples are presented.-Meta-analyses of cluster randomization trials. Power considerations.",1
"I derive the exact distribution of the unpaired t-statistic computed when the data actually come from a paired design. I use this to prove a result Diehr et al. obtained by simulation, namely that the type I error rate of this procedure is no greater than alpha regardless of the sample size. I provide a formula to use in computation of power and type I error rate.-On the distribution of the unpaired t-statistic with paired data.",1
"Identifying a control group when cases come from a specialized hospital is a challenge for epidemiologists. The authors compared controls recruited by using a commercial database with those recruited by random digit dialing in the context of a hospital-based case-control study of ovarian cancer. This part of the study was conducted in 1997-1998 among women aged 18 years or older who resided in the New York metropolitan area. A mailing list owner grouped cases into ""lifestyle"" clusters based on US zip+4 postal code microneighborhoods and generated a random sample of potential controls with the same distribution across the clusters. Controls recruited from the commercial database (n = 82) and from random digit dialing (n = 90) were similar in age and race. Women from the commercial database had somewhat more education and higher incomes and were more similar to the cases on these measures. The control groups resembled each other closely in terms of oral contraceptive use, nulliparity, and religion and differed from the cases on these measures. Response rates were similar for the two groups. Only 28% of the cases were included on the mailing list, indicating that it did not reflect the source population of the cases. Use of a commercial database provided a control group whose socioeconomic factors were similar to those of cases at a lower cost than when random digit dialing was used but did not result in a higher response rate.-Selection of control groups by using a commercial database and random digit dialing.",0
"Cluster randomised crossover trials have been utilised in recent years in the health and social sciences. Methods for analysis have been proposed; however, for binary outcomes, these have received little assessment of their appropriateness. In addition, methods for determination of sample size are currently limited to balanced cluster sizes both between clusters and between periods within clusters. This article aims to extend this work to unbalanced situations and to evaluate the properties of a variety of methods for analysis of binary data, with a particular focus on the setting of potential trials of near-universal interventions in intensive care to reduce in-hospital mortality. We derive a formula for sample size estimation for unbalanced cluster sizes, and apply it to the intensive care setting to demonstrate the utility of the cluster crossover design. We conduct a numerical simulation of the design in the intensive care setting and for more general configurations, and we assess the performance of three cluster summary estimators and an individual-data estimator based on binomial-identity-link regression. For settings similar to the intensive care scenario involving large cluster sizes and small intra-cluster correlations, the sample size formulae developed and analysis methods investigated are found to be appropriate, with the unweighted cluster summary method performing well relative to the more optimal but more complex inverse-variance weighted method. More generally, we find that the unweighted and cluster-size-weighted summary methods perform well, with the relative efficiency of each largely determined systematically from the study design parameters. Performance of individual-data regression is adequate with small cluster sizes but becomes inefficient for large, unbalanced cluster sizes. When outcome prevalences are 6% or less and the within-cluster-within-period correlation is 0.05 or larger, all methods display sub-nominal confidence interval coverage, with the less prevalent the outcome the worse the coverage. As with all simulation studies, conclusions are limited to the configurations studied. We confined attention to detecting intervention effects on an absolute risk scale using marginal models and did not explore properties of binary random effects models. Cluster crossover designs with binary outcomes can be analysed using simple cluster summary methods, and sample size in unbalanced cluster size settings can be determined using relatively straightforward formulae. However, caution needs to be applied in situations with low prevalence outcomes and moderate to high intra-cluster correlations.-Cluster randomised crossover trials with binary data and unbalanced cluster sizes: application to studies of near-universal interventions in intensive care.",1
"Subjects in tumour studies are often misclassified with respect to histologic features that are not routinely recorded in diagnostic reports and that display heterogeneity within tumours. Pathologic analysis of the tumours may miss the feature of interest if the pathologist was not alerted to detail the microscopic feature of interest or if it is not present in the selected specimens. In this setting, only the subjects for whom the outcome is not found are potentially misclassified. Analyses of associations between the observed, potentially misclassified, outcome and a second outcome are invalid if the probability of misclassification depends on the second outcome. Three natural tests of association based on the observed data depend on different numbers of nuisance parameters. Most promising is a test based on the ratio of proportions of the observed feature. We illustrate this test using a study of the association of imaging parameters with genetic features in subjects with oligodendroglioma, a common brain tumour. In this study, calcification, a feature related to the imaging parameters, was potentially misclassified as not present.-Tests of association under misclassification: application to histological sampling in oncology.",0
"Expression quantitative trait loci (eQTL) studies are performed to identify single-nucleotide polymorphisms that modify average expression values of genes, proteins, or metabolites, depending on the genotype. As expression values are often not normally distributed, statistical methods for eQTL studies should be valid and powerful in these situations. Adaptive tests are promising alternatives to standard approaches, such as the analysis of variance or the Kruskal-Wallis test. In a two-stage procedure, skewness and tail length of the distributions are estimated and used to select one of several linear rank tests. In this study, we compare two adaptive tests that were proposed in the literature using extensive Monte Carlo simulations of a wide range of different symmetric and skewed distributions. We derive a new adaptive test that combines the advantages of both literature-based approaches. The new test does not require the user to specify a distribution. It is slightly less powerful than the locally most powerful rank test for the correct distribution and at least as powerful as the maximin efficiency robust rank test. We illustrate the application of all tests using two examples from different eQTL studies.-Adaptive linear rank tests for eQTL studies.",0
Commitment contracts as a way to health.,0
Appropriate analysis and reporting of cluster randomised trials.,1
"Reconciling two quantitative enzyme-linked immunosorbent assay tests for an antibody to an RNA virus, in a situation without a gold standard and where false negatives may occur, is the motivation for this work. False negatives occur when access of the antibody to the binding site is blocked. On the basis of the mechanism of the assay, a mixture of four bivariate normal distributions is proposed with the mixture probabilities depending on a two-stage latent variable model including the prevalence of the antibody in the population and the probabilities of blocking on each test. There is prior information on the prevalence of the antibody, and also on the probability of false negatives, and so a Bayesian analysis is used. The dependence between the two tests is modeled to be consistent with the biological mechanism. Bayesian decision theory is utilized for classification.The proposed method is applied to the motivating data set to classify the data into two groups: those with and those without the antibody. Simulation studies describe the properties of the estimation and the classification. Sensitivity to the choice of the prior distribution is also addressed by simulation. The same model with two levels of latent variables is applicable in other testing procedures such as quantitative polymerase chain reaction tests, where false negatives occur when there is a mutation in the primer sequence.-Bayesian analysis and classification of two enzyme-linked immunosorbent assay tests without a gold standard.",0
"Interferon-alpha therapy, which is used to treat metastatic malignant melanoma, can cause patients to develop two distinct neurobehavioral symptom complexes: a mood syndrome and a neurovegetative syndrome. Interferon-alpha effects on serotonin metabolism appear to contribute to the mood and anxiety syndrome, while the neurovegetative syndrome appears to be related to interferon-alpha effects on dopamine. Our goal is to propose a design for utilizing a sequential, multiple assignment, randomized trial design for patients with malignant melanoma to test the relative efficacy of drugs that target serotonin versus dopamine metabolism during 4 weeks of intravenous, then 8 weeks of subcutaneous, interferon-alpha therapy. Patients will be offered participation in a double-blinded, randomized, controlled, 14-week trial involving two treatment phases. During the first month of intravenous interferon-alpha therapy, we will test the hypotheses that escitalopram will be more effective in reducing depressed mood, anxiety, and irritability, whereas methylphenidate will be more effective in diminishing interferon-alpha-induced neurovegetative symptoms, such as fatigue and psychomotor slowing. During the next 8 weeks of subcutaneous interferon therapy, participants whose symptoms do not improve significantly will be randomized to the alternate agent alone versus escitalopram and methylphenidate together. We present a prototype for a single-center, sequential, multiple assignment, randomized trial, which seeks to determine the efficacy of sequenced and targeted treatment for the two distinct symptom complexes suffered by patients treated with interferon-alpha. Because we cannot completely control for external factors, a relevant question is whether or not 'short-term' neuropsychiatric interventions can increase the number of interferon-alpha doses tolerated and improve long-term survival. This sequential, multiple assignment, randomized trial proposes a framework for developing optimal treatment strategies; however, additional studies are needed to determine the best strategy for treating or preventing neurobehavioral symptoms induced by the immunotherapy interferon-alpha.-Sequential multiple-assignment randomized trial design of neurobehavioral treatment for patients with metastatic malignant melanoma undergoing high-dose interferon-alpha therapy.",0
"Dietary guidelines emphasize selecting lean (low-fat) meats to reduce saturated fat and cholesterol, but growing evidence suggests that health effects may relate to other ingredients, such as sodium, heme iron, or L-carnitine. Understanding how meats influence health, and on which nutrients this relationship depends, is essential to advise consumer choices, set guidelines, and inform food reformulations. A recent study published in BMC Medicine involving 448,568 participants in 10 European countries, provides important evidence in this regard. After multivariate adjustment, intake of unprocessed red meat was not significantly associated with total or cause-specific mortality; conversely, intake of processed meat was associated with a 30% higher rate of cardiovascular disease (CVD) (per 50 g/day, relative risk 1.30, 95% confidence interval 1.17 to 1.45) and also higher cancer mortality. These findings are consistent with our previous meta-analysis, based on smaller studies, showing strong associations of processed meats, but not unprocessed meats, with CVD. Preservatives are the notable difference; the calculated blood-pressure effects of sodium differences (around 400% higher in processed meats) explain most of the observed higher risk. Although unprocessed red meats seem to be relatively neutral for CVD, healthier choices are available, including fish, nuts, legumes, fruits, and vegetables. Public-health guidance should prioritize avoidance of processed meats, including the low-fat deli meats currently marketed as healthy choices, and the food industry should substantially reduce sodium and other preservatives in processed meats.See related research article here http://www.biomedcentral.com/1741-7015/11/63.-Processing of meats and cardiovascular risk: time to focus on preservatives.",0
"Recent theory and methodology for inferences concerning the interclass correlation coefficient are reviewed, focusing on the case of a single individual in one class and a variable number of individuals in the other. Topics discussed include point and interval estimation, as well as significance-testing, with emphasis on application to data arising from family studies.-Review of inference procedures for the interclass correlation coefficient with emphasis on applications to family studies.",1
"The overall goal of the Supporting Adolescent Adherence in Vietnam (SAAV) study is to improve understanding of an adherence feedback mHealth intervention designed to help adolescents living with HIV (ALHIV) maintain high adherence to antiretroviral therapy (ART), critical to effective treatment. Specifically, we aim to: (1) conduct formative research with Vietnamese ALHIV and their caregivers to better understand adherence challenges and refine the personalized mHealth intervention package; and (2) assess the feasibility, acceptability, and efficacy of the intervention to improve ART adherence by implementing a randomized controlled trial (RCT). The study will utilize mixed methods. The formative phase will include 40 in-depth interviews (IDIs) with 20 adolescent (12-17 years)/caregiver dyads and eight focus group discussions with adolescents, caregivers, and clinicians at the National Hospital for Pediatrics (NHP) in Hanoi, Vietnam. We will also conduct 20 IDIs with older adolescents (18-21 years) who have transitioned to adult care at outpatient clinics in Hanoi. We will then implement a seven-month RCT at NHP. We will recruit 80 adolescents on ART, monitor their adherence for one month to establish baseline adherence using a wireless pill container (WPC), and then randomize participants to intervention versus control within optimal (? 95% on-time doses) versus suboptimal (&lt; 95% on-time doses) baseline adherence strata. Intervention participants will receive a reminder of their choice (cellphone text message/call or bottle-based flash/alarm), triggered when they miss a dose, and engage in monthly counseling informed by their adherence data. Comparison participants will receive usual care and offer of counseling at routine monthly clinic visits. After six months, we will compare ART adherence, CD4 count, and HIV viral suppression between arms, in addition to acceptability and feasibility of the intervention. Findings will contribute valuable information on perceived barriers and facilitators affecting adolescents' ART adherence, mHealth approaches as adherence support tools for ALHIV, and factors affecting adolescents' ART adherence. This information will be useful to researchers, medical personnel, and policy-makers as they develop and implement adherence programs for ALHIV, with potential relevance to other chronic diseases during transition from adolescent to adult care. ClinicalTrials.gov, NCT03031197 . Registered on 21 January 2017.-The Supporting Adolescent Adherence in Vietnam (SAAV) study: study protocol for a randomized controlled trial assessing an mHealth approach to improving adherence for adolescents living with HIV in Vietnam.",0
"There is evidence that social anxiety disorder (SAD) is a prevalent and disabling disorder. However, most of the available data on the epidemiology of this condition originate from high income countries in the West. The World Mental Health (WMH) Survey Initiative provides an opportunity to investigate the prevalence, course, impairment, socio-demographic correlates, comorbidity, and treatment of this condition across a range of high, middle, and low income countries in different geographic regions of the world, and to address the question of whether differences in SAD merely reflect differences in threshold for diagnosis. Data from 28 community surveys in the WMH Survey Initiative, with 142,405 respondents, were analyzed. We assessed the 30-day, 12-month, and lifetime prevalence of SAD, age of onset, and severity of role impairment associated with SAD, across countries. In addition, we investigated socio-demographic correlates of SAD, comorbidity of SAD with other mental disorders, and treatment of SAD in the combined sample. Cross-tabulations were used to calculate prevalence, impairment, comorbidity, and treatment. Survival analysis was used to estimate age of onset, and logistic regression and survival analyses were used to examine socio-demographic correlates. SAD 30-day, 12-month, and lifetime prevalence estimates are 1.3, 2.4, and 4.0% across all countries. SAD prevalence rates are lowest in low/lower-middle income countries and in the African and Eastern Mediterranean regions, and highest in high income countries and in the Americas and the Western Pacific regions. Age of onset is early across the globe, and persistence is highest in upper-middle income countries, Africa, and the Eastern Mediterranean. There are some differences in domains of severe role impairment by country income level and geographic region, but there are no significant differences across different income level and geographic region in the proportion of respondents with any severe role impairment. Also, across countries SAD is associated with specific socio-demographic features (younger age, female gender, unmarried status, lower education, and lower income) and with similar patterns of comorbidity. Treatment rates for those with any impairment are lowest in low/lower-middle income countries and highest in high income countries. While differences in SAD prevalence across countries are apparent, we found a number of consistent patterns across the globe, including early age of onset, persistence, impairment in multiple domains, as well as characteristic socio-demographic correlates and associated psychiatric comorbidities. In addition, while there are some differences in the patterns of impairment associated with SAD across the globe, key similarities suggest that the threshold for diagnosis is similar regardless of country income levels or geographic location. Taken together, these cross-national data emphasize the international clinical and public health significance of SAD.-The cross-national epidemiology of social anxiety disorder: Data from the World Mental Health Survey Initiative.",0
"Outcome-dependent sampling (ODS) scheme is a cost-effective sampling scheme where one observes the exposure with a probability that depends on the outcome. The well-known such design is the case-control design for binary response, the case-cohort design for the failure time data, and the general ODS design for a continuous response. While substantial work has been carried out for the univariate response case, statistical inference and design for the ODS with multivariate cases remain under-developed. Motivated by the need in biological studies for taking the advantage of the available responses for subjects in a cluster, we propose a multivariate outcome-dependent sampling (multivariate-ODS) design that is based on a general selection of the continuous responses within a cluster. The proposed inference procedure for the multivariate-ODS design is semiparametric where all the underlying distributions of covariates are modeled nonparametrically using the empirical likelihood methods. We show that the proposed estimator is consistent and developed the asymptotically normality properties. Simulation studies show that the proposed estimator is more efficient than the estimator obtained using only the simple-random-sample portion of the multivariate-ODS or the estimator from a simple random sample with the same sample size. The multivariate-ODS design together with the proposed estimator provides an approach to further improve study efficiency for a given fixed study budget. We illustrate the proposed design and estimator with an analysis of association of polychlorinated biphenyl exposure to hearing loss in children born to the Collaborative Perinatal Study. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Statistical inferences for data from studies conducted with an aggregated multivariate outcome-dependent sample design.",0
"Current guidelines for treatment of multidrug-resistant tuberculosis (MDR-TB) are largely based on expert opinion and observational data. Fluoroquinolones remain an essential part of MDR-TB treatment, but the optimal dose of fluoroquinolones as part of the regimen has not been defined. We designed a randomized, blinded, phase II trial in MDR-TB patients comparing across levofloxacin doses of 11, 14, 17 and 20 mg/kg/day, all within an optimized background regimen. We assess pharmacokinetics, efficacy, safety and tolerability of regimens containing each of these doses. The primary efficacy outcome is time to culture conversion over the first 6 months of treatment. The study aims to determine the area under the curve (AUC) of the levofloxacin serum concentration in the 24 hours after dosing divided by the minimal inhibitory concentration of the patient's Mycobacterium tuberculosis isolate that inhibits &gt; 90% of organisms (AUC/MIC) that maximizes efficacy and the AUC that maximizes safety and tolerability in the context of an MDR-TB treatment regimen. Fluoroquinolones are an integral part of recommended MDR-TB regimens. Little is known about how to optimize dosing for efficacy while maintaining acceptable toxicity. This study will provide evidence to support revised dosing guidelines for the use of levofloxacin as part of combination regimens for treatment of MDR-TB. The novel methodology can be adapted to elucidate the effect of other single agents in multidrug antibiotic treatment regimens. ClinicalTrials.gov, NCT01918397 . Registered on 5 August 2013.-An optimized background regimen design to evaluate the contribution of levofloxacin to multidrug-resistant tuberculosis treatment regimens: study protocol for a randomized controlled trial.",0
"Recent studies have investigated the small sample properties of models for clustered data, such as multilevel models and generalized estimating equations. These studies have focused on parameter bias when the number of clusters is small, but very few studies have addressed the methods' properties with sparse data: a small number of observations within each cluster. In particular, studies have yet to address the properties of generalized estimating equations, a possible alternative to multilevel models often overlooked in behavioral sciences, with sparse data. This article begins with a discussion of population-averaged and cluster-specific models, provides a brief overview of both multilevel models and generalized estimating equations, and then conducts a simulation study on the sparse data properties of generalized estimating equations, multilevel models, and single-level regression models for both normal and binary outcomes. The simulation found generalized estimating equations estimate regression coefficients and their standard errors without bias with as few as 2 observations per cluster, provided that the number of clusters was reasonably large. Similar to the previous studies, multilevel models tended to overestimate the between-cluster variance components when the cluster size was below about 5.-Modeling sparsely clustered data: design-based, model-based, and single-level methods.",1
"The transition from high school to college is a developmentally sensitive period that is high-risk for the escalation of alcohol use. Although risky drinking is a common problem among freshmen, engagement in treatment services is very low. College alcohol interventions target drinking directly at a time when students may be uninterested in changing their drinking. Approaches that indirectly target drinking may be particularly effective. Behavioral activation (BA) is an intervention that indirectly addresses mental health conditions by guiding individuals to engage in reinforcing activities that align with their values (Lejuez et al., 2001). A pilot study of a BA intervention administered in a semester-long freshman orientation course reported a significant decrease in drinking-related problems compared to students in standard orientation course (Reynolds et al., 2011). The cluster-randomized trial will test the efficacy of BA administered in a semester-long (16?week) freshman orientation course, compared to a standard orientation course in 540 freshmen spread over 36 course sections (18 sections each). A 5-month post-treatment assessment will measure durability of effects. The study will test a promising BA intervention that addresses factors limiting participation in other programs by targeting alcohol use indirectly and by integrating an intervention into college curriculum. This study represents a first step toward developing an intervention course that could be widely disseminated to address the persistent college drinking problem and its consequences. NCT04038190.-A behavioral activation intervention administered in a 16-week freshman orientation course: Study protocol.",0
The double jeopardy of clustered measurement and cluster randomisation.,1
"Many endocrine systems are regulated by pulsatile hormones - hormones that are secreted intermittently in boluses rather than continuously over time. To study pulsatile secretion, blood is drawn every few minutes for an extended period. The result is a time series of hormone concentrations for each individual. The goal is to estimate pulsatile hormone secretion features such as frequency, location, duration, and amount of pulsatile and non-pulsatile secretion and compare these features between groups. Various statistical approaches to analyzing these data have been proposed, but validation has generally focused on one hormone. Thus, we lack a broad understanding of each method's performance. By using simulated data with features seen in reproductive and stress hormones, we investigated the performance of three recently developed statistical approaches for analyzing pulsatile hormone data and compared them to a frequently used deconvolution approach. We found that methods incorporating a changing baseline modeled both constant and changing baseline shapes well; however, the added model flexibility resulted in a slight increase in bias in other model parameters. When pulses were well defined and baseline constant, Bayesian approaches performed similar to the existing deconvolution method. The increase in computation time of Bayesian approaches offered improved estimation and more accurate quantification of estimation variation in situations where pulse locations were not clearly identifiable. Within the class of deconvolution models for fitting pulsatile hormone data, the Bayesian approach with a changing baseline offered adequate results over the widest range of data.-A comparison of methods for analyzing time series of pulsatile hormone data.",0
"Shared decision making (SDM) implementation remains challenging. The factors that promote or hinder implementation of SDM tools for use during the consultation, including contextual factors such as clinician burnout and organizational support, remain unclear. We explored these factors in the context of a practical multicenter randomized trial evaluating the effectiveness of an SDM conversation tool for patients with atrial fibrillation considering anticoagulation therapy. In this cross-sectional study, we recruited clinicians who were regularly involved in conversations with patients regarding anticoagulation for atrial fibrillation. Clinicians reported their characteristics and burnout symptoms using the two-item Maslach Burnout Inventory. Clinicians were trained in using the SDM tool, and they recorded their perceptions of the tool's normalization potential using the Normalization MeAsure Development (NoMAD) survey instrument and verbally reflected on their answers to these survey questions. When possible, the training sessions and clinicians' verbal responses to the conversation tool were recorded. Our study comprised 183 clinicians recruited into the trial (168 with survey responses and 112 with recordings). Overall, clinicians gave high scores to the normalization potential of the intervention; they endorsed all domains of normalization to the same extent, regardless of site, clinician characteristics, or burnout ratings. In interviews, clinicians paid significant attention to making sense of the tool. Tool buy-in seemed to depend heavily on their ability to see the tool as accurate and ""evidence-based"" and their perceptions of having time in the consultation to use it. While time in the consultation remains a barrier, we did not find a significant association between burnout symptoms and normalization of an SDM conversation tool. Possible areas for improving the normalization of SDM conversation tools in clinical practice include enabling collaboration among clinicians to implement the tool and reporting how clinicians elsewhere use the tool. Direct measures of normalization (i.e., observing how often clinicians access the tool in practice outside of the clinical trial) may further elucidate the role that contextual factors, such as clinician burnout, play in the implementation of SDM. ClinicalTrials.gov, NCT02905032. Registered on 9 September 2016.-Normalization of a conversation tool to promote shared decision making about anticoagulation in patients with atrial fibrillation within a practical randomized trial of its effectiveness: a cross-sectional study.",0
"Disease incidence or disease mortality rates for small areas are often displayed on maps. Maps of raw rates, disease counts divided by the total population at risk, have been criticized as unreliable due to non-constant variance associated with heterogeneity in base population size. This has led to the use of model-based Bayes or empirical Bayes point estimates for map creation. Because the maps have important epidemiological and political consequences, for example, they are often used to identify small areas with unusually high or low unexplained risk, it is important that the assumptions of the underlying models be scrutinized. We review the use of posterior predictive model checks, which compare features of the observed data to the same features of replicate data generated under the model, for assessing model fitness. One crucial issue is whether extrema are potentially important epidemiological findings or merely evidence of poor model fit. We propose the use of the cross-validation posterior predictive distribution, obtained by reanalyzing the data without a suspect small area, as a method for assessing whether the observed count in the area is consistent with the model. Because it may not be feasible to actually reanalyze the data for each suspect small area in large data sets, two methods for approximating the cross-validation posterior predictive distribution are described.-Posterior predictive model checks for disease mapping models.",0
"One measure of the severity of a pandemic influenza outbreak at the individual level is the risk of death among people infected by the new virus. However, there are complications in estimating both the numerator and denominator. Regarding the numerator, statistical estimates of the excess deaths associated with influenza virus infections tend to exceed the number of deaths associated with laboratory-confirmed infection. Regarding the denominator, few infections are laboratory confirmed, while differences in case definitions and approaches to case ascertainment can lead to wide variation in case fatality risk estimates. Serological surveillance can be used to estimate the cumulative incidence of infection as a denominator that is more comparable across studies. We estimated that the first wave of the influenza A(H1N1)pdm09 virus in 2009 was associated with approximately 232 (95% confidence interval: 136, 328) excess deaths of all ages in Hong Kong, mainly among the elderly. The point estimates of the risk of death on a per-infection basis increased substantially with age, from below 1 per 100,000 infections in children to 1,099 per 100,000 infections in those 60-69 years of age. Substantial variation in the age-specific infection fatality risk complicates comparison of the severity of different influenza strains.-Infection fatality risk of the pandemic A(H1N1)2009 virus in Hong Kong.",0
Self-rated health may be adequate for broad assessments of social inequalities in health.,0
"Determination of the prevalence of accumulated antiretroviral drug resistance among persons infected with human immunodeficiency virus (HIV) is complicated by the lack of routine measurement in clinical care. By using data from 8 clinic-based cohorts from the North American AIDS Cohort Collaboration on Research and Design, drug-resistance mutations from those with genotype tests were determined and scored using the Genotypic Resistance Interpretation Algorithm developed at Stanford University. For each year from 2000 through 2005, the prevalence was calculated using data from the tested subset, assumptions that incorporated clinical knowledge, and multiple imputation methods to yield a complete data set. A total of 9,289 patients contributed data to the analysis; 3,959 had at least 1 viral load above 1,000 copies/mL, of whom 2,962 (75%) had undergone at least 1 genotype test. Using these methods, the authors estimated that the prevalence of accumulated resistance to 2 or more antiretroviral drug classes had increased from 14% in 2000 to 17% in 2005 (P &lt; 0.001). In contrast, the prevalence of resistance in the tested subset declined from 57% to 36% for 2 or more classes. The authors' use of clinical knowledge and multiple imputation methods revealed trends in HIV drug resistance among patients in care that were markedly different from those observed using only data from patients who had undergone genotype tests.-Missing data on the estimation of the prevalence of accumulated human immunodeficiency virus drug resistance in patients treated with antiretroviral drugs in north america.",0
CONSORT statement: extension to cluster randomised trials.,1
"A compelling hypothesis was proposed that childhood brain tumours are associated with maternal exposure to N-nitroso compounds during the prenatal period. Many common drugs, such as antihistamines, aspirin, and antibiotics, are nitrosatable and depending upon the product, potentially carcinogenic. We hypothesized that maternal ingestion of certain subgroups of nitrosatable drug products during pregnancy increases the risk of brain tumour development in offspring. Data were collected as part of a population-based case-control study of childhood brain tumours and mothers' self-reported exposure to therapeutic drugs and dietary nitrites. Cases were enrolled from three US West Coast SEER tumour registries: Seattle-Puget sound, Los Angeles County, and the San Francisco-Oakland Bay Area. Tumours were grouped into three major histological tumour subtypes: astroglial, primitive neural ectodermal tumours, and all remaining glial tumours ('other glial'). Therapeutic drugs reported by mothers were translated into active chemical compounds and classified as secondary amines, tertiary amines, amides, or none of the three. Risk estimates were computed according to classes of nitrosatability, potential carcinogenicity, teratogenicity, and predicted end product. We found no significant association between maternal use of nitrosatable drugs, either overall or within any of the nitrosatable drug classifications, and subsequent development of brain tumours in children. Nitrite consumption from cured meats was not an effect modifier. However, exposure to nitrosoephedrine during pregnancy was associated with significantly increased risk of 'other glial' tumours (OR = 3.1; 95% CI: 1.1-9.2). These findings do not support an association between maternal use of nitrosatable drugs during pregnancy and brain tumour risk in offspring. While exposure to the nitrosation end product nitrosoephedrine was associated with increased risk for other glial tumours, the finding was not specific to any one type of tumour.-Maternal prenatal exposure to nitrosatable drugs and childhood brain tumours.",0
"All studies classified as research involving human participants require research ethics review. Most regulation and guidance on ethical oversight of research involving human participants was written for pharmacotherapy interventions. Interpretation of such guidance for cluster-randomized trials and stepped-wedge trials, which commonly evaluate complex non-therapeutic interventions such as knowledge translation, public health, or health service delivery interventions, can pose challenges to researchers and regulators. The Ottawa Statement on the Ethical Design and Conduct of Cluster-Randomized Trials provides guidance on the ethical oversight and consent procedures for cluster-randomized trials, and while not explicit, this includes stepped-wedge trials. Yet, stepped-wedge trials have unique characteristics that differentiate them from standard cluster-randomized trials. In particular, they can be used to evaluate knowledge translation interventions within the context of a routine health system rollout; they may have a non-randomized design; and the decision to implement the intervention is not always made by the researcher. Many stepped-wedge trials do not undergo ethical review and do not report trial registration. This suggests that those undertaking these studies and research ethics committees perceive them as non-research activities. Through an ethical analysis of two case studies, we argue that stepped-wedge trials, like parallel arm cluster trials, are systematic investigations designed to produce generalizable knowledge. We contend that stepped-wedge trials usually include human research participants, which may be patients, health care providers, or both. Stepped-wedge trials are therefore research involving human participants for the purpose of ethical review. Nevertheless, the use of a waiver or alteration of consent may be appropriate in many stepped-wedge trials due to the infeasibility of obtaining informed consent and the low-risk nature of the interventions. To ensure that traditional ethical principles such as respect for persons are upheld, these studies must undergo research ethics review.-Stepped-wedge trials should be classified as research for the purpose of ethical review",3
"A requirement for calculating sample sizes for cluster randomized trials (CRTs) conducted over multiple periods of time is the specification of a form for the correlation between outcomes of subjects within the same cluster, encoded via the within-cluster correlation structure. Previously proposed within-cluster correlation structures have made strong assumptions; for example, the usual assumption is that correlations between the outcomes of all pairs of subjects are identical (""uniform correlation""). More recently, structures that allow for a decay in correlation between pairs of outcomes measured in different periods have been suggested. However, these structures are overly simple in settings with continuous recruitment and measurement. We propose a more realistic ""continuous-time correlation decay"" structure whereby correlations between subjects' outcomes decay as the time between these subjects' measurement times increases. We investigate the use of this structure on trial planning in the context of a primary care diabetes trial, where there is evidence of decaying correlation between pairs of patients' outcomes over time. In particular, for a range of different trial designs, we derive the variance of the treatment effect estimator under continuous-time correlation decay and compare this to the variance obtained under uniform correlation. For stepped wedge and cluster randomized crossover designs, incorrectly assuming uniform correlation will underestimate the required sample size under most trial configurations likely to occur in practice. Planning of CRTs requires consideration of the most appropriate within-cluster correlation structure to obtain a suitable sample size.-Accounting for a decaying correlation structure in cluster randomized trials with continuous recruitment",1
"There is often substantial uncertainty about the impacts of health system and policy interventions. Despite that, randomized controlled trials (RCTs) are uncommon in this field, partly because experiments can be difficult to carry out. An alternative method for impact evaluation is the interrupted time-series (ITS) design. Little is known, however, about how results from the two methods compare. Our aim was to explore whether ITS studies yield results that differ from those of randomized trials. We conducted single-arm ITS analyses (segmented regression) based on data from the intervention arm of cluster randomized trials (C-RCTs), that is, discarding control arm data. Secondarily, we included the control group data in the analyses, by subtracting control group data points from intervention group data points, thereby constructing a time series representing the difference between the intervention and control groups. We compared the results from the single-arm and controlled ITS analyses with results based on conventional aggregated analyses of trial data. The findings were largely concordant, yielding effect estimates with overlapping 95% confidence intervals (CI) across different analytical methods. However, our analyses revealed the importance of a concurrent control group and of taking baseline and follow-up trends into account in the analysis of C-RCTs. The ITS design is valuable for evaluation of health systems interventions, both when RCTs are not feasible and in the analysis and interpretation of data from C-RCTs.-A reanalysis of cluster randomized trials showed interrupted time-series studies were valuable in health system evaluation.",1
"It has become increasingly common in epidemiological studies to pool specimens across subjects to achieve accurate quantitation of biomarkers and certain environmental chemicals. In this article, we consider the problem of fitting a binary regression model when an important exposure is subject to pooling. We take a regression calibration approach and derive several methods, including plug-in methods that use a pooled measurement and other covariate information to predict the exposure level of an individual subject, and normality-based methods that make further adjustments by assuming normality of calibration errors. Within each class we propose two ways to perform the calibration (covariate augmentation and imputation). These methods are shown in simulation experiments to effectively reduce the bias associated with the naive method that simply substitutes a pooled measurement for all individual measurements in the pool. In particular, the normality-based imputation method performs reasonably well in a variety of settings, even under skewed distributions of calibration errors. The methods are illustrated using data from the Collaborative Perinatal Project.-Binary regression analysis with pooled exposure measurements: a regression calibration approach.",0
"The stepped-wedge cluster randomised trial design has received substantial attention in recent years. Although various extensions to the original design have been proposed, no guidance is available on the design of stepped-wedge cluster randomised trials with interim analyses. In an individually randomised trial setting, group sequential methods can provide notable efficiency gains and ethical benefits. We address this by discussing how established group sequential methodology can be adapted for stepped-wedge designs. Utilising the error spending approach to group sequential trial design, we detail the assumptions required for the determination of stepped-wedge cluster randomised trials with interim analyses. We consider early stopping for efficacy, futility, or efficacy and futility. We describe first how this can be done for any specified linear mixed model for data analysis. We then focus on one particular commonly utilised model and, using a recently completed stepped-wedge cluster randomised trial, compare the performance of several designs with interim analyses to the classical stepped-wedge design. Finally, the performance of a quantile substitution procedure for dealing with the case of unknown variance is explored. We demonstrate that the incorporation of early stopping in stepped-wedge cluster randomised trial designs could reduce the expected sample size under the null and alternative hypotheses by up to 31% and 22%, respectively, with no cost to the trial's type-I and type-II error rates. The use of restricted error maximum likelihood estimation was found to be more important than quantile substitution for controlling the type-I error rate. The addition of interim analyses into stepped-wedge cluster randomised trials could help guard against time-consuming trials conducted on poor performing treatments and also help expedite the implementation of efficacious treatments. In future, trialists should consider incorporating early stopping of some kind into stepped-wedge cluster randomised trials according to the needs of the particular trial.-Group sequential designs for stepped-wedge cluster randomised trials.",3
"Influenza has a long history of causing morbidity and mortality in the human population through routine seasonal spread and global pandemics. The high mutation rate of the RNA genome of the influenza virus, combined with assortment of its multiple genomic segments, promote antigenic diversity and new subtypes, allowing the virus to evade vaccines and become resistant to antiviral drugs. There is thus a continuing need for new anti-influenza therapy using novel targets and creative strategies. In this review, we summarize prospective future therapeutic regimens based on recent molecular and genomic discoveries.-New treatments for influenza.",0
Intraclass correlation values for planning group-randomized trials in education,1
"The intent of this review is to update the science of emerging cardiometabolic risk factors that were listed in the National Cholesterol Education Program (NCEP) Adult Treatment Panel-III (ATP-III) report of 2001 (updated in 2004). At the time these guidelines were published, the evidence was felt to be insufficient to recommend these risk factors for routine screening of cardiovascular disease risk. However, the panel felt that prudent use of these biomarkers for patients at intermediate risk of a major cardiovascular event over the subsequent 10 years might help identify patients who needed more aggressive low density lipoprotein (LDL) or non-high density lipoprotein (HDL) cholesterol lowering therapy. While a number of other emerging risk factors have been identified, this review will be limited to assessing the data and recommendations for the use of apolipoprotein B, lipoprotein (a), homocysteine, pro-thrombotic factors, inflammatory factors, impaired glucose metabolism, and measures of subclinical atherosclerotic cardiovascular disease for further cardiovascular disease risk stratification.-Update on the NCEP ATP-III emerging cardiometabolic risk factors.",0
Cohort Profile: The Women's Interagency HIV Study (WIHS).,0
"A critical issue in the analysis of clinical trials is patients' noncompliance to assigned treatments. In the context of a binary treatment with all or nothing compliance, the intent-to-treat analysis is a straightforward approach to estimating the effectiveness of the trial. In contrast, there exist 3 commonly used estimators with varying statistical properties for the efficacy of the trial, formally known as the complier-average causal effect. The instrumental variable estimator may be unbiased but can be extremely variable in many settings. The as treated and per protocol estimators are usually more efficient than the instrumental variable estimator, but they may suffer from selection bias. We propose a synthetic approach that incorporates all 3 estimators in a data-driven manner. The synthetic estimator is a linear convex combination of the instrumental variable, per protocol, and as treated estimators, resembling the popular model-averaging approach in the statistical literature. However, our synthetic approach is nonparametric; thus, it is applicable to a variety of outcome types without specific distributional assumptions. We also discuss the construction of the synthetic estimator using an analytic form derived from a simple normal mixture distribution. We apply the synthetic approach to a clinical trial for post-traumatic stress disorder.-A synthetic estimator for the efficacy of clinical trials with all-or-nothing compliance.",0
"Despite evidence linking obesity to impaired immune function, little is known about the specific mechanisms. Because of emerging evidence that immune responses are epigenetically regulated, we hypothesized that DNA methylation changes are involved in obesity induced immune dysfunction and aimed to identify these changes. We conducted a genome wide methylation analysis on seven obese cases and seven lean controls aged 14 to 18 years from extreme ends of the obesity distribution and performed further validation of six CpG sites from six genes in 46 obese cases and 46 lean controls aged 14 to 30 years. In comparison with the lean controls, we observed one CpG site in the UBASH3A gene showing higher methylation levels and one CpG site in the TRIM3 gene showing lower methylation levels in the obese cases in both the genome wide step (P = 5 ? 10(-6) and P = 2 ? 10(-5) for the UBASH3A and the TRIM3 gene respectively) and the validation step (P = 0.008 and P = 0.001 for the UBASH3A and the TRIM3 gene respectively). Our results provide evidence that obesity is associated with methylation changes in blood leukocyte DNA. Further studies are warranted to determine the causal direction of this relationship as well as whether such methylation changes can lead to immune dysfunction.-Obesity related methylation changes in DNA of peripheral blood leukocytes.",0
"The standard approach for analysing a randomized clinical trial is based on intent-to-treat (ITT) where subjects are analysed according to their assigned treatment group regardless of actual adherence to the treatment protocol. For therapeutic equivalence trials, it is a common concern that an ITT analysis increases the chance of erroneously concluding equivalence. In this paper, we formally investigate the impact of non-compliance on an ITT analysis of equivalence trials with a binary outcome. We assume 'all-or-none' compliance and independence between compliance and the outcome. Our results indicate that non-compliance does not always make it easier to demonstrate equivalence. The direction and magnitude of changes in the type I error rate and power of the study depend on the patterns of non-compliance, event probabilities, the margin of equivalence and other factors.-The effects of non-compliance on intent-to-treat analysis of equivalence trials.",0
"The analysis of repeated measure or clustered data is often complicated by the presence of correlation. Further complications arise for discrete responses, where the marginal probability-dependent Fr?chet bounds impose feasibility limits on the correlation that are often more restrictive than the positive definite range. Some popular statistical methods, such as generalized estimating equations (GEE), ignore these bounds, and as such can generate erroneous estimates and lead to incorrect inferential results. In this paper, we discuss two alternative strategies: (i) using QIC to select a data-driven correlation value within the Fr?chet bounds, and (ii) the use of likelihood-based latent variable modeling, such as multivariate probit, to get around the problem all together. We provide two examples of the repercussions of incorrectly using existing GEE software in the presence of correlated binary responses.-What can go wrong when ignoring correlation bounds in the use of generalized estimating equations.",1
"A common problem encountered in many medical applications is the comparison of survival curves. Often, rather than comparison of the entire survival curves, interest is focused on the comparison at a fixed point in time. In most cases, the naive test based on a difference in the estimates of survival is used for this comparison. In this note, we examine the performance of alternatives to the naive test. These include tests based on a number of transformations of the survival function and a test based on a generalized linear model for pseudo-observations. The type I errors and power of these tests for a variety of sample sizes are compared by a Monte Carlo study. We also discuss how these tests may be extended to situations where the data are stratified. The pseudo-value approach is also applicable in more detailed regression analysis of the survival probability at a fixed point in time. The methods are illustrated on a study comparing survival for autologous and allogeneic bone marrow transplants.-Analyzing survival curves at a fixed point in time.",0
"For power and sample-size calculations, most practicing researchers rely on power and sample-size software programs to design their studies. There are many factors that affect the statistical power that, in many situations, go beyond the coverage of commercial software programs. Factors commonly known as design effects influence statistical power by inflating the variance of the test statistics. The authors quantify how these factors affect the variances so that researchers can adjust the statistical power or sample size accordingly. The authors review design effects for factorial design, crossover design, cluster randomization, unequal sample-size design, multiarm design, logistic regression, Cox regression, and the linear mixed model, as well as missing data in various designs. To design a study, researchers can apply these design effects, also known as variance inflation factors to adjust the power or sample size calculated from a two-group parallel design using standard formulas and software.-An overview of variance inflation factors for sample-size calculation.",1
"How to Design, Analyse and Report Cluster Randomised Trials in Medicine and Health Related Research",1
"Most cancer research now involves one or more assays profiling various biological molecules, e.g., messenger RNA and micro RNA, in samples collected on the same individuals. The main interest with these genomic data sets lies in the identification of a subset of features that are active in explaining the dependence between platforms. To quantify the strength of the dependency between two variables, correlation is often preferred. However, expression data obtained from next-generation sequencing platforms are integer with very low counts for some important features. In this case, the sample Pearson correlation is not a valid estimate of the true correlation matrix, because the sample correlation estimate between two features/variables with low counts will often be close to zero, even when the natural parameters of the Poisson distribution are, in actuality, highly correlated. We propose a model-based approach to correlation estimation between two non-normal data sets, via a method we call Probabilistic Correlations ANalysis, or PCAN. PCAN takes into consideration the distributional assumption about both data sets and suggests that correlations estimated at the model natural parameter level are more appropriate than correlations estimated directly on the observed data. We demonstrate through a simulation study that PCAN outperforms other standard approaches in estimating the true correlation between the natural parameters. We then apply PCAN to the joint analysis of a microRNA (miRNA) and a messenger RNA (mRNA) expression data set from a squamous cell lung cancer study, finding a large number of negative correlation pairs when compared to the standard approaches.-PCAN: Probabilistic correlation analysis of two non-normal data sets.",0
"For cluster randomized trials with a continuous outcome, the sample size is often calculated as if an analysis of the outcomes at the end of the treatment period (follow-up scores) would be performed. However, often a baseline measurement of the outcome is available or feasible to obtain. An analysis of covariance (ANCOVA) using both the baseline and follow-up score of the outcome will then have more power. We calculate the efficiency of an ANCOVA analysis using the baseline scores compared with an analysis on follow-up scores only. The sample size for such an ANCOVA analysis is a factor r2 smaller, where r is the correlation of the cluster means between baseline and follow-up. This correlation can be expressed in clinically interpretable parameters: the correlation between baseline and follow-up of subjects (subject autocorrelation) and that of clusters (cluster autocorrelation). Because of this, subject matter knowledge can be used to provide (range of) plausible values for these correlations, when estimates from previous studies are lacking. Depending on how large the subject and cluster autocorrelations are, analysis of covariance can substantially reduce the number of clusters needed.-A simple sample size formula for analysis of covariance in cluster randomized trials.",1
"Randomized trials are often designed to assess an intervention's ability to change patient knowledge, behaviour or health. The study outcome will then need to be measured at least twice for each subject--prior to random assignment and following implementation of the intervention. In this paper we consider methods for modelling change when data are obtained from cluster randomization trials where the unit of allocation is a family, school or community. Attention focuses on mixed effects linear regression extensions of (i) two-sample t-tests and (ii) analysis of covariance, in both cases accounting for dependencies among cluster members. Algebraic expressions for tests of the intervention effect are derived for the special case where there are a fixed number of subjects per cluster while simulation studies are used to compare the power of these procedures in the more realistic case where there is variability in cluster size. A key conclusion is that there can be considerable gains in power when allowing for different individual-level and cluster-level associations between the baseline and follow-up assessments. The discussion is illustrated using data from a school-based smoking prevention trial.-Methods for modelling change in cluster randomization trials.",1
"Recurrent events data are commonly encountered in medical studies. In many applications, only the number of events during the follow-up period rather than the recurrent event times is available. Two important challenges arise in such studies: (a) a substantial portion of subjects may not experience the event, and (b) we may not observe the event count for the entire study period due to informative dropout. To address the first challenge, we assume that underlying population consists of two subpopulations: a subpopulation nonsusceptible to the event of interest and a subpopulation susceptible to the event of interest. In the susceptible subpopulation, the event count is assumed to follow a Poisson distribution given the follow-up time and the subject-specific characteristics. We then introduce a frailty to account for informative dropout. The proposed semiparametric frailty models consist of three submodels: (a) a logistic regression model for the probability such that a subject belongs to the nonsusceptible subpopulation; (b) a nonhomogeneous Poisson process model with an unspecified baseline rate function; and (c) a Cox model for the informative dropout time. We develop likelihood-based estimation and inference procedures. The maximum likelihood estimators are shown to be consistent. Additionally, the proposed estimators of the finite-dimensional parameters are asymptotically normal and the covariance matrix attains the semiparametric efficiency bound. Simulation studies demonstrate that the proposed methodologies perform well in practical situations. We apply the proposed methods to a clinical trial on patients with myelodysplastic syndromes.-Semiparametric frailty models for zero-inflated event count data in the presence of informative dropout.",0
"Assessing and comparing the performance of correlated predictive scores are of current interest in precision medicine. Given the limitations of available theoretical approaches for assessing and comparing the predictive accuracy, numerical methods are highly desired which, however, have not been systematically developed due to technical challenges. The main challenges include the lack of a general strategy on effectively simulating many kinds of correlated predictive scores each with some given level of predictive accuracy in either concordance index or the area under a receiver operating characteristic curve area under the curves (AUC). To fill in this important knowledge gap, this paper is to provide a general copula-based numeric framework for assessing and comparing predictive performance of correlated predictive or risk scores. The new algorithms are designed to effectively simulate correlated predictive scores with given levels of predictive accuracy as measured in terms of concordance indices or time-dependent AUC for predicting survival outcomes. The copula-based numerical strategy is convenient for numerically evaluating and comparing multiple measures of predictive accuracy of correlated risk scores and for investigating finite-sample properties of test statistics and confidence intervals as well as assessing for optimism of given performance measures using cross-validation or bootstrap.-A numerical strategy to evaluate performance of predictive scores via a copula-based approach.",0
"We derived sample size formulae for detecting main effects in group-based randomized clinical trials with different levels of data hierarchy between experimental and control arms. Such designs are necessary when experimental interventions need to be administered to groups of subjects whereas control conditions need to be administered to individual subjects. This type of trial, often referred to as a partially nested or partially clustered design, has been implemented for management of chronic diseases such as diabetes and is beginning to emerge more commonly in wider clinical settings. Depending on the research setting, the level of hierarchy of data structure for the experimental arm can be three or two, whereas that for the control arm is two or one. Such different levels of data hierarchy assume correlation structures of outcomes that are different between arms, regardless of whether research settings require two or three level data structure for the experimental arm. Therefore, the different correlations should be taken into account for statistical modeling and for sample size determinations. To this end, we considered mixed-effects linear models with different correlation structures between experimental and control arms to theoretically derive and empirically validate the sample size formulae with simulation studies.-Sample size determinations for group-based randomized clinical trials with different levels of data hierarchy between experimental and control arms.",2
"Group-randomized study designs are useful when individually randomized designs are either not possible, or will not be able to estimate the parameters of interest. Blocked and/or stratified (for example, pair-matched) designs have been used, and their properties statistically evaluated by many researchers. Group-randomized trials often have small numbers of experimental units, and strong, geographically induced between-unit correlation, which increase the chance of obtaining a ""bad"" randomization outcome. This article describes a procedure--random selection from a list of acceptable allocations--to allocate treatment conditions in a way that ensures balance on relevant covariates. Numerous individual- and group-level covariates can be balanced using exact or caliper criteria. Simulation results indicate that this method has good frequency properties, but some care may be needed not to overly constrain the randomization. There is a trade-off between achieving good balance through a highly constrained design, and jeopardizing the appearance of impartiality of the investigator and potentially departing from the nominal Type I error.-Covariate-based constrained randomization of group-randomized trials.",1
"Consider a randomized clinical trial to evaluate the benefit of screening an asymptomatic population. Suppose that the subjects are randomized into a usual care and a study group. The study group receives one or more periodic early detection examinations aimed at diagnosing disease early, when there are no signs or symptoms. Early detection clinical trials differ from therapeutic trials in that power is affected by: i) the number of exams, ii) the time between exams and iii) the ages at which exams will be given. These design options do not exist in therapeutic trials. Furthermore; long-term follow-up may result in a reduction of power. In general, power increases with number of examinations, and the optimal follow-up time is dependent on the spacing between examinations. Clinical trials in which the usual care group receives benefit are also discussed. Two designs are discussed, for example the 'up-front design' in which all subjects receive an initial exam and then are randomized to the usual care and study groups and the 'close-out design' in which the usual care group receives an exam which is timed to be given at the same time as the last exam in the study group. Both families of designs significantly reduce the power. Power calculations are made for two clinical trials, which actually used these two designs.-Planning of randomized early detection trials.",0
"Previous reviews of cluster randomised trials have been critical of the quality of the trials reviewed, but none has explored determinants of the quality of these trials in a specific field over an extended period of time. Recent work suggests that correct conduct and reporting of these trials may require more than published guidelines. In this review, our aim was to assess the quality of cluster randomised trials conducted in residential facilities for older people, and to determine whether (1) statistician involvement in the trial and (2) strength of journal endorsement of the Consolidated Standards of Reporting Trials (CONSORT) statement influence quality. We systematically identified trials randomising residential facilities for older people, or parts thereof, without language restrictions, up to the end of 2010, using National Library of Medicine (Medline) via PubMed and hand-searching. We based quality assessment criteria largely on the extended CONSORT statement for cluster randomised trials. We assessed statistician involvement based on statistician co-authorship, and strength of journal endorsement of the CONSORT statement from journal websites. 73 trials met our inclusion criteria. Of these, 20 (27%) reported accounting for clustering in sample size calculations and 54 (74%) in the analyses. In 29 trials (40%), methods used to identify/recruit participants were judged by us to have potentially caused bias or reporting was unclear to reach a conclusion. Some elements of quality improved over time but this appeared not to be related to the publication of the extended CONSORT statement for these trials. Trials with statistician/epidemiologist co-authors were more likely to account for clustering in sample size calculations (unadjusted odds ratio 5.4, 95% confidence interval 1.1 to 26.0) and analyses (unadjusted OR 3.2, 1.2 to 8.5). Journal endorsement of the CONSORT statement was not associated with trial quality. Despite international attempts to improve methods in cluster randomised trials, important quality limitations remain amongst these trials in residential facilities. Statistician involvement on trial teams may be more effective in promoting quality than further journal endorsement of the CONSORT statement. Funding bodies and journals should promote statistician involvement and co-authorship in addition to adherence to CONSORT guidelines.-A systematic review of cluster randomised trials in residential facilities for older people suggests how to improve quality.",1
"We present closed form sample size and power formulas motivated by the study of a psycho-social intervention in which the experimental group has the intervention delivered in teaching subgroups whereas the control group receives usual care. This situation is different from the usual clustered randomized trial because subgroup heterogeneity only exists in one arm. We take this modification into consideration and present formulas for the situation in which we compare a continuous outcome at both a single point in time and longitudinally over time. In addition, we present the optimal combination of parameters such as the number of subgroups and number of time points for minimizing sample size and maximizing power subject to constraints such as the maximum number of measurements that can be taken (i.e., a proxy for cost).-Sample size estimation in educational intervention trials with subgroup heterogeneity in only one arm.",2
"Compared with individual-patient randomized controlled trials, cluster randomized controlled trials have unique methodological and ethical considerations. We evaluated the rationale, methodological quality, and reporting of cluster randomized controlled trials in critical care studies. Systematic searches of Medline, Embase, and Cochrane Central Register were performed. We included all cluster randomized controlled trials conducted in adult, pediatric, or neonatal critical care units from January 2005 to September 2019. Two reviewers independently screened citations, reviewed full texts, protocols, and supplements of potentially eligible studies, abstracted data, and assessed methodology of included studies. From 1,902 citations, 59 cluster randomized controlled trials met criteria. Most focused on quality improvement (24, 41%), antimicrobial therapy (9, 15%), or infection control (9, 15%) interventions. Designs included parallel-group (25, 42%), crossover (21, 36%), and stepped-wedge (13, 22%). Concealment of allocation was reported in 21 studies (36%). Thirteen studies (22%) reported at least one method of blinding. The median total sample size was 1,660 patients (interquartile range, 813-4,295); the median number of clusters was 12 (interquartile range, 5-24); and the median patients per cluster was 141 (interquartile range, 54-452). Sample size calculations were reported in 90% of trials, but only 54% met Consolidated Standards of Reporting Trials guidance for sample size reporting. Twenty-seven of the studies (46%) identified a fixed number of available clusters prior to trial commencement, and only nine (15%) prespecified both the number of clusters and patients required to detect the expected effect size. Overall, 36 trials (68%) achieved the total prespecified sample size. When analyzing data, 44 studies (75%) appropriately adjusted for clustering when analyzing the primary outcome. Only 12 (20%) reported an intracluster coefficient (median 0.047 [interquartile range, 0.01-0.13]). Cluster randomized controlled trials in critical care typically involve a small and fixed number of relatively large clusters. The reporting of key methodological aspects of these trials is often inadequate.-Rationale, Methodological Quality, and Reporting of Cluster-Randomized Controlled Trials in Critical Care Medicine: A Systematic Review",1
"Plasmodium falciparum infections with slow parasite clearance following artemisinin-based therapies are widespread in the Greater Mekong Subregion. A molecular marker of the slow clearance phenotype has been identified: single genetic changes within the propeller region of the Kelch13 protein (pfk13; Pf3D7_1343700). Global searches have identified almost 200 different non-synonymous mutant pfk13 genotypes. Most mutations occur at low prevalence and have uncertain functional significance. To characterize the impact of different pfk13 mutations on parasite clearance, we conducted an individual patient data meta-analysis of the associations between parasite clearance half-life (PC1/2) and pfk13 genotype based on a large set of individual patient records from Asia and Africa. A systematic literature review following the PRISMA protocol was conducted to identify studies published between 2000 and 2017 which included frequent parasite counts and pfk13 genotyping. Four databases (Ovid Medline, PubMed, Ovid Embase, and Web of Science Core Collection) were searched. Eighteen studies (15 from Asia, 2 from Africa, and one multicenter study with sites on both continents) met inclusion criteria and were shared. Associations between the log transformed PC1/2 values and pfk13 genotype were assessed using multivariable regression models with random effects for study site. Both the pfk13 genotypes and the PC1/2 were available from 3250 (95%) patients (n = 3012 from Asia (93%), n = 238 from Africa (7%)). Among Asian isolates, all pfk13 propeller region mutant alleles observed in five or more specific isolates were associated with a 1.5- to 2.7-fold longer geometric mean PC1/2 compared to the PC1/2 of wild type isolates (all p ? 0.002). In addition, mutant allele E252Q located in the P. falciparum region of pfk13 was associated with 1.5-fold (95%CI 1.4-1.6) longer PC1/2. None of the isolates from four countries in Africa showed a significant difference between the PC1/2 of parasites with or without pfk13 propeller region mutations. Previously, the association of six pfk13 propeller mutant alleles with delayed parasite clearance had been confirmed. This analysis demonstrates that 15 additional pfk13 alleles are associated strongly with the slow-clearing phenotype in Southeast Asia. Pooled analysis associated 20 pfk13 propeller region mutant alleles with the slow clearance phenotype, including 15 mutations not confirmed previously.-Association of mutations in the Plasmodium falciparum Kelch13 gene (Pf3D7_1343700) with parasite clearance rates after artemisinin-based treatments-a WWARN individual patient data meta-analysis.",0
"It is well recognized that the conventional summary of treatment effect by averaging across individual patients has its limitation in ignoring the heterogeneous responses to the treatment in the target population. However, there are few alternative metrics in the literature that are designed to capture such heterogeneity. We propose the concept of treatment benefit rate (TBR) and treatment harm rate (THR) that characterize both the overall treatment effect and the magnitude of heterogeneity. We discuss a method to estimate TBR and THR that easily incorporates a sensitivity analysis scheme, and illustrate the idea through analysis of a randomized trial that evaluates the implantable cardioverter-defibrillator (ICD) in reducing mortality. A simulation study is presented to assess the performance of the proposed method.-Treatment benefit and treatment harm rate to characterize heterogeneity in treatment effect.",0
"Recent studies have reported increases in cancer incidence in adults under 50 years. However, there remains uncertainty about whether these are true increases or a result of incidental findings from increased medical imaging. To evaluate these trends, we propose an alternative method to age-period-cohort analyses based on survival modeling. Simulations show that our method is capable of quantifying cohort effects within various backgrounds including increasing medical imaging. We applied the method to analyze the changes in cancer incidence rates for 44 anatomic sites, stratified by sex, by birth cohort for individuals born from 1945 to 1969 in the US based on incidence data from the Surveillance, Epidemiology, and End Results (SEER) program, and tested the validity of our models using later birth cohorts (1970-1974 and 1975-1979). We found that cancer risks have increased significantly in 15 sites (9 in men and 11 in women) for 25-49 year-olds. These results were consistent with previous findings from age-period-cohort analyses. Furthermore, based on our simulations, these increases were independent of increased medical imaging and support substantial, increased extrinsic risks in the identified cancers. Although our approach has several limitations including the restriction to the younger age range and requirement of complete data for all ages of interest, we demonstrate many advantages of our approach including the ease in implementation and interpretation of cohort effects, robustness to various period backgrounds, and ability to make predictions. Our approach should help epidemiologists evaluate cohort effects using incidence data for cancer or other diseases.-Survival model methods for analyses of cancer incidence trends in young adults.",0
"In stepped-wedge trials (SWTs), the intervention is rolled out in a random order over more than 1 time-period. SWTs are often analysed using mixed-effects models that require strong assumptions and may be inappropriate when the number of clusters is small. We propose a non-parametric within-period method to analyse SWTs. This method estimates the intervention effect by comparing intervention and control conditions in a given period using cluster-level data corresponding to exposure. The within-period intervention effects are combined with an inverse-variance-weighted average, and permutation tests are used. We present an example and, using simulated data, compared the method to (1) a parametric cluster-level within-period method, (2) the most commonly used mixed-effects model, and (3) a more flexible mixed-effects model. We simulated scenarios where period effects were common to all clusters, and when they varied according to a distribution informed by routinely collected health data. The non-parametric within-period method provided unbiased intervention effect estimates with correct confidence-interval coverage for all scenarios. The parametric within-period method produced confidence intervals with low coverage for most scenarios. The mixed-effects models' confidence intervals had low coverage when period effects varied between clusters but had greater power than the non-parametric within-period method when period effects were common to all clusters. The non-parametric within-period method is a robust method for analysing SWT. The method could be used by trial statisticians who want to emphasise that the SWT is a randomised trial, in the common position of being uncertain about whether data will meet the assumptions necessary for mixed-effect models.-Robust analysis of stepped wedge trials using cluster-level summaries within periods.",3
"HIV transmission behaviors and health practices of HIV-infected youths were examined over a period of 15 months after they received a preventive intervention. HIV-infected youths aged 13 to 24 years (n = 310; 27% African American, 37% Latino) were assigned by small cohort to (1) a 2-module (""Stay Healthy"" and ""Act Safe"") intervention totaling 23 sessions or (2) a control condition. Among those in the intervention condition, 73% attended at least 1 session. Subsequent to the ""Stay Healthy"" module, number of positive lifestyle changes and active coping styles increased more often among females who attended the intervention condition than among those in the control condition. Social support coping also increased significantly among males and females attending the intervention condition compared with those attending the control condition. Following the ""Act Safe"" module, youths who attended the intervention condition reported 82% fewer unprotected sexual acts, 45% fewer sexual partners, 50% fewer HIV-negative sexual partners, and 31% less substance use, on a weighted index, than those in the control condition. Prevention programs can effectively reduce risk acts among HIV-infected youths. Alternative formats need to be identified for delivering interventions (e.g., telephone groups, individual sessions).-Efficacy of a preventive intervention for youths living with HIV.",0
"The standard approach in a randomized controlled trial (RCT) is to randomize individuals to intervention and control groups. Yet, nursing and other health interventions are often implemented at the levels of health service organizational unit or geographical area. It may be more appropriate to conduct a cluster RCT. However, cluster randomization requires consideration of a number of important issues. The objective of this study was to show how critical issues in relation to design and analysis can be addressed. Two cluster RCTs conducted by the authors are used as examples. Guidance on the conduct and reporting of cluster RCTs is also offered. A rationale for choosing this design was provided, and issues in relation to study design, calculation of sample size, and statistical analysis were clarified. A decision tree and checklist are provided to guide researchers through essential steps in conducting a cluster RCT. Cluster RCTs present special challenges in relation to design, conduct, and analysis. Nevertheless, they are an appropriate and potentially powerful tool for nursing research. With careful attention to the issues addressed in this article, researchers can use this approach successfully.-Planning a cluster randomized controlled trial: methodological issues.",1
"To estimate the incidence, risk factors, and outcomes associated with in-hospital cardiac arrest and cardiopulmonary resuscitation in critically ill adults with coronavirus disease 2019 (covid-19). Multicenter cohort study. Intensive care units at 68 geographically diverse hospitals across the United States. Critically ill adults (age ?18 years) with laboratory confirmed covid-19. In-hospital cardiac arrest within 14 days of admission to an intensive care unit and in-hospital mortality. Among 5019 critically ill patients with covid-19, 14.0% (701/5019) had in-hospital cardiac arrest, 57.1% (400/701) of whom received cardiopulmonary resuscitation. Patients who had in-hospital cardiac arrest were older (mean age 63 (standard deviation 14) v 60 (15) years), had more comorbidities, and were more likely to be admitted to a hospital with a smaller number of intensive care unit beds compared with those who did not have in-hospital cardiac arrest. Patients who received cardiopulmonary resuscitation were younger than those who did not (mean age 61 (standard deviation 14) v 67 (14) years). The most common rhythms at the time of cardiopulmonary resuscitation were pulseless electrical activity (49.8%, 199/400) and asystole (23.8%, 95/400). 48 of the 400 patients (12.0%) who received cardiopulmonary resuscitation survived to hospital discharge, and only 7.0% (28/400) survived to hospital discharge with normal or mildly impaired neurological status. Survival to hospital discharge differed by age, with 21.2% (11/52) of patients younger than 45 years surviving compared with 2.9% (1/34) of those aged 80 or older. Cardiac arrest is common in critically ill patients with covid-19 and is associated with poor survival, particularly among older patients.-In-hospital cardiac arrest in critically ill patients with covid-19: multicenter cohort study.",0
"Treatment comparisons in clinical trials often involve multiple endpoints. By making use of bootstrap tests, we develop a new non-parametric approach to multiple-endpoint testing that can be used to demonstrate non-inferiority of a new treatment for all endpoints and superiority for some endpoint when it is compared to an active control. It is shown that this approach does not incur a large multiplicity cost in sample size to achieve reasonable power and that it can incorporate complex dependencies in the multivariate distributions of all outcome variables for the two treatments via bootstrap resampling.-A combined superiority and non-inferiority approach to multiple endpoints in clinical trials.",0
"The additive hazards model can be easier to interpret and in some cases fits better than the proportional hazards model. However, sample size formulas for clinical trials with time to event outcomes are currently based on either the proportional hazards assumption or an assumption of constant hazards. The goal is to provide sample size formulas for superiority and non-inferiority trials assuming an additive hazards model but no specific distribution, along with evaluations of the performance of the formulas. Formulas are presented that determine the required sample size for a given scenario under the additive hazards model. Simulations are conducted to ensure that the formulas attain the desired power. For illustration, the non-inferiority sample size formula is applied to the calculations in the SPORTIF III trial of stroke prevention in atrial fibrillation. Simulation results show that the sample size calculations lead to the correct power. Sample size is easily calculated using a tool that is available on the web at http://leemcdaniel.github.io/samplesize.html.-Sample size under the additive hazards model.",0
A brief history of the cluster randomised trial design.,1
"Clustered or correlated samples with binary data are frequently encountered in biomedical studies. The clustering may be due to repeated measurements of individuals over time or may be due to subsampling of the primary sampling units. Individuals in the same cluster tend to behave more alike than individuals who belong to different clusters. This exhibition of intracluster correlation decreases the amount of information about the effect of the intervention. In the analysis of randomized cluster trials one must adjust the variance of estimator of the mean for the effect of the positive intraclass correlation p;. We review selected alternative methods to the typical Pearson's chi2 analysis, illustrate these alternatives, and out line an alternative analysis algorithm. We have written and tested a FORTRAN program that produces the statistics outlined in this paper. The program is available in an executable format and is available from the author on request.-Eliminating bias in randomized cluster trials with correlated binomial outcomes.",1
This commentary discusses the D. J. Bauer and P. J. Curran (2003) investigation of growth mixture modeling. Single-class modeling of nonnormal outcomes is compared with modeling with multiple latent trajectory classes. New statistical tests of multiple-class models are discussed. Principles for substantive investigation of growth mixture model results are presented and illustrated by an example of high school dropout predicted by low mathematics achievement development in Grades 7-10.-Statistical and substantive checking in growth mixture modeling: comment on Bauer and Curran (2003).,0
"Background This article studies the design of trials that compare three treatment conditions that are delivered by two types of health professionals. The one type of health professional delivers one treatment, and the other type delivers two treatments, hence, this design is a combination of a nested and crossed design. As each health professional treats multiple patients, the data have a nested structure. This nested structure has thus far been ignored in the design of such trials, which may result in an underestimate of the required sample size. In the design stage, the sample sizes should be determined such that a desired power is achieved for each of the three pairwise comparisons, while keeping costs or sample size at a minimum. Methods The statistical model that relates outcome to treatment condition and explicitly takes the nested data structure into account is presented. Mathematical expressions that relate sample size to power are derived for each of the three pairwise comparisons on the basis of this model. The cost-efficient design achieves sufficient power for each pairwise comparison at lowest costs. Alternatively, one may minimize the total number of patients. The sample sizes are found numerically and an Internet application is available for this purpose. The design is also compared to a nested design in which each health professional delivers just one treatment. Results Mathematical expressions show that this design is more efficient than the nested design. For each pairwise comparison, power increases with the number of health professionals and the number of patients per health professional. The methodology of finding a cost-efficient design is illustrated using a trial that compares treatments for social phobia. The optimal sample sizes reflect the costs for training and supervising psychologists and psychiatrists, and the patient-level costs in the three treatment conditions. Conclusion This article provides the methodology for designing trials that compare three treatment conditions while taking the nesting of patients within health professionals into account. As such, it helps to avoid underpowered trials. To use the methodology, a priori estimates of the total outcome variances and intraclass correlation coefficients must be obtained from experts' opinions or findings in the literature.-Cost-efficient designs for three-arm trials with treatment delivered by health professionals: Sample sizes for a combination of nested and crossed designs.",1
"We present a global test for disease clustering with power to identify disturbances from the null population distribution which accounts for the lag time between the date of exposure and the date of diagnosis. Location at diagnosis is often used as a surrogate for the location of exposure; however, the causative exposure could have occurred at a previous address in a case's residential history. We incorporate models for the incubation distribution of a disease to weight each address into the residential history by the corresponding probability of the exposure occurring at that address. We then introduce a test statistic which uses these incubation-weighted addresses to test for a difference between the spatial distribution of the cases and the spatial distribution of the controls, or the background population. We follow the construction of the M statistic to evaluate the significance of these new distance distributions. Our results show that gains in detection power when residential history is accounted for are of such a degree that it might make the qualitative difference between the presence of spatial clustering being detected or not, thus making a strong argument for the inclusion of residential history in the analysis of such data.-Improving the power of chronic disease surveillance by incorporating residential history.",0
"Participant attrition from randomized controlled trials reduces the statistical power of the study and can potentially introduce bias. Early identification of potential causes of attrition can help reduce patient attrition. We performed secondary analyses of two trials involving cancer patients. To identify predictors of attrition during two early phases, i.e., from consent to screening (Phase-1), and from screening to intake interview (Phase-2) in two clinical trials. Cancer patients undergoing chemotherapy were asked to enroll in one of two clinical trials. In each trial the benefits of a cognitive behavioral intervention were compared with a psycho-educational intervention to assist patients to manage cancer and treatment-related symptoms. Following consent patients were screened for their symptoms' severity to determine their eligibility. Of the 885 consenters 785 completed screening and of the 782 eligible for participation, 713 completed intake interview. In the first phase, longer delays between consent and first contact attempt, lower levels of patient education, minority race, and prolonged duration of screening increased the likelihood of dropping out with a significantly stronger effect on minorities than white patients. In the second phase, low education, being a minority, longer screening delays, and impact of symptom severity on enjoyment of life significantly increased probability of attrition. Participant reported causes of attrition were not modeled; however, exclusion of patients who died during the time period of this research meant that most patients leaving the study made a conscious decision to do so. To assure preservation of external validity, the time between consent and randomization into the arms of a trial must be held to a minimum. Delays between contacts and run in time, that may include screening patients to assure they will benefit from a trial, must be balanced against rates of attrition. Compressing intervals between contacts is particularly important to retain minorities.-Early participant attrition from clinical trials: role of trial design and logistics.",0
"Permutation tests are very useful when parametric assumptions are violated or distributions of test statistics are mathematically intractable. The major advantage of permutation tests is that the procedure is so general that it is applicable to most test statistics. The computational expense is, however, impractical in high-dimensional settings such as genomewide association studies. This study provides a comprehensive review of existing methods that can compute very small p-values efficiently. A common issue with existing methods is that they can only be applied to a specific test statistic. To fill in the knowledge gap, we propose a hybrid method of the sequential Monte Carlo and the Edgeworth expansion approximation for a studentized statistic, which is applicable to a variety of test statistics. The simulation results show that the proposed method performs better than competing methods. Furthermore, applications of the proposed method are demonstrated by statistical analysis on the genomewide association studies data from the Study of Addiction: Genetics and Environment (SAGE).-A hybrid method of the sequential Monte Carlo and the Edgeworth expansion for computation of very small p-values in permutation tests.",0
"Where patients receive therapy as a group, there are good theoretical reasons to believe that variation in the outcome will be smaller for patients treated in the same group than for patients treated in different groups. Similarly, where different therapists treat different groups of patients, outcome for patients treated by the same therapist may differ less than outcome for patients treated by different therapists. Clinical trials evaluating such therapies need to consider this potential lack of independence. As with cluster-randomized trials, this has implications for the precision of treatment effects estimates and statistical power. There are nevertheless differences between clustering due to the organization of treatment and that due to randomization. In cluster-randomized trials the distribution of cluster sizes in each treatment arm should be similar as a consequence of randomization unless there is differential loss to follow-up. With clustering due to therapy group or therapist, cluster size may differ systematically between treatment arms, due to size of therapy groups or differing health professional caseload. Intra-cluster correlation may also differ between treatment arms. The implications of differential cluster size and intracluster correlation for design and analysis will be illustrated by data from two trials, the first comparing nurse practitioner care with general practitioner care, and the second comparing a group therapy with individual treatment as usual. The special case where a group therapy or therapist is compared with an unclustered treatment is examined in detail using a simulation study. The implications of differential clustering effects for sample size and power are addressed. It is argued that the design and analysis of this type of trial should take account of possible heterogeneity in cluster size and intracluster correlation.-Design and analysis of clinical trials with clustering effects due to treatment.",2
"To test rapid approaches that use Drugs@FDA (a public database of approved drugs) and ClinicalTrials.gov to identify trials and to compare these two sources with bibliographic databases as an evidence base for a systematic review and network meta-analysis (NMA). We searched bibliographic databases, Drugs@FDA, and ClinicalTrials.gov for eligible trials on first-line glaucoma medications. We extracted data, assessed risk of bias, and examined the completeness and consistency of information provided by different sources. We fitted random-effects NMA models separately for trials identified from each source and for all unique trials from three sources. We identified 138 unique trials including 29,394 participants on 15 first-line glaucoma medications. For a given trial, information reported was sometimes inconsistent across data sources. Journal articles provided the most information needed for a systematic review; trial registrations provided the least. Compared to an NMA including all unique trials, we were able to generate reasonably precise effect estimates and similar relative rankings for available interventions using trials from Drugs@FDA alone (but not ClinicalTrials.gov). A rapid NMA approach using data from Drugs@FDA is feasible but has its own limitations. Reporting of trial design and results can be improved in both the drug approval packages and on ClinicalTrials.gov.-Rapid network meta-analysis using data from Food and Drug Administration approval packages is feasible but with limitations.",0
"Objective?To investigate the association between in utero exposure to antidepressants and risk of psychiatric disorders.Design?Population based cohort study.Setting?Danish national registers.Participants?905 383 liveborn singletons born during 1998-2012 in Denmark and followed from birth until July 2014, death, emigration, or date of first psychiatric diagnosis, whichever came first. The children were followed for a maximum of 16.5 years and contributed 8.1?106 person years at risk.Exposures for observational studies?Children were categorised into four groups according to maternal antidepressant use within two years before and during pregnancy: unexposed, antidepressant discontinuation (use before but not during pregnancy), antidepressant continuation (use both before and during pregnancy), and new user (use only during pregnancy).Main outcome measure?First psychiatric diagnosis in children, defined as first day of inpatient or outpatient treatment for psychiatric disorders. Hazard ratios of psychiatric disorders were estimated using Cox regression models.Results?Overall, psychiatric disorders were diagnosed in 32 400 children. The adjusted 15 year cumulative incidence of psychiatric disorders was 8.0% (95% confidence interval 7.9% to 8.2%) in the unexposed group, 11.5% (10.3% to 12.9%) in the antidepressant discontinuation group, 13.6% (11.3% to 16.3%) in the continuation group, and 14.5% (10.5% to 19.8%) in the new user group. The antidepressant continuation group had an increased risk of psychiatric disorders (hazard ratio 1.27, 1.17 to 1.38), compared with the discontinuation group.Conclusions?In utero exposure to antidepressants was associated with increased risk of psychiatric disorders. The association may be attributable to the severity of underlying maternal disorders in combination with antidepressant exposure in utero. The findings suggest that focusing solely on a single psychiatric disorder among offspring in studies of in utero antidepressant exposure may be too restrictive.-Antidepressant use during pregnancy and psychiatric disorders in offspring: Danish nationwide register based cohort study.",0
Why we must cluster and cross over.,1
"To consider the effects of contamination on the magnitude and statistical significance (or precision) of the estimated effect of an educational intervention, to investigate the mechanisms of contamination, and to consider how contamination can be avoided. Major electronic databases were searched up to May 2005. An exploratory literature search was conducted. The results of trials included in previous relevant systematic reviews were then analysed to see whether studies that avoided contamination resulted in larger effect estimates than those that did not. Experts' opinions were elicited about factors more or less likely to lead to contamination. We simulated contamination processes to compare contamination biases between cluster and individually randomised trials. Statistical adjustment was made for contamination using Complier Average Causal Effect analytic methods, using published and simulated data. The bias and power of cluster and individually randomised trials were compared, as were Complier Average Causal Effect, intention-to-treat and per protocol methods of analysis. Few relevant studies quantified contamination. Experts largely agreed on where contamination was more or less likely. Simulation of contamination processes showed that, with various combinations of timing, intensity and baseline dependence of contamination, cluster randomised trials might produce biases greater than or similar to those of individually randomised trials. Complier Average Causal Effect analyses produced results that were less biased than intention-to-treat or per protocol analyses. They also showed that individually randomised trials would in most situations be more powerful than cluster randomised trials despite contamination. The probability, nature and process of contamination should be considered when designing and analysing controlled trials of educational interventions in health. Cluster randomisation may or may not be appropriate and should not be uncritically assumed always to be a solution. Complier Average Causal Effect models are an appropriate way to adjust for contamination if it can be measured. When conducting such trials in future, it is a priority to report the extent, nature and effects of contamination.-Contamination in trials of educational interventions.",1
"In cluster randomized trials (CRTs), the outcome of interest is often a count at the cluster level. This occurs, for example, in evaluating an intervention with the outcome being the number of infections of a disease such as HIV or dengue or the number of hospitalizations in the cluster. Standard practice analyzes these counts through cluster outcome rates using an appropriate denominator (eg, population size). However, such denominators are sometimes unknown, particularly when the counts depend on a passive community surveillance system. We consider direct comparison of the counts without knowledge of denominators, relying on randomization to balance denominators. We also focus on permutation tests to allow for small numbers of randomized clusters. However, such approaches are subject to bias when there is differential ascertainment of counts across arms, a situation that may occur in CRTs that cannot implement blinded interventions. We suggest the use of negative control counts as a method to remove, or reduce, this bias, discussing the key properties necessary for an effective negative control. A current example of such a design is the recent extension of test-negative designs to CRTs testing community-level interventions. Via simulation, we compare the performance of new and standard estimators based on CRTs with negative controls to approaches that only use the original counts. When there is no differential ascertainment by intervention arm, the count-only approaches perform comparably to those using debiasing negative controls. However, under even modest differential ascertainment, the count-only estimators are no longer reliable.-Analysis of counts for cluster randomized trials: Negative controls and test-negative designs",1
"The importance of developing personalized risk prediction estimates has become increasingly evident in recent years. In general, patient populations may be heterogenous and represent a mixture of different unknown subtypes of disease. When the source of this heterogeneity and resulting subtypes of disease are unknown, accurate prediction of survival may be difficult. However, in certain disease settings, the onset time of an observable short-term event may be highly associated with these unknown subtypes of disease and thus may be useful in predicting long-term survival. One approach to incorporate short-term event information along with baseline markers for the prediction of long-term survival is through a landmark Cox model, which assumes a proportional hazards model for the residual life at a given landmark point. In this paper, we use this modeling framework to develop procedures to assess how a patient's long-term survival trajectory may change over time given good short-term outcome indications along with prognosis on the basis of baseline markers. We first propose time-varying accuracy measures to quantify the predictive performance of landmark prediction rules for residual life and provide resampling-based procedures to make inference about such accuracy measures. Simulation studies show that the proposed procedures perform well in finite samples. Throughout, we illustrate our proposed procedures by using a breast cancer dataset with information on time to metastasis and time to death. In addition to baseline clinical markers available for each patient, a chromosome instability genetic score, denoted by CIN25, is also available for each patient and has been shown to be predictive of survival for various types of cancer. We provide procedures to evaluate the incremental value of CIN25 for the prediction of residual life and examine how the residual life profile changes over time. This allows us to identify an informative landmark point, t(0) , such that accurate risk predictions of the residual life could be made for patients who survive past t(0) without metastasis.-Landmark risk prediction of residual life for breast cancer survival.",0
"Accelerated failure time model is a popular model to analyze censored time-to-event data. Analysis of this model without assuming any parametric distribution for the model error is challenging, and the model complexity is enhanced in the presence of large number of covariates. We developed a nonparametric Bayesian method for regularized estimation of the regression parameters in a flexible accelerated failure time model. The novelties of our method lie in modeling the error distribution of the accelerated failure time nonparametrically, modeling the variance as a function of the mean, and adopting a variable selection technique in modeling the mean. The proposed method allowed for identifying a set of important regression parameters, estimating survival probabilities, and constructing credible intervals of the survival probabilities. We evaluated operating characteristics of the proposed method via simulation studies. Finally, we apply our new comprehensive method to analyze the motivating breast cancer data from the Surveillance, Epidemiology, and End Results Program, and estimate the five-year survival probabilities for women included in the Surveillance, Epidemiology, and End Results database who were diagnosed with breast cancer between 1990 and 2000.-Bayesian variable selection in the accelerated failure time model with an application to the surveillance, epidemiology, and end results breast cancer data.",0
"We consider semiparametric regression for periodic longitudinal data. Parametric fixed effects are used to model the covariate effects and a periodic nonparametric smooth function is used to model the time effect. The within-subject correlation is modeled using subject-specific random effects and a random stochastic process with a periodic variance function. We use maximum penalized likelihood to estimate the regression coefficients and the periodic nonparametric time function, whose estimator is shown to be a periodic cubic smoothing spline. We use restricted maximum likelihood to simultaneously estimate the smoothing parameter and the variance components. We show that all model parameters can be easily obtained by fitting a linear mixed model. A common problem in the analysis of longitudinal data is to compare the time profiles of two groups, e.g., between treatment and placebo. We develop a scaled chi-squared test for the equality of two nonparametric time functions. The proposed model and the test are illustrated by analyzing hormone data collected during two consecutive menstrual cycles and their performance is evaluated through simulations.-Semiparametric regression for periodic longitudinal hormone data from multiple menstrual cycles.",0
"Research is needed to identify promising recruitment strategies to reach and engage diverse young adults in diabetes clinical research. The aim of this study was to examine the relative strengths and weaknesses of three recruitment strategies used in a diabetes self-management clinical trial: social media advertising (Facebook), targeted mailing, and in-person solicitation of clinic patients. Strategies were compared in terms of (1) cost-effectiveness (i.e. cost of recruitment/number of enrolled participants), (2) ability to yield participants who would not otherwise be reached by alternative strategies, and (3) likelihood of participants recruited through each strategy to adhere to study procedures. We further explored the appeal (overall and among age and gender subgroups) of social media advertisement features. In-person recruitment of clinic patients was overall the most cost-effective strategy. However, differences in demographic, clinical, and psychosocial characteristics of participants recruited via different strategies suggest that the combination of these approaches yielded a more diverse sample than would any one strategy alone. Once successfully enrolled, there was no difference in study completion and intervention adherence between individuals recruited by the three recruitment strategies. Ultimately, the utility of a recruitment strategy is defined by its ability to effectively attract people representative of the target population who are willing to enroll in and complete the study. Leveraging a variety of recruitment strategies appears to produce a more representative sample of young adults, including those who are less engaged in diabetes care.-Effectiveness of social media (Facebook), targeted mailing, and in-person solicitation for the recruitment of young adult in a diabetes self-management clinical trial.",0
[Anticipations: risk of bias in cluster randomized trials].,1
"Cluster randomised trials, in which groups of individuals are randomised, are increasingly being used in the health field. Adopting a clustered approach has implications for the design of such trials, and sample size calculations need to be inflated to accommodate for the clustering effect. Reliable estimates of intracluster correlation coefficients (ICCs) are required for robust sample size calculations to be made; however, little empirical evidence is available on their likely size, and on factors which influence their magnitude. The aim of this study was to generate empirical estimates of ICCs and to explore factors which may affect their magnitude. Empirical estimates of ICCs were calculated for both process variables and patient outcomes from a number of datasets of primary and secondary care implementation studies. Estimates of ICCs varied according to setting and type of outcome. Estimates of ICCs for process variables were higher than those for patient outcomes, and estimates derived from secondary care were higher than those from primary care. ICCs for process variables in primary care were of the order of 0.05-0.15, whilst those in secondary care were of the order of 0.3. Estimates for patient outcomes in primary care were generally lower than 0.05. Adopting cluster randomisation has implications for the design, size and analysis of clinical trials. This study gives an insight into the potential size of ICCs in primary and secondary care, and provides a practical guide to researchers to aid the planning of future studies in this area.-Sample size calculations for cluster randomised trials. Changing Professional Practice in Europe Group (EU BIOMED II Concerted Action).",1
"Receiver operating characteristic (ROC) analysis is widely used to evaluate the performance of diagnostic tests with continuous or ordinal responses. A popular study design for assessing the accuracy of diagnostic tests involves multiple readers interpreting multiple diagnostic test results, called the multi-reader, multi-test design. Although several different approaches to analyzing data from this design exist, few methods have discussed the sample size and power issues. In this article, we develop a power formula to compare the correlated areas under the ROC curves (AUC) in a multi-reader, multi-test design. We present a nonparametric approach to estimate and compare the correlated AUCs by extending DeLong et al.'s (1988, Biometrics 44, 837-845) approach. A power formula is derived based on the asymptotic distribution of the nonparametric AUCs. Simulation studies are conducted to demonstrate the performance of the proposed power formula and an example is provided to illustrate the proposed procedure.-Power calculation for comparing diagnostic accuracies in a multi-reader, multi-test design.",0
An Analysis of Variance Pitfall - the Fixed Effects Analysis in a Nested Design,1
"To date, no cross-national RCT has addressed the mechanisms underlying the relative success of pharmacological and psychotherapeutic interventions for depression. A multi-site clinical trial that includes psychotherapy as one of the treatments presents numerous challenges related to cross-site consistency and communication. This report describes how those challenges were met in the study ""Depression: The Search for Treatment Relevant Phenotypes'', being carried out at the University of Pittsburgh and the University of Pisa, Italy. Implementing the study required the investigators to address methodological and practical challenges related to the different requirements of the two Institutional Review Boards (IRBs), psychotherapy training, independent evaluator training, patient recruitment, development of common tools for data entry, quality control and generation of weekly reports of patient progress as well as establishing a similar clinical and research framework in two countries with substantially different health care systems. By having bilingual investigators and staff members who spent time at one another's sites, making use of frequent conference-call staff meetings and being flexible within the bounds of the sometimes contradictory requirements of the IRBs, the investigators were able to meet the human subjects protection requirements of both institutions, surmount language barriers to consistent therapist and evaluator training and develop common tools for study management. As a result, recruitment goals were met at both sites and retention rates were high. One instance of inconsistent implementation of the protocol was corrected within the first year. This study was conducted in two Western cultures by researchers with long-standing collaboration. Our findings may not be generalizable to other countries or research settings. The implementation of a cross-national protocol and the adoption and maintenance of common procedures is possible when investigators are aware of the challenges this may present and are proactive in trying to address them.-Addressing the challenges of a cross-national investigation: lessons from the Pittsburgh-Pisa study of treatment-relevant phenotypes of unipolar depression.",0
"Cluster randomised trials can be susceptible to a range of methodological problems. These problems are not commonly recognised by many researchers. In this paper we discuss the issues that can lead to bias in cluster trials. We used a sample of cluster randomised trials from a recent review and from a systematic review of hip protectors. We compared the mean age of participants between intervention groups in a sample of 'good' cluster trials with a sample of potentially biased trials. We also compared the effect sizes, in a funnel plot, between hip protector trials that used individual randomisation compared with those that used cluster randomisation. There is a tendency for cluster trials, with evidence methodological biases, to also show an age imbalance between treatment groups. In a funnel plot we show that all cluster trials show a large positive effect of hip protectors whilst individually randomised trials show a range of positive and negative effects, suggesting that cluster trials may be producing a biased estimate of effect. Methodological biases in the design and execution of cluster randomised trials is frequent. Some of these biases associated with the use of cluster designs can be avoided through careful attention to the design of cluster trials. Firstly, if possible, individual allocation should be used. Secondly, if cluster allocation is required, then ideally participants should be identified before random allocation of the clusters. Third, if prior identification is not possible, then an independent recruiter should be used to recruit participants.-Methodological bias in cluster randomised trials.",1
"Unbiased estimation of causal effects with observational data requires adjustment for confounding variables that are related to both the outcome and treatment assignment. Standard variable selection techniques aim to maximize predictive ability of the outcome model, but they ignore covariate associations with treatment and may not adjust for important confounders weakly associated to outcome. We propose a novel method for estimating causal effects that simultaneously considers models for both outcome and treatment, which we call the bilevel spike and slab causal estimator (BSSCE). By using a Bayesian formulation, BSSCE estimates the posterior distribution of all model parameters and provides straightforward and reliable inference. Spike and slab priors are used on each covariate coefficient which aim to minimize the mean squared error of the treatment effect estimator. Theoretical properties of the treatment effect estimator are derived justifying the prior used in BSSCE. Simulations show that BSSCE can substantially reduce mean squared error over numerous methods and performs especially well with large numbers of covariates, including situations where the number of covariates is greater than the sample size. We illustrate BSSCE by estimating the causal effect of vasoactive therapy vs. fluid resuscitation on hypotensive episode length for patients in the Multiparameter Intelligent Monitoring in Intensive Care III critical care database.-Variable selection and estimation in causal inference using Bayesian spike and slab priors.",0
"Cluster randomized trials (CRTs) refer to experiments with randomization carried out at the cluster or the group level. While numerous statistical methods have been developed for the design and analysis of CRTs, most of the existing methods focused on testing the overall treatment effect across the population characteristics, with few discussions on the differential treatment effect among subpopulations. In addition, the sample size and power requirements for detecting differential treatment effect in CRTs remain unclear, but are helpful for studies planned with such an objective. In this article, we develop a new sample size formula for detecting treatment effect heterogeneity in two-level CRTs for continuous outcomes, continuous or binary covariates measured at cluster or individual level. We also investigate the roles of two intraclass correlation coefficients (ICCs): the adjusted ICC for the outcome of interest and the marginal ICC for the covariate of interest. We further derive a closed-form design effect formula to facilitate the application of the proposed method, and provide extensions to accommodate multiple covariates. Extensive simulations are carried out to validate the proposed formula in finite samples. We find that the empirical power agrees well with the prediction across a range of parameter constellations, when data are analyzed by a linear mixed effects model with a treatment-by-covariate interaction. Finally, we use data from the HF-ACTION study to illustrate the proposed sample size procedure for detecting heterogeneous treatment effects.-Sample size requirements for detecting treatment effect heterogeneity in cluster randomized trials",1
This paper explores the role of balancing covariates between treatment groups in the design of cluster randomized trials. General expressions are obtained for two criteria to evaluate designs for parallel group studies with two treatments. The first is the variance of the estimated treatment effect and the second is the extent to which the estimated treatment effect is changed by adjusting for covariates. It is argued that the second of these is more important for cluster randomized trials. Methods of obtaining balanced designs from covariates which are available at the start of a study are proposed. An imbalance measure is used to compare the extent to which designs balance important covariates between the arms of a trial. Several approaches to selecting a well balanced design are possible. A method that randomly selects one member from the class of designs with acceptable bias will allow randomization inference as well as model-based inference. The methods are illustrated with data from a trial of school-based sex education.-Balance in cluster randomized trials.,1
"""Cluster randomized trials,"" in which groups of patients are randomly assigned to different therapeutic interventions, provide a powerful way of evaluating drugs. CRTs have not been widely used, in good part because of concerns about whether patients must give informed consent to participate in them. A better understanding of how CRTs fit into clinical practice resolves the concerns.-Comparing drug effectiveness at health plans: the ethics of cluster randomized trials.",1
"In cluster-randomized trials, intervention effects are often formulated by specifying marginal models, fitting them under a working independence assumption, and using robust variance estimates to address the association in the responses within clusters. We develop sample size criteria within this framework, with analyses based on semiparametric Cox regression models fitted with event times subject to right censoring. At the design stage, copula models are specified to enable derivation of the asymptotic variance of estimators from a marginal Cox regression model and to compute the number of clusters necessary to satisfy power requirements. Simulation studies demonstrate the validity of the sample size formula in finite samples for a range of cluster sizes, censoring rates, and degrees of within-cluster association among event times. The power and relative efficiency implications of copula misspecification is studied, as well as the effect of within-cluster dependence in the censoring times. Sample size criteria and other design issues are also addressed for the setting where the event status is only ascertained at periodic assessments and times are interval censored.-Sample size and robust marginal methods for cluster-randomized trials with censored event times.",1
"We used a biomarker of activity-related energy expenditure (AREE) to assess measurement properties of self-reported physical activity and to determine the usefulness of AREE regression calibration equations in the Women's Health Initiative. Biomarker AREE, calculated as the total energy expenditure from doubly labeled water minus the resting energy expenditure from indirect calorimetry, was assessed in 450 Women's Health Initiative participants (2007-2009). Self-reported AREE was obtained from the Arizona Activity Frequency Questionnaire (AAFQ), the 7-Day Physical Activity Recall (PAR), and the Women's Health Initiative Personal Habits Questionnaire (PHQ). Eighty-eight participants repeated the protocol 6 months later. Reporting error, measured as log(self-report AREE) minus log(biomarker AREE), was regressed on participant characteristics for each instrument. Body mass index was associated with underreporting on the AAFQ and PHQ but overreporting on PAR. Blacks and Hispanics underreported physical activity levels on the AAFQ and PAR, respectively. Underreporting decreased with age for the PAR and PHQ. Regressing logbiomarker AREE on logself-reported AREE revealed that self-report alone explained minimal biomarker variance (R(2) = 7.6, 4.8, and 3.4 for AAFQ, PAR, and PHQ, respectively). R(2) increased to 25.2, 21.5, and 21.8, respectively, when participant characteristics were included. Six-month repeatability data adjusted for temporal biomarker variation, improving R(2) to 79.4, 67.8, and 68.7 for AAFQ, PAR, and PHQ, respectively. Calibration equations ""recover"" substantial variation in average AREE and valuably enhance AREE self-assessment.-Physical activity assessment: biomarkers and self-report of activity-related energy expenditure in the WHI.",0
Outcomes that patients perceive and value are systematically unassessed in randomized clinical trials of endocrine-related illnesses: a?systematic review.,0
"Fecal microbiota transplantation is a highly effective intervention for patients suffering from recurrent Clostridium difficile, a common hospital-acquired infection. Fecal microbiota transplantation's success as a therapy for C. difficile has inspired interest in performing clinical trials that experiment with fecal microbiota transplantation as a therapy for other conditions like inflammatory bowel disease, obesity, diabetes, and Parkinson's disease. Results from clinical trials that use fecal microbiota transplantation to treat inflammatory bowel disease suggest that, for at least one condition beyond C. difficile, most fecal microbiota transplantation donors produce stool that is not efficacious. The optimal strategies for identifying and using efficacious donors have not been investigated. We therefore examined the optimal Bayesian response-adaptive strategy for allocating patients to donors and formulated a computationally tractable myopic heuristic. This heuristic computes the probability that a donor is efficacious by updating prior expectations about the efficacy of fecal microbiota transplantation, the placebo rate, and the fraction of donors that produce efficacious stool. In simulations designed to mimic a recent fecal microbiota transplantation clinical trial, for which traditional power calculations predict [Formula: see text] statistical power, we found that accounting for differences in donor stool efficacy reduced the predicted statistical power to [Formula: see text]. For these simulations, using the heuristic Bayesian allocation strategy more than quadrupled the statistical power to [Formula: see text]. We use the results of similar simulations to make recommendations about the number of patients, the number of donors, and the choice of clinical endpoint that clinical trials should use to optimize their ability to detect if fecal microbiota transplantation is effective for treating a condition.-Designing fecal microbiota transplant trials that account for differences in donor stool efficacy.",0
"Cluster randomized trials (CRTs) were originally proposed for use when randomization at the subject level is practically infeasible or may lead to a severe estimation bias of the treatment effect. However, recruiting an additional cluster costs more than enrolling an additional subject in an individually randomized trial. Under budget constraints, researchers have proposed the optimal sample sizes in two-level CRTs. CRTs may have a three-level structure, in which two levels of clustering should be considered. In this paper, we propose optimal designs in three-level CRTs with a binary outcome, assuming a nested exchangeable correlation structure in generalized estimating equation models. We provide the variance of estimators of three commonly used measures: risk difference, risk ratio, and odds ratio. For a given sampling budget, we discuss how many clusters and how many subjects per cluster are necessary to minimize the variance of each measure estimator. For known association parameters, the locally optimal design is proposed. When association parameters are unknown but within predetermined ranges, the MaxiMin design is proposed to maximize the minimum of relative efficiency over the possible ranges, that is, to minimize the risk of the worst scenario.-Optimal designs in three-level cluster randomized trials with a binary outcome",1
"Trials in which treatments induce clustering of observations in one of two treatment arms, such as when comparing group therapy with pharmacological treatment or with a waiting-list group, are examined with respect to the efficiency loss caused by varying cluster sizes. When observations are (approximately) normally distributed, treatment effects can be estimated and tested through linear mixed model analysis. For maximum likelihood estimation, the asymptotic relative efficiency of unequal versus equal cluster sizes is derived. In an extensive Monte Carlo simulation for small sample sizes, the asymptotic relative efficiency turns out to be accurate for the treatment effect, but less accurate for the random intercept variance. For the treatment effect, the efficiency loss due to varying cluster sizes rarely exceeds 10 per cent, which can be regained by recruiting 11 per cent more clusters for one arm and 11 per cent more persons for the other. For the intercept variance the loss can be 16 per cent, which requires recruiting 19 per cent more clusters for one arm, with no additional recruitment of subjects for the other arm.-Varying cluster sizes in trials with clusters in one treatment arm: sample size adjustments when testing treatment effects with linear mixed models.",1
"Outcome-dependent sampling (ODS) study designs are commonly implemented with rare diseases or when prospective studies are infeasible. In longitudinal data settings, when a repeatedly measured binary response is rare, an ODS design can be highly efficient for maximizing statistical information subject to resource limitations that prohibit covariate ascertainment of all observations. This manuscript details an ODS design where individual observations are sampled with probabilities determined by an inexpensive, time-varying auxiliary variable that is related but is not equal to the response. With the goal of validly estimating marginal model parameters based on the resulting biased sample, we propose a semi-parametric, sequential offsetted logistic regressions (SOLR) approach. The SOLR strategy first estimates the relationship between the auxiliary variable and the response and covariate data by using an offsetted logistic regression analysis where the offset is used to adjust for the biased design. Results from the auxiliary variable model are then combined with the known or estimated sampling probabilities to formulate a second offset that is used to correct for the biased design in the ultimate target model relating the longitudinal binary response to covariates. Because the target model offset is estimated with SOLR, we detail asymptotic standard error estimates that account for uncertainty associated with the auxiliary variable model. Motivated by an analysis of the BioCycle Study (Gaskins et al., Effect of daily fiber intake on reproductive function: the BioCycle Study. American Journal of Clinical Nutrition 2009; 90(4): 1061-1069) that aims to describe the relationship between reproductive health (determined by luteinizing hormone levels) and fiber consumption, we examine properties of SOLR estimators and compare them with other common approaches.-Outcome-dependent sampling for longitudinal binary response data based on a time-varying auxiliary variable.",0
"The Mann-Whitney U-test is ubiquitous in statistical practice for the comparison of measures of location for two samples where the assumption of normality is questionable. Frequently, one has replicate data for each individual in a group and would like to compare measures of central tendency between groups without assuming normality. For this purpose, we present a generalization of the Mann-Whitney U-test for clustered data. The test is performed by computing zc = (Wc - mu c)/sigma c, approximately N(0, 1) under H0, where Wc, mu c are the observed and expected Mann-Whitney U-statistic based on a comparison of all pairs of replicates in the two groups and sigma c is the standard deviation of Wc that is modified to account for clustering effects within a cluster. We obtain an explicit variance formula that is a function of four clustering parameters. We validate the properties of the test procedure in a simulation study. We illustrate the methods with an example comparing the baseline Humphrey visual field between two treatment groups in a randomized clinical trial of patients with retinitis pigmentosa (RP).-Use of the Mann-Whitney U-test for clustered data.",1
"Various papers have addressed pros and cons of the stepped wedge cluster randomized trial design (SWD). However, some issues have not or only limitedly been addressed. Our aim was to provide a comprehensive overview of all merits and limitations of the SWD to assist researchers, reviewers and medical ethics committees when deciding on the appropriateness of the SWD for a particular study. We performed an initial search to identify articles with a methodological focus on the SWD, and categorized and discussed all reported advantages and disadvantages of the SWD. Additional aspects were identified during multidisciplinary meetings in which ethicists, biostatisticians, clinical epidemiologists and health economists participated. All aspects of the SWD were compared to the parallel group cluster randomized design. We categorized the merits and limitations of the SWD to distinct phases in the design and conduct of such studies, highlighting that their impact may vary depending on the context of the study or that benefits may be offset by drawbacks across study phases. Furthermore, a real-life illustration is provided. New aspects are identified within all disciplines. Examples of newly identified aspects of an SWD are: the possibility to measure a treatment effect in each cluster to examine the (in)consistency in effects across clusters, the detrimental effect of lower than expected inclusion rates, deviation from the ordinary informed consent process and the question whether studies using the SWD are likely to have sufficient social value. Discussions are provided on e.g. clinical equipoise, social value, health economical decision making, number of study arms, and interim analyses. Deciding on the use of the SWD involves aspects and considerations from different disciplines not all of which have been discussed before. Pros and cons of this design should be balanced in comparison to other feasible design options as to choose the optimal design for a particular intervention study.-The need to balance merits and limitations from different disciplines when considering the stepped wedge cluster randomized trial design.",3
"Drug-induced liver injury (DILI) is a common complication of tuberculosis treatment. We utilised data from the REMoxTB clinical trial to describe the incidence of predisposing factors and the natural history in patients with liver enzyme levels elevated in response to tuberculosis treatment. Patients received either standard tuberculosis treatment (2EHRZ/4HR), or a 4-month regimen in which moxifloxacin replaced either ethambutol (isoniazid arm, 2MHRZ/2MHR) or isoniazid (ethambutol arm, 2EMRZ/2MR). Hepatic enzymes were measured at 0, 2, 4, 8, 12 and 17 weeks and as clinically indicated during reported adverse events. Patients included were those receiving at least one dose of drug and with two or more hepatic enzyme measurements. A total of 1928 patients were included (639 2EHRZ/4HR, 654 2MHRZ/2MHR and 635 2EMRZ/2MR). DILI was defined as peak alanine aminotransferase (ALT) ? 5 times the upper limit of normal (5 ? ULN) or ALT ? 3 ? ULN with total bilirubin &gt; 2 ? ULN. DILI was identified in 58 of the 1928 (3.0%) patients at a median time of 28 days (interquartile range IQR 14-56). Of 639 (6.4%) patients taking standard tuberculosis therapy, 41 experienced clinically significant enzyme elevations (peak ALT ? 3 ? ULN). On standard therapy, 21.1% of patients aged &gt;55 years developed a peak ALT/aspartate aminotransferase (AST) ? 3 ? ULN (p = 0.01) and 15% of HIV-positive patients experienced a peak ALT/AST ? 3 ? ULN compared to 9% of HIV-negative patients (p = 0.160). The median peak ALT/AST was higher in isoniazid-containing regimens vs no-isoniazid regimens (p &lt; 0.05), and lower in moxifloxacin-containing arms vs no-moxifloxacin arms (p &lt; 0.05). Patients receiving isoniazid reached a peak ALT ? 3 ? ULN 9.5 days earlier than those on the ethambutol arm (median time of 28 days vs 18.5 days). Of the 67 Asian patients with a peak ALT/AST ? 3 ? ULN, 57 (85.1%) were on an isoniazid-containing regimen (p = 0.008). Our results provide evidence of the risk of DILI in tuberculosis patients on standard treatment. Older patients on standard therapy, HIV-positive patients, Asian patients and those receiving isoniazid were at higher risk of elevated enzyme levels. Monitoring hepatic enzymes during the first 2 months of standard therapy detected approximately 75% of patients with a peak enzyme elevation ?3 ? ULN, suggesting this should be a standard of care. These results provide evidence for the potential of moxifloxacin in hepatic sparing.-Liver toxicity associated with tuberculosis chemotherapy in the REMoxTB study.",0
Structuring Sexual Pleasure: Equitable Access to Biomedical HIV Prevention for Black Men Who Have Sex with Men.,0
"The statistical power of cluster randomized trials depends on two sample size components, the number of clusters per group and the numbers of individuals within clusters (cluster size). Variable cluster sizes are common and this variation alone may have significant impact on study power. Previous approaches have taken this into account by either adjusting total sample size using a designated design effect or adjusting the number of clusters according to an assessment of the relative efficiency of unequal versus equal cluster sizes. This article defines a relative efficiency of unequal versus equal cluster sizes using noncentrality parameters, investigates properties of this measure, and proposes an approach for adjusting the required sample size accordingly. We focus on comparing two groups with normally distributed outcomes using t-test, and use the noncentrality parameter to define the relative efficiency of unequal versus equal cluster sizes and show that statistical power depends only on this parameter for a given number of clusters. We calculate the sample size required for an unequal cluster sizes trial to have the same power as one with equal cluster sizes. Relative efficiency based on the noncentrality parameter is straightforward to calculate and easy to interpret. It connects the required mean cluster size directly to the required sample size with equal cluster sizes. Consequently, our approach first determines the sample size requirements with equal cluster sizes for a pre-specified study power and then calculates the required mean cluster size while keeping the number of clusters unchanged. Our approach allows adjustment in mean cluster size alone or simultaneous adjustment in mean cluster size and number of clusters, and is a flexible alternative to and a useful complement to existing methods. Comparison indicated that we have defined a relative efficiency that is greater than the relative efficiency in the literature under some conditions. Our measure of relative efficiency might be less than the measure in the literature under some conditions, underestimating the relative efficiency. The relative efficiency of unequal versus equal cluster sizes defined using the noncentrality parameter suggests a sample size approach that is a flexible alternative and a useful complement to existing methods.-Relative efficiency and sample size for cluster randomized trials with variable cluster sizes.",1
"Systematic reviews and meta-analyses are labor-intensive and time-consuming. Automated extraction of quantitative data from primary studies can accelerate this process. ClinicalTrials.gov, launched in 2000, is the world's largest trial repository of results data from clinical trials; it has been used as a source instead of journal articles. We have developed a Web application called EXACT (EXtracting Accurate efficacy and safety information from ClinicalTrials.gov) that allows users without advanced programming skills to automatically extract data from ClinicalTrials.gov in analysis-ready format. We have also used the automatically extracted data to examine the reproducibility of meta-analyses in three published systematic reviews. We developed a Python-based software application (EXACT) that automatically extracts data required for meta-analysis from the ClinicalTrials.gov database in a spreadsheet format. We confirmed the accuracy of the extracted data and then used those data to repeat meta-analyses in three published systematic reviews. To ensure that we used the same statistical methods and outcomes as the published systematic reviews, we repeated the meta-analyses using data manually extracted from the relevant journal articles. For the outcomes whose results we were able to reproduce using those journal article data, we examined the usability of ClinicalTrials.gov data. EXACT extracted data at ClincalTrials.gov with 100% accuracy, and it required 60% less time than the usual practice of manually extracting data from journal articles. We found that 87% of the data elements extracted using EXACT matched those extracted manually from the journal articles. We were able to reproduce 24 of 28 outcomes using the journal article data. Of these 24 outcomes, we were able to reproduce 83.3% of the published estimates using data at ClinicalTrials.gov. EXACT (http://bio-nlp.org/EXACT) automatically and accurately extracted data elements from ClinicalTrials.gov and thus reduced time in data extraction. The ClinicalTrials.gov data reproduced most meta-analysis results in our study, but this conclusion needs further validation.-Automatic extraction of quantitative data from ClinicalTrials.gov to conduct meta-analyses.",0
Erratum to: Sample size calculations for cluster randomised controlled trials with a fixed number of clusters.,1
"In this tutorial, we describe regression-based methods for analysing multiple source data arising from complex sample survey designs. We use the term 'multiple-source' data to encompass all cases where data are simultaneously obtained from multiple informants, or raters (e.g. self-reports, family members, health care providers, administrators) or via different/parallel instruments, indicators or methods (e.g. symptom rating scales, standardized diagnostic interviews, or clinical diagnoses). We review regression models for analysing multiple source risk factors or multiple source outcomes and show that they can be considered special cases of generalized linear models, albeit with correlated outcomes. We show how these methods can be extended to handle the common survey features of stratification, clustering, and sampling weights. We describe how to fit regression models with multiple source reports derived from complex sample surveys using general purpose statistical software. Finally, the methods are illustrated using data from two studies: the Stirling County Study and the Eastern Connecticut Child Survey.-Regression analysis of multiple source and multiple informant data from complex survey samples.",0
"Currently, there is considerable interest in studies that use the community as the experimental unit. Health promotion programmes are one example. Because such activities are expensive, the number of experimental units (communities) is usually very small. Investigators often match communities on demographic variables in order to improve the power of their studies. Matching is known to improve power in certain circumstances. However, we show here that if the number of communities is small, the matched design will probably have less power than the unmatched design. This is due primarily to the loss of degrees of freedom in the matched design, which outweighs the benefits of matching on any but the strongest correlates of changes in behaviour. In the community intervention situation, even small differences in sample size between the matched and unmatched analyses can have expensive consequences.-The effect of matching on the power of randomized community intervention studies.",1
"Time-dependent receiver operating characteristic (ROC) curves and their area under the curve (AUC) are important measures to evaluate the prediction accuracy of biomarkers for time-to-event endpoints (e.g., time to disease progression or death). In this article, we propose a direct method to estimate AUC(t) as a function of time t using a flexible fractional polynomials model, without the middle step of modeling the time-dependent ROC. We develop a pseudo partial-likelihood procedure for parameter estimation and provide a test procedure to compare the predictive performance between biomarkers. We establish the asymptotic properties of the proposed estimator and test statistics. A major advantage of the proposed method is its ease to make inference and to compare the prediction accuracy across biomarkers, rendering our method particularly appealing for studies that require comparing and screening a large number of candidate biomarkers. We evaluate the finite-sample performance of the proposed method through simulation studies and illustrate our method in an application to AIDS Clinical Trials Group 175 data.-A direct method to evaluate the time-dependent predictive accuracy for biomarkers.",0
"Behaviour modification is often delivered to teaching subgroups. For example, experimental and control smoking cessation programmes may be given to 15 classes (subgroups) with 10 (otherwise independent) individuals. We present general statistical tests and power estimates to compare continuous outcomes from two interventions in settings where the magnitude of teaching subgroup heterogeneity, number of subgroups and subgroup size can differ between intervention arms. An application is made to data from a trial to reduce disease-transmitting sexual behaviour. The statistical impact of teaching subgroup heterogeneity effect increases as the (a) number of participants in a subgroup increases, and (b) ratio of 'averaged experimental and control subgroup effect variance' to study subject variance increases. If plausible levels of subgroup teaching effect heterogeneity are ignored, the true sizes of tests with nominal 0.05 two-sided type I errors range from 0.055 to 0.47, while when planning studies, estimated sample sizes are only 11.1-95.2 per cent of the true requirements.-Clinical trials of behavioural interventions with heterogeneous teaching subgroup effects.",2
Cluster randomized trials with a small number of clusters: which analyses should be used?,1
"Cluster randomized trials (CRTs) are useful in practice-based research network translational research. However, simple or stratified randomization often yields study groups that differ on key baseline variables when the number of clusters is small. Unbalanced study arms constitute a potentially serious methodological problem for CRTs. Covariate constrained randomization with data on relevant variables before randomization was used to achieve balanced study arms in 2 pragmatic CRTs. In study 1, 16 counties in Colorado were randomized to practice-based or population-based reminder recall for vaccinating children ages 19 to 35 months. In study 2, 18 primary care practices were randomized to computer decision support plus practice facilitation versus computer decision support alone to improve care for patients with stage 3 and 4 chronic kidney disease. For each study, a set of optimal randomizations, which minimized differences of key variables between study arms, was identified from the set of all possible randomizations. Differences between study arms were smaller in the optimal versus remaining randomizations. Even for the randomization in the optimal set with the largest difference between groups, study arms did not differ significantly on any variable for either study (P &gt; .05). Covariate constrained randomization, which restricts the full randomization set to a subset in which differences between study arms are minimized, is a useful tool for achieving balanced study arms in CRTs. Because of the increasing recognition of the risk of imbalance in CRTs and implications for interpreting study findings, procedures of this type should be considered in designing practice-based or community-based trials.-Pragmatic Cluster Randomized Trials Using Covariate Constrained Randomization: A Method for Practice-based Research Networks (PBRNs).",1
"Three-level cluster randomized trials (CRTs) are increasingly used in implementation science, where 2fold-nested-correlated data arise. For example, interventions are randomly assigned to practices, and providers within the same practice who provide care to participants are trained with the assigned intervention. Teerenstra et al proposed a nested exchangeable correlation structure that accounts for two levels of clustering within the generalized estimating equations (GEE) approach. In this article, we utilize GEE models to test the treatment effect in a two-group comparison for continuous, binary, or count data in three-level CRTs. Given the nested exchangeable correlation structure, we derive the asymptotic variances of the estimator of the treatment effect for different types of outcomes. When the number of clusters is small, researchers have proposed bias-corrected sandwich estimators to improve performance in two-level CRTs. We extend the variances of two bias-corrected sandwich estimators to three-level CRTs. The equal provider and practice sizes were assumed to calculate number of practices for simplicity. However, they are not guaranteed in practice. Relative efficiency (RE) is defined as the ratio of variance of the estimator of the treatment effect for equal to unequal provider and practice sizes. The expressions of REs are obtained from both asymptotic variance estimation and bias-corrected sandwich estimators. Their performances are evaluated for different scenarios of provider and practice size distributions through simulation studies. Finally, a percentage increase in the number of practices is proposed due to efficiency loss from unequal provider and/or practice sizes.-Sample size calculation in three-level cluster randomized trials using generalized estimating equation models.",1
"Mechanism-driven low-dimensional ordinary differential equation (ODE) models are often used to model viral dynamics at cellular levels and epidemics of infectious diseases. However, low-dimensional mechanism-based ODE models are limited for modeling infectious diseases at molecular levels such as transcriptomic or proteomic levels, which is critical to understand pathogenesis of diseases. Although linear ODE models have been proposed for gene regulatory networks (GRNs), nonlinear regulations are common in GRNs. The reconstruction of large-scale nonlinear networks from time-course gene expression data remains an unresolved issue. Here, we use high-dimensional nonlinear additive ODEs to model GRNs and propose a 4-step procedure to efficiently perform variable selection for nonlinear ODEs. To tackle the challenge of high dimensionality, we couple the 2-stage smoothing-based estimation method for ODEs and a nonlinear independence screening method to perform variable selection for the nonlinear ODE models. We have shown that our method possesses the sure screening property and it can handle problems with non-polynomial?dimensionality. Numerical performance of the proposed method is illustrated with simulated data and a real data example for identifying the dynamic GRN of Saccharomyces cerevisiae.-Independence screening for high dimensional nonlinear additive ODE models with applications to dynamic gene regulatory networks.",0
"In 2012, the National Cancer Institute (NCI) engaged the scientific community to provide a vision for cancer epidemiology in the 21st century. Eight overarching thematic recommendations, with proposed corresponding actions for consideration by funding agencies, professional societies, and the research community emerged from the collective intellectual discourse. The themes are (i) extending the reach of epidemiology beyond discovery and etiologic research to include multilevel analysis, intervention evaluation, implementation, and outcomes research; (ii) transforming the practice of epidemiology by moving toward more access and sharing of protocols, data, metadata, and specimens to foster collaboration, to ensure reproducibility and replication, and accelerate translation; (iii) expanding cohort studies to collect exposure, clinical, and other information across the life course and examining multiple health-related endpoints; (iv) developing and validating reliable methods and technologies to quantify exposures and outcomes on a massive scale, and to assess concomitantly the role of multiple factors in complex diseases; (v) integrating ""big data"" science into the practice of epidemiology; (vi) expanding knowledge integration to drive research, policy, and practice; (vii) transforming training of 21st century epidemiologists to address interdisciplinary and translational research; and (viii) optimizing the use of resources and infrastructure for epidemiologic studies. These recommendations can transform cancer epidemiology and the field of epidemiology, in general, by enhancing transparency, interdisciplinary collaboration, and strategic applications of new technologies. They should lay a strong scientific foundation for accelerated translation of scientific discoveries into individual and population health benefits.-Transforming epidemiology for 21st century medicine and public health.",1
Screening for HIV infection.,0
"To describe the concerns raised by health plan members, providers and purchasers related to studying the comparative effectiveness of therapeutics using cluster randomized trials (CRTs) within health plans. An additional goal was to develop recommendations for increasing acceptability. Eighty-four qualitative in-depth telephone interviews were conducted; 50 with health plan members, 21 with providers, and 13 with purchasers. Interviews focused on stakeholders' concerns about and recommendations for conducting CRTs in health plans. Members expressed concerns that CRTs might compromise their healthcare. Providers and purchasers recognized the value of and the need for comparative effectiveness research. Providers expressed concerns that they would not have sufficient time to discuss a CRT with patients, and that participation in such a study could negatively impact their relationships with patients. Purchasers would want assurances that study participation would not result in members receiving lesser care, and that benefits would remain equitable for all members. This study provides insight into how health plan members, providers and purchasers might react to a CRT being conducted in their health plan. The recommendations reported here provide guidance for researchers and policy makers considering this methodological approach and suggest that with sufficient preparation and planning CRTs can be an acceptable and efficient methodology for studying the comparative effectiveness of therapeutics in real world settings.-Cluster randomized trials to study the comparative effectiveness of therapeutics: stakeholders' concerns and recommendations.",1
"Several studies in low-income populations report the somewhat counterintuitive finding that positive income gains adversely affect adult health. The literature posits that receipt of a large portion of annual income increases, in the short term, risk-taking behaviour and/or the consumption of health-damaging goods. This work implies the hypothesis that persons with an unexpected gain in income will exhibit an elevated risk of accidental death-the fifth leading cause of death in the USA. We test this hypothesis directly by capitalizing on a natural experiment in which Cherokee Indians in rural North Carolina received discrete lump sum payments from a new casino. We applied Poisson regression to the monthly count of accidental deaths among Cherokee Indians over 204 months spanning 1990-2006. We controlled for temporal patterns in accidental deaths (e.g. seasonality and trend) as well as changes in population size. As hypothesized, the risk of accidental death rises above expected levels during months of the large casino payments (relative risk = 2.62; 95% confidence interval = 1.54-4.47). Exploratory analyses of ethnographic interviews and behavioural surveys support that increased vehicular travel and consumption of health-damaging goods may account for the rise in accident proneness. Although long-term income gains may improve health in this population, our findings indicate that acute responses to large income gains, in the short term, increase risk-taking and accident proneness. We encourage further investigation of natural experiments to identify causal economic antecedents of population health.-Positive income shocks and accidental deaths among Cherokee Indians: a natural experiment.",0
"The relationship between homicide and suicide has been studied extensively, but with conflicting results. The primary objective of this study was to examine the correlation between homicide and suicide rates in a large cross-sectional sample of UN member states. The study used age-standardized data on homicide and suicide for 65 international locales compiled by the World Health Organization. Weighted correlation coefficients between homicide and suicide rates were computed by sex, income level, and geographic region. The overall correlation between homicide and suicide rates was weak and statistically insignificant (rho = -0.08, P = 0.5178). However, when analysed by geographic region the data revealed two distinct patterns: homicide and suicide rates were positively correlated in European countries (rho = 0.89, P &lt; 0.0001), but negatively correlated in the Asia Pacific Region (rho = -0.97, P &lt; 0.0001), and the Americas (rho = -0.62, P &lt; 0.005). The strength and direction of the relationship between homicide and suicide vary significantly with geographic region. The divergent geographic patterns in the relationship between homicide and suicide might be due to regional differences in social and cultural variables.-Correlating homicide and suicide.",0
"Multivariate failure time data often arise in research. Cox proportional hazards modelling is a widely used method of analysing failure time data for independent observations. However, when failure times are correlated the Cox proportional hazards model does not yield valid estimates of standard errors or significance tests. Many methods for the analysis of multivariate failure time data have been proposed. These methods commonly test hypotheses about the regression parameters, a practice which averages the treatment effect across time. The purpose of this paper is to examine the bootstrap method for obtaining standard errors in the multivariate failure time case, particularly when the focus is the survival probability or the treatment effect at a single time point such as in a surgical trial. Our motivating example comes from the Asymptomatic Carotid and Atherosclerosis Study (ACAS) in which the outcome of stroke or perioperative complications could be observed for either or both carotid arteries within each patient. Extensive simulation studies were conducted to examine the bootstrap procedure for analysing correlated failure time data under a variety of conditions including a range of treatment effects, cluster sizes, intercluster correlation values and for both proportional and non-proportional data. We found that the bootstrap method was able to estimate the standard error adequately for survival probabilities at a specific time and the standard error for the survival difference and the relative risk at a specific time. We illustrated the bootstrap method for calculating the standard error for the survival probability and statistical testing at a specific time value by analysing the two arteries per patient from the ACAS study.-Bootstrap analysis of multivariate failure time data.",0
"In the analysis of complex and high-dimensional data, graphical models have been commonly adopted to describe associations among variables. When common factors exist which make the associations dense, the single factor graphical model has been proposed, which first extracts the common factor and then conducts graphical modeling. Under other simpler contexts, it has been recognized that results generated from analyzing a single dataset are often unsatisfactory, and integrating multiple datasets can effectively improve variable selection and estimation. In graphical modeling, the increased number of parameters makes the ""lack of information"" problem more severe. In this article, we integrate multiple datasets and conduct the approximate single factor graphical model analysis. A novel penalization approach is developed for the identification and estimation of important loadings and edges. An effective computational algorithm is developed. A wide spectrum of simulations and the analysis of breast cancer gene expression datasets demonstrate the competitive performance of the proposed approach. Overall, this study provides an effective new venue for taking advantage of multiple datasets and improving graphical model analysis.-Integrating approximate single factor graphical models.",0
"In the cluster randomised study design, the data collected have a hierarchical structure and often include multivariate outcomes. We present a flexible modelling strategy that permits several normally distributed outcomes to be analysed simultaneously, in which intervention effects as well as individual-level and cluster-level between-outcome correlations are estimated. This is implemented in a Bayesian framework which has several advantages over a classical approach, for example in providing credible intervals for functions of model parameters and in allowing informative priors for the intracluster correlation coefficients. In order to declare such informative prior distributions, and fit models in which the between-outcome covariance matrices are constrained, priors on parameters within the covariance matrices are required. Careful specification is necessary however, in order to maintain non-negative definiteness and symmetry between the different outcomes. We propose a novel solution in the case of three multivariate outcomes, and present a modified existing approach and novel alternative for four or more outcomes. The methods are applied to an example of a cluster randomised trial in the prevention of coronary heart disease. The modelling strategy presented would also be useful in other situations involving hierarchical multivariate outcomes.-Modelling multivariate outcomes in hierarchical data, with application to cluster randomised trials.",1
"Three arguments are usually invoked in favour of stepped wedge cluster randomised controlled trials: the logistic convenience of implementing an intervention in phases, the ethical benefit of providing the intervention to all clusters, and the potential to enhance the social acceptability of cluster randomised controlled trials. Are these alleged benefits real? We explored the logistic, ethical, and political dimensions of stepped wedge trials using case studies of six recent evaluations. We identified completed or ongoing stepped wedge evaluations using two systematic reviews. We then purposively selected six with a focus on public health in high, middle, and low-income settings. We interviewed their authors about the logistic, ethical, and social issues faced by their teams. Two authors reviewed interview transcripts, identified emerging issues through qualitative thematic analysis, reflected upon them in the context of the literature, and invited all participants to co-author the manuscript. Our analysis raises three main points. First, the phased implementation of interventions can alleviate problems linked to simultaneous roll-out, but also brings new challenges. Issues to consider include the feasibility of organising intervention activities according to a randomised sequence, estimating time lags in implementation and effects, and accommodating policy changes during the trial period. Second, stepped wedge trials, like parallel cluster trials, require equipoise: without it, randomising participants to a control condition, even for a short time, remains problematic. In stepped wedge trials, equipoise is likely to lie in the degree of effect, effectiveness in a specific operational milieu, and the balance of benefit and harm, including the social value of better evaluation. Third, the strongest arguments for a stepped wedge design are logistic and political rather than ethical. The design is advantageous when simultaneous roll-out is impractical and when it increases the acceptability of using counterfactuals. The logistic convenience of phased implementation is context-dependent, and may be vitiated by the additional requirements of phasing. The potential for stepped wedge trials to enhance the social acceptability of cluster randomised trials is real, but their ethical legitimacy still rests on demonstrating equipoise and its configuration for each research question and setting.-Logistic, ethical, and political dimensions of stepped wedge trials: critical review and case studies.",3
"The past decades of research have seen an increase in statistical tools to explore the complex dynamics of mental health from patient data, yet the application of these tools in clinical practice remains uncommon. This is surprising, given that clinical reasoning, e.g., case conceptualizations, largely coincides with the dynamical system approach. We argue that the gap between statistical tools and clinical practice can partly be explained by the fact that current estimation techniques disregard theoretical and practical considerations relevant to psychotherapy. To address this issue, we propose that case conceptualizations should be formalized. We illustrate this approach by introducing a computational model of functional analysis, a framework commonly used by practitioners to formulate case conceptualizations and design patient-tailored treatment. We outline the general approach of formalizing idiographic theories, drawing on the example of a functional analysis for a patient suffering from panic disorder. We specified the system using a series of differential equations and simulated different scenarios; first, we simulated data without intervening in the system to examine the effects of avoidant coping on the development of panic symptomatic. Second, we formalized two interventions commonly used in cognitive behavioral therapy (CBT; exposure and cognitive reappraisal) and subsequently simulated their effects on the system. The first simulation showed that the specified system could recover several aspects of the phenomenon (panic disorder), however, also showed some incongruency with the nature of panic attacks (e.g., rapid decreases were not observed). The second simulation study illustrated differential effects of CBT interventions for this patient. All tested interventions could decrease panic levels in the system. Formalizing idiographic theories is promising in bridging the gap between complexity science and clinical practice and can help foster more rigorous scientific practices in psychotherapy, through enhancing theory development. More precise case conceptualizations could potentially improve intervention planning and treatment outcomes. We discuss applications in psychotherapy and future directions, amongst others barriers for systematic theory evaluation and extending the framework to incorporate interactions between individual systems, relevant for modeling social learning processes. With this report, we hope to stimulate future efforts in formalizing clinical frameworks.-Bridging the gap between complexity science and clinical practice by formalizing idiographic theories: a computational model of functional analysis.",0
"To evaluate the validity of the Patient-Reported Outcomes Measurement Information System (PROMIS) Physical Function measures using longitudinal data collected in six chronic health conditions. Individuals with rheumatoid arthritis (RA), major depressive disorder (MDD), back pain, chronic obstructive pulmonary disease (COPD), chronic heart failure (CHF), and cancer completed the PROMIS Physical Function computerized adaptive test or fixed-length short form at baseline and at the end of clinically relevant follow-up intervals. Anchor items were also administered to assess change in physical function and general health. Linear mixed-effects models and standardized response means were estimated at baseline and follow-up. A total of 1,415 individuals participated (COPD n?=?121; CHF n?=?57; back pain n?=?218; MDD n?=?196; RA n?=?521; cancer n?=?302). The PROMIS Physical Function scores improved significantly for treatment of CHF and back pain patients but not for patients with MDD or COPD. Most of the patient subsamples that reported improvement or worsening on the anchors showed a corresponding positive or negative change in PROMIS Physical Function. This study provides evidence that the PROMIS Physical Function measures are sensitive to change in intervention studies where physical function is expected to change and able to distinguish among different clinical samples. The results inform the estimation of meaningful change, enabling comparative effectiveness research.-Validity of PROMIS physical function measured in diverse clinical samples.",0
Continual reassessment and related designs in dose-finding studies.,0
"The ethics of the Flexibility In duty hour Requirements for Surgical Trainees (FIRST) trial have been vehemently debated. Views on the ethics of the FIRST trial range from it being completely unethical to wholly unproblematic. The FIRST trial illustrates the complex ethical challenges posed by cluster randomised trials (CRTs) of policy interventions involving healthcare professionals. In what follows, we have three objectives. First, we critically review the FIRST trial controversy, finding that commentators have failed to sufficiently identify and address many of the relevant ethical issues. The 2012 Ottawa Statement on the Ethical Design and Conduct of Cluster Randomized Trials provides researchers and research ethics committees with specific guidance for the ethical design and conduct of CRTs. Second, we aim to demonstrate how the Ottawa Statement provides much-needed clarity to the ethical issues in the FIRST trial, including: research participant identification; consent requirements; gatekeeper roles; benefit-harm analysis and identification of vulnerable participants. We nonetheless also find that the FIRST trial raises ethical issues not adequately addressed by the Ottawa Statement. Hence, third and finally, we raise important questions requiring further ethical analysis and guidance, including: Does clinical equipoise apply to policy interventions with little or no evidence-base? Do healthcare providers have an obligation to participate in research? Does the power-differential in certain healthcare settings render healthcare providers vulnerable to duress and coercion to participant in research? If so, what safeguards might be implemented to protect providers, while allowing important research to proceed?-Thinking clearly about the FIRST trial: addressing ethical challenges in cluster randomised trials of policy interventions involving health providers.",1
"The analysis of very small samples of Gaussian repeated measurements can be challenging. First, due to a very small number of independent subjects contributing outcomes over time, statistical power can be quite small. Second, nuisance covariance parameters must be appropriately accounted for in the analysis in order to maintain the nominal test size. However, available statistical strategies that ensure valid statistical inference may lack power, whereas more powerful methods may have the potential for inflated test sizes. Therefore, we explore an alternative approach to the analysis of very small samples of Gaussian repeated measurements, with the goal of maintaining valid inference while also improving statistical power relative to other valid methods. This approach uses generalized estimating equations with a bias-corrected empirical covariance matrix that accounts for all small-sample aspects of nuisance correlation parameter estimation in order to maintain valid inference. Furthermore, the approach utilizes correlation selection strategies with the goal of choosing the working structure that will result in the greatest power. In our study, we show that when accurate modeling of the nuisance correlation structure impacts the efficiency of regression parameter estimation, this method can improve power relative to existing methods that yield valid inference. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-On the analysis of very small samples of Gaussian repeated measurements: an alternative approach.",0
"To describe the characteristics and quality of reporting of cluster randomized trials (CRTs) in children published from 2004 to 2010. Four databases were searched for reports of CRTs in children (0-18 years). Characteristics of the studies were summarized and the quality of reporting assessed using consolidated standards of reporting trial-CRT (CONSORT-CRT). Of 1,949 identified references, 106 were included. The number of published CRTs in children increased since 2004. The greatest proportion of CRTs was undertaken in Europe (29%), whereas 40% was conducted in low- and middle-income countries. Most studies were of complex rather than simple interventions (83%); were preventive rather than treatment interventions (76%); and most frequently addressed infectious disease (21%), diet/physical activity interventions (19%), health-risk behaviors (15%), and undernutrition (13%). The majority used schools as units of randomization (72%) and enrolled 1,000-10,000 children per study (51%). Reporting was generally poor, with 34% of CRTs inadequately reporting on more than half of the CONSORT-CRT criteria. Although 85% of CRTs reported that they had ethics approval for the study, consent or assent was not obtained from children in most studies. Children-specific elements of reporting are needed to improve the quality of reporting of CRTs and consequently their planning and implementation.-Characteristics and quality of reporting of cluster randomized trials in children: reporting needs improvement.",1
"The standard 6-month four-drug regimen for the treatment of drug-sensitive tuberculosis has remained unchanged for decades and is inadequate to control the epidemic. Shorter, simpler regimens are urgently needed to defeat what is now the world's greatest infectious disease killer. We describe the Phase IIC Selection Trial with Extended Post-treatment follow-up (STEP) as a novel hybrid phase II/III trial design to accelerate regimen development. In the Phase IIC STEP trial, the experimental regimen is given for the duration for which it will be studied in phase III (presently 3 or 4 months) and patients are followed for clinical outcomes of treatment failure and relapse for a total of 12 months from randomisation. Operating characteristics of the trial design are explored assuming a classical frequentist framework as well as a Bayesian framework with flat and sceptical priors. A simulation study is conducted using data from the RIFAQUIN phase III trial to illustrate how such a design could be used in practice. With 80 patients per arm, and two (2.5 %) unfavourable outcomes in the STEP trial, there is a probability of 0.99 that the proportion of unfavourable outcomes in a potential phase III trial would be less than 12 % and a probability of 0.91 that the proportion of unfavourable outcomes would be less than 8 %. With six (7.5 %) unfavourable outcomes, there is a probability of 0.82 that the proportion of unfavourable outcomes in a potential phase III trial would be less than 12 % and a probability of 0.41 that it would be less than 8 %. Simulations using data from the RIFAQUIN trial show that a STEP trial with 80 patients per arm would have correctly shown that the Inferior Regimen should not proceed to phase III and would have had a high chance (0.88) of either showing that the Successful Regimen could proceed to phase III or that it might require further optimisation. Collection of definitive clinical outcome data in a relatively small number of participants over only 12 months provides valuable information about the likelihood of success in a future phase III trial. We strongly believe that the STEP trial design described herein is an important tool that would allow for more informed decision-making and accelerate regimen development.-A new trial design to accelerate tuberculosis drug development: the Phase IIC Selection Trial with Extended Post-treatment follow-up (STEP).",0
"A project that originated with the aim of documenting the implications of dropouts for tests of significance based on general linear mixed model procedures resulted in recognition of problems in the use of SAS PROC.MIXED for this purpose. In responding to suggestions and criticisms, we have further analyzed simulated clinical trial data with realistic autoregressive structure, using alternative error model formulations, different approaches to the use of covariates to model dropout patterns, and different ways to include the critical time variable in the mixed model. Results emphasize the sensitivity of the PROC.MIXED tests of significance for GROUP and TIME x GROUP equal slopes hypothesis to less than optimal modeling of the error covariance structure. Even with the authoritatively recommended best available modeling of the error structure, model formulations that made use of the REPEATED statement did not maintain conservative test sizes when covariates were required to model dropout data patterns. Random coefficients models that employed the RANDOM statement did permit appropriate covariate controls, but the tests of significance for treatment effects were lacking in power. After examining a variety of alternative PROC.MIXED model formulations, it is concluded that none provided both Type I error protection and power comparable to that of simple two-stage analysis of covariance (ANCOVA) procedures for confirming the presence of true treatment effects in controlled clinical trials. Other issues examined in this article concern treating baseline scores as both covariate and initial repeated measurement to which a linear means model is fitted, failure to take advantage of the regression of repeated measurements on time in modeling time as an unordered categorical variable, and fitting linear regression models to nonlinear response patterns.-Issues in use of SAS PROC.MIXED to test the significance of treatment effects in controlled clinical trials.",1
"We describe a methodology for analysing self-reported risk behaviour transitional patterns in a binary outcome variable, subject to misclassification and a large loss to follow-up. The motivation stems from the analysis of self-reported transitional patterns in responses to the question 'have you ever smoked a whole cigarette?' in a cohort of South African school children. The partially complete records analysis (PCRA) introduced, estimates the transitional probability as: the ratio of the joint probability of the response at two time points based on the complete records for this time sequence over the marginal probabilities of the response based on the complete records at the first time point, and assumes a non-informative missing pattern. A comparison was made using un-weighted complete records and inverse probability weighted logistic regression. The estimates of the probabilities of reporting ever having smoked a cigarette obtained from the three methods were similar for a particular transition. The PCRA method lacked precision compared with the inverse probability weighted logistic regression. A simulation study indicated an association between bias and reporting error in all three methods. The PCRA method can be considered as a method for the estimation of transition probabilities in a cohort study where there is consistency in the self-reported risk behaviour pattern and the sample size is large at baseline. The inverse probability weighting approach is more precise and is suitable for this setting in order to determine risk factors for the incidence of self-reported substance used in a cohort with a high dropout rate.-Bias in a binary risk behaviour model subject to inconsistent reports and dropout in a South African high school cohort study.",0
This paper presents sample size formulae for both continuous and dichotomous endpoints obtained from intervention studies that use the cluster as the unit of randomization. The formulae provide the required number of clusters or the required number of individuals per cluster when the other number is given. The proposed formulae derive from Student's t-test with use of cluster summary measures and a variance that consists of within and between cluster components. Power contours are provided to help in the design of intervention studies that use cluster randomization. Sample size formulae for designs with and without stratification of clusters appear separately.-Sample size formulae for intervention studies with the cluster as unit of randomization.,1
Optimal allocation of clusters in cohort stepped wedge designs,3
"Kappa coefficients are measures of correlation between categorical variables often used as reliability or validity coefficients. We recapitulate development and definitions of the K (categories) by M (ratings) kappas (K x M), discuss what they are well- or ill-designed to do, and summarize where kappas now stand with regard to their application in medical research. The 2 x M(M&gt;/=2) intraclass kappa seems the ideal measure of binary reliability; a 2 x 2 weighted kappa is an excellent choice, though not a unique one, as a validity measure. For both the intraclass and weighted kappas, we address continuing problems with kappas. There are serious problems with using the K x M intraclass (K&gt;2) or the various K x M weighted kappas for K&gt;2 or M&gt;2 in any context, either because they convey incomplete and possibly misleading information, or because other approaches are preferable to their use. We illustrate the use of the recommended kappas with applications in medical research.-Kappa coefficients in medical research.",0
"The Wilcoxon signed rank test is a frequently used nonparametric test for paired data (e.g., consisting of pre- and posttreatment measurements) based on independent units of analysis. This test cannot be used for paired comparisons arising from clustered data (e.g., if paired comparisons are available for each of two eyes of an individual). To incorporate clustering, a generalization of the randomization test formulation for the signed rank test is proposed, where the unit of randomization is at the cluster level (e.g., person), while the individual paired units of analysis are at the subunit within cluster level (e.g., eye within person). An adjusted variance estimate of the signed rank test statistic is then derived, which can be used for either balanced (same number of subunits per cluster) or unbalanced (different number of subunits per cluster) data, with an exchangeable correlation structure, with or without tied values. The resulting test statistic is shown to be asymptotically normal as the number of clusters becomes large, if the cluster size is bounded. Simulation studies are performed based on simulating correlated ranked data from a signed log-normal distribution. These studies indicate appropriate type I error for data sets with &gt; or =20 clusters and a superior power profile compared with either the ordinary signed rank test based on the average cluster difference score or the multivariate signed rank test of Puri and Sen. Finally, the methods are illustrated with two data sets, (i) an ophthalmologic data set involving a comparison of electroretinogram (ERG) data in retinitis pigmentosa (RP) patients before and after undergoing an experimental surgical procedure, and (ii) a nutritional data set based on a randomized prospective study of nutritional supplements in RP patients where vitamin E intake outside of study capsules is compared before and after randomization to monitor compliance with nutritional protocols.-The Wilcoxon signed rank test for paired comparisons of clustered data.",1
"Screening and recruitment for clinical trials can be costly and time-consuming. Inpatient trials present additional challenges because enrollment is time sensitive based on length of stay. We hypothesized that using an automated prescreening algorithm to identify eligible subjects would increase screening efficiency and enrollment and be cost-effective compared to manual review of a daily admission list. Using a before-and-after design, we compared time spent screening, number of patients screened, enrollment rate, and cost-effectiveness of each screening method in an inpatient diabetes trial conducted at Massachusetts General Hospital. Manual chart review (CR) involved reviewing a daily list of admitted patients to identify eligible subjects. The automated prescreening (APS) method used an algorithm to generate a daily list of patients with glucose levels ? 180 mg/dL, an insulin order, and/or admission diagnosis of diabetes mellitus. The census generated was then manually screened to confirm eligibility and eliminate patients who met our exclusion criteria. We determined rates of screening and enrollment and cost-effectiveness of each method based on study sample size. Total screening time (prescreening and screening) decreased from 4 to 2 h, allowing subjects to be approached earlier in the course of the hospital stay. The average number of patients prescreened per day increased from 13 ? 4 to 30 ? 16 (P &lt; 0.0001). Rate of enrollment increased from 0.17 to 0.32 patients per screening day. Developing the computer algorithm added a fixed cost of US$3000 to the study. Based on our screening and enrollment rates, the algorithm was cost-neutral after enrolling 12 patients. Larger sample sizes further favored screening with an algorithm. By contrast, higher recruitment rates favored individual CR. Because of the before-and-after design of this study, it is possible that unmeasured factors contributed to increased enrollment. Using a computer algorithm to identify eligible patients for a clinical trial in the inpatient setting increased the number of patients screened and enrolled, decreased the time required to enroll them, and was less expensive. Upfront investment in developing a computerized algorithm to improve screening may be cost-effective even for relatively small trials, especially when the recruitment rate is expected to be low.-Efficacy and cost-effectiveness of an automated screening algorithm in an inpatient clinical trial.",0
"The study of gene-environment interactions is an increasingly important aspect of genetic epidemiological investigation. Historically, it has been difficult to study gene-environment interactions using a family-based design for quantitative traits or when parent-offspring trios were incomplete. The QBAT-I provides researchers a tool to estimate and test for a gene-environment interaction in families of arbitrary structure that are sampled without regard to the phenotype of interest, but is vulnerable to inflated type I error if families are ascertained on the basis of the phenotype. In this study, we verified the potential for type I error of the QBAT-I when applied to samples ascertained on a trait of interest. The magnitude of the inflation increases as the main genetic effect increases and as the ascertainment becomes more extreme. We propose an ascertainment-corrected score test that allows the use of the QBAT-I to test for gene-environment interactions in ascertained samples. Our results indicate that the score test and an ad hoc method we propose can often restore the nominal type I error rate, and in cases where complete restoration is not possible, dramatically reduce the inflation of the type I error rate in ascertained samples.-Testing gene-environment interactions in family-based association studies using trait-based ascertained samples.",0
"Stepped wedge trials (SWTs) can be considered as a variant of a clustered randomised trial, although in many ways they embed additional complications from the point of view of statistical design and analysis. While the literature is rich for standard parallel or clustered randomised clinical trials (CRTs), it is much less so for SWTs. The specific features of SWTs need to be addressed properly in the sample size calculations to ensure valid estimates of the intervention effect. We critically review the available literature on analytical methods to perform sample size and power calculations in a SWT. In particular, we highlight the specific assumptions underlying currently used methods and comment on their validity and potential for extensions. Finally, we propose the use of simulation-based methods to overcome some of the limitations of analytical formulae. We performed a simulation exercise in which we compared simulation-based sample size computations with analytical methods and assessed the impact of varying the basic parameters to the resulting sample size/power, in the case of continuous and binary outcomes and assuming both cross-sectional data and the closed cohort design. We compared the sample size requirements for a SWT in comparison to CRTs based on comparable number of measurements in each cluster. In line with the existing literature, we found that when the level of correlation within the clusters is relatively high (for example, greater than 0.1), the SWT requires a smaller number of clusters. For low values of the intracluster correlation, the two designs produce more similar requirements in terms of total number of clusters. We validated our simulation-based approach and compared the results of sample size calculations to analytical methods; the simulation-based procedures perform well, producing results that are extremely similar to the analytical methods. We found that usually the SWT is relatively insensitive to variations in the intracluster correlation, and that failure to account for a potential time effect will artificially and grossly overestimate the power of a study. We provide a framework for handling the sample size and power calculations of a SWT and suggest that simulation-based procedures may be more effective, especially in dealing with the specific features of the study at hand. In selected situations and depending on the level of intracluster correlation and the cluster size, SWTs may be more efficient than comparable CRTs. However, the decision about the design to be implemented will be based on a wide range of considerations, including the cost associated with the number of clusters, number of measurements and the trial duration.-Sample size calculation for a stepped wedge trial.",3
"Recurrent events are common in medical research for subjects who are followed for the duration of a study. For example, cardiovascular patients with an implantable cardioverter defibrillator (ICD) experience recurrent arrhythmic events that are terminated by shocks or antitachycardia pacing delivered by the device. In a published randomized clinical trial, a recurrent-event model was used to study the effect of a drug therapy in subjects with ICDs, who were experiencing recurrent symptomatic arrhythmic events. Under this model, one expects the robust variance for the estimated treatment effect to diminish when the duration of the trial is extended, due to the additional events observed. However, as shown in this article, that is not always the case. We investigate this phenomenon using large datasets from this arrhythmia trial and from a diabetes study, with some analytical results, as well as through simulations. Some insights are also provided on existing sample size formulae using our results.-Insights on the robust variance estimator under recurrent-events model.",0
"This paper describes some statistical considerations for the Child and Adolescent Trial for Cardiovascular Health (CATCH), a large-scale community health trial sponsored by the National Heart, Lung, and Blood Institute. The trial involves randomization of entire schools rather than individual students to the experimental arms. The paper discussed the implications of this form of randomization for the design and analysis of the trial. The power calculations and analysis plan for the trial are presented in detail. The handling of outmigrating and immigrating students is also discussed.-Statistical design of the Child and Adolescent Trial for Cardiovascular Health (CATCH): implications of cluster randomization.",1
"To evaluate the long term effect of ongoing intervention to improve treatment of depression in primary care. Randomised controlled trial. Twelve primary care practices across the United States. 211 adults beginning a new treatment episode for major depression; 94% of patients assigned to ongoing intervention participated. Practices assigned to ongoing intervention encouraged participating patients to engage in active treatment, using practice nurses to provide care management over 24 months. Patients' report of remission and functioning. Ongoing intervention significantly improved both symptoms and functioning at 24 months, increasing remission by 33 percentage points (95% confidence interval 7% to 46%), improving emotional functioning by 24 points (11 to 38) and physical functioning by 17 points (6 to 28). By 24 months, 74% of patients in enhanced care reported remission, with emotional functioning exceeding 90% of population norms and physical functioning approaching 75% of population norms. Ongoing intervention increased remission rates and improved indicators of emotional and physical functioning. Studies are needed to compare the cost effectiveness of ongoing depression management with other chronic disease treatment routinely undertaken by primary care.-Managing depression as a chronic disease: a randomised trial of ongoing treatment in primary care.",0
"The stereotype regression model for categorical outcomes, proposed by Anderson (J. Roy. Statist. Soc. B. 1984; 46:1-30) is nested between the baseline-category logits and adjacent category logits model with proportional odds structure. The stereotype model is more parsimonious than the ordinary baseline-category (or multinomial logistic) model due to a product representation of the log-odds-ratios in terms of a common parameter corresponding to each predictor and category-specific scores. The model could be used for both ordered and unordered outcomes. For ordered outcomes, the stereotype model allows more flexibility than the popular proportional odds model in capturing highly subjective ordinal scaling which does not result from categorization of a single latent variable, but are inherently multi-dimensional in nature. As pointed out by Greenland (Statist. Med. 1994; 13:1665-1677), an additional advantage of the stereotype model is that it provides unbiased and valid inference under outcome-stratified sampling as in case-control studies. In addition, for matched case-control studies, the stereotype model is amenable to classical conditional likelihood principle, whereas there is no reduction due to sufficiency under the proportional odds model. In spite of these attractive features, the model has been applied less, as there are issues with maximum likelihood estimation and likelihood-based testing approaches due to non-linearity and lack of identifiability of the parameters. We present comprehensive Bayesian inference and model comparison procedure for this class of models as an alternative to the classical frequentist approach. We illustrate our methodology by analyzing data from The Flint Men's Health Study, a case-control study of prostate cancer in African-American men aged 40-79 years. We use clinical staging of prostate cancer in terms of Tumors, Nodes and Metastasis as the categorical response of interest.-Bayesian inference for the stereotype regression model: Application to a case-control study of prostate cancer.",0
"The sample size required for a cluster randomised trial is inflated compared with an individually randomised trial because outcomes of participants from the same cluster are correlated. Sample size calculations for longitudinal cluster randomised trials (including stepped wedge trials) need to take account of at least two levels of clustering: the clusters themselves and times within clusters. We derive formulae for sample size for repeated cross-section and closed cohort cluster randomised trials with normally distributed outcome measures, under a multilevel model allowing for variation between clusters and between times within clusters. Our formulae agree with those previously described for special cases such as crossover and analysis of covariance designs, although simulation suggests that the formulae could underestimate required sample size when the number of clusters is small. Whether using a formula or simulation, a sample size calculation requires estimates of nuisance parameters, which in our model include the intracluster correlation, cluster autocorrelation, and individual autocorrelation. A cluster autocorrelation less than 1 reflects a situation where individuals sampled from the same cluster at different times have less correlated outcomes than individuals sampled from the same cluster at the same time. Nuisance parameters could be estimated from time series obtained in similarly clustered settings with the same outcome measure, using analysis of variance to estimate variance components. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Sample size calculation for stepped wedge and other longitudinal cluster randomised trials.",3
"Considerable attention has been recently paid to the use of surrogate endpoints in clinical research. We deal with the situation where the two endpoints are both right censored. While proportional hazards analyses are typically used for this setting, their use leads to several complications. In this article, we propose the use of the accelerated failure time model for analysis of surrogate endpoints. Based on the model, we then describe estimation and inference procedures for several measures of surrogacy. A complication is that potentially both the independent and dependent variable are subject to censoring. We adapt the Theil-Sen estimator to this problem, develop the associated asymptotic results, and propose a novel resampling-based technique for calculating the variances of the proposed estimators. The finite-sample properties of the estimation methodology are assessed using simulation studies, and the proposed procedures are applied to data from an acute myelogenous leukemia clinical trial.-Semiparametric inference for surrogate endpoints with bivariate censored data.",0
"Major depression is one of the leading causes of disability worldwide, yet epidemiologic data are not available for many countries, particularly low- to middle-income countries. In this paper, we present data on the prevalence, impairment and demographic correlates of depression from 18 high and low- to middle-income countries in the World Mental Health Survey Initiative. Major depressive episodes (MDE) as defined by the Diagnostic and Statistical Manual of Mental Disorders, fourth edition (DMS-IV) were evaluated in face-to-face interviews using the World Health Organization Composite International Diagnostic Interview (CIDI). Data from 18 countries were analyzed in this report (n = 89,037). All countries surveyed representative, population-based samples of adults. The average lifetime and 12-month prevalence estimates of DSM-IV MDE were 14.6% and 5.5% in the ten high-income and 11.1% and 5.9% in the eight low- to middle-income countries. The average age of onset ascertained retrospectively was 25.7 in the high-income and 24.0 in low- to middle-income countries. Functional impairment was associated with recency of MDE. The female: male ratio was about 2:1. In high-income countries, younger age was associated with higher 12-month prevalence; by contrast, in several low- to middle-income countries, older age was associated with greater likelihood of MDE. The strongest demographic correlate in high-income countries was being separated from a partner, and in low- to middle-income countries, was being divorced or widowed. MDE is a significant public-health concern across all regions of the world and is strongly linked to social conditions. Future research is needed to investigate the combination of demographic risk factors that are most strongly associated with MDE in the specific countries included in the WMH.-Cross-national epidemiology of DSM-IV major depressive episode.",0
"Regression analysis of survival data, and more generally event history data, is typically based on Cox's regression model. We here review some recent methodology, focusing on the limitations of Cox's regression model. The key limitation is that the model is not well suited to represent time-varying effects. We start by considering classical and also more recent goodness-of-fit procedures for the Cox model that will reveal when the Cox model does not capture important aspects of the data, such as time-varying effects. We present recent regression models that are able to deal with and describe such time-varying effects. The introduced models are all applied to data on breast cancer from the Norwegian cancer registry, and these analyses clearly reveal the shortcomings of Cox's regression model and the need for other supplementary analyses with models such as those we present here.-Flexible survival regression modelling.",0
"Onset of systemic lupus erythematosus (SLE) is preceded by a preclinical phase characterized by expression of autoantibodies and nonspecific clinical symptoms. Hydroxychloroquine is a treatment for lupus that is widely used based on longstanding experience and a very good safety profile. Existing data suggest that treatment with hydroxychloroquine may postpone the onset of disease. However, prospective studies that prove and quantify the efficacy of hydroxychloroquine in the preclinical phase of lupus have not been done. This study will test the hypothesis that early hydroxychloroquine use can prevent accumulation of clinical abnormalities and modify immune responses that define SLE. A randomized, double-blind, placebo-controlled trial of hydroxychloroquine vs placebo will be conducted. Participants will have incomplete lupus erythematosus as defined by the presence of antinuclear antibody (ANA) positivity at a titer of 1:80 or greater, as well as one or two additional criteria from the 2012 Systemic Lupus International Collaborating Clinics (SLICC) classification criteria. The age range will be 15-45 years and the treatment phase will be 96 weeks. The primary endpoint will be the increase in the number of features of SLE defined by the 2012 SLICC classification schema. Secondary outcomes will include the proportion of participants who transition to a classification of SLE as defined by SLICC criteria. A major challenge for improving therapies in patients with SLE is early detection of disease. The ANA test that is widely used to screen for SLE has low specificity and interpretation of its significance is challenging. The Study of Anti-Malarials in Incomplete Lupus Erythematosus (SMILE) trial will provide insights into the appropriate target population for intervention, and will assess whether hydroxychloroquine can slow progression as measured by the accumulation of criteria. Ophthalmologic safety in this population will be assessed. The study will investigate candidate biomarkers that will guide treatment decisions and will accumulate a specimen biobank that will be available to the lupus research community for further in-depth mechanistic studies. This trial is a first step toward testing the feasibility of disease prevention strategies in SLE. ClinicalTrials.gov, NCT 03030118 . Registered on 24 January 2017.-Study of Anti-Malarials in Incomplete Lupus Erythematosus (SMILE): study protocol for a randomized controlled trial.",0
"The Ottawa Statement is the first guidance document for the ethical and scientific conduct of cluster-randomized trials (CRTs). However, not all recommendations are straightforward to implement. In this paper we will reflect in particular on the recommendation on identifying human research subjects and the issue to what extent the randomization process should be disclosed if there is a risk of contamination. The Ottawa Statement was thoroughly evaluated within a multidisciplinary research team, consisting amongst others of epidemiologists and ethicists. Patients in a CRT may also be considered as research subjects if they are indirectly affected by the studied interventions in a CRT. Second, health care workers are research subjects in CRTs but have a different moral status compared with ordinary research participants. This different status has implications for withdrawal and the choice of the primary objective. Third, modified informed consent for CRTs may be obtained when researchers can demonstrate that disclosure of the randomization process would affect the validity of a CRT. Recommendations of the Ottawa Statement on identifying the research subject and providing informed consent can and should be refined.-The ethics of cluster-randomized trials requires further evaluation: a refinement of the Ottawa Statement.",1
"The choice of weights in estimating equations for multivariate survival data is considered. Specifically, we consider families of weight functions which are constant on fixed time intervals, including the special case of time-constant weights. For a fixed set of time intervals, the optimal weights are identified as the solution to a system of linear equations. The optimal weights are computed for several scenarios. It is found that for the scenarios examined, the gains in efficiency using the optimal weights are quite small relative to simpler approaches except under extreme dependence, and that a simple estimator of an exchangeable approximation to the weights also performs well.-Optimal weight functions for marginal proportional hazards analysis of clustered failure time data.",1
"The 13C-urea breath test (UBT) is currently regarded as one of the most important noninvasive diagnostic methods for detecting Helicobacter pylori (H. pylori) infection in adults and children. However, for infants and young children, the standard for UBT interpretation has not been validated, and its reliability has not been established for diagnosing H. pylori infection in this group. The primary outcome data from UBT consist of mixture data, which come from subjects whose H. pylori infection classifications are unconfirmed. In this paper, we propose the finite mixture distribution method to identify a reliable UBT cut-off value in a large baseline sample in which gastric biopsy is not available to confirm the H. pylori infection in younger children. Maximum likelihood estimators of the parameters in the mixture model were obtained using an expectation maximization (EM) algorithm. The standard deviation of the cut-off point was estimated by bootstrap methods. We applied the same analytical methods to the UBT results yielded from the follow up, as well as the overall UBT results in the longitudinal cohort data. The cut-off points from those UBT data sets are similar. The advantage of the finite mixture model is that it may be used to calculate sensitivity and specificity in the absence of other diagnostic tests.-13C-urea breath test for Helicobacter pylori in young children: cut-off point determination by finite mixture model.",0
"Vitamin D deficiency is highly prevalent across the globe. Existing studies suggest that a low vitamin D level is associated with more than 130 outcomes. Exploring the causal role of vitamin D in health outcomes could support or question vitamin D supplementation. We carried out a systematic literature review of previous Mendelian-randomization studies on vitamin D. We then implemented a Mendelian Randomization-Phenome Wide Association Study (MR-PheWAS) analysis on data from 339?256 individuals of White British origin from UK Biobank. We first ran a PheWAS analysis to test the associations between a 25(OH)D polygenic risk score and 920 disease outcomes, and then nine phenotypes (i.e. systolic blood pressure, diastolic blood pressure, risk of hypertension, T2D, ischaemic heart disease, body mass index, depression, non-vertebral fracture and all-cause mortality) that met the pre-defined inclusion criteria for further analysis were examined by multiple MR analytical approaches to explore causality. The PheWAS analysis did not identify any health outcome associated with the 25(OH)D polygenic risk score. Although a selection of nine outcomes were reported in previous Mendelian-randomization studies or umbrella reviews to be associated with vitamin D, our MR analysis, with substantial study power (&gt;80% power to detect an association with an odds ratio &gt;1.2 for per standard deviation increase of log-transformed 25[OH]D), was unable to support an interpretation of causal association. We investigated the putative causal effects of vitamin D on multiple health outcomes in a White population. We did not support a causal effect on any of the disease outcomes tested. However, we cannot exclude small causal effects or effects on outcomes that we did not have enough power to explore due to the small number of cases.-Phenome-wide Mendelian-randomization study of genetically determined vitamin D on multiple health outcomes using the UK Biobank study.",0
"In cluster randomized trials, clusters of subjects are randomized rather than subjects themselves, and missing outcomes are a concern as in individual randomized trials. We assessed strategies for handling missing data when analysing cluster randomized trials with a binary outcome; strategies included complete case, adjusted complete case, and simple and multiple imputation approaches. We performed a simulation study to assess bias and coverage rate of the population-averaged intervention-effect estimate. Both multiple imputation with a random-effects logistic regression model or classical logistic regression provided unbiased estimates of the intervention effect. Both strategies also showed good coverage properties, even slightly better for multiple imputation with a random-effects logistic regression approach. Finally, this latter approach led to a slightly negatively biased intracluster correlation coefficient estimate but less than that with a classical logistic regression model strategy. We applied these strategies to a real trial randomizing households and comparing ivermectin and malathion to treat head lice.-A comparison of imputation strategies in cluster randomized trials with missing binary outcomes.",1
"Suppose we have a binary treatment used to influence an outcome. Given data from an observational or controlled study, we wish to determine whether or not there exists some subset of observed covariates in which the treatment is more effective than the standard practice of no treatment. Furthermore, we wish to quantify the improvement in population mean outcome that will be seen if this subgroup receives treatment and the rest of the population remains untreated. We show that this problem is surprisingly challenging given how often it is an (at least implicit) study objective. Blindly applying standard techniques fails to yield any apparent asymptotic results, while using existing techniques to confront the non-regularity does not necessarily help at distributions where there is no treatment effect. Here, we describe an approach to estimate the impact of treating the subgroup which benefits from treatment that is valid in a nonparametric model and is able to deal with the case where there is no treatment effect. The approach is a slight modification of an approach that recently appeared in the individualized medicine literature.-Evaluating the impact of treating the optimal subgroup.",0
"In order to design multicentre studies an estimate of the correlation of the observations within each centre is necessary. A standard measure of the correlation between observations within each centre is the Intraclass Correlation Coefficient (ICC). We used the National Trauma Data Bank (NTDB). By 2004, 448 trauma centres (including 110 level I and 123 level II trauma centres) from 43 states and US territories contributed over 1.2 million records to the NTDB. Data of patients directly transported from the scene of injury to level I or II trauma centres were used to calculate the ICC of in-hospital trauma fatality and emergency department (ED) shock rate. The ICCs of ED shock and in-hospital fatality rate were 0.010 (95% confidence interval (CI): 0.003-0.018) and 0.039 (95% CI: 0.028-0.050), respectively. The ICC of shock in the ED was the highest for penetrating injuries (0.017, 95% CI: 0.003-0.032) and the lowest for women (0.008, 95% CI: 0.002-0.013) although the observed difference between men and women was not statistically significant. The ICC of trauma fatality was the highest for penetrating injuries (0.073, 95% CI: 0.047-0.098), and the lowest for blunt injuries (0.029, 95% CI: 0.020-0.037). Although the calculated ICCs might seem so small as to be ignored, the required sample size in studies with exclusively exposed or non-exposed clusters depends on the ICC and the average number of subjects within clusters. Therefore, investigators should be aware of the influence that these ICCs might have on sample size and power of their studies.-Analysis of clustered data in multicentre trauma studies.",1
"Missing observations are common in cluster randomised trials. The problem is exacerbated when modelling bivariate outcomes jointly, as the proportion of complete cases is often considerably smaller than the proportion having either of the outcomes fully observed. Approaches taken to handling such missing data include the following: complete case analysis, single-level multiple imputation that ignores the clustering, multiple imputation with a fixed effect for each cluster and multilevel multiple imputation. We contrasted the alternative approaches to handling missing data in a cost-effectiveness analysis that uses data from a cluster randomised trial to evaluate an exercise intervention for care home residents. We then conducted a simulation study to assess the performance of these approaches on bivariate continuous outcomes, in terms of confidence interval coverage and empirical bias in the estimated treatment effects. Missing-at-random clustered data scenarios were simulated following a full-factorial design. Across all the missing data mechanisms considered, the multiple imputation methods provided estimators with negligible bias, while complete case analysis resulted in biased treatment effect estimates in scenarios where the randomised treatment arm was associated with missingness. Confidence interval coverage was generally in excess of nominal levels (up to 99.8%) following fixed-effects multiple imputation and too low following single-level multiple imputation. Multilevel multiple imputation led to coverage levels of approximately 95% throughout. ? 2016 The Authors. Statistics in Medicine Published by John Wiley &amp; Sons Ltd.-Multiple imputation methods for bivariate outcomes in cluster randomised trials.",1
We introduce a novel approach for describing patterns of HIV genetic variation using regression modeling techniques. Parameters are defined for describing genetic variation within and between viral populations by generalizing Simpson's index of diversity. Regression models are specified for these variation parameters and the generalized estimating equation framework is used for estimating both the regression parameters and their corresponding variances. Conditions are described under which the usual asymptotic approximations to the distribution of the estimators are met. This approach provides a formal statistical framework for testing hypotheses regarding the changing patterns of HIV genetic variation over time within an infected patient. The application of these methods for testing biologically relevant hypotheses concerning HIV genetic variation is demonstrated in an example using sequence data from a subset of patients from the Multicenter AIDS Cohort Study.-A regression modeling approach for describing patterns of HIV genetic variation.,0
Experimental Design for Unit and Cluster Randomid Trials,1
"Missing data are a potential source of bias, and their handling in the statistical analysis can have an important impact on both the likelihood and degree of such bias. Inadequate handling of the missing data may also result in invalid variance estimation. The handling of missing values is more complex in cluster randomised trials, but there are no reviews of practice in this field. A systematic review of published trials was conducted to examine how missing data are reported and handled in cluster randomised trials. We systematically identified cluster randomised trials, published in English in 2011, using the National Library of Medicine (MEDLINE) via PubMed. Non-randomised and pilot/feasibility trials were excluded, as were reports of secondary analyses, interim analyses, and economic evaluations and those where no data were at the individual level. We extracted information on missing data and the statistical methods used to deal with them from a random sample of the identified studies. We included 132 trials. There was evidence of missing data in 95 (72%). Only 32 trials reported handling missing data, 22 of them using a variety of single imputation techniques, 8 using multiple imputation without accommodating the clustering and 2 stating that their likelihood-based complete case analysis accounted for missing values because the data were assumed Missing-at-Random. The results presented in this study are based on a large random sample of cluster randomised trials published in 2011, identified in electronic searches and therefore possibly missing some trials, most likely of poorer quality. Also, our results are based on information in the main publication for each trial. These reports may omit some important information on the presence of, and reasons for, missing data and on the statistical methods used to handle them. Our extraction methods, based on published reports, could not distinguish between missing data in outcomes and missing data in covariates. This distinction may be important in determining the assumptions about the missing data mechanism necessary for complete case analyses to be valid. Missing data are present in the majority of cluster randomised trials. However, they are poorly reported, and most authors give little consideration to the assumptions under which their analysis will be valid. The majority of the methods currently used are valid under very strong assumptions about the missing data, whose plausibility is rarely discussed in the corresponding reports. This may have important consequences for the validity of inferences in some trials. Methods which result in valid inferences under general Missing-at-Random assumptions are available and should be made more accessible.-Are missing data adequately handled in cluster randomised trials? A systematic review and guidelines.",1
"Group randomized trials (GRT) are often designed with relatively little preliminary data available to estimate key parameters. In this paper, however, the opposite situation is considered-very good baseline data are available on the primary outcome of interest. These data can then be used to inform key design and analysis decisions such as (i) should the trial be designed as an unmatched or pair-matched study, or stratified in some other fashion; (ii) is analysis of ""change from baseline"" preferable to using end-of-study data alone; and (iii) what power might be expected by pursuing these various strategies. The results are applied to a GRT for sexually transmitted diseases prevention in Peru.-Using baseline data to design a group randomized trial.",1
"In cluster randomized trials (CRTs), individuals belonging to the same cluster are very likely to resemble one another, not only in terms of outcomes but also in terms of treatment compliance behavior. Although the impact of resemblance in outcomes is well acknowledged, little attention has been given to the possible impact of resemblance in compliance behavior. This study defines compliance intraclass correlation as the level of resemblance in compliance behavior among individuals within clusters. On the basis of Monte Carlo simulations, it is demonstrated how compliance intraclass correlation affects power to detect intention-to-treat (ITT) effect in the CRT setting. As a way of improving power to detect ITT effect in CRTs accompanied by noncompliance, this study employs an estimation method, where ITT effect estimates are obtained based on compliance-type-specific treatment effect estimates. A multilevel mixture analysis using an ML-EM estimation method is used for this estimation.-Intention-to-treat analysis in cluster randomized trials with noncompliance.",1
"Type 2 diabetes mellitus (T2DM) accelerates brain aging and increases the risk for dementia. Insulin is a key neurotrophic factor in the brain, where it modulates energy metabolism, neurovascular coupling, and regeneration. Impaired insulin-mediated brain signaling and central insulin resistance may contribute to cognitive and functional decline in T2DM. Intranasal insulin (INI) has emerged as a potential therapy for treating T2DM-related cognitive impairment. Ongoing from 2015, a prospective, two-center, randomized, double-blind, placebo-controlled trial of 210 subjects (120 T2DM and 90 non-diabetic older adults) randomized into four treatment arms (60 T2DM-INI, 60 T2DM-Placebo, 45 Control-INI, and 45 Control-Placebo) evaluating the long-term effects of daily intranasal administration of 40 International Units (IU) of human insulin, as compared to placebo (sterile saline) over 24?weeks and 24?weeks of post-treatment follow-up. Study outcomes are: 1) long-term INI effects on cognition, daily functionality, and gait speed; 2) identifying a clinically relevant phenotype that predicts response to INI therapy; 3) long-term safety. This study addresses an important knowledge gap about the long-term effects of intranasal insulin on memory and cognition in older people with T2DM and non-diabetic controls, and may provide a novel therapeutic target for prevention and treatment of cognitive and functional decline and dementia. Trial Registration NCT02415556.-Memory advancement by intranasal insulin in type 2 diabetes (MemAID) randomized controlled clinical trial: Design, methods and rationale.",0
"Our first purpose was to determine whether, in the context of a group-randomized trial (GRT) with Gaussian errors, permutation or mixed-model regression methods fare better in the presence of measurable confounding in terms of their Monte Carlo type I error rates and power. Our results indicate that given a proper randomization, the type I error rate is similar for both methods, whether unadjusted or adjusted, even in small studies. However, our results also show that should the investigator face the unfortunate circumstance in which modest confounding exists in the only realization available, the unadjusted analysis risks a type I error; in this regard, there was little to distinguish the two methods. Finally, our results show that power is similar for the two methods and, not surprisingly, better for the adjusted tests. Our second purpose was to examine the relative performance of permutation and mixed-model regression methods in the context of a GRT when the normality assumptions underlying the mixed model are violated. Published studies have examined the impact of violation of this assumption at the member level only. Our findings indicate that both methods perform well when the assumption is violated so long as the ICC is very small and the design is balanced at the group level. However, at ICC&gt;or=0.01, the permutation test carries the nominal type I error rate while the model-based test is conservative and so less powerful. Binomial group- and member-level errors did not otherwise change the relative performance of the two methods with regard to confounding.-A comparison of permutation and mixed-model regression methods for the analysis of simulated data in the context of a group-randomized trial.",1
"We established Project Viva to examine prenatal diet and other factors in relation to maternal and child health. We recruited pregnant women at their initial prenatal visit in eastern Massachusetts between 1999 and 2002. Exclusion criteria included multiple gestation, inability to answer questions in English, gestational age ?22 weeks at recruitment and plans to move away before delivery. We completed in-person visits with mothers during pregnancy in the late first (median 9.9 weeks of gestation) and second (median 27.9 weeks) trimesters. We saw mothers and children in the hospital during the delivery admission and during infancy (median age 6.3 months), early childhood (median 3.2 years) and mid-childhood (median 7.7 years). We collected information from mothers via interviews and questionnaires, performed anthropometric and neurodevelopmental assessments and collected biosamples. We have collected additional information from medical records and from mailed questionnaires sent annually to mothers between in-person visits and to children beginning at age 9 years. From 2341 eligible women, there were 2128 live births; 1279 mother-child pairs provided data at the mid-childhood visit. Primary study outcomes include pregnancy outcomes, maternal mental and cardiometabolic health and child neurodevelopment, asthma/atopy and obesity/cardiometabolic health. Investigators interested in learning more about how to obtain Project Viva data can contact Project_Viva@hphc.org.-Cohort profile: project viva.",0
"In randomised controlled trials with only few randomisation units, treatment allocation may be challenging if balanced distributions of many covariates or baseline outcome measures are desired across all treatment groups. Both traditional approaches, stratified randomisation and allocation by minimisation, have their own limitations. A third method for achieving balance consists of randomly choosing from a preselected list of sufficiently balanced allocations. As with minimisation, this method requires that heterogeneity between treatment groups is measured by specified imbalance metrics. Although certain imbalance measures are more commonly used than others, to the author's knowledge there is no generally accepted ""gold standard"", neither for categorical and even less so for continuous variables. An intuitive and easily accessible web-based software tool was developed which allows for balancing multiple variables of different types and using various imbalance metrics. Different metrics were compared in a simulation study. Using simulated data, it could be shown that for categorical variables, ?2-based imbalance measures seem to be viable alternatives to the established ""quadratic imbalance"" metric. For continuous variables, using the area between the empirical cumulative distribution functions or the largest difference in the three pairs of quartiles is recommended to measure imbalance. Another imbalance metric suggested in the literature for continuous variables, the (symmetrised) Kullback-Leibler divergence, should be used with caution. The Shiny Balancer offers the possibility to visually explore the balancing properties of several well established or newly suggested imbalance metrics, and its use is particularly advocated in clinical studies with few randomisation units, as it is typically the case in cluster randomised trials.-The Shiny Balancer - software and imbalance criteria for optimally balanced treatment allocation in small RCTs and cRCTs",1
"This paper examines the implications of using robust estimators (REs) of standard errors in the presence of clustering when cluster membership is unclear as may commonly occur in clustered randomized trials. For example, in such trials, cluster membership may not be recorded for one or more treatment arms and/or cluster membership may be dynamic. When clusters are well defined, REs have properties that are robust to misspecification of the correlation structure. To examine whether results were sensitive to assumptions about the clustering membership, we conducted simulation studies for a two-arm clinical trial, where the number of clusters, the intracluster correlation (ICC), and the sample size varied. REs of standard errors that incorrectly assumed clustering of data that were truly independent yielded type I error rates of up to 40%. Partial and complete misspecifications of membership (where some and no knowledge of true membership were incorporated into assumptions) for data generated from a large number of clusters (50) with a moderate ICC (0.20) yielded type I error rates that ranged from 7.2% to 9.1% and 10.5% to 45.6%, respectively; incorrectly assuming independence gave a type I error rate of 10.5%. REs of standard errors can be useful when the ICC and knowledge of cluster membership are high. When the ICC is weak, a number of factors must be considered. Our findings suggest guidelines for making sensible analytic choices in the presence of clustering.-On the use of robust estimators for standard errors in the presence of clustering when clustering membership is misspecified.",1
"For clinical trials of interventions that could affect mortality or major morbidity, Data Monitoring Committees have an important role in safeguarding patient interests and enhancing trial integrity and credibility. In trials overseen by an independent DMC it is widely recognized that interim data should remain confidential to the DMC and to the statistical group preparing reports. However, we have found that the principle of confidentiality is not always followed in practice, particularly where the interim data include complete results on a short-term outcome measure. To discuss the reasoning and evidence supporting the principle of confidentiality of interim data with emphasis on the setting where the interim data include complete results on a short-term outcome. We review the reasons why wider access to interim data can increase the risk of false positive or false negative conclusions and discuss the types of harm which can occur. We provide illustrations and insights from recent experiences and discuss the level of consensus in the research community. The arguments in favor of early release of interim data include the need to provide reliable data in a timely manner to patients and physicians, the potential to increase the enthusiasm of trial investigators, and to restore equipoise. However interim data, even where these include complete results on a short-term outcome measure, provide an unreliable and biased assessment of the overall benefit-to-risk profile of the trial treatments. Pre-judgment based on over-interpretation of such interim data can affect recruitment, treatment delivery, and follow-up, risking the ability of the trial to achieve its goals. In order to preserve the integrity of a trial and safeguard the interests of patients, interim data, including complete data on short-term outcomes, should remain confidential to the DMC and the statistical group responsible for preparing interim reports until the trial has achieved its primary objectives.-Maintaining confidentiality of interim data to enhance trial integrity and credibility.",0
"Studies of viral dynamics are important to our understanding of the pathogenesis of HIV-1 infection and in assessment of the potency of antiretroviral therapies. Although many different viral dynamic models and methods for estimating viral dynamic parameters have been proposed and used in various studies, none has been entirely satisfactory. We propose here a segmental model to describe the viral load, and estimate the dynamic parameters by using a mixed-effects model. We address the relation between the baseline viral load and the decay rate of the first phase of viral load decay, and divide patients into three categories on the basis of their changing viral load patterns.-Segmental modeling of changing viral load to assess drug resistance in HIV infection.",0
"To systematically review the account of center and provider effects in large surgical and interventional randomized controlled trials. A systematic review of multicenter interventional randomized trials of 200+ patients. The search included Medline from 1 January 2000 through 11 October 2005 and a hand search of the bibliographies of retrieved articles. One author abstracted all data using standardized abstraction forms; a second reviewer assessed a random sample of reports as quality-assurance procedure. Sixty-eight reports met inclusion criteria. The trials predominantly reported on cardiology (n=23, 34%). The number of participating providers was reported in 11 trials (16%). Both the performed control and performed interventional procedures were described in 43 trials (63%). The use of stratified random allocation on center was reported in 26 trials (38%) and on provider in 6 trials (9%). The analysis was adjusted for center in four trials (6%) and for provider in three trials (4%). Only few trials account for center or provider effect in the design and analysis. Authors and journal editors could play an important role in improving the reporting of trials.-The account for provider and center effects in multicenter interventional and surgical randomized controlled trials is in need of improvement: a review.",1
Performing baseline testing in cluster randomised controlled trials,1
"In a cluster randomized cross-over trial, all participating clusters receive both intervention and control treatments consecutively, in separate time periods. Patients recruited by each cluster within the same time period receive the same intervention, and randomization determines order of treatment within a cluster. Such a design has been used on a number of occasions. For analysis of the trial data, the approach of analysing cluster-level summary measures is appealing on the grounds of simplicity, while hierarchical modelling allows for the correlation of patients within periods within clusters and offers flexibility in the model assumptions. We consider several cluster-level approaches and hierarchical models and make comparison in terms of empirical precision, coverage, and practical considerations. The motivation for a cluster randomized trial to employ cross-over of trial arms is particularly strong when the number of clusters available is small, so we examine performance of the methods under small, medium and large (6, 18, 30) numbers of clusters. One hierarchical model and two cluster-level methods were found to perform consistently well across the designs considered. These three methods are efficient, provide appropriate standard errors and coverage, and continue to perform well when incorporating adjustment for an individual-level covariate. We conclude that choice between hierarchical models and cluster-level methods should be influenced by the extent of complexity in the planned analysis.-Analysis of cluster randomized cross-over trial data: a comparison of methods.",1
"For many large-scale behavioral interventions, random assignment to intervention condition occurs at the group level. Data analytic models that ignore potential non-independence of observations provide inefficient parameter estimates and often produce biased test statistics. For studies in which individuals are randomized by groups to treatment condition, multilevel models (MLMs) provide a flexible approach to statistically evaluating program effects. This article presents an explanation of the need for MLM's for such nested designs and uses data from the Safer Choices study to illustrate the application of MLMs for both continuous and dichotomous outcomes. When designing studies, researchers who are considering group-randomized interventions should also consider the features of the multilevel analytic models they might employ.-Multilevel models and unbiased tests for group based interventions: Examples from the Safer Choices Study",1
"The Cohort Multiple Randomised Controlled Trial (cmRCT) is a newly proposed pragmatic trial design; recently several cmRCT have been initiated. This study tests the unresolved question of whether differential refusal in the intervention arm leads to bias or loss of statistical power and how to deal with this. We conduct simulations evaluating a hypothetical cluster cmRCT in patients at risk of cardiovascular disease (CVD). To deal with refusal, we compare the analysis methods intention to treat (ITT), per protocol (PP) and two instrumental variable (IV) methods: two stage predictor substitution (2SPS) and two stage residual inclusion (2SRI) with respect to their bias and power. We vary the correlation between treatment refusal probability and the probability of experiencing the outcome to create different scenarios. We found ITT to be biased in all scenarios, PP the most biased when correlation is strong and 2SRI the least biased on average. Trials suffer a drop in power unless the refusal rate is factored into the power calculation. The ITT effect in routine practice is likely to lie somewhere between the ITT and IV estimates from the trial which differ significantly depending on refusal rates. More research is needed on how refusal rates of experimental interventions correlate with refusal rates in routine practice to help answer the question of which analysis more relevant. We also recommend updating the required sample size during the trial as more information about the refusal rate is gained.-Cohort Multiple Randomised Controlled Trials (cmRCT) design: efficient but biased? A simulation study to evaluate the feasibility of the Cluster cmRCT design.",1
"Whether there are specific genes involved in response to different environmental agents and how such genes regulate developmental trajectories during lifetime are of fundamental importance in health, clinical and pharmaceutical research. In this article, we present a novel statistical model for monitoring environment-induced genes of major effects on longitudinal outcomes of a trait. This model is derived within the maximum likelihood framework, incorporated by mathematical aspects of growth and developmental processes. A typical structural model is implemented to approximate time-dependent covariance matrices for the longitudinal trait. This model allows for a number of biologically meaningful hypothesis tests regarding the effects of major genes on overall growth trajectories or particular stages of development. It can be used to test whether and how major genetic effects are expressed differently under altered environmental agents. In a well-designed case-control study, our model has been employed to detect cocaine-dependent genes that affect growth trajectories for head circumference during childhood. The detected gene triggers significant effects on growth curves in both cocaine-exposed (case) and unexposed groups (control), but with different extents. Significant genotype-environment interactions due to this so-called environment-sensitive gene are promising for further studies toward its genomic mapping using polymorphic molecular markers.-A framework to monitor environment-induced major genes for developmental trajectories: implication for a prenatal cocaine exposure study.",0
"Clinical trial designs involving correlated data often arise in biomedical research. The intracluster correlation needs to be taken into account to ensure the validity of sample size and power calculations. In contrast to the fixed-sample designs, we propose a flexible trial design with adaptive monitoring and inference procedures. The total sample size is not predetermined, but adaptively re-estimated using observed data via a systematic mechanism. The final inference is based on a weighted average of the block-wise test statistics using generalized estimating equations, where the weight for each block depends on cumulated data from the ongoing trial. When there are no significant treatment effects, the devised stopping rule allows for early termination of the trial and acceptance of the null hypothesis. The proposed design updates information regarding both the effect size and within-cluster correlation based on the cumulated data in order to achieve a desired power. Estimation of the parameter of interest and its confidence interval are proposed. We conduct simulation studies to examine the operating characteristics and illustrate the proposed method with an example.-Adaptive design and estimation in randomized clinical trials with correlated observations.",1
"Often only a limited number of clusters can be obtained in cluster randomised trials, although many potential participants can be recruited within each cluster. Thus, active recruitment is feasible within the clusters. To obtain an efficient sample size in a cluster randomised trial, the cluster level and individual level variance should be known before the study starts, but this is often not the case. We suggest using an internal pilot study design to address this problem of unknown variances. A pilot can be useful to re-estimate the variances and re-calculate the sample size during the trial. Using simulated data, it is shown that an initially low or high power can be adjusted using an internal pilot with the type I error rate remaining within an acceptable range. The intracluster correlation coefficient can be re-estimated with more precision, which has a positive effect on the sample size. We conclude that an internal pilot study design may be used if active recruitment is feasible within a limited number of clusters.-Re-estimating sample size in cluster randomised trials with active recruitment within clusters.",1
"Because the odds ratio (OR) possesses certain desirable statistical properties, the OR has been recommended elsewhere to measure the relative treatment effect in establishing non-inferiority. For cost efficiency, we may often employ a cluster randomized trial (CRT), in which randomized units are clusters of patients. Furthermore, it is not uncommon to encounter data in which there are patients not complying with their assigned treatment. Under the Dirichlet multinomial model, we have developed a test statistic for assessing non-inferiority based on the OR between two treatments under a CRT with noncompliance. We have further derived a sample size formula accounting for both noncompliance and the intraclass correlation for a desired power 1 - ? of detecting non-inferiority with respect to the OR at a nominal ? level. Using Monte Carlo simulation, we have evaluated the performance of the proposed test statistic and sample size formula. Finally, we use the CRT studying the effect of vitamin A supplementation on mortality among preschool children to illustrate the use of the sample size formula given here.-Test non-inferiority and sample size determination based on the odds ratio under a cluster randomized trial with noncompliance.",1
"To determine how often stepped-wedge cluster randomized controlled trials reach their planned sample size, and what reasons are reported for choosing a stepped-wedge trial design. We conducted a PubMed literature search (period 2012 to 2017) and included articles describing the results of a stepped-wedge cluster randomized trial. We calculated the percentage of studies reaching their prespecified number of participants and clusters, and we summarized the reasons for choosing the stepped-wedge trial design as well as difficulties during enrollment. Forty-six individual stepped-wedge studies from a total of 53 articles were included in our review. Of the 35 studies, for which recruitment rate could be calculated, 69% recruited their planned number of participants, with 80% having recruited the planned number of clusters. Ethical reasons were the most common motivation for choosing the stepped-wedge trial design. Most important difficulties during study conduct were dropout of clusters and delayed implementation of the intervention. About half of recently published stepped-wedge trials reached their planned sample size indicating that recruitment is also a major problem in these trials. Still, the stepped-wedge trial design can yield practical, ethical, and methodological advantages.-Systematic review showed that stepped-wedge cluster randomized trials often did not reach their planned sample size",3
"With the publication of the CONSORT statement there is now increased awareness of the need to adequately report the findings of randomised controlled trials. The CONSORT statement includes a checklist of items that should be addressed in the trial report. The original CONSORT statement was developed to ensure the appropriate reporting of parallel group randomised controlled trials in which individual participants are allocated to different intervention groups. In cluster randomised trials, however, groups of participants, rather than individuals, are randomly allocated to study groups. The process of allocating groups of participants raises additional reporting considerations and led to the publication of an extension to the CONSORT statement specifically for cluster randomised trials. In this paper we review the CONSORT extension to cluster randomised trials, outlining the special features of the cluster randomised trial which must be considered.-[The CONSORT statement for cluster randomised trials].",1
"Children diagnosed with cancer are living longer and the survivor population is growing. However, most survivors develop late effects of radiation and chemotherapy shortly to years after completion of therapy, and the receipt of follow-up visits that are recommended by the Children's Oncology Group (COG) is suboptimal nationally. The aims of this study are to: 1) evaluate the impact of a patient-controlled electronic personal health record (ePHR) and system (SurvivorLink) on care visit attendance, risk-based surveillance, and other secondary outcomes (i.e., patient activation, quality of life (QOL)); 2) measure the use, acceptability, and perceived usefulness of, and satisfaction with SurvivorLink; and 3) assess facilitators and barriers to implementation. This hybrid effectiveness-implementation, clustered randomized control trial (RCT) evaluates the effect of SurvivorLink among pediatric cancer survivors and their parents on receipt of follow-up cancer care. We will recruit 20 pediatric survivor clinics with half receiving the intervention and half acting as a waitlist control. Parents of survivors and survivors will complete baseline, 3 and 12?month surveys that assess SurvivorLink use, patient self-efficacy, and intentions to return for follow-up. We will use mixed methods and multi-informant assessment to assess implementation outcomes (i.e., acceptability, feasibility, appropriateness). New approaches are needed to facilitate the receipt of long-term follow-up care among pediatric cancer survivors. This study will assess whether SurvivorLink is effective in increasing receipt of follow-up cancer care. Moreover, it will explore the influences of context and other moderators of clinical practice change in pediatric cancer survivorship.-Scalability of cancer SurvivorLink?: A cluster randomized trial among pediatric cancer clinics.",0
"In the statistical analysis of twinship and familial data, one often encounters the need for regression methods that control for the possibly different aggregation of the twins and the members of the families according to their type (e.g., monozygotic, dizygotic for twinship data). We present the maximum likelihood solution for the regression analysis of data in the presence of heterogeneous intraclass correlations. This work extends previous results for the case of a homogeneous intraclass correlation. An application of the methods for the analysis of twinship data is included.-Regression analysis in the presence of heterogeneous intraclass correlations.",1
"The case-cohort study design has often been used in studies of a rare disease or for a common disease with some biospecimens needing to be preserved for future studies. A case-cohort study design consists of a random sample, called the subcohort, and all or a portion of the subjects with the disease of interest. One advantage of the case-cohort design is that the same subcohort can be used for studying multiple diseases. Stratified random sampling is often used for the subcohort. Additive hazards models are often preferred in studies where the risk difference, instead of relative risk, is of main interest. Existing methods do not use the available covariate information fully. We propose a more efficient estimator by making full use of available covariate information for the additive hazards model with data from a stratified case-cohort design with rare (the traditional situation) and non-rare (the generalized situation) diseases. We propose an estimating equation approach with a new weight function. The proposed estimators are shown to be consistent and asymptotically normally distributed. Simulation studies show that the proposed method using all available information leads to efficiency gain and stratification of the subcohort improves efficiency when the strata are highly correlated with the covariates. Our proposed method is applied to data from the Atherosclerosis Risk in Communities study.-Improving the efficiency of estimation in the additive hazards model for stratified case-cohort design with multiple diseases.",0
"Clustered data are the rule in many clinical specialties such as ophthalmology. Methods have been developed for the treatment of clustered continuous or binary outcome data. Less attention has been given to ordinal outcomes which occur frequently in ophthalmology. For example, grading systems of cataract and diabetic retinopathy are commonly used where a photograph is graded by comparison with a series of reference photographs of increasing severity. Some commonly used methods for the analysis of ordered categorical data include the proportional odds and continuation ratio models. It is difficult, however, to incorporate clustering effects into these models. Instead, for clusters of size two, we propose a generalization of the adjacent category model given by log[Pr(i + 1,j)/Pr(i,j)] = ui + (j - 1) lambda + beta' x, where Pr(i,j) denotes the probability that the right (left) eye has grade i(j), x is a vector of (person or eye-specific) covariates for the right eye, u and beta are vectors of location and covariate parameters and lambda is a clustering parameter. Based on this model, and a similar model interchanging the role of i and j, we derived a closed-form expression for Pr(i,j) as a function of u, lambda and beta and use Newton-Raphson method to maximize the likelihood. An extension of the method allows for extra agreement along the diagonal and is then a generalization of the agreement plus linear-by-linear association model proposed by Agresti in the setting of no covariates. We apply these methods to a data set of 43 diabetic subjects from the Harvard Clinical Cataract Research Center, where cortical cataract grade was the outcome. We also extend this methodology to a survival setting, where both censored and uncensored outcomes are available for individual cluster members, and one wishes to take clustering into account. We apply the survival analysis model to a data set of 1807 children (two ears per child) in the greater Boston area, who were followed for the development of otitis media over the first year of life.-Multivariate methods for clustered ordinal data with applications to survival analysis.",1
Cvcrand and Cptest: Commands for Efficient Design and Analysis of Cluster Randomized Trials Using Constrained Randomization and Permutation Tests,1
"Among the different approaches to the study of cardiovascular disease prevention are community-based programs. This type of program concerns a whole community and the intervention takes advantage of the existing service structure and community organization. The evaluation assesses the feasibility, effects on risk factor and disease reduction, costs, process, and other consequences associated with the program. Several such programs have recently been launched in the United States and some other countries. The first major community-based control program was the North Karelia project in Finland, started in 1972 and recently evaluated for its first five-year period. This paper discusses the problems in evaluating community-based CVD control programs on the experiences obtained in the North Karelia project.-Evaluating community-based preventive cardiovascular programs: problems and experiences from the North Karelia project.",1
"Cluster randomized trials assess the effect of an intervention that is carried out at the group or cluster level. Ajzen's theory of planned behavior is often used to model the effect of the intervention as an indirect effect mediated in turn by attitude, norms and behavioral intention. Structural equation modeling (SEM) is the technique of choice to estimate indirect effects and their significance. However, this is a large sample technique, and its application in a cluster randomized trial assumes a relatively large number of clusters. In practice, the number of clusters in these studies tends to be relatively small, e.g., much less than fifty. This study uses simulation methods to find the lowest number of clusters needed when multilevel SEM is used to estimate the indirect effect. Maximum likelihood estimation is compared to Bayesian analysis, with the central quality criteria being accuracy of the point estimate and the confidence interval. We also investigate the power of the test for the indirect effect. We conclude that Bayes estimation works well with much smaller cluster level sample sizes such as 20 cases than maximum likelihood estimation; although the bias is larger the coverage is much better. When only 5-10 clusters are available per treatment condition even with Bayesian estimation problems occur.-Analyzing indirect effects in cluster randomized trials. The effect of estimation method, number of groups and group sizes on accuracy and power.",1
"Cluster randomized trials (CRTs) have been widely used in field experiments treating a cluster of individuals as the unit of randomization. This study focused particularly on situations where CRTs are accompanied by a common complication, namely, treatment noncompliance or, more generally, intervention nonadherence. In CRTs, compliance may be related not only to individual characteristics but also to the environment of clusters individuals belong to. Therefore, analyses ignoring the connection between compliance and clustering may not provide valid results. Although randomized field experiments often suffer from both noncompliance and clustering of the data, these features have been studied as separate rather than concurrent problems. On the basis of Monte Carlo simulations, this study demonstrated how clustering and noncompliance may affect statistical inferences and how these two complications can be accounted for simultaneously. In particular, the effect of the intervention on individuals who not only were assigned to active intervention but also abided by this intervention assignment (complier average causal effect) was the focus. For estimation of intervention effects considering noncompliance and data clustering, an ML-EM estimation method was employed.-Cluster randomized trials with treatment noncompliance.",1
"The sequential parallel clinical trial is a novel clinical trial design being used in psychiatric diseases that are known to have potentially high placebo response rates. The design consists of an initial parallel trial of placebo versus drug augmented by a second parallel trial of placebo versus drug in the placebo non-responders from the initial trial. Statistical research on the design has focused on hypothesis tests. However, an equally important output from any clinical trial is the estimate of treatment effect and variability around that estimate. In the sequential parallel trial, the most important treatment effect is the effect in the overall population. This effect can be estimated by considering only the first phase of the trial, but this ignores useful information from the second phase of the trial. We develop estimates of treatment effect that incorporate data from both phases of the trial. Our simulations and a real data example suggest that there can be substantial gains in precision by incorporating data from both phases. The potential gains appear to be greatest in moderate-sized trials, which would typically be the case in phase II trials.-Estimation of treatment effect for the sequential parallel design.",0
"Sitting behaviours have been linked with increased risk of all-cause mortality independent of moderate to vigorous physical activity (MVPA). Previous studies have tended to examine single indicators of sitting or all sitting behaviours combined. This study aims to enhance the evidence base by examining the type-specific prospective associations of four different sitting behaviours as well as total sitting with the risk of all-cause mortality. Participants (3720 men and 1412 women) from the Whitehall II cohort study who were free from cardiovascular disease provided information on weekly sitting time (at work, during leisure time, while watching TV, during leisure time excluding TV, and at work and during leisure time combined) and covariates in 1997-99. Cox proportional hazards models were used to investigate prospective associations between sitting time (h/week) and mortality risk. Follow-up was from date of measurement until (the earliest of) death, date of censor or July 31 2014. Over 81 373 person-years of follow-up (mean follow-up time 15.7 ? 2.2 years) a total of 450 deaths were recorded. No associations were observed between any of the five sitting indicators and mortality risk, either in unadjusted models or models adjusted for covariates including MVPA. Sitting time was not associated with all-cause mortality risk. The results of this study suggest that policy makers and clinicians should be cautious about placing emphasis on sitting behaviour as a risk factor for mortality that is distinct from the effect of physical activity.-Associations of sitting behaviours with all-cause mortality over a 16-year follow-up: the Whitehall II study.",0
Peer led programme for asthma education in adolescents.  Papers describing cluster randomised trials must be peer reviewed by statisticians.,1
"This article reviews three traditional methods for the analysis of multicenter trials with persons nested within clusters, i.e., centers, namely na?ve regression (persons as units of analysis), fixed effects regression, and the use of summary measures (clusters as units of analysis), and compares these methods with multilevel regression. The comparison is made for continuous (quantitative) outcomes, and is based on the estimator of the treatment effect and its standard error, because these usually are of main interest in intervention studies. When the results of the experiment have to be valid for some larger population of centers, the centers in the intervention study have to present a random sample from this population and multilevel regression may be used. It is shown that the treatment effect and especially its standard error, are generally incorrectly estimated by the traditional methods, which should, therefore, not in general be used as an alternative to multilevel regression.-A comparison between traditional methods and multilevel regression for the analysis of multicenter intervention studies.",1
"Cluster randomised trials (CRTs) often use geographical areas as the unit of randomisation, however explicit consideration of the location and spatial distribution of observations is rare. In many trials, the location of participants will have little importance, however in some, especially against infectious diseases, spillover effects due to participants being located close together may affect trial results. This review aims to identify spatial analysis methods used in CRTs and improve understanding of the impact of spatial effects on trial results. A systematic review of CRTs containing spatial methods, defined as a method that accounts for the structure, location, or relative distances between observations. We searched three sources: Ovid/Medline, Pubmed, and Web of Science databases. Spatial methods were categorised and details of the impact of spatial effects on trial results recorded. We identified ten papers which met the inclusion criteria, comprising thirteen trials. We found that existing approaches fell into two categories; spatial variables and spatial modelling. The spatial variable approach was most common and involved standard statistical analysis of distance measurements. Spatial modelling is a more sophisticated approach which incorporates the spatial structure of the data within a random effects model. Studies tended to demonstrate the importance of accounting for location and distribution of observations in estimating unbiased effects. There have been a few attempts to control and estimate spatial effects within the context of human CRTs, but our overall understanding is limited. Although spatial effects may bias trial results, their consideration was usually a supplementary, rather than primary analysis. Further work is required to evaluate and develop the spatial methodologies relevant to a range of CRTs.-Spatial analysis of cluster randomised trials: a systematic review of analysis methods.",1
"Researchers often have informative hypotheses in mind when comparing means across treatment groups, such as H1 : ?A &lt; ?B &lt; ?C and H2 : ?B &lt; ?A &lt; ?C, and want to compare these hypotheses to each other directly. This can be done by means of Bayesian inference. This article discusses the disadvantages of the frequentist approach to null hypothesis testing and the advantages of the Bayesian approach. It demonstrates how to use the Bayesian approach to hypothesis testing in the setting of cluster-randomized trials. The data from a school-based smoking prevention intervention with four treatment groups are used to illustrate the Bayesian approach. The main advantage of the Bayesian approach is that it provides a degree of evidence from the collected data in favor of an informative hypothesis. Furthermore, a simulation study was conducted to investigate how Bayes factors behave with cluster-randomized trials. The results from the simulation study showed that the Bayes factor increases with increasing number of clusters, cluster size, and effect size, and decreases with increasing intraclass correlation coefficient. The effect of the number of clusters is stronger than the effect of cluster size. With a small number of clusters, the effect of increasing cluster size may be weak, especially when the intraclass correlation coefficient is large. In conclusion, the study showed that the Bayes factor is affected by sample size and intraclass correlation similarly to how these parameters affect statistical power in the frequentist approach of null hypothesis significance testing. Bayesian evaluation may be used as an alternative to null hypotheses testing.-Bayesian evaluation of informative hypotheses in cluster-randomized trials.",1
"Many dental studies have assessed the effectiveness of community- or group-based interventions such as community water fluoridation. These cluster trials, of which group-randomized trials (GRTs) are one type, have design and analysis considerations not found in studies with randomization of treatments to individuals (randomized controlled trials, RCTs). The purpose of this paper is to review analytic methods used for the analysis of binary outcomes from cluster trials and to illustrate these concepts and analytical methods using a school-based GRT. We examine characteristics of GRTs including intra-class correlation (ICC), their most distinctive feature, and review analytical methods for GRTs including group-level analysis, adjusted chi-square test and multivariable analysis (mixed effect models and generalized estimating equations) for correlated binary data. We consider two- and three-level modeling of data from a cross-sectional cluster design. We apply the concepts reviewed using a GRT designed to determine the effect of incentives on response rates in a school-based dental study. We compare the results of analyses using methods for correlated binary data with those from traditional methods that do not account for ICC. Application of traditional analytic methods to the dental GRT used as an example for this paper led to a substantial overstatement of the effectiveness of the intervention. Ignoring the ICC among members of the same group in the analysis of public health intervention studies can lead to erroneous conclusions where groups are the unit of assignment. Special consideration is needed in the analysis of data from these cluster trials. Randomization of treatments to groups also should receive more consideration in the design of cluster trials in dental public health.-Multilevel analysis of group-randomized trials with binary outcomes.",1
"To determine the advantages and disadvantages of a stepped wedge design for a specific clinical application. The clinical application was a pragmatic cluster randomized surgical trial intending to find an increased percentage of curable recurrences in patients in follow-up after colorectal cancer. Advantages and disadvantages of the stepped wedge design were evaluated, and for this application, new advantages and disadvantages were presented. A main advantage of the stepped wedge design was that the intervention rolls out to all participants, motivating patients and doctors, and a large number of patients who were included in this study. The stepped wedge design increased the complexity of the data analysis, and there were concerns regarding the informed consent procedure. The repeated measurements may bring burden to patients in terms of quality of life, satisfaction, and costs. The stepped wedge design is a strong alternative for pragmatic cluster randomized trials. The known advantages hold, whereas most of the disadvantages were not applicable to this application. The main advantage was that we were able to include a large number of patients. Main disadvantages were that the informed consent procedure can be problematic and that the analysis of the data can be complex.-Strengths and weaknesses of a stepped wedge cluster randomized design: its application in a colorectal cancer follow-up study.",3
"Inadequacy in mental health care in low and middle income countries has been an important contributor to the rising global burden of disease. The treatment gap is salient in resource-poor settings, especially when providing care for conflict-affected forced migrant populations. Primary care is often the only available service option for the majority of forced migrants, and integration of mental health into primary care is a difficult task. The proposed pilot study aims to explore the feasibility of integrating mental health care into primary care by providing training to primary care practitioners serving displaced populations, in order to improve identification, treatment, and referral of patients with common mental disorders via the World Health Organization Mental Health Gap Action Programme (mhGAP). This pilot randomized controlled trial will recruit 86 primary care practitioners (PCP) serving in the Puttalam and Mannar districts of Sri Lanka (with displaced and returning conflict-affected populations). The intervention arm will receive a structured training program based on the mhGAP intervention guide. Primary outcomes will be rates of correct identification, adequate management based on set criteria, and correct referrals of common mental disorders. A qualitative study exploring the attitudes, views, and perspectives of PCP on integrating mental health and primary care will be nested within the pilot study. An economic evaluation will be carried out by gathering service utilization information. In post-conflict Sri Lanka, an important need exists to provide adequate mental health care to conflict-affected internally displaced persons who are returning to their areas of origin after prolonged displacement. The proposed study will act as a local demonstration project, exploring the feasibility of formulating a larger-scale intervention study in the future, and is envisaged to provide information on engaging PCP, and data on training and evaluation including economic costs, patient recruitment, and acceptance and follow-up rates. The study should provide important information on the WHO mhGAP intervention guide to add to the growing evidence base of its implementation. SLCTR/2013/025.-An intervention to improve mental health care for conflict-affected forced migrants in low-resource primary care settings: a WHO MhGAP-based pilot study in Sri Lanka (COM-GAP study).",0
"We study the use of simultaneous confidence bands for low-dose risk estimation with quantal response data, and derive methods for estimating simultaneous upper confidence limits on predicted extra risk under a multistage model. By inverting the upper bands on extra risk, we obtain simultaneous lower bounds on the benchmark dose (BMD). Monte Carlo evaluations explore characteristics of the simultaneous limits under this setting, and a suite of actual data sets are used to compare existing methods for placing lower limits on the BMD.-Confidence bands for low-dose risk estimation with quantal response data.",0
"We examined the exposure to tobacco direct mail marketing and its effect on subsequent smoking behaviors in a US Midwest regional cohort of young adults. Data were collected from 2622 young adults (mean age = 24 years) in 2010 to 2011 (baseline) and 2011 to 2012 (follow-up). We collected information on demographics, tobacco use, and exposure to tobacco direct mail materials in the previous 6 months at baseline. Smoking behaviors were reassessed at follow-up. We investigated the characteristics associated with receiving these materials at baseline, and the associations between receiving cigarette coupons in the mail at baseline and smoking behaviors at follow-up. Thirteen percent of participants reported receiving tobacco direct mail materials in the previous 6 months. Receipt of these materials was associated with age, education, and tobacco use (P &lt; .05). Among those who received these materials, 77% and 56% reported receiving coupons for cigarettes and other tobacco products, respectively. Among baseline nonsmokers and ex-smokers, receiving coupons was associated with becoming current smokers at follow-up (P &lt; .05). Among baseline current smokers, receiving coupons was associated with lower likelihood of smoking cessation at follow-up (P &lt; .05). Tobacco direct mail marketing promoted and sustained smoking behaviors among US Midwest young adults. Regulating this marketing strategy might reduce the prevalence of smoking in this population.-Frequency and characteristics associated with exposure to tobacco direct mail marketing and its prospective effect on smoking behaviors among young adults from the US Midwest.",0
"Many studies in psychological and educational research aim to estimate population average treatment effects (PATE) using data from large complex survey samples, and many of these studies use propensity score methods. Recent advances have investigated how to incorporate survey weights with propensity score methods. However, to this point, that work had not been well summarized, and it was not clear how much difference the different PATE estimation methods would make empirically. The purpose of this study is to systematically summarize the appropriate use of survey weights in propensity score analysis of complex survey data and use a case study to empirically compare the PATE estimates using multiple analysis methods that include ordinary least squares regression, weighted least squares regression, and various propensity score applications. We first summarize various propensity score methods that handle survey weights. We then demonstrate the performance of various analysis methods using a nationally representative data set, the Early Childhood Longitudinal Study-Kindergarten to estimate the effects of preschool on children's academic achievement. The correspondence of the results was evaluated using multiple criteria. It is important for researchers to think carefully about their estimand of interest and use methods appropriate for that estimand. If interest is in drawing inferences to the survey target population, it is important to take the survey weights into account, particularly in the outcome analysis stage for estimating the PATE. The case study shows, however, not much difference among various analysis methods in one applied example.-Using Propensity Score Analysis of Survey Data to Estimate Population Average Treatment Effects: A Case Study Comparing Different Methods.",0
"To determine the concordance between two methods to measure drug exposure duration from pharmacy claim data. We conducted a cohort study using 2002-2007 US Medicaid data. Initiators of eight drug groups were identified: statins, metformin, atypical antipsychotics, warfarin, proton pump inhibitors (PPIs), angiotensin-converting enzyme (ACE) inhibitors, nonsteroidal anti-inflammatory drugs (ns-NSAIDs), and coxibs. For each patient, we calculated two measures of exposure duration using (1) observed days' supply available in US pharmacy claims and (2) the World Health Organisation's Defined Daily Dose (DDD) methodology. We used Wilcoxon signed rank tests to compare medians and Spearman correlations to assess correlation between the two measures. Cohort sizes ranged from 143,885 warfarin users to &gt;3,000,000 ns-NSAID users. Similar median exposure durations were observed for ACE inhibitors (70 days vs.75 days), PPIs (44 days vs. 45 days), and coxibs (44 days vs. 45 days). The DDD method overestimated exposure duration for ns-NSAIDs and underestimated for the remaining drug groups, relative to days' supply. Spearman correlation coefficients ranged from 0.2 to 0.8. Using DDDs to estimate drug exposure duration can result in misclassification. The magnitude of this misclassification might depend on doses used which can vary according to factors such as local prescribing practices, renal function, and age.-Measuring drug exposure: concordance between defined daily dose and days' supply depended on drug class.",0
"There is little rigorous evidence to underpin clinical guidelines for palliative care. However, research in palliative care is difficult, especially with dying patients. Consent is a major issue, since staff do not wish to invite dying patients to participate in trials. We, therefore, conducted a feasibility study in two units within the North West Wales NHS Trust. We explored two novel approaches to research in palliative care -cluster randomisation and randomised consent. All patients admitted to the two units during the study were asked for permission to use their data for research. We allocated the two units, at random, to use cluster randomisation or randomised consent for three months, and then to crossover to the other design. Of 24 patients dying during cluster-randomised phases, 13 gave consent on admission to use their data and were, thus, eligible to enter the trial; however, defined eligibility criteria reduced these to six active participants. Of 29 patients dying during randomised consent phases, seven gave consent on admission to use their data; although two were eligible for randomisation, neither entered the trial. We judge that cluster randomisation is the more effective design for research with dying patients. Computer simulation, based on data from 1500 dying patients on the Welsh Integrated Care Pathway, shows that crossover cluster trials need much smaller samples than simple cluster trials. Furthermore, this study has shown that crossover cluster trials are entirely feasible. We recommend a 'definitive' trial to test the crossover design more widely.-Design of trials with dying patients: a feasibility study of cluster randomisation versus randomised consent.",1
"Portal surveys, defined as assessments occurring proximal to the entry point to a high-risk locale and immediately on exit, can be used in different settings to measure characteristics and behavior of attendees at an event of interest. This methodology has been developed to assess alcohol and other drug (AOD) use at specific events and has included measuring intentions to use collected at entry and reported use on exit, as well as chemical tests for AOD consumption at both entrance and exit. Recent applications of the portal survey procedure to electronic music dance events that occur in established venues (e.g., bars or nightclubs) are discussed.-Portal surveys of time-out drinking locations: a tool for studying binge drinking and AOD use.",0
"We generalize the well-known R(2) measure for linear regression to linear mixed effects models. Our work was motivated by a cluster-randomized study conducted by the Eastern Cooperative Oncology Group, to compare two different versions of informed consent document. We quantify the variation in the response that is explained by the covariates under the linear mixed model, and study three types of measures to estimate such quantities. The first type of measures make direct use of the estimated variances; the second type of measures use residual sums of squares in analogy to the linear regression; the third type of measures are based on the Kullback-Leibler information gain. All the measures can be easily obtained from software programs that fit linear mixed models. We study the performance of the measures through Monte Carlo simulations, and illustrate the usefulness of the measures on data sets.-Measuring explained variation in linear mixed effects models.",1
"Preterm birth (gestational age &lt;37 weeks) has previously been associated with cardiometabolic and neuropsychiatric disorders into adulthood, but has seldom been examined in relation to sleep disorders. We conducted the first population-based study of preterm birth in relation to sleep-disordered breathing (SDB) from childhood into mid-adulthood. A national cohort study was conducted of all 4?186?615 singleton live births in Sweden during 1973-2014, who were followed for SDB ascertained from nationwide inpatient and outpatient diagnoses through 2015 (maximum age 43 years). Cox regression was used to examine gestational age at birth in relation to SDB while adjusting for other perinatal and maternal factors, and co-sibling analyses assessed for potential confounding by unmeasured shared familial factors. There were 171?100 (4.1%) persons diagnosed with SDB in 86.0 million person-years of follow-up. Preterm birth was associated with increased risk of SDB from childhood into mid-adulthood, relative to full-term birth (39-41 weeks) [adjusted hazard ratio (aHR), ages 0-43 years: 1.43; 95% confidence interval (CI), 1.40, 1.46; P &lt;0.001; ages 30-43 years: 1.40; 95% CI, 1.34, 1.47; P &lt;0.001]. Persons born extremely preterm (&lt;28 weeks) had more than 2-fold risks (aHR, ages 0-43 years: 2.63; 95% CI, 2.41, 2.87; P &lt;0.001; ages 30-43 years: 2.22; 95% CI, 1.64, 3.01; P &lt;0.001). These associations affected both males and females, but accounted for more SDB cases among males (additive interaction, P = 0.003). Co-sibling analyses suggested that these findings were only partly due to shared genetic or environmental factors in families. Preterm-born children and adults need long-term follow-up for anticipatory screening and potential treatment of SDB.-Preterm birth and risk of sleep-disordered breathing from childhood into mid-adulthood.",0
"Postmarket device surveillance studies often have important primary objectives tied to estimating a survival function at some future time                        $$T$$                     with a certain amount of precision. This article presents the details and various operating characteristics of a Bayesian adaptive design for device surveillance, as well as a method for estimating a sample size vector (determined by the maximum sample size and a preset number of interim looks) that will deliver the desired power. We adopt a Bayesian adaptive framework, which recognizes the fact that persons enrolled in a study report their results over time, not all at once. At each interim look, we assess whether we expect to achieve our goals with only the current group or the achievement of such goals is extremely unlikely even for the maximum sample size. Our Bayesian adaptive design can outperform two nonadaptive frequentist methods currently recommended by Food and Drug Administration (FDA) guidance documents in many settings. Our method's performance can be sensitive to model misspecification and changes in the trial's enrollment rate. The proposed design provides a more efficient framework for conducting postmarket surveillance of medical devices.-Bayesian adaptive design for device surveillance.",0
"In 2001, the U.S. Office of Personnel Management required all health plans participating in the Federal Employees Health Benefits Program to offer mental health and substance abuse benefits on par with general medical benefits. The initial evaluation found that, on average, parity did not result in either large spending increases or increased service use over the four-year observational period. However, some groups of enrollees may have benefited from parity more than others. To address this question, we propose a Bayesian two-part latent class model to characterize the effect of parity on mental health use and expenditures. Within each class, we fit a two-part random effects model to separately model the probability of mental health or substance abuse use and mean spending trajectories among those having used services. The regression coefficients and random effect covariances vary across classes, thus permitting class-varying correlation structures between the two components of the model. Our analysis identified three classes of subjects: a group of low spenders that tended to be male, had relatively rare use of services, and decreased their spending pattern over time; a group of moderate spenders, primarily female, that had an increase in both use and mean spending after the introduction of parity; and a group of high spenders that tended to have chronic service use and constant spending patterns. By examining the joint 95% highest probability density regions of expected changes in use and spending for each class, we confirmed that parity had an impact only on the moderate spender class.-A bayesian two-part latent class model for longitudinal medical expenditure data: assessing the impact of mental health and substance abuse parity.",0
"When a likelihood ratio is used to measure the strength of evidence for one hypothesis over another, its reliability (i.e. how often it produces misleading evidence) depends on the specification of the working model. When the working model happens to be the 'true' or 'correct' model, the probability of observing strong misleading evidence is low and controllable. But this is not necessarily the case when the working model is misspecified. Royall and Tsou (J. R. Stat. Soc., Ser. B 2003; 65:391-404) show how to adjust working models to make them robust to misspecification. Likelihood ratios derived from their 'robust adjusted likelihood' are just as reliable (asymptotically) as if the working model were correctly specified in the first place. In this paper, we apply and extend these ideas to the generalized linear model (GLM) regression setting. We provide several illustrations (both from simulated data and real data concerning rates of parasitic infection in Philippine adolescents), show how the required adjustment factor can be obtained from standard statistical software, and draw some connections between this approach and the 'sandwich estimator' for robust standard errors of regression parameters. This substantially broadens the availability and the viability of likelihood methods for measuring statistical evidence in regression settings.-Statistical evidence for GLM regression parameters: a robust likelihood approach.",0
"The cluster randomized crossover design has been proposed to improve efficiency over the traditional parallel cluster randomized design, which often involves a limited number of clusters. In recent years, the cluster randomized crossover design has been increasingly used to evaluate the effectiveness of health care policy or programs, and the interest often lies in quantifying the population-averaged intervention effect. In this paper, we consider the two-treatment two-period crossover design, and develop sample size procedures for continuous and binary outcomes corresponding to a population-averaged model estimated by generalized estimating equations, accounting for both within-period and interperiod correlations. In particular, we show that the required sample size depends on the correlation parameters through an eigenvalue of the within-cluster correlation matrix for continuous outcomes and through two distinct eigenvalues of the correlation matrix for binary outcomes. We demonstrate that the empirical power corresponds well with the predicted power by the proposed formulae for as few as eight clusters, when outcomes are analyzed using the matrix-adjusted estimating equations for the correlation parameters concurrently with a suitable bias-corrected sandwich variance estimator.-Power and sample size requirements for GEE analyses of cluster randomized crossover trials.",1
"The Minnesota Heart Health Program is a 13-year research and demonstration project to reduce morbidity and mortality from coronary heart disease in whole communities in the upper Midwest. Six communities were selected for the study: three intervention and three comparison sites, matched to increase baseline comparability. After 2-4 years of baseline observations, a 5- to 6-year program of intensive intervention was introduced in the three intervention communities. Periodic cross-sectional and cohort surveys provided data on risk factors and related behaviors. Regression adjustments within and between communities reduced the confounding influences of important covariates and the variance inflation associated with the nesting of individuals within communities and surveys. Post hoc stratification allowed exploration of the main and strata-specific effects of the intervention program. Finally, the intervention effect was modeled as a departure from the trend line fit to the nonintervention city-year means. Together, these procedures explicitly acknowledged the component of variance associated with communities, and so avoided a major source of bias created in the usual analysis when that variation is ignored. They also increased the interpretability of the analyses and reduced the mean square errors used to assess the treatment effects.-Assessing intervention effects in the Minnesota Heart Health Program.",1
"Missing outcomes are a commonly occurring problem for cluster randomised trials, which can lead to biased and inefficient inference if ignored or handled inappropriately. Two approaches for analysing such trials are cluster-level analysis and individual-level analysis. In this study, we assessed the performance of unadjusted cluster-level analysis, baseline covariate-adjusted cluster-level analysis, random effects logistic regression and generalised estimating equations when binary outcomes are missing under a baseline covariate-dependent missingness mechanism. Missing outcomes were handled using complete records analysis and multilevel multiple imputation. We analytically show that cluster-level analyses for estimating risk ratio using complete records are valid if the true data generating model has log link and the intervention groups have the same missingness mechanism and the same covariate effect in the outcome model. We performed a simulation study considering four different scenarios, depending on whether the missingness mechanisms are the same or different between the intervention groups and whether there is an interaction between intervention group and baseline covariate in the outcome model. On the basis of the simulation study and analytical results, we give guidance on the conditions under which each approach is valid. ? 2017 The Authors. Statistics in Medicine Published by John Wiley &amp; Sons Ltd.-Missing binary outcomes under covariate-dependent missingness in cluster randomised trials.",1
"In oncology clinical trials, both short-term response and long-term survival are important. We propose an urn-based adaptive randomization design to incorporate both of these two outcomes. While short-term response can update the randomization probability quickly to benefit the trial participants, long-term survival outcome can also change the randomization to favor the treatment arm with definitive therapeutic benefit. Using generalized Friedman's urn, we derive an explicit formula for the limiting distribution of the number of subjects assigned to each arm. With prior or hypothetical knowledge on treatment effects, this formula can be used to guide the selection of parameters for the proposed design to achieve desirable patient number ratios between different treatment arms, and thus optimize the operating characteristics of the trial design. Simulation studies show that the proposed design successfully assign more patients to the treatment arms with either better short-term tumor response or long-term survival outcome or both.-An oncology clinical trial design with randomization adaptive to both short- and long-term responses.",0
Negative values of the intraclass correlation coefficient are not theoretically possible.,1
"This article addresses the analytic problems associated with a design in which one identifiable group is allocated to each treatment condition and members of those groups are measured to assess the intervention. Such designs are often called quasi-experiments if the groups are not randomized to conditions and group-randomized trials if the groups are randomized. They present special problems, and previous reports have argued against their use in efficacy or effectiveness trials. Even so, this design still appears with surprising frequency. This article presents the results from a new simulation study that underscores the analytic problems associated with this design.-An evaluation of analysis options for the one-group-per-condition design. Can any of the alternatives overcome the problems inherent in this design?",1
"Numerous methods for joint analysis of longitudinal measures of a continuous outcome y and a time to event outcome T have recently been developed either to focus on the longitudinal data y while correcting for nonignorable dropout, to predict the survival outcome T using the longitudinal data y, or to examine the relationship between y and T. The motivating problem for our work is in joint modeling of the serial measurements of pulmonary function (FEV1% predicted) and survival in cystic fibrosis (CF) patients using registry data. Within the CF registry data, an additional complexity is that not all patients have been followed from birth; therefore, some patients have delayed entry into the study while others may have been missed completely, giving rise to a left truncated distribution. This paper shows in joint modeling situations where y and T are not independent, that it is necessary to account for this left truncation to obtain valid parameter estimates related to both survival and the longitudinal marker. We assume a linear random effects model for FEV1% predicted, where the random intercept and slope of FEV1% predicted, along with a specified transformation of the age at death follow a trivariate normal distribution. We develop an expectation-maximization algorithm for maximum likelihood estimation of parameters, which takes left truncation and right censoring of survival times into account. The methods are illustrated using simulation studies and using data from CF patients in a registry followed at Rainbow Babies and Children's Hospital, Cleveland, OH.-Jointly modeling the relationship between longitudinal and survival data subject to left truncation with applications to cystic fibrosis.",0
"To investigate the association between recurrent AIDS-defining events and a semicompeting risk of death in patients with advanced, multidrug-resistant human immunodeficiency virus infection and to identify individuals at increased risk for these events using a joint frailty model. Three hundred sixty-eight patients with antiretroviral treatment failure in the Options in Management of Antiretrovirals Trial randomized to two antiretroviral treatment strategies using a 2??2 factorial design, intensive vs. standard and interruption vs. continuation, and followed for development of AIDS-defining events and death. Participants were heterogeneous for risk of AIDS-defining events and death (P?&lt;?0.001), and AIDS-defining events were strongly associated with death (P?&lt;?0.001), irrespective of treatment. The frailty model was used to classify individuals into high- and low-risk groups based on unobserved heterogeneity. Low-risk individuals were unlikely to die (0%) or have an AIDS-defining event (&lt;4%), whereas high-risk individuals had event rates approaching 70%. About one-third of high-risk individuals had accelerated mortality, all who died before experiencing an AIDS-defining event. High-risk was associated with being immunocompromised and higher predicted 5-year mortality. The joint frailty model permits classification of individuals into risk groups based on unobserved heterogeneity that may be identifiable based on observed covariates, providing advantages over the traditional Cox model.-A joint frailty model provides for risk stratification of human immunodeficiency virus-infected patients based on unobserved heterogeneity.",0
"Stepped-wedge (SW) designs have been steadily implemented in a variety of trials. A SW design typically assumes a three-level hierarchical data structure where participants are nested within times or periods which are in turn nested within clusters. Therefore, statistical models for analysis of SW trial data need to consider two correlations, the first and second level correlations. Existing power functions and sample size determination formulas had been derived based on statistical models for two-level data structures. Consequently, the second-level correlation has not been incorporated in conventional power analyses. In this paper, we derived a closed-form explicit power function based on a statistical model for three-level continuous outcome data. The power function is based on a pooled overall estimate of stratified cluster-specific estimates of an intervention effect. The sampling distribution of the pooled estimate is derived by applying a fixed-effect meta-analytic approach. Simulation studies verified that the derived power function is unbiased and can be applicable to varying number of participants per period per cluster. In addition, when data structures are assumed to have two levels, we compare three types of power functions by conducting additional simulation studies under a two-level statistical model. In this case, the power function based on a sampling distribution of a marginal, as opposed to pooled, estimate of the intervention effect performed the best. Extensions of power functions to binary outcomes are also suggested.-Sample size determinations for stepped-wedge clinical trials from a three-level data hierarchy perspective.",3
"Human infections with avian influenza A(H7N9) virus are associated with severe illness and high mortality. To better inform triage decisions of hospitalization and management, we developed a clinical prediction rule for diagnosing patients with A(H7N9) and determined its predictive performance. Clinical details on presentation of adult patients hospitalized with either A(H7N9)(n = 121) in China from March to May 2013 or other causes of acute respiratory infections (n = 2,603) in Jingzhou City, China from January 2010 through September 2012 were analyzed. A clinical prediction rule was developed using a two-step coefficient-based multivariable logistic regression scoring method and evaluated with internal validation by bootstrapping. In step 1, predictors for A(H7N9) included male sex, poultry exposure history, and fever, haemoptysis, or shortness of breath on history and physical examination. In step 2, haziness or pneumonic consolidation on chest radiographs and leukopenia were also associated with a higher probability of A(H7N9). The observed risk of A(H7N9) was 0.3% for those assigned to the low-risk group and 2.5%, 4.3%, and 44.0% for tertiles 1 through 3, respectively, in the high-risk group. This prediction rule achieved good model performance, with an optimism-corrected sensitivity of 0.93, a specificity of 0.80, and an area under the receiver-operating characteristic curve of 0.96. A simple decision rule based on data readily obtainable in the setting of patients' first clinical presentations from the first wave of the A/H7N9 epidemic in China has been developed. This prediction rule has achieved good model performance in predicting their risk of A(H7N9) infection and should be useful in guiding important clinical and public health decisions in a timely and objective manner. Data to be gathered with its use in the current evolving second wave of the A/H7N9 epidemic in China will help to inform its performance in the field and contribute to its further refinement.-A clinical prediction rule for diagnosing human infections with avian influenza A(H7N9) in a hospital emergency department setting.",0
Randomization Inference for Stepped-Wedge Cluster-Randomized Trials: An Application to Community-Based Health Insurance.,3
"The first applications of cluster randomized trials with three instead of two levels are beginning to appear in health research, for instance, in trials where different strategies to implement best-practice guidelines are compared. In such trials, the strategy is implemented in health care units ('clusters') and aims at changing the behavior of health care professionals working in this unit ('subjects'), while the effects are measured at patient level ('evaluations'). To guide the choice of number of clusters, number of subjects per cluster, and number of evaluations per subject. We derive a sample size formula and investigate the influence of sample allocation on power or number of clusters required. The required sample size is the product of the sample size in absence of correlation and two variance inflation factors (VIFs) that describe the clustering of evaluations within subjects and of subjects within cluster, respectively. Because each VIF is expressed in terms of an interpretable Pearson correlation, subject matter knowledge can be incorporated. Moreover, these Pearson's correlations are related to intracluster correlations (ICCs) from comparable, but 2-level cluster randomized trials. Formulas are obtained to guide the sample allocation (number of clusters, subjects, and evaluations) for minimizing total sample size, minimizing the number of clusters, or maximizing power given a budget constraint. Empirical estimates of variance components or ICCs from 3-level cluster trials are scarce which limits reliably powering. When parameterized in terms of Pearson correlations, the two variance inflation factors give quantitative insight into the impact of the number of clusters, subjects and evaluations on power. Moreover, subject matter knowledge as well as ICCs from 2-level cluster randomized trials can be incorporated in the sample size calculation, when empirical estimates of variance components or ICCs from a pilot or comparable 3-level study are lacking.-Sample size calculations for 3-level cluster randomized trials.",1
"Although it is common in community psychology research to have data at both the community, or cluster, and individual level, the analysis of such clustered data often presents difficulties for many researchers. Since the individuals within the cluster cannot be assumed to be independent, the use of many traditional statistical techniques that assumes independence of observations is problematic. Further, there is often interest in assessing the degree of dependence in the data resulting from the clustering of individuals within communities. In this paper, a random-effects regression model is described for analysis of clustered data. Unlike ordinary regression analysis of clustered data, random-effects regression models do not assume that each observation is independent, but do assume data within clusters are dependent to some degree. The degree of this dependency is estimated along with estimates of the usual model parameters, thus adjusting these effects for the dependency resulting from the clustering of the data. Models are described for both continuous and dichotomous outcome variables, and available statistical software for these models is discussed. An analysis of a data set where individuals are clustered within firms is used to illustrate features of random-effects regression analysis, relative to both individual-level analysis which ignores the clustering of the data, and cluster-level analysis which aggregates the individual data.-Analysis of clustered data in community psychology: with an example from a worksite smoking cessation project.",1
"Stepped wedge and cluster randomised crossover trials are examples of cluster randomised designs conducted over multiple time periods that are being used with increasing frequency in health research. Recent systematic reviews of both of these designs indicate that the within-cluster correlation is typically taken account of in the analysis of data using a random intercept mixed model, implying a constant correlation between any two individuals in the same cluster no matter how far apart in time they are measured: within-period and between-period intra-cluster correlations are assumed to be identical. Recently proposed extensions allow the within- and between-period intra-cluster correlations to differ, although these methods require that all between-period intra-cluster correlations are identical, which may not be appropriate in all situations. Motivated by a proposed intensive care cluster randomised trial, we propose an alternative correlation structure for repeated cross-sectional multiple-period cluster randomised trials in which the between-period intra-cluster correlation is allowed to decay depending on the distance between measurements. We present results for the variance of treatment effect estimators for varying amounts of decay, investigating the consequences of the variation in decay on sample size planning for stepped wedge, cluster crossover and multiple-period parallel-arm cluster randomised trials. We also investigate the impact of assuming constant between-period intra-cluster correlations instead of decaying between-period intra-cluster correlations. Our results indicate that in certain design configurations, including the one corresponding to the proposed trial, a correlation decay can have an important impact on variances of treatment effect estimators, and hence on sample size and power. An R Shiny app allows readers to interactively explore the impact of correlation decay.-Impact of non-uniform correlation structure on sample size and power in multiple-period cluster randomised trials.",3
"Statistical methods have been developed for cost-effectiveness analyses of cluster randomised trials (CRTs) where baseline covariates are balanced. However, CRTs may show systematic differences in individual and cluster-level covariates between the treatment groups. This paper presents three methods to adjust for imbalances in observed covariates: seemingly unrelated regression with a robust standard error, a 'two-stage' bootstrap approach combined with seemingly unrelated regression and multilevel models. We consider the methods in a cost-effectiveness analysis of a CRT with covariate imbalance, unequal cluster sizes and a prognostic relationship that varied by treatment group. The cost-effectiveness results differed according to the approach for covariate adjustment. A simulation study then assessed the relative performance of methods for addressing systematic imbalance in baseline covariates. The simulations extended the case study and considered scenarios with different levels of confounding, cluster size variation and few clusters. Performance was reported as bias, root mean squared error and CI coverage of the incremental net benefit. Even with low levels of confounding, unadjusted methods were biased, but all adjusted methods were unbiased. Multilevel models performed well across all settings, and unlike the other methods, reported CI coverage close to nominal levels even with few clusters of unequal sizes.-Methods for covariate adjustment in cost-effectiveness analysis that use cluster randomised trials.",1
"Examining covariate balance is the prescribed method for determining the degree to which propensity score methods should be successful at reducing bias. This study assessed the performance of various balance measures, including a proposed balance measure based on the prognostic score (similar to a disease risk score), to determine which balance measures best correlate with bias in the treatment effect estimate. The correlations of multiple common balance measures with bias in the treatment effect estimate produced by weighting by the odds, subclassification on the propensity score, and full matching on the propensity score were calculated. Simulated data were used, based on realistic data settings. Settings included both continuous and binary covariates and continuous covariates only. The absolute standardized mean difference (ASMD) in prognostic scores, the mean ASMD (in covariates), and the mean t-statistic all had high correlations with bias in the effect estimate. Overall, prognostic scores displayed the highest correlations with bias of all the balance measures considered. Prognostic score measure performance was generally not affected by model misspecification, and the prognostic score measure performed well under a variety of scenarios. Researchers should consider using prognostic score-based balance measures for assessing the performance of propensity score methods for reducing bias in nonexperimental studies.-Prognostic score-based balance measures can be a useful diagnostic for propensity score methods in comparative effectiveness research.",0
"Multiple births are an important subgroup to consider in trials aimed at reducing preterm birth or its consequences. Including multiples results in a unique mixture of independent and clustered data, which has implications for the design, analysis and reporting of the trial. We aimed to determine how multiple births were taken into account in the design and analysis of recent trials involving preterm infants, and whether key information relevant to multiple births was reported. We conducted a systematic review of multicentre randomised trials involving preterm infants published between 2008 and 2013. Information relevant to multiple births was extracted. Of the 56 trials included in the review, 6 (11%) excluded multiples and 24 (43%) failed to indicate whether multiples were included. Among the 26 trials that reported multiples were included, only one (4%) accounted for clustering in the sample size calculations and eight (31%) took the clustering into account in the analysis of the primary outcome. Of the 20 trials that randomised infants, 12 (60%) failed to report how infants from the same birth were randomised. Information on multiple births is often poorly reported in trials involving preterm infants, and clustering due to multiple births is rarely taken into account. Since ignoring clustering could result in inappropriate recommendations for clinical practice, clustering should be taken into account in the design and analysis of future neonatal and perinatal trials including infants from a multiple birth.-Accounting for multiple births in randomised trials: a systematic review.",1
"School-based alcohol use prevention studies frequently employ designs in which schools are assigned to treatment conditions while observations are made on individuals. The nesting of schools within treatment conditions requires that the treatment effect be assessed against the between-school variance; unfortunately, that variance is usually larger than for randomly constituted groups and its precision is usually less than that for the within-school variance. These factors often combine to reduce power substantially. To address these problems, investigators need good estimates for the intraclass correlation which together with the number of observations per school determines the magnitude of the extra variation in the nested design. This article presents estimates of school-level intraclass correlation for measures related to alcohol use among ninth and twelfth grade students and discusses their use in planning new studies and analyzing previous or current studies.-Intraclass correlation among measures related to alcohol use by school aged adolescents: estimates, correlates and applications in intervention studies.",1
"A mathematical model of HIV/sexually transmitted infections (STI) transmission was used to examine how linearity or nonlinearity in the relationship between the number of unprotected sex acts (or the number of sex partners) and the risk of acquiring HIV or a highly infectious STI (such as gonorrhea or chlamydia) affects the utility of sexual behavior change measures as indicators of the effectiveness of HIV/STI risk-reduction interventions. Findings indicate that the risk of acquiring HIV through vaginal intercourse is essentially a linear function of the number of unprotected sex acts and is nearly independent of the number of sex partners. Consequently, the number of unprotected sex acts is an excellent marker for the risk of acquiring HIV through vaginal intercourse, whereas the number of sex partners is largely uninformative. In general, the number of unprotected sex acts is not an adequate marker for the risk of acquiring a highly infectious STI due to the highly nonlinear per act transmission dynamics of these STIs. The number of sex partners is a reasonable indicator of STI risk only under highly circumscribed conditions. A theoretical explanation for this pattern of results is provided. The contrasting extent to which HIV and highly infectious STIs deviate from the linearity assumption that underlies sexual behavior outcome measures has important implications for the use of these measures to assess the effectiveness of HIV/STI risk-reduction interventions.-Linearity and nonlinearity in HIV/STI transmission: implications for the evaluation of sexual risk reduction interventions.",0
"Drawing inferences for high-dimensional models is challenging as regular asymptotic theories are not applicable. This article proposes a new framework of simultaneous estimation and inferences for high-dimensional linear models. By smoothing over partial regression estimates based on a given variable selection scheme, we reduce the problem to low-dimensional least squares estimations. The procedure, termed as Selection-assisted Partial Regression and Smoothing (SPARES), utilizes data splitting along with variable selection and partial regression. We show that the SPARES estimator is asymptotically unbiased and normal, and derive its variance via a nonparametric delta method. The utility of the procedure is evaluated under various simulation scenarios and via comparisons with the de-biased LASSO estimators, a major competitor. We apply the method to analyze two genomic datasets and obtain biologically meaningful results.-Drawing inferences for high-dimensional linear models: A selection-assisted partial regression and smoothing approach.",0
"A unified statistical methodology of sample size determination is developed for hierarchical designs that are frequently used in many areas, particularly in medical and health research studies. The solid foundation of the proposed methodology opens a new horizon for power analysis in presence of various conditions. Important features such as joint significance testing, unequal allocations of clusters across intervention groups, and differential attrition rates over follow up time points are integrated to address some useful questions that investigators often encounter while conducting such studies. Proposed methodology is shown to perform well in terms of maintaining type I error rates and achieving the target power under various conditions. Proposed method is also shown to be robust with respect to violation of distributional assumptions of random-effects.-Sample size determination for multilevel hierarchical designs using generalized linear mixed models.",1
A menu-driven facility for power and detectable difference calculations in stepped-wedge randomized trials,3
"We study regularized estimation in high-dimensional longitudinal classification problems, using the lasso and fused lasso regularizers. The constructed coefficient estimates are piecewise constant across the time dimension in the longitudinal problem, with adaptively selected change points (break points). We present an efficient algorithm for computing such estimates, based on proximal gradient descent. We apply our proposed technique to a longitudinal data set on Alzheimer's disease from the Cardiovascular Health Study Cognition Study. Using data analysis and a simulation study, we motivate and demonstrate several practical considerations such as the selection of tuning parameters and the assessment of model stability. While race, gender, vascular and heart disease, lack of caregivers, and deterioration of learning and memory are all important predictors of dementia, we also find that these risk factors become more relevant in the later stages of life.-High-dimensional longitudinal classification with the multinomial fused lasso.",0
"In epidemiological studies, subjects are often followed for a period during which study outcomes are measured at selected time points, such as by diagnostic testing performed on biological samples collected at each visit. Although test results may indicate the presence or absence of a disease or condition, they cannot provide information on when exactly it occurred. Such study designs generate arbitrarily censored time-to-event data, which can include left, interval and right censoring. Adding to this complexity, the data may be clustered such that observations within the same cluster are not independent, such as time to recovery of an infectious disease of family or community members. This data structure is observed when evaluating circumcision's effect on clearance of penile high risk human papillomavirus (HR-HPV) infections using data collected from the male circumcision(MC) trial conducted in Rakai, Uganda, where the multiple infections within individual and HPV testings performed at trial follow-up visits gave rise to the clustered data with arbitrary censoring. We describe the use of parametric proportional hazards frailty models and accelerated failure time frailty models to examine the relationship between explanatory variables and the survival outcomes that are subject to arbitrary censoring, while accounting for the correlation within clusters. Standard software such as SAS can be used for parameter estimation. Circumcision's effect on HPV infection was a secondary end point in the Rakai MC trial, and HPV genotyping was conducted for penile samples of a subset of trial participants collected at enrollment, 6, 12 and 24-month follow up visits. At enrollment, 36.7% intervention arm men (immediate circumcision) and 36.6% control arm men (delayed circumcision at 2 years) were infected with HR-HPV, with the number of infections per man being 1-5. The proposed models were used to examine whether MC facilitated clearance of the prevalent infections. Results show that clearance of multiple infections within each man is highly correlated, and clearance was 60% faster if a man was circumcised. Parametric frailty models provide viable ways to study the relationship between exposure variables and clustered survival outcome that is subject to arbitrary censoring, as is often observed in HPV epidemiology studies.-Parametric frailty models for clustered data with arbitrary censoring: application to effect of male circumcision on HPV clearance.",1
"To explore the constructs underlying a self-report assessment of multimorbidity. We conducted a cross-sectional survey of 352 HMO members aged 65 years or more with, at a minimum, diabetes, depression, and osteoarthritis. We assessed self-reported 'disease burden' (a severity-adjusted count of conditions) as a function of biopsychosocial factors, two data-based comorbidity indices, and demographic variables. In multivariate regression, age, 'compound effects of conditions' (treatments and symptoms interfering with each other), self-efficacy, financial constraints, and physical functioning were significantly (p&lt;or=0.05) associated with disease burden. An ICD-9-based morbidity index did not significantly contribute to disease burden, and a pharmacy-data-based morbidity index was minimally significant. This measure of self-reported disease burden represents an amalgamation of functional capabilities, social considerations, and medical conditions that are not captured by two administrative data-based measures of morbidity. This suggests that (a) self-reported descriptions of multimorbidity incorporate biopsychosocial constructs that reflect the perceived burden of multimorbidity, (b) a simple count of diagnoses should be supplemented by an assessment of activity limitations imposed by these conditions, and (c) choice of the morbidity measurement instrument should be based on the outcome of interest rather than on the most convenient method of measurement.-Seniors' self-reported multimorbidity captured biopsychosocial factors not incorporated into two other data-based morbidity measures.",0
"A generalized least squares regression approach is proposed for the analysis of data arising from experimental studies involving cluster randomization and non-experimental studies in which the major treatment factor corresponds to a characteristic which applies at the cluster level. This approach is more flexible than that provided by the analysis of variance, and unlike ordinary least squares, provides significance levels which are adjusted for the correlation among elements within the same cluster. Two examples are presented.-A regression approach to the analysis of data arising from cluster randomization.",1
"The Lorenz curve is a graphical tool that is widely used to characterize the concentration of a measure in a population, such as wealth. It is frequently the case that the measure of interest used to rank experimental units when estimating the empirical Lorenz curve, and the corresponding Gini coefficient, is subject to random error. This error can result in an incorrect ranking of experimental units which inevitably leads to a curve that exaggerates the degree of concentration (variation) in the population. We consider a specific data configuration with a hierarchical structure where multiple observations are aggregated within experimental units to form the outcome whose distribution is of interest. Within this context, we explore this bias and discuss several widely available statistical methods that have the potential to reduce or remove the bias in the empirical Lorenz curve. The properties of these methods are examined and compared in a simulation study. This work is motivated by a health outcomes application that seeks to assess the concentration of black patient visits among primary care physicians. The methods are illustrated on data from this study.-Estimating the empirical Lorenz curve and Gini coefficient in the presence of error with nested data.",0
"During the recruitment phase of a randomized breast cancer trial, investigating the time to recurrence, we found a strong suggestion that the failure probabilities used at the design stage were too high. Since most of the methodological research involving sample size re-estimation has focused on normal or binary outcomes, we developed a method which preserves blinding to re-estimate sample size in our time to event trial. A mistakenly high estimate of the failure rate at the design stage may reduce the power unacceptably for a clinically important hazard ratio. We describe an ongoing trial and an application of a sample size re-estimation method that combines current trial data with prior trial data or assumes a parametric model to re-estimate failure probabilities in a blinded fashion. Using our current blinded trial data and additional information from prior studies, we re-estimate the failure probabilities to be used in sample size re-calculation. We employ bootstrap re-sampling to quantify uncertainty in the re-estimated sample sizes. At the time of re-estimation data from 278 patients were available, averaging 1.2 years of follow up. Using either method, we estimated a sample size increase of zero for the hazard ratio because the estimated failure probabilities at the time of re-estimation differed little from what was expected. We show that our method of blinded sample size re-estimation preserves the type I error rate. We show that when the initial guess of the failure probabilities are correct, the median increase in sample size is zero. Either some prior knowledge of an appropriate survival distribution shape or prior data is needed for re-estimation. In trials when the accrual period is lengthy, blinded sample size re-estimation near the end of the planned accrual period should be considered. In our examples, when assumptions about failure probabilities and HRs are correct the methods usually do not increase sample size or otherwise increase it by very little. Clinical Trials 2010; 7: 219. http://ctj.sagepub.com.-Sample size re-estimation in a breast cancer trial.",0
"To assess aspects of the internal validity of recently published cluster randomised trials and explore the reporting of information useful in assessing the external validity of these trials. Review of 34 cluster randomised trials in primary care published in 2004 and 2005 in seven journals (British Medical Journal, British Journal of General Practice, Family Practice, Preventive Medicine, Annals of Internal Medicine, Journal of General Internal Medicine, Pediatrics). National Library of Medicine (Medline) via PubMed. To assess aspects of internal validity we extracted data on appropriateness of sample size calculations and analyses, methods of identifying and recruiting individual participants, and blinding. To explore reporting of information useful in assessing external validity we extracted data on cluster eligibility, cluster inclusion and retention, cluster generalisability, and the feasibility and acceptability of the intervention to health providers in clusters. 21 (62%) trials accounted for clustering in sample size calculations and 30 (88%) in the analysis; about a quarter were potentially biased because of procedures surrounding recruitment and identification of patients; individual participants were blind to allocation status in 19 (56%) and outcome assessors were blind in 15 (44%). In almost half the reports, information relating to generalisability of clusters was poorly reported, and in two fifths there was no information about the feasibility and acceptability of the intervention. Cluster randomised trials are essential for evaluating certain types of interventions. Issues affecting their internal validity, such as appropriate sample size calculations and analysis, have been widely disseminated and are now better addressed by researchers. Blinding of those identifying and recruiting patients to allocation status is recommended but is not always carried out. There may be fewer barriers to internal validity in trials in which individual participants are not recruited. External validity seems poorly addressed in many trials, yet is arguably as important as internal validity in judging quality as a basis for healthcare intervention.-Internal and external validity of cluster randomised trials: systematic review of recent trials.",1
"Individual randomized trials (IRTs) and cluster randomized trials (CRTs) with binary outcomes arise in a variety of settings and are often analyzed by logistic regression (fitted using generalized estimating equations for CRTs). The effect of stratification on the required sample size is less well understood for trials with binary outcomes than for continuous outcomes. We propose easy-to-use methods for sample size estimation for stratified IRTs and CRTs and demonstrate the use of these methods for a tuberculosis prevention CRT currently being planned. For both IRTs and CRTs, we also identify the ratio of the sample size for a stratified trial vs a comparably powered unstratified trial, allowing investigators to evaluate how stratification will affect the required sample size when planning a trial. For CRTs, these can be used when the investigator has estimates of the within-stratum intracluster correlation coefficients (ICCs) or by assuming a common within-stratum ICC. Using these methods, we describe scenarios where stratification may have a practically important impact on the required sample size. We find that in the two-stratum case, for both IRTs and for CRTs with very small cluster sizes, there are unlikely to be plausible scenarios in which an important sample size reduction is achieved when the overall probability of a subject experiencing the event of interest is low. When the probability of events is not small, or when cluster sizes are large, however, there are scenarios where practically important reductions in sample size result from stratification.-Sample size estimation for stratified individual and cluster randomized trials with binary outcomes",1
Robust covariate control in cluster-randomized trials,1
"The authors report the reliability and convergent validity in a sample of college students for 27 composite scales and two items covering alcohol use, cigarette smoking, marijuana use, and other drug use; beliefs relating to alcohol use; perceived norms for alcohol-related behavior; harm prevention skills; intentions to take prevention action; harm prevention action taken; risk taken; experienced harm; and other health-related behaviors and person characteristics. Data quality assessment strategies and missing data procedures were illustrated for large, multivariate, longitudinal data sets. Results indicate 23 of the 27 composite scales had at least acceptable reliability, and the remaining 4 composite scales had at least marginally acceptable reliability. At least moderate construct validity was demonstrated for 25 scales.-Data quality in evaluation of an alcohol-related harm prevention program.",0
"Online estimators update a current estimate with a new incoming batch of data without having to revisit past data thereby providing streaming estimates that are scalable to big data. We develop flexible, ensemble-based online estimators of an infinite-dimensional target parameter, such as a regression function, in the setting where data are generated sequentially by a common conditional data distribution given summary measures of the past. This setting encompasses a wide range of time-series models and, as special case, models for independent and identically distributed data. Our estimator considers a large library of candidate online estimators and uses online cross-validation to identify the algorithm with the best performance. We show that by basing estimates on the cross-validation-selected algorithm, we are asymptotically guaranteed to perform as well as the true, unknown best-performing algorithm. We provide extensions of this approach including online estimation of the optimal ensemble of candidate online estimators. We illustrate excellent performance of our methods using simulations and a real data example where we make streaming predictions of infectious disease incidence using data from a large database. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-Online cross-validation-based ensemble learning.",0
"Conventional approaches for handling missingness in substance use disorder trials commonly rely upon a single deterministic ""worst value"" imputation that posits a perfect relationship between missingness and drug use (""missing value = presumed drug use""); this yields biased estimates of treatment effects and their standard errors. Instead, deterministic imputations should be replaced by probabilistic versions that encode researchers prior beliefs that those with missing data are more likely to be using drugs at those occasions. Motivated by this problem, we present a method for handling non-monotone missing binary data in longitudinal studies. Specifically, we consider a joint model that combines a not missing at random (NMAR) selection model with a generalized linear mixed model for longitudinal binary data. The selection model links the distribution of a missing outcome to the corresponding distribution of the outcome for those observed at that occasion via a fixed and known sensitivity parameter. The mixed model for longitudinal binary data assumes the random effects have bridge distributions; the latter yields regression parameters that have both subject-specific and marginal interpretations. This approach is completely transparent about what is being assumed about missing data and can be used as the basis for sensitivity analysis.-Sensitivity analysis for non-monotone missing binary data in longitudinal studies: Application to the NIDA collaborative cocaine treatment study.",0
"objective measures are needed to quantify dietary adherence during caloric restriction (CR) while participants are freeliving. One method to monitor adherence is to compare observed weight loss to the expected weight loss during a prescribed level of CR. Normograms (graphs) of expected weight loss can be created from mathematical modeling of weight change to a given level of CR, conditional on the individual's set of baseline characteristics. These normograms can then be used by counselors to help the participant adhere to their caloric target. (1) To develop models of weight loss over a year of caloric restriction-given demographics, and well-defined measurements of body mass index, total daily energy expenditure (TDEE) and %CR. (2) To utilize these models to develop normograms, given the level of caloric restriction prescribed, and measures of these variables. Seventy-seven individuals completing a 6-12-month caloric restriction intervention (CALERIE) at three sites (Pennington Biomedical Research Center, Tufts University, and Washington University) and had body weight and body composition measured frequently. Energy intake (and %CR) was estimated from TDEE (by doubly labeled water) and body composition (by DXA) at baseline and months 1, 3, 6, and 12. Bodyweight was modeled to determine the predictors and distribution of the expected trajectory of percent weight change over 12 months of CR. As expected, CR was related to change in body weight. Controlling for time-varying measures, initially simple models of the functional form indicated that the trajectory of percent weight change was predicted by a nonlinear function of age, TDEE, %CR, and sex. Using these estimates, normograms for the weight change were developed. Our model estimates that the mean weight loss (% change from baseline weight) for an individual adherent to a 25% CR regimen is -10.9 ? 6.3% for females and -13.9 + 6.4% for men after 12 months. There are several limitations. Sample sizes are small (n = 77), and, by design, the protocols, including prescribed CR, for the interventions differed by site, and not all subjects completed a year of follow-up. In addition, the inclusion of subjects by age and initial BMI was constricted, so that these results may not generalize to other populations including older and obese subjects. The trajectory of percent weight change during CR interventions in the presence of well-measured covariates can be modeled using simple nonlinear functions, and is related level of CR, the percent change in TDEE, gender, and age. Displayed on a normogram, individually tailored trajectories can be used by counselors and participants to monitor weight loss and adherence to a CR regimen.-Development of adherence metrics for caloric restriction interventions.",0
"Reliable estimates of intracluster correlation coefficients (ICCs) for specific outcome measures are crucial for sample size calculations of future cluster randomized trials. ICCs indicate the proportion of data variability that is explained by defined levels of clustering. In this manuscript, we present potentially valuable and reliable estimates of ICCs for specific baseline and follow-up data. ICCs were estimated from linear and generalized linear mixed models using maximum likelihood estimation for common measures used in stroke research, including modified Rankin Scale (mRS), National Institutes of Health Stroke Scale (NIHSS), and Barthel Index (BI). Data were available for 11 841 patients with ischemic stroke from 11 randomized trials. After adjusting for age, thrombolysis, and baseline NIHSS, the median ICC for follow-up data, using center as the level of clustering, ranged from 0?007 to 0?041. The ICCs using trial, continent or year of enrollment as level of clustering were distinctly lower. Less than 1% of the variability of mRS, NIHSS, and BI was explained by any of these three cluster levels. This compendium of relevant ICC estimates should assist trial planning. For example, the sample size for a cluster trial with 150 patients per center using ordinal analysis of mRS should be inflated by 2?0 due to the ICC of 0?007; whereas the ICC of 0?031 using mRS dichotomized above mRS 0-1, requires inflation by 5?6. The low contribution of trials, year or continent of enrollment to overall variation in outcome offers reassurance that analyses using pooled data from multiple trials in VISTA are unlikely to suffer from bias from these sources.-Intracluster correlation coefficients and reliability of randomized multicenter stroke trials within VISTA.",1
University of Pennsylvania 7th annual conference on statistical issues in clinical trials: Current issues regarding the use of biomarkers and surrogate endpoints in clinical trials (morning panel discussion).,0
"In disease screening and prevention trials, subjects in the experimental condition are frequently nested within therapy groups, whereas subjects in the control group receive individual or no therapy and are therefore not nested within groups. Outcomes of subjects within the same therapy group are expected to be more alike than outcomes of subjects within different therapy groups. Ignoring this dependency in the design stage may result in less powerful designs. This paper presents a multilevel model for analyzing such trials and sample size formulae for continuous and binary outcomes with unequal variances and costs across groups. The proposed optimal design ensures that there is adequate power to detect a treatment effect with either minimal cost or a minimal number of subjects. We apply our strategy and design an improved trial where all subjects with musculoskeletal pain received conventional therapy and subjects in the intervention arm participated in a group-learning program.-Sample size formulae for trials comparing group and individual treatments in a multilevel model.",1
"This paper examines whether an ""environmental indicator""--a survey of grocery store product displays--can provide a realistic alternative to individual-level telephone surveys for the evaluation of community-based nutrition programs. Telephone and grocery store measures were used separately to evaluate three community-level dietary interventions that were part of the Kaiser Family Foundation Community Health Promotion Grants Program (CHPGP). Both surveys were conducted in the three intervention and seven control communities at three points in time: 1988, 1990, and 1992. The grocery store survey recorded the relative availability of low-fat and high-fiber products and the amount of store-provided health-education information. Self-reported dietary intake of residents was obtained in the same communities using a telephone survey. In the one community in which the intervention seemed to have contributed to reduced fat consumption, the grocery store and telephone surveys showed very similar relative changes for the only variable they had in common, low-fat milk consumption. In another community, both survey approaches indicated that there was no change or perhaps a slight worsening in the treatment relative to the controls. The third community produced the only contradictory results: the telephone survey suggested no change or perhaps a worsening, while the grocery store results were generally positive, though not statistically significant. These results, combined with the much lower cost of the grocery store survey, justify further pursuit of environmental indicators as an evaluation tool.-Evaluating community-based nutrition programs: comparing grocery store and individual-level survey measures of program impact.",1
"Trials carried out in primary care typically involve complex interventions that require considerable planning if they are to be implemented successfully. The role of the statistician in promoting both robust study design and appropriate statistical analysis is an important contribution to a multi-disciplinary primary care research group. Issues in the design of complex interventions have been addressed in the Medical Research Council's new guidance document and over the past 7 years by the Royal Statistical Society's Primary Health Care Study Group. With the aim of raising the profile of statistics and building research capability in this area, particularly with respect to methodological issues, the study group meetings have covered a wide range of topics that have been of interest to statisticians and non-statisticians alike. The aim of this article is to provide an overview of the statistical issues that have arisen over the years related to the design and evaluation of trials in primary care, to provide useful examples and references for further study and ultimately to promote good practice in the conduct of complex interventions carried out in primary care and other health care settings. Throughout we have given particular emphasis to statistical issues related to the design of cluster randomised trials.-Trials in primary care: statistical issues in the design, conduct and evaluation of complex interventions.",1
"The sandwich estimator of variance may be used to create robust Wald-type tests from estimating equations that are sums of K independent or approximately independent terms. For example, for repeated measures data on K individuals, each term relates to a different individual. These tests applied to a parameter may have greater than nominal size if K is small, or more generally if the parameter to be tested is essentially estimated from a small number of terms in the estimating equation. We offer some practical modifications to these robust Wald-type tests, which asymptotically approach the usual robust Wald-type tests. We show that one of these modifications provides exact coverage for a simple case and examine by simulation the modifications applied to the generalized estimating equations of Liang and Zeger (1986), conditional logistic regression, and the Cox proportional hazard model.-Small-sample adjustments for Wald-type tests using sandwich estimators.",1
"The Trial for Activity in Adolescent Girls (TAAG) is a group-randomized trial (GRT) to reduce the usual decline in moderate to vigorous physical activity (MVPA) among middle school girls. We report the school-level intraclass correlation (ICC) for MVPA from the TAAG baseline survey of sixth grade girls and describe the relationship between the schedule of data collection and the ICC. Each of six sites recruited six schools and randomly selected 60 sixth grade girls from each school; 74.2% participated. Girls were grouped in waves defined by the date measurements began and asked to wear an Actigraph accelerometer for 6 d. Occasional missing data were replaced by imputation, and counts above 1500 per 30 s were treated as MVPA, converted into metabolic equivalents (METs), and summed over 6 a.m.-midnight to provide MET-minutes per 18-h day. Mixed-model regression was used to estimate ICC. The school-level ICC were higher when estimated from a single wave compared with three waves (e.g., 0.057 vs 0.022) and across weekdays compared with weekend days (e.g., 0.024 vs 0.012). Power in a new trial would be greater with some schedules (e.g., 88% given three waves and 6 d) than with others (e.g., 23% given one wave and Tuesday only). The schedule of data collection can have a dramatic effect on the ICC for MVPA. In turn, this can have a dramatic effect on the standard error for an intervention effect and on power. Investigators will need to consider the expected magnitude of the ICC and the validity of the MVPA estimates associated with their data collection schedule in planning a new study.-School-level intraclass correlation for physical activity in sixth grade girls.",1
"Background A recent focus in the health sciences has been the development of personalized medicine, which includes determining the population for which a given treatment is effective. Due to limited data, identifying the true benefiting population is a challenging task. To tackle this difficulty, the credible subgroups approach provides a pair of bounding subgroups for the true benefiting subgroup, constructed so that one is contained by the benefiting subgroup while the other contains the benefiting subgroup with high probability. However, the method has so far only been developed for parametric linear models. Methods In this article, we develop the details required to follow the credible subgroups approach in more realistic settings by considering nonlinear and semiparametric regression models, supported for regulatory science by conditional power simulations. We also present an improved multiple testing approach using a step-down procedure. We evaluate our approach via simulations and apply it to data from four trials of Alzheimer's disease treatments carried out by AbbVie. Results Semiparametric modeling yields credible subgroups that are more robust to violations of linear treatment effect assumptions, and careful choice of the population of interest as well as the step-down multiple testing procedure result in a higher rate of detection of benefiting types of patients. The approach allows us to identify types of patients that benefit from treatment in the Alzheimer's disease trials. Conclusion Attempts to identify benefiting subgroups of patients in clinical trials are often met with skepticism due to a lack of multiplicity control and unrealistically restrictive assumptions. Our proposed approach merges two techniques, credible subgroups, and semiparametric regression, which avoids these problems and makes benefiting subgroup identification practical and reliable.-Multiplicity-adjusted semiparametric benefiting subgroup identification in clinical trials.",0
"We propose a robust two-stage design to identify the optimal biological dose for phase I/II clinical trials evaluating both toxicity and efficacy outcomes. In the first stage of dose finding, we use the Bayesian model averaging continual reassessment method to monitor the toxicity outcomes and adopt an isotonic regression method based on the efficacy outcomes to guide dose escalation. When the first stage ends, we use the Dirichlet-multinomial distribution to jointly model the toxicity and efficacy outcomes and pick the candidate doses based on a three-dimensional volume ratio. The selected candidate doses are then seamlessly advanced to the second stage for dose validation. Both toxicity and efficacy outcomes are continuously monitored so that any overly toxic and/or less efficacious dose can be dropped from the study as the trial continues. When the phase I/II trial ends, we select the optimal biological dose as the dose obtaining the minimal value of the volume ratio within the candidate set. An advantage of the proposed design is that it does not impose a monotonically increasing assumption on the shape of the dose-efficacy curve. We conduct extensive simulation studies to examine the operating characteristics of the proposed design. The simulation results show that the proposed design has desirable operating characteristics across different shapes of the underlying true dose-toxicity and dose-efficacy curves. The software to implement the proposed design is available upon request. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-A robust two-stage design identifying the optimal biological dose for phase I/II clinical trials.",0
"Previous reviews have identified problems in the design and analysis of group-randomized trials in a number of areas. Similar problems may exist in cancer research, but there have been no comprehensive reviews. We searched Medline and PubMed for group-randomized trials focused on cancer prevention and control that were published between 2002 and 2006. We located and reviewed 75 articles to determine whether articles included evidence of taking group randomization into account in establishing the size of the trial, such as reporting the expected intraclass correlation, the group component of variance, or the variance inflation factor. We also examined the analytical approaches to determine their appropriateness. Only 18 (24%) of the 75 articles documented appropriate methods for sample size calculations. Only 34 (45%) limited their reports to analyses judged to be appropriate. Fully 26 (34%) failed to report any analyses that were judged to be appropriate. The most commonly used inappropriate analysis was an analysis at the individual level that ignored the groups altogether. Nine articles (12%) did not provide sufficient information. Many investigators who use group-randomized trials in cancer research do not adequately attend to the special design and analytic challenges associated with these trials. Failure to do so can lead to reporting type I errors as real effects, mislead investigators and policy-makers, and slow progress toward control and prevention of cancer. A collaborative effort by investigators, statisticians, and others will be required to ensure that group-randomized trials are planned and analyzed using appropriate methods so that the scientific community can have confidence in the published results.-Design and analysis of group-randomized trials in cancer: a review of current practices.",1
"The goal of this article is to improve the practice and reporting of cost estimates of prevention programs. It reviews the steps in estimating the costs of an intervention and the principles that should guide estimation. The authors then review prior efforts to estimate intervention costs using a sample of well-known but diverse studies. Finally, the authors illustrate the principles with an example, the Family Bereavement Program. They conclude that example by discussing whether and how the costs of the intervention might differ when implemented in a real-world setting.-Estimating the costs of preventive interventions.",0
"As the USA grapples with an opioid epidemic, medical emergency departments (EDs) have become a critical setting for intervening with opioid-dependent patients. Brief interventions designed to bridge the gap from acute ED care to longer-term treatment have shown limited efficacy for this population. Strength-based case management (SBCM) has shown strong effects on treatment linkage among patients with substance use disorders in other healthcare settings. This study aimed to investigate whether SBCM is an effective model for linking opioid-dependent ED patients with addiction treatment and pharmacotherapy. Here, we describe the implementation and challenges of adapting SBCM for the ED (SBCM-ED). Study rationale, design, and baseline characteristics are also described. This study compared the effects of SBCM-ED to screening, assessment, and referral alone (SAR) on treatment linkage, substance use, and functioning. We recruited participants from a public hospital in NYC. Working alliance between case managers and participants and the feasibility of SBCM implementation were evaluated. Baseline data from the randomized sample were analyzed for group equivalency. Outcomes analyses are forthcoming. Three hundred adult participants meeting DSM-IV criteria for opioid dependence were randomly assigned to either SBCM, in which they received a maximum of six case management sessions within 90 days of enrollment, or SAR, in which they received a comprehensive referral list and pamphlet outlining drug use consequences. No significant differences were found between groups at baseline on demographic or substance use characteristics. All SAR participants and 92.6% of SBCM-ED participants initiated their assigned intervention. Over half of SBCM-ED first sessions occurred in the ED on the day of enrollment. Case managers developed a strong working alliance with SBCM-ED participants after just one session. Interventions that exceed SBIRT were accepted by an opioid-dependent patient population seen in an urban medical ED. At the time of study funding, this trial was one of the first to focus specifically on this population in this challenging setting. The successful implementation of SBCM demonstrates its adaptability to the ED and may serve as a potential model for EDs seeking to adopt an intervention that overcomes the barrier between the ED encounter and more intensive treatment. ClinicalTrials.gov NCT02586896 . Registered on 27 October 2015.-Implementation of strength-based case management for opioid-dependent patients presenting in medical emergency departments: rationale and study design of a randomized trial.",0
"There has been great interest in developing nonlinear structural equation models and associated statistical inference procedures, including estimation and model selection methods. In this paper a general semiparametric structural equation model (SSEM) is developed in which the structural equation is composed of nonparametric functions of exogenous latent variables and fixed covariates on a set of latent endogenous variables. A basis representation is used to approximate these nonparametric functions in the structural equation and the Bayesian Lasso method coupled with a Markov Chain Monte Carlo (MCMC) algorithm is used for simultaneous estimation and model selection. The proposed method is illustrated using a simulation study and data from the Affective Dynamics and Individual Differences (ADID) study. Results demonstrate that our method can accurately estimate the unknown parameters and correctly identify the true underlying model.-Bayesian lasso for semiparametric structural equation models.",0
"In this paper we propose formulae for calculating the expected number of events or, alternatively, the required trial duration, for clinical trials involving two treatment groups in which patients may potentially experience multiple events and the data will be analysed using a multiplicative intensity (MI) model. We use a partial likelihood-based approach and examine in detail two MI models: one that includes a binary treatment variable as the only covariate and a three-state Markov process model in which a binary time-varying covariate is added to the previous model. For the simpler model, our formula coincides with those derived by Cook using full likelihood methods. We present applications of the derived formulae to chronic granulomatous disease and breast cancer data sets.-Sample size calculations for the two-sample problem using the multiplicative intensity model.",0
"Data from a controlled clinical trial in liver cirrhosis are used to illustrate that multi-state models may be a useful tool in the analysis of data where survival is the ultimate outcome of interest but where intermediate, transient states are identified. We compare models for the marginal survival time distribution with models including transient states, both with respect to their clinical interpretation and with respect to the precision of survival probability estimates obtained from the various models.-Multi-state models for bleeding episodes and mortality in liver cirrhosis.",0
"Racial and ethnic minorities remain underrepresented in clinical research, yet few recruitment strategies have been rigorously evaluated. We experimentally tested whether targeted recruitment letters acknowledging diabetes health disparities and health risks specific to recipients' racial/ethnic group improved two metrics of trial participation: willingness to be screened and enrollment. This experiment was efficiently nested within a randomized clinical trial examining a preventive lifestyle intervention among women at high risk for diabetes. Pregnant women with gestational diabetes or impaired glucose tolerance (N = 445) were randomized to receive a targeted recruitment letter with health risk information specific to their racial/ethnic group (n = 216), or a standard letter with risk information for the general population (n = 229). All letters were bilingual in English and Spanish. The targeted as compared to the standard letter did not improve screening or enrollment rates overall or within separate racial/ethnic groups. Among Latina women who preferred Spanish, the targeted letter showed trends for improved screening (66.7% vs 33.3%, p = .06) and enrollment rates (38.9% vs 13.3%, p = .13). In contrast, among Latina women who preferred English, the targeted letter significantly lowered screening (29.6% vs 57.1%, p = .04) and showed trends for lowered enrollment rates (25.9% vs 50.0%, p = .07). Results from this randomized study appear to suggest that recruitment letters with diabetes health risk information targeted to recipients' race/ethnicity may improve one metric of clinical trial participation among Latina women who prefer Spanish, but not English. Larger experimental studies, incorporating input from diverse participant stakeholders, are needed to develop evidence-based minority recruitment strategies.-Outreach to diversify clinical trial participation: A randomized recruitment study.",0
"In analyses of time-to-failure data with competing risks, cumulative incidence functions may be used to estimate the time-dependent cumulative probability of failure due to specific causes. These functions are commonly estimated using nonparametric methods, but in cases where events due to the cause of primary interest are infrequent relative to other modes of failure, nonparametric methods may result in rather imprecise estimates for the corresponding subdistribution. In such cases, it may be possible to model the cause-specific hazard of primary interest parametrically, while accounting for the other modes of failure using nonparametric estimators. The cumulative incidence estimators so obtained are simple to compute and are considerably more efficient than the usual nonparametric estimator, particularly with regard to interpolation of cumulative incidence at early or intermediate time points within the range of data used to fit the function. More surprisingly, they are often nearly as efficient as fully parametric estimators. We illustrate the utility of this approach in the analysis of patients treated for early stage breast cancer.-Semiparametric models for cumulative incidence functions.",0
"Binary classification rules based on covariates typically depend on simple loss functions such as zero-one misclassification. Some cases may require more complex loss functions. For example, individual-level monitoring of HIV-infected individuals on antiretroviral therapy requires periodic assessment of treatment failure, defined as having a viral load (VL) value above a certain threshold. In some resource limited settings, VL tests may be limited by cost or technology, and diagnoses are based on other clinical markers. Depending on scenario, higher premium may be placed on avoiding false-positives, which brings greater cost and reduced treatment options. Here, the optimal rule is determined by minimizing a weighted misclassification loss/risk. We propose a method for finding and cross-validating optimal binary classification rules under weighted misclassification loss. We focus on rules comprising a prediction score and an associated threshold, where the score is derived using an ensemble learner. Simulations and examples show that our method, which derives the score and threshold jointly, more accurately estimates overall risk and has better operating characteristics compared with methods that derive the score first and the cutoff conditionally on the score especially for finite samples.-Classification using ensemble learning under weighted misclassification loss.",0
"Despite more lifestyle intervention trials, there is little published information on the development of the comparison group intervention. This article describes the comparison group intervention, termed Diabetes Support and Education Intervention and its development for the Action for HEAlth in Diabetes (Look AHEAD) trial. Look AHEAD, a randomized, controlled, multicenter trial, was designed to determine whether an Intensive Lifestyle Intervention to reduce weight and increase physical activity reduces cardiovascular morbidity and mortality in overweight volunteers with type 2 diabetes compared to the Diabetes Support and Education Intervention. The Diabetes Support and Education Committee was charged with developing the Diabetes Support and Education Intervention with the primary aim of participant retention. The objectives were to design the Diabetes Support and Education Intervention sessions, standardize delivery across the 16 clinics, review quality and protocol adherence and advise on staffing and funding. Following a mandatory session on basic diabetes education, three optional sessions were offered on nutrition, physical activity, and support yearly for 4 years. For each session, guidelines, objectives, activities, and a resource list were created. Participant evaluations were very positive with hands on experiences being the most valuable. Retention so far at years 1 and 4 has been excellent and only slightly lower in the Diabetes Support and Education Intervention arm. The comparison group plays an important role in the success of a clinical trial. Understanding the effort needed to develop and implement the comparison group intervention will facilitate its implementation in future lifestyle intervention trials, particularly multicenter trials. Retention rates may improve by developing the comparison intervention simultaneously with the lifestyle intervention.-The development and description of the comparison group in the Look AHEAD trial.",0
"Diabetes mellitus is a chronic disease of aging that affects more than 20% of people over 65. In older patients with diabetes, comorbidities are highly prevalent and their presence may alter the relative importance, effectiveness, and safety of treatments for diabetes. Randomized controlled trials have shown that intensive glucose control produces microvascular and cardiovascular benefits but typically after extended treatment periods (five to nine years) and with exposure to short term risks such as mortality (in one trial) and hypoglycemia. Decision analysis, health economics, and observational studies have helped to illustrate the importance of acknowledging life expectancy, hypoglycemia, and treatment burden when setting goals in diabetes. Guidelines recommend that physicians individualize the intensity of glucose control and treatments on the basis of the prognosis (for example, three tiers based on comorbidities and functional impairments) and preferences of individual patients. Very few studies have attempted to formally implement and study these concepts in clinical practice. To better meet the treatment needs of older patients with diabetes and comorbidities, more research is needed to determine the risks and benefits of intensifying, maintaining, or de-intensifying treatments in this population. This research effort should extend to the development and study of decision support tools as well as targeted care management.-Management of diabetes mellitus in older people with comorbidities.",0
"Electronic medical records (EMRs) have the potential to facilitate the design of large cluster-randomized trials (CRTs). To describe the design of a CRT of clinical decision support to improve diabetes care and outcomes. In the Diabetes Improvement Group-Intervention Trial (DIG-IT), we identified and balanced preassignment characteristics of 12,675 diabetic patients cared for by 147 physicians in 24 practices of 2 systems using the same vendor's EMR. EMR-facilitated disease management was system A's experimental intervention; system B interventions involved patient empowerment, with or without disease management. For our sample, we: (1) identified characteristics associated with response to interventions or outcomes; (2) summarized feasible partitions of 10 system A practices (2 groups) and 14 system B practices (3 groups) using intra-cluster correlation coefficients (ICCs) and standardized differences; (3) selected (blinded) partitions to effectively balance the characteristics; and (4) randomly assigned groups of practices to interventions. In System A, 4,306 patients, were assigned to 2 groups of practices; 8,369 patients in system B were assigned to 3 groups of practices. Nearly all baseline outcome variables and covariates were well-balanced, including several not included in the initial design. DIG-IT's balance was superior to alternative partitions based on volume, geography or demographics alone. EMRs facilitated rigorous CRT design by identifying large numbers of patients with diabetes and enabling fair comparisons through preassignment balancing of practice sites. Our methods can be replicated in other settings and for other conditions, enhancing the power of other translational investigations.-Electronic medical record-assisted design of a cluster-randomized trial to improve diabetes care and outcomes.",1
"This article presents marginal structural models with inverse propensity weighting (IPW) for assessing mediation. Generally, individuals are not randomly assigned to levels of the mediator. Therefore, confounders of the mediator and outcome may exist that limit causal inferences, a goal of mediation analysis. Either regression adjustment or IPW can be used to take confounding into account, but IPW has several advantages. Regression adjustment of even one confounder of the mediator and outcome that has been influenced by treatment results in biased estimates of the direct effect (i.e., the effect of treatment on the outcome that does not go through the mediator). One advantage of IPW is that it can properly adjust for this type of confounding, assuming there are no unmeasured confounders. Further, we illustrate that IPW estimation provides unbiased estimates of all effects when there is a baseline moderator variable that interacts with the treatment, when there is a baseline moderator variable that interacts with the mediator, and when the treatment interacts with the mediator. IPW estimation also provides unbiased estimates of all effects in the presence of nonrandomized treatments. In addition, for testing mediation we propose a test of the null hypothesis of no mediation. Finally, we illustrate this approach with an empirical data set in which the mediator is continuous, as is often the case in psychological research.-Assessing mediation using marginal structural models in the presence of confounding and moderation.",0
"In many areas of medical research, 'gold standard' diagnostic tests do not exist and so evaluating the performance of standardized diagnostic criteria or algorithms is problematic. In this paper we propose an approach to evaluating the operating characteristics of diagnoses using a latent class model. By defining 'true disease' as our latent variable, we are able to estimate sensitivity, specificity and negative and positive predictive values of the diagnostic test. These methods are applied to diagnostic criteria for depression using Baltimore's Epidemiologic Catchment Area Study Wave 3 data.-Methods for evaluating the performance of diagnostic tests in the absence of a gold standard: a latent class model approach.",0
Mandatory addiction treatment for people who use drugs: global health and human rights analysis.,0
"The generalized estimating equation (GEE) approach is widely used in regression analyses with correlated response data. Under mild conditions, the resulting regression coefficient estimator is consistent and asymptotically normal with its variance being consistently estimated by the so-called sandwich estimator. Statistical inference is thus accomplished by using the asymptotic Wald chi-squared test. However, it has been noted in the literature that for small samples the sandwich estimator may not perform well and may lead to much inflated type I errors for the Wald chi-squared test. Here we propose using an approximate t- or F-test that takes account of the variability of the sandwich estimator. The level of type I error of the proposed t- or F-test is guaranteed to be no larger than that of the Wald chi-squared test. The satisfactory performance of the proposed new tests is confirmed in a simulation study. Our proposal also has some advantages when compared with other new approaches based on direct modifications of the sandwich estimator, including the one that corrects the downward bias of the sandwich estimator. In addition to hypothesis testing, our result has a clear implication on constructing Wald-type confidence intervals or regions.-Small-sample adjustments in using the sandwich variance estimator in generalized estimating equations.",1
"Medical students, residents, postdoctoral fellows, and faculty commonly consult with biostatistical experts about study design and data analysis when conducting clinical research. The role of biostatistical training during these consultations is examined, and characterizations of the connections between biostatistical consultation and education are reviewed. The presence and kinds of teaching efforts during biostatistical consults at four academic research institutions over various periods of time between 1999 and 2005 (237 consultations in total) were recorded and are described. By site, 67, 70, 78, and 100 per cent of the consulting sessions included biostatistical training, with an overall 78 per cent (95 per cent CI: 73-83 per cent) of consultations including an educational component when all consultations were combined. Training covered a wide range of biostatistical topics. Seventy-five per cent of the consultations with faculty (120/161), 79 per cent with fellows and residents (31/39), and 100 per cent with medical students (10/10) included some degree of instruction in study design or statistical analysis topics. Results show that both the need and the opportunity exist for specialized biostatistical instruction during one-on-one sessions between a consulting biostatistician and physicians, medical students, and research staff. Academic researchers are ideally positioned to absorb this kind of training when they initiate a request for assistance with their own research project.-The role of education in biostatistical consulting.",0
"This article introduces two simple scatter plots for model diagnosis in structural equation modeling. One plot contrasts a residual-based M-distance of the structural model with the M-distance for the factor score. It contains information on outliers, good leverage observations, bad leverage observations, and normal cases. The other plot contrasts the residual-based M-distance with the quantile of a chi distribution. It allows the researcher to visually identify clusters of potential outliers. The article further studies the effect of the potential outliers on the overall model evaluation when they are removed according to the order of the clusters exhibited in the plot. Suggestions are provided on determining the outlier status of outstanding cases in real data analysis. Recommendations are also made on the choice of robust methods and maximum likelihood following outlier removal.-Fitting data to model: structural equation modeling diagnosis using two scatter plots.",0
"Objective measures of oxygen consumption and carbon dioxide production by mammals are used to predict their energy expenditure. Since energy expenditure is not directly observable, it can be viewed as a latent construct with multiple physical indirect measures such as respiratory quotient, volumetric oxygen consumption, and volumetric carbon dioxide production. Metabolic rate is defined as the rate at which metabolism occurs in the body. Metabolic rate is also not directly observable. However, heat is produced as a result of metabolic processes within the body. Therefore, metabolic rate can be approximated by heat production plus some errors. While energy expenditure and metabolic rates are correlated, they are not equivalent. Energy expenditure results from physical function, while metabolism can occur within the body without the occurrence of physical activities. In this manuscript, we present a novel approach for studying the relationship between metabolic rate and indicators of energy expenditure. We do so by extending our previous work on MIMIC ME models to allow responses that are sparsely observed functional data, defining the sparse functional multiple indicators, multiple cause measurement error (FMIMIC ME) models. The mean curves in our proposed methodology are modeled using basis splines. A novel approach for estimating the variance of the classical measurement error based on functional principal components is presented. The model parameters are estimated using the EM algorithm and a discussion of the model's identifiability is provided. We show that the defined model is not a trivial extension of longitudinal or functional data methods, due to the presence of the latent construct. Results from its application to data collected on Zucker diabetic fatty rats are provided. Simulation results investigating the properties of our approach are also presented.-Functional multiple indicators, multiple causes measurement error models.",0
"We take a functional data approach to longitudinal studies with complex bivariate outcomes. This work is motivated by data from a physical activity study that measured 2 responses over time in 5-minute intervals. One response is the proportion of time active in each interval, a continuous proportions with excess zeros and ones. The other response, energy expenditure rate in the interval, is a continuous variable with excess zeros and skewness. This outcome is complex because there are 3 possible activity patterns in each interval (inactive, partially active, and completely active), and those patterns, which are observed, induce both nonrandom and random associations between the responses. More specifically, the inactive pattern requires a zero value in both the proportion for active behavior and the energy expenditure rate; a partially active pattern means that the proportion of activity is strictly between zero and one and that the energy expenditure rate is greater than zero and likely to be moderate, and the completely active pattern means that the proportion of activity is exactly one, and the energy expenditure rate is greater than zero and likely to be higher. To address these challenges, we propose a 3-part functional data joint modeling approach. The first part is a continuation-ratio model to reorder the ordinal valued 3 activity patterns. The second part models the proportions when they are in interval (0,1). The last component specifies the skewed continuous energy expenditure rate with Box-Cox transformations when they are greater than zero. In this 3-part model, the regression structures are specified as smooth curves measured at various time points with random effects that have a correlation structure. The smoothed random curves for each variable are summarized using a few important principal components, and the association of the 3 longitudinal components is modeled through the association of the principal component scores. The difficulties in handling the ordinal and proportional variables are addressed using a quasi-likelihood type approximation. We develop an efficient algorithm to fit the model that also involves the selection of the number of principal components. The method is applied to physical activity data and is evaluated empirically by a simulation study.-Three-part joint modeling methods for complex functional data mixed with zero-and-one-inflated proportions and zero-inflated continuous outcomes with skewness.",0
"When contamination is present, randomization on a patient level leads to dilution of the treatment effect. The usual solution is to randomize on a cluster level, but at the cost of efficiency and more importantly, this may introduce selection bias. Furthermore, it may slow down recruitment in the clusters that are randomized to the ""less interesting"" treatment. We discuss an alternative randomization procedure to approach these problems. Pseudo cluster randomization is a two-stage randomization procedure that balances between individual randomization and cluster randomization. For common scenarios, the design factors needed to calculate the appropriate sample size are tabulated. A pseudo cluster randomized design can reduce selection bias and contamination, while maintaining good efficiency and possibly improving enrollment. To make a well-informed choice of randomization procedure, we discuss the advantages of each method and provide a decision flow chart. When contamination is thought to be substantial in an individually randomized setting and a cluster randomized design would suffer from selection bias and/or slow recruitment, pseudo cluster randomization can be considered.-Pseudo cluster randomization dealt with selection bias and contamination in clinical trials.",1
Bias in identifying and recruiting participants in cluster randomised trials: what can be done?,1
"Latinos are now the largest minority in the United States, but their distinctive health needs and mortality patterns remain poorly understood. Proportional hazards regressions were used to compare Latino versus White risk- and income-adjusted mortality over 25 years' follow-up from 5,846 Latino and 300,647 White men screened for the Multiple Risk Factor Intervention Trial. Men were aged 35-57 years and residing in 14 states when screened in 1973-1975. Data on coronary heart disease risk factors, self-reported race/ethnicity, and home addresses were obtained at baseline; income was estimated by linking addresses to census data. Mortality follow-up through 1999 was obtained using the National Death Index. The fully adjusted Latino/White hazard ratio for all-cause mortality was 0.82 (95% confidence interval (CI): 0.77, 0.87), based on 1,085 Latino and 73,807 White deaths; this pattern prevailed over time and across states (thus, likely across Latino subgroups). Hazard ratios were significantly greater than one for stroke (hazard ratio = 1.30, 95% CI: 1.01, 1.68), liver cancer (hazard ratio = 2.02, 95% CI: 1.21, 3.37), and infection (hazard ratio = 1.69, 95% CI: 1.24, 2.32). A substudy found only minor racial/ethnic differences in the quality of Social Security numbers, birth dates, soundex-adjusted names, and National Death Index searches. Results were not likely an artifact of return migration or incomplete mortality data.-Latino risk-adjusted mortality in the men screened for the Multiple Risk Factor Intervention Trial.",0
"The cluster randomized crossover (CRXO) design is gaining popularity in trial settings where individual randomization or parallel group cluster randomization is not feasible or practical. In a CRXO trial, not only are clusters of individuals rather than individuals themselves randomized to trial arms, but also each cluster participates in each arm of the trial at least once in separate periods of time.We will review publications of clinical trials undertaken in humans that have used the CRXO design. The aim of this systematic review is to summarize, as reported: the motivations for using the CRXO design, the values of the CRXO design parameters, the justification and methodology for the sample size calculations and analyses, and the quality of reporting the CRXO design aspects. We will identify reports of CRXO trials by systematically searching MEDLINE, PubMed, Cochrane Methodology Register, EMBASE, and CINAHL Plus. In addition, we will search for methodological articles that describe the CRXO design and conduct citation searches to identify any further CRXO trials. The references of all eligible trials will also be searched. We will screen the identified abstracts, and retrieve and assess for inclusion the full text for any potentially relevant articles. Data will be extracted from the full text independently by two reviewers. Descriptive summary statistics will be presented for the extracted data. This systematic review will inform both researchers addressing CRXO methodology and trialists considering implementing the design. The results will allow focused methodological research of the CRXO design, provide practical examples for researchers of how CRXO trials have been conducted, including any shortcomings, and highlight areas where reporting and conduct may be improved.-The use of the cluster randomized crossover design in clinical trials: protocol for a systematic review.",1
"In persons with type 1 diabetes (T1D), hypoglycemia is the major limiting factor in achieving optimal glycemic control. All persons with T1D are at risk for hypoglycemia (blood glucose level &lt; 70 mg/dl), which is life-threatening and accompanied by serious physical and psychological symptoms, resulting in profound fear of hypoglycemia (FOH) and reduced quality of life. Young adults with T1D are at risk for FOH and have worse glycemic control and self-management behavior than other age groups with T1D. FOH also results in increased glycemic variability (GV). A major gap exists in how to manage FOH. Our overall objective is to reduce FOH and improve diabetes self-management, glycemic control, and GV in young adults with T1D to reduce or delay diabetes complications and improve quality of life. We aim to (1) determine the feasibility and acceptability of an eight-week cognitive behavioral therapy (CBT)-based Fear Reduction Efficacy Evaluation (FREE) intervention in young adults with T1D who experience FOH; and (2) determine the impact of the FREE intervention, compared to an attention control group, on the outcomes FOH, self-management, glycemic control (A1C), and glycemic variability (continuous glucose monitoring recordings). A randomized controlled trial in 50 young adults aged 18 to 35 years with T1D will be used. Eligible subjects will be randomized to the intervention program (Fear Reduction Efficacy Evaluation [FREE]) or attention control group. A one-week run-in phase is planned, with baseline measures of FOH, self-management behavior, A1C, and real-time continuous glucose monitoring recordings (RT-CGM) to calculate GV for both groups. The intervention group will participate in eight weekly individual one-hour sessions using CBT and exposure treatment for specific fears. RT-CGM and a daily FOH diary will be used as feedback cues as part of the FREE program. The attention control group will participate in eight weekly individual one-hour diabetes self-management education (DSME) sessions and wear a RT-CGM device (to measure GV only) over 8 weeks. At completion, FOH will be measured, and RT-CGM recordings will be analyzed to determine differences between the FREE and control groups. Findings from this proposed pilot study will serve as the foundation for a larger trial to reduce FOH and improve self-management, glycemic control, and GV. ClinicalTrials.gov: A cognitive behavioral therapy (CBT) intervention to reduce fear of hypoglycemia in type 1 diabetes, NCT03549104. Registered June 7, 2018.-A cognitive behavioral therapy intervention to reduce fear of hypoglycemia in young adults with type 1 diabetes (FREE): study protocol for a randomized controlled trial.",0
"To test the reliability and validity of 8-day and 30-day self-report measures of adherence to daily isoniazid (INH) for treatment of latent tuberculosis infection (LTBI). Participants were 286 Latino adolescents (ages 13-18, 55.6% male) with LTBI recruited from 10 public middle and high schools in San Diego County. INH adherence was measured monthly for up to 9 months by interview and urine specimens at unannounced visits. Reliability and validity analyses were performed within 5 consecutive months. Reliability was assessed by correlating: (1) 8- and 30-day INH adherence measures within each month; and (2) each of the two adherence measures across months. Validity was assessed by correlating reported measures with biological assays within each month. Reliability tests yielded significant correlation coefficients (p &lt; .05 to .001), both across measures (r = 0.71-0.93) and across time (r = 0.29-0.64 for 8-day recall; r = 0.32-0.69 for 30-day recall). Validity tests of both adherence measures were also significant (p &lt; .05 to .001): 8-day recall (r(pb) = 0.52-0.72) and 30-day recall (r(pb) = 0.37-0.71). Results suggest that impromptu recall measures of INH adherence, combined with urine collection, are reliable and valid in Latino adolescents.-Self-report INH adherence measures were reliable and valid in Latino adolescents with latent tuberculosis infection.",0
"To hasten Ebola containment, mobilize survivors.",0
"In this article, we present new methods to analyze data from an experiment using rodent models to investigate the role of p27, an important cell-cycle mediator, in early colon carcinogenesis. The responses modeled here are essentially functions nested within a two-stage hierarchy. Standard functional data analysis literature focuses on a single stage of hierarchy and conditionally independent functions with near white noise. However, in our experiment, there is substantial biological motivation for the existence of spatial correlation among the functions, which arise from the locations of biological structures called colonic crypts: this possible functional correlation is a phenomenon we term crypt signaling. Thus, as a point of general methodology, we require an analysis that allows for functions to be correlated at the deepest level of the hierarchy. Our approach is fully Bayesian and uses Markov chain Monte Carlo methods for inference and estimation. Analysis of this data set gives new insights into the structure of p27 expression in early colon carcinogenesis and suggests the existence of significant crypt signaling. Our methodology uses regression splines, and because of the hierarchical nature of the data, dimension reduction of the covariance matrix of the spline coefficients is important: we suggest simple methods for overcoming this problem.-Bayesian hierarchical spatially correlated functional data analysis with application to colon carcinogenesis.",0
"Interrupted time series (ITS) is a strong quasi-experimental research design, which is increasingly applied to estimate the effects of health services and policy interventions. We describe and illustrate two methods for estimating confidence intervals (CIs) around absolute and relative changes in outcomes calculated from segmented regression parameter estimates. We used multivariate delta and bootstrapping methods (BMs) to construct CIs around relative changes in level and trend, and around absolute changes in outcome based on segmented linear regression analyses of time series data corrected for autocorrelated errors. Using previously published time series data, we estimated CIs around the effect of prescription alerts for interacting medications with warfarin on the rate of prescriptions per 10,000 warfarin users per month. Both the multivariate delta method (MDM) and the BM produced similar results. BM is preferred for calculating CIs of relative changes in outcomes of time series studies, because it does not require large sample sizes when parameter estimates are obtained correctly from the model. Caution is needed when sample size is small.-Methods for estimating confidence intervals in interrupted time series analyses of health interventions.",0
"Sometimes interventions in randomized clinical trials are not allocated to individual patients, but rather to patients in groups. This is called cluster allocation, or cluster randomization, and is particularly common in health services research. Similarly, in some types of observational studies, patients (or observations) are found in naturally occurring groups, such as neighborhoods. In either situation, observations within a cluster tend to be more alike than observations selected entirely at random. This violates the assumption of independence that is at the heart of common methods of statistical estimation and hypothesis testing. Failure to account for the dependence between individual observations and the cluster to which they belong can have profound implications on the design and analysis of such studies. Their p-values will be too small, confidence intervals too narrow, and sample size estimates too small, sometimes to a dramatic degree. This problem is similar to that caused by the more familiar ""unit of analysis error"" seen when observations are repeated on the same subjects, but are treated as independent. The purpose of this paper is to provide an introduction to the problem of clustered data in clinical research. It provides guidance and examples of methods for analyzing clustered data and calculating sample sizes when planning studies. The article concludes with some general comments on statistical software for cluster data and principles for planning, analyzing, and presenting such studies.-Advanced statistics: statistical methods for analyzing cluster and cluster-randomized data.",1
"Studies designed to evaluate HIV and STD prevention interventions often involve random assignment of groups such as neighborhoods or communities to study conditions (e.g., to intervention or control). Investigators who design group-randomized trials (GRTs) must take the expected intraclass correlation coefficient (ICC) into account in sample size estimation to have adequate power; however, few published ICC estimates exist for outcome variables related to HIV and STD prevention. The Prevention Options for Women Equal Rights (POWER) study was a GRT designed to evaluate a campaign to increase awareness and use of condoms among young African American and Hispanic women. The authors used precampaign and postcampaign data from the POWER study to estimate ICCs (unadjusted and adjusted for covariates) for a variety of sexual behavior and other variables. To illustrate the impact of ICCs on power, the authors present sample-size calculations and demonstrate how ICCs of differing magnitude will affect estimates of required sample size.-Estimates of intraclass correlation for variables related to behavioral HIV/STD prevention in a predominantly African American and Hispanic sample of young women.",2
"The stepped-wedge design (SWD) of clinical trials has become very popular in recent years, particularly in health services research. Typically, study participants are randomly allotted in clusters to the different treatment options. The basic principles of the stepped wedge design and related statistical techniques are described here on the basis of pertinent publications retrieved by a selective search in PubMed and in the CIS statistical literature database. In a typical SWD trial, the intervention is begun at a time point that varies from cluster to cluster. Until this time point is reached, all participants in the cluster belong to the control arm of the trial. Once the intervention is begun, it is continued with- out change until the end of the trial period. The starting time for the intervention in each cluster is determined by randomization. At the first time point of measurement, no intervention has yet begun in any cluster; at the last one, the intervention is in prog- ress in all clusters. The treatment effect can be optimally assessed under the assumption of an identical correlation at all time points. A method is available to calculate the power and the number of clusters that would be necessary in order to achieve statistical significance by the appropriate type of significance test. All of the statistical techniques presented here are based on the assumptions of a normal distribution of cluster means and of a constant intervention effect across all time points of measure- ment. The necessary statistical tools for the planning and evaluation of SWD trials now stand at our disposal. Such trials nevertheless are subject to major risks, as valid results can be obtained only if the far-reaching assumptions of the model are, in fact, justified.-Planning and Analysis of Trials Using a Stepped Wedge Design",3
"The National Institutes of Health (NIH) Health Care Systems Research Collaboratory (NIH Collaboratory) seeks to produce generalizable knowledge about the conduct of pragmatic research in health systems. This analysis applied the PRECIS-2 pragmatic trial criteria to five NIH Collaboratory pragmatic trials to better understand 1) the pragmatic aspects of the design and implementation of treatments delivered in real world settings and 2) the usability of the PRECIS-2 criteria for assessing pragmatic features across studies and across time. Using the PRECIS-2 criteria, five pragmatic trials were each rated by eight raters. For each trial, we reviewed the original grant application and a required progress report written at the end of a 1-year planning period that included changes to the protocol or implementation approach. We calculated median scores and interrater reliability for each PRECIS domain and for the overall trial at both time points, as well as the differences in scores between the two time points. We also reviewed the rater comments associated with the scores. All five trials were rated to be more pragmatic than explanatory, with comments indicating that raters generally perceived them to closely mirror routine clinical care across multiple domains. The PRECIS-2 domains for which the trials were, on average, rated as most pragmatic on the 1 to 5 scale at the conclusion of the planning period included primary analysis (mean = 4.7 (range = 4.5 to 4.9)), recruitment (4.3 (3.6 to 4.8)), eligibility (4.1 (3.4 to 4.8)), setting (4.1 (4.0 to 4.4)), follow-up (4.1 (3.4 to 4.9)), and primary outcome (4.1 (3.5 to 4.9)). On average, the less pragmatic domains were organization (3.3 (2.6 to 4.4)), flexibility of intervention delivery (3.5 (2.1-4.5)), and flexibility of intervention adherence (3.8 (2.8-4.5)). Interrater agreement was modest but statistically significant for four trials (Gwet's AC1 statistic range 0.23 to 0.40) and the intraclass correlation coefficient ranged from 0.05 to 0.31. Rating challenges included assigning a single score for domains that may relate to both patients and care settings (that is, eligibility or recruitment) and determining to what extent aspects of complex research interventions differ from usual care. These five trials in diverse healthcare settings were rated as highly pragmatic using the PRECIS-2 criteria. Applying the tool generated insightful discussion about real-world design decisions but also highlighted challenges using the tool. PRECIS-2 raters would benefit from additional guidance about how to rate the interwoven patient and practice-level considerations that arise in pragmatic trials. Clinicaltrials.gov trial registrations: NCT02019225 , NCT01742065 , NCT02015455 , NCT02113592 , NCT02063867 .-Use of PRECIS ratings in the National Institutes of Health (NIH) Health Care Systems Research Collaboratory.",0
"Intraclass correlation for measures from a worksite health promotion study: estimates, correlates, and applications.",1
"The vast majority of settings for which frequentist statistical properties are derived assume a fixed, a priori known sample size. Familiar properties then follow, such as, for example, the consistency, asymptotic normality, and efficiency of the sample average for the mean parameter, under a wide range of conditions. We are concerned here with the alternative situation in which the sample size is itself a random variable which may depend on the data being collected. Further, the rule governing this may be deterministic or probabilistic. There are many important practical examples of such settings, including missing data, sequential trials, and informative cluster size. It is well known that special issues can arise when evaluating the properties of statistical procedures under such sampling schemes, and much has been written about specific areas (Grambsch P. Sequential sampling based on the observed Fisher information to guarantee the accuracy of the maximum likelihood estimator. Ann Stat 1983; 11: 68-77; Barndorff-Nielsen O and Cox DR. The effect of sampling rules on likelihood statistics. Int Stat Rev 1984; 52: 309-326). Our aim is to place these various related examples into a single framework derived from the joint modeling of the outcomes and sampling process and so derive generic results that in turn provide insight, and in some cases practical consequences, for different settings. It is shown that, even in the simplest case of estimating a mean, some of the results appear counterintuitive. In many examples, the sample average may exhibit small sample bias and, even when it is unbiased, may not be optimal. Indeed, there may be no minimum variance unbiased estimator for the mean. Such results follow directly from key attributes such as non-ancillarity of the sample size and incompleteness of the minimal sufficient statistic of the sample size and sample sum. Although our results have direct and obvious implications for estimation following group sequential trials, there are also ramifications for a range of other settings, such as random cluster sizes, censored time-to-event data, and the joint modeling of longitudinal and time-to-event data. Here, we use the simplest group sequential setting to develop and explicate the main results. Some implications for random sample sizes and missing data are also considered. Consequences for other related settings will be considered elsewhere.-On random sample size, ignorability, ancillarity, completeness, separability, and degeneracy: sequential trials, random sample sizes, and missing data.",1
"We summarized and appraised evidence regarding HIV prevention interventions for adults with criminal justice involvement. We included randomized and quasi-randomized controlled trials that evaluated an HIV prevention intervention, enrolled participants with histories of criminal justice involvement, and reported biological or behavioral outcomes. We used Cochrane methods to screen 32,271 citations from 16 databases and gray literature. We included 37 trials enrolling n = 12,629 participants. Interventions were 27 psychosocial, 7 opioid substitution therapy, and 3 HIV-testing programs. Eleven programs significantly reduced sexual risk taking, 4 reduced injection drug risks, and 4 increased testing. Numerous interventions may reduce HIV-related risks among adults with criminal justice involvement. Future research should consider process evaluations, programs involving partners or families, and interventions integrating biomedical, psychosocial, and structural approaches.-HIV prevention for adults with criminal justice involvement: a systematic review of HIV risk-reduction interventions in incarceration and community settings.",0
"Few studies have comprehensively reported intracluster correlation coefficient (ICC) estimates for outcomes collected in primary care settings. Using data from a large primary care study, we aimed to: a) report ICCs for process-of-care and clinical outcome measures related to cardiovascular disease management and prevention, and b) investigate the impact of practice structure and rurality on ICC estimates. We used baseline data from the Improved Delivery of Cardiovascular Care (IDOCC) trial to estimate ICC values. Data on 5,140 patients from 84 primary care practices across Eastern Ontario, Canada were collected through chart abstraction. ICC estimates were calculated using an ANOVA approach and were calculated for all patients and separately for patient subgroups defined by condition (i.e., coronary artery disease, diabetes, chronic kidney disease, hypertension, dyslipidemia, and smoking). We compared ICC estimates between practices in which data were collected from a single physician versus those that had multiple participating physicians and between urban versus rural practices. ICC estimates ranged from 0 to 0.173, with a median of 0.056. The median ICC estimate for dichotomous process outcomes (0.088) was higher than that for continuous clinical outcomes (0.035). ICC estimates calculated for single physician practices were higher than those for practices with multiple physicians for both process (average 3.9-times higher) and clinical measures (average 1.9-times higher). Urban practices tended to have higher process-of-care ICC estimates than rural practices, particularly for measuring lipid profiles and estimated glomerular filtration rates. To our knowledge, this is the most comprehensive summary of cardiovascular-related ICCs to be reported from Canadian primary care practices. Differences in ICC estimates based on practice structure and location highlight the importance of understanding the context in which external ICC estimates were determined prior to their use in sample size calculations. Failure to choose appropriate ICC estimates can have substantial implications for the design of a cluster randomized trial.-Intracluster correlation coefficients for sample size calculations related to cardiovascular disease prevention and management in primary care practices.",1
"Field trials in tropical medicine are often designed so that intact social units (e.g., families, schools, communities) rather than independent individuals are randomized to an intervention group. Reasons are diverse, but include administrative convenience, a desire to reduce the effect of treatment contamination, and the need to avoid ethical issues that might otherwise arise. Dependencies among cluster members typical of such designs must be considered when determining sample size and analysing the resulting data. Failure to do so can result in false conclusions that the treatment is effective. The purpose of this paper is to compare different methods which can be used to construct tests of the effect of treatment when outcomes are binary (e.g., infected/uninfected). The discussion will be illustrated using data from a trial which randomly assigned families to either a control group or a screening and treatment programme for imported intestinal parasites.-Cluster randomization trials in tropical medicine: a case study.",1
"Screening substantially reduces cervical cancer incidence and mortality. More than half of invasive cervical cancers are attributable to infrequent screening or not screening at all. The current study, My Body My Test (MBMT), evaluates the impact of mailed kits for self-collection of samples for human papillomavirus (HPV) testing on completion of cervical cancer screening in low-income, North Carolina women overdue for cervical cancer screening. The study will enroll at least 510 US women aged 25-64 years who report no Pap test in the last 4 years and no HPV test in the last 6 years. We will randomize participants to an intervention or control arm. The intervention arm will receive kits to self-collect a sample at home and mail it for HPV testing. In both the intervention and control arms, participants will receive assistance in scheduling an appointment for screening in clinic. Study staff will deliver HPV self-collection results by phone and assist in scheduling participants for screening in clinic. The primary outcome is completion of cervical cancer screening. Specifically, completion of screening will be defined as screening in clinic or receipt of negative HPV self-collection results. Women with HPV-negative self-collection results will be considered screening-complete. All other participants will be considered screening-complete if they obtain co-testing or Pap test screening at a study-affiliated institution or other clinic. We will assess whether the self-collection intervention influences participants' perceived risk of cervical cancer and whether perceived risk mediates the relationship between HPV self-collection results and subsequent screening in clinic. We also will estimate the incremental cost per woman screened of offering at-home HPV self-collection kits with scheduling assistance as compared to offering scheduling assistance alone. If mailed self-collection of samples for HPV testing is an effective strategy for increasing cervical cancer screening among women overdue for screening, this method has the potential to reduce cervical cancer incidence and mortality in medically underserved women at higher risk of developing cervical cancer. ClinicalTrials.gov NCT02651883, Registered on 11 January 2016.-Impact of human papillomavirus (HPV) self-collection on subsequent cervical cancer screening completion among under-screened US women: MyBodyMyTest-3 protocol for a randomized controlled trial.",0
"In an individually randomised controlled trial where the treatment is delivered by a health professional it seems likely that the effectiveness of the treatment, independent of any treatment effect, could depend on the skill, training or even enthusiasm of the health professional delivering it. This may then lead to a potential clustering of the outcomes for patients treated by the same health professional, but similar clustering may not occur in the control arm. Using four case studies, we aim to provide practical guidance and recommendations for the analysis of trials with some element of clustering in one arm. Five approaches to the analysis of outcomes from an individually randomised controlled trial with clustering in one arm are identified in the literature. Some of these methods are applied to four case studies of completed randomised controlled trials with clustering in one arm with sample sizes ranging from 56 to 539. Results are obtained using the statistical packages R and Stata and summarised using a forest plot. The intra-cluster correlation coefficient (ICC) for each of the case studies was small (&lt;0.05) indicating little dependence on the outcomes related to cluster allocations. All models fitted produced similar results, including the simplest approach of ignoring clustering for the case studies considered. A partially clustered approach, modelling the clustering in just one arm, most accurately represents the trial design and provides valid results. Modelling homogeneous variances between the clustered and unclustered arm is adequate in scenarios similar to the case studies considered. We recommend treating each participant in the unclustered arm as a single cluster. This approach is simple to implement in R and Stata and is recommended for the analysis of trials with clustering in one arm only. However, the case studies considered had small ICC values, limiting the generalisability of these results.-Recommendations for the analysis of individually randomised controlled trials with clustering in one arm - a case of continuous outcomes.",2
"Analysis of clustered data focusing on inference of the marginal distribution may be problematic when the risk of the outcome is related to the cluster size, termed as informative cluster size. In the absence of censoring, Hoffman et al. proposed a within-cluster resampling method, which is asymptotically equivalent to a weighted generalized estimating equations score equation. We investigate the estimation of the marginal distribution for multivariate survival data with informative cluster size using cluster-weighted Weibull and Cox proportional hazards models. The cluster-weighted Cox model can be implemented using standard software. Simulation results demonstrate that the proposed methods produce unbiased parameter estimation in the presence of informative cluster size. To illustrate the proposed approach, we analyze survival data from a lymphatic filariasis study in Recife, Brazil.-Modeling survival data with informative cluster size.",1
"Inverse probability weights used to fit marginal structural models are typically estimated using logistic regression. However, a data-adaptive procedure may be able to better exploit information available in measured covariates. By combining predictions from multiple algorithms, ensemble learning offers an alternative to logistic regression modeling to further reduce bias in estimated marginal structural model parameters. We describe the application of two ensemble learning approaches to estimating stabilized weights: super learning (SL), an ensemble machine learning approach that relies on V-fold cross validation, and an ensemble learner (EL) that creates a single partition of the data into training and validation sets. Longitudinal data from two multicenter cohort studies in Spain (CoRIS and CoRIS-MD) were analyzed to estimate the mortality hazard ratio for initiation versus no initiation of combined antiretroviral therapy among HIV positive subjects. Both ensemble approaches produced hazard ratio estimates further away from the null, and with tighter confidence intervals, than logistic regression modeling. Computation time for EL was less than half that of SL. We conclude that ensemble learning using a library of diverse candidate algorithms offers an alternative to parametric modeling of inverse probability weights when fitting marginal structural models. With large datasets, EL provides a rich search over the solution space in less time than SL with comparable results.-Ensemble learning of inverse probability weights for marginal structural modeling in large observational datasets.",0
"The methods of evaluating change and improvement strategies are not well described. The design and conduct of a range of experimental and non-experimental quantitative designs are considered. Such study designs should usually be used in a context where they build on appropriate theoretical, qualitative and modelling work, particularly in the development of appropriate interventions. A range of experimental designs are discussed including single and multiple arm randomised controlled trials and the use of more complex factorial and block designs. The impact of randomisation at both group and individual levels and three non-experimental designs (uncontrolled before and after, controlled before and after, and time series analysis) are also considered. The design chosen will reflect both the needs (and resources) in any particular circumstances and also the purpose of the evaluation. The general principle underlying the choice of evaluative design is, however, simple-those conducting such evaluations should use the most robust design possible to minimise bias and maximise generalisability.-Research designs for studies evaluating the effectiveness of change and improvement strategies.",1
"The stepped wedge (SW) cluster randomized controlled trial (CRCT) design is being used with increasing frequency. However, there is limited published research on the quality of reporting of SW-CRCTs. We address this issue by conducting a literature review. Medline, Ovid, Web of Knowledge, the Cochrane Library, PsycINFO, the ISRCTN registry, and ClinicalTrials.gov were searched to identify investigations employing the SW-CRCT design up to February 2015. For each included completed study, information was extracted on a selection of criteria, based on the CONSORT extension to CRCTs, to assess the quality of reporting. A total of 123 studies were included in our review, of which 39 were completed trial reports. The standard of reporting of SW-CRCTs varied in quality. The percentage of trials reporting each criterion varied to as low as 15.4%, with a median of 66.7%. There is much room for improvement in the quality of reporting of SW-CRCTs. This is consistent with recent findings for CRCTs. A CONSORT extension for SW-CRCTs is warranted to standardize the reporting of SW-CRCTs.-Stepped wedge cluster randomized controlled trial designs: a review of reporting quality and design features.",3
"Expected value of sample information (EVSI) measures the anticipated net benefit gained from conducting new research with a specific design to add to the evidence on which reimbursement decisions are made. Cluster randomized trials raise specific issues for EVSI calculations because 1) a hierarchical model is necessary to account for between-cluster variability when incorporating new evidence and 2) heterogeneity between clusters needs to be carefully characterized in the cost-effectiveness analysis model. Multi-arm trials provide parameter estimates that are correlated, which needs to be accounted for in EVSI calculations. Furthermore, EVSI is computationally intensive when the net benefit function is nonlinear, due to the need for an inner-simulation step. We develop a method for the computation of EVSI that avoids the inner simulation step for cluster randomized multi-arm trials with a binary outcome, where the net benefit function is linear in the probability of an event but nonlinear in the log-odds ratio parameters. We motivate and illustrate the method with an example of a cluster randomized 2 ? 2 factorial trial for interventions to increase attendance at breast screening in the UK, using a previously reported cost-effectiveness model. We highlight assumptions made in our approach, extensions to individually randomized trials and inclusion of covariates, and areas for further developments. We discuss computation time, the research-design space, and the ethical implications of an EVSI approach. We suggest that EVSI is a practical and appropriate tool for the design of cluster randomized trials.-Expected value of sample information for multi-arm cluster randomized trials with binary outcomes.",1
"The functional role of oxidative stress in cancer pathogenesis has long been a hotly debated topic. A study published this month in BMC Cancer by Goh et al., directly addresses this issue by using a molecular genetic approach, via an established mouse animal model of human breast cancer. More specifically, alleviation of mitochondrial oxidative stress, via transgenic over-expression of catalase (an anti-oxidant enzyme) targeted to mitochondria, was sufficient to lower tumor grade (from high-to-low) and to dramatically reduce metastatic tumor burden by &gt;12-fold. Here, we discuss these new findings and place them in the context of several other recent studies showing that oxidative stress directly contributes to tumor progression and metastasis. These results have important clinical and translational significance, as most current chemo-therapeutic agents and radiation therapy increase oxidative stress, and, therefore, could help drive tumor recurrence and metastasis. Similarly, chemo- and radiation-therapy both increase the risk for developing a secondary malignancy, such as leukemia and/or lymphoma. To effectively reduce mitochondrial oxidative stress, medical oncologists should now re-consider the use of powerful anti-oxidants as a key component of patient therapy and cancer prevention. Please see related research article: http://www.biomedcentral.com/1471-2407/11/191.-Mitochondrial oxidative stress drives tumor progression and metastasis: should we use antioxidants as a key component of cancer treatment and prevention?",0
"We used simulation to compare accuracy of estimation and confidence interval coverage of several methods for analysing binary outcomes from cluster randomized trials. The following methods were used to estimate the population-averaged intervention effect on the log-odds scale: marginal logistic regression models using generalized estimating equations with information sandwich estimates of standard error (GEE); unweighted cluster-level mean difference (CL/U); weighted cluster-level mean difference (CL/W) and cluster-level random effects linear regression (CL/RE). Methods were compared across trials simulated with different numbers of clusters per trial arm, numbers of subjects per cluster, intraclass correlation coefficients (rho), and intervention versus control arm proportions. Two thousand data sets were generated for each combination of design parameter values. The results showed that the GEE method has generally acceptable properties, including close to nominal levels of confidence interval coverage, when a simple adjustment is made for data with relatively few clusters. CL/U and CL/W have good properties for trials where the number of subjects per cluster is sufficiently large and rho is sufficiently small. CL/RE also has good properties in this situation provided a t-distribution multiplier is used for confidence interval calculation in studies with small numbers of clusters. For studies where the number of subjects per cluster is small and rho is large all cluster-level methods may perform poorly for studies with between 10 and 50 clusters per trial arm.-A simulation study of odds ratio estimation for binary outcomes from cluster randomized trials.",1
"With censored event time observations, the logrank test is the most popular tool for testing the equality of two underlying survival distributions. Although this test is asymptotically distribution free, it may not be powerful when the proportional hazards assumption is violated. Various other novel testing procedures have been proposed, which generally are derived by assuming a class of specific alternative hypotheses with respect to the hazard functions. The test considered by Pepe and Fleming (1989) is based on a linear combination of weighted differences of the two Kaplan-Meier curves over time and is a natural tool to assess the difference of two survival functions directly. In this article, we take a similar approach but choose weights that are proportional to the observed standardized difference of the estimated survival curves at each time point. The new proposal automatically makes weighting adjustments empirically. The new test statistic is aimed at a one-sided general alternative hypothesis and is distributed with a short right tail under the null hypothesis but with a heavy tail under the alternative. The results from extensive numerical studies demonstrate that the new procedure performs well under various general alternatives with a caution of a minor inflation of the type I error rate when the sample size is small or the number of observed events is small. The survival data from a recent cancer comparative study are utilized for illustrating the implementation of the process.-A versatile test for equality of two survival functions based on weighted differences of Kaplan-Meier curves.",0
"A robust likelihood approach for the analysis of overdispersed correlated count data that takes into account cluster varying covariates is proposed. We emphasise two characteristics of the proposed method: That the correlation structure satisfies the constraints on the second moments and that the estimation of the correlation structure guarantees consistent estimates of the regression coefficients. In addition we extend the mean specification to include within- and between-cluster effects. The method is illustrated through the analysis of data from two studies. In the first study, cross-sectional count data from a randomised controlled trial are analysed to evaluate the efficacy of a communication skills training programme. The second study involves longitudinal count data which represent counts of damaged hand joints in patients with psoriatic arthritis. Motivated by this study, we generalize our model to accommodate for a subpopulation of patients who are not susceptible to the development of damaged hand joints.-Regression analysis of overdispersed correlated count data with subject specific covariates.",1
Alzheimer's disease beyond amyloid: strategies for future therapeutic interventions.,0
"To describe the inaugural comparative effectiveness research (CER) cohort study of Washington State's Comparative Effectiveness Research Translation Network (CERTAIN), which compares invasive with noninvasive treatments for peripheral artery disease, and to focus on the patient centeredness of this cohort study by describing it within the context of a newly published conceptual framework for patient-centered outcomes research (PCOR). The peripheral artery disease study was selected because of clinician-identified uncertainty in treatment selection and differences in desired outcomes between patients and clinicians. Patient centeredness is achieved through the ""Patient Voices Project,"" a CERTAIN initiative through which patient-reported outcome (PRO) instruments are administered for research and clinical purposes, and a study-specific patient advisory group where patients are meaningfully engaged throughout the life cycle of the study. A clinician-led research advisory panel follows in parallel. Primary outcomes are PRO instruments that measure function, health-related quality of life, and symptoms, the latter developed with input from the patients. Input from the patient advisory group led to revised retention procedures, which now focus on short-term (3-6 months) follow-up. The research advisory panel is piloting a point-of-care, patient assessment checklist, thereby returning study results to practice. The cohort study is aligned with the tenets of one of the new conceptual frameworks for conducting PCOR. The CERTAIN's inaugural cohort study may serve as a useful model for conducting PCOR and creating a learning health care network.-A model for incorporating patient and stakeholder voices in a learning health care network: Washington State's Comparative Effectiveness Research Translation Network.",0
"Zero-inflated regression models have emerged as a popular tool within the parametric framework to characterize count data with excess zeros. Despite their increasing popularity, much of the literature on real applications of these models has centered around the latent class formulation where the mean response of the so-called at-risk or susceptible population and the susceptibility probability are both related to covariates. While this formulation in some instances provides an interesting representation of the data, it often fails to produce easily interpretable covariate effects on the overall mean response. In this article, we propose two approaches that circumvent this limitation. The first approach consists of estimating the effect of covariates on the overall mean from the assumed latent class models, while the second approach formulates a model that directly relates the overall mean to covariates. Our results are illustrated by extensive numerical simulations and an application to an oral health study on low income African-American children, where the overall mean model is used to evaluate the effect of sugar consumption on caries indices.-Marginal mean models for zero-inflated count data.",0
"Most school-based smoking prevention studies employ designs in which schools or classrooms are assigned to different treatment conditions while observations are made on individual students. This design requires that the treatment effect be assessed against the between-school variance. However, the between-school variance is usually larger than the variance that would be obtained if students were individually randomized to different conditions. Consequently, the power of the test for a treatment effect is reduced, and it becomes difficult to detect important treatment effects. To assess the potential loss of power or to calculate appropriate sample sizes, investigators need good estimates of the intraclass correlations for the variables of interest. The authors calculated intraclass correlations for some common outcome variables in a school-based smoking prevention study, using a three-level model-i.e., students nested within classrooms and classrooms nested within schools. The authors present the intraclass correlation estimates for the entire data set, as well as separately by sex and ethnicity. They also illustrate the use of these estimates in the planning of future studies.-Intraclass correlation estimates in a school-based smoking prevention study. Outcome and mediating variables, by sex and ethnicity.",1
"Stepped-wedge design (SWD) cluster-randomized trials have traditionally been used for evaluating a single intervention. We aimed to explore design variants suitable for evaluating multiple interventions in an SWD trial. We identified four specific variants of the traditional SWD that would allow two interventions to be conducted within a single cluster-randomized trial: concurrent, replacement, supplementation, and factorial SWDs. These variants were chosen to flexibly accommodate study characteristics that limit a one-size-fits-all approach for multiple interventions. In the concurrent SWD, each cluster receives only one intervention, unlike the other variants. The replacement SWD supports two interventions that will not or cannot be used at the same time. The supplementation SWD is appropriate when the second intervention requires the presence of the first intervention, and the factorial SWD supports the evaluation of intervention interactions. The precision for estimating intervention effects varies across the four variants. Selection of the appropriate design variant should be driven by the research question while considering the trade-off between the number of steps, number of clusters, restrictions for concurrent implementation of the interventions, lingering effects of each intervention, and precision of the intervention effect estimates.-Proposed variations of the stepped-wedge design can be used to accommodate multiple interventions.",3
"To calculate sample sizes in cluster randomized trials (CRTs), the cluster sizes are usually assumed to be identical across all clusters for simplicity. However, equal cluster sizes are not guaranteed in practice, especially when the number of clusters is limited. Therefore, it is important to understand the relative efficiency (RE) of equal versus unequal cluster sizes when designing CRTs with a limited number of clusters. In this paper, we are interested in the RE of two bias-corrected sandwich estimators of the treatment effect in the Generalized Estimating Equation (GEE) models for CRTs with a small number of clusters. Specifically, we derive the RE of two bias-corrected sandwich estimators for binary, continuous, or count data in CRTs under the assumption of an exchangeable working correlation structure. We consider different scenarios of cluster size distributions and investigate RE performance through simulation studies. We conclude that the number of clusters could be increased by as much as 42% to compensate for efficiency loss due to unequal cluster sizes. Finally, we propose an algorithm of increasing the number of clusters when the coefficient of variation of cluster sizes is known and unknown.-Relative efficiency of equal versus unequal cluster sizes in cluster randomized trials with a small number of clusters",1
A practical guide to cluster randomized trials in health research,1
Incorrect Analyses of Cluster-Randomized Trials that Do Not Take Clustering and Nesting into Account Likely Lead to p-Values that Are Too Small,1
"The nature of variability in menstrual function has not been adequately described or quantified across the reproductive life span. This article evaluates the applicability of the bipartite model approach to the analysis of menstrual data and the relative importance of within-woman variability across the reproductive life span using data from the Tremin Trust data, a large prospective study in which women maintained menstrual diaries throughout their reproductive life. We first consider how the boundaries of the Gaussian portion of the distribution change with age, and reflect upon the implications of these distribution changes for definitions of normal cycling. We next estimate the change in mean cycle length, in between- and within-woman variance and in the probability of having a nonstandard cycle across the reproductive life span. Finally, we characterize the dynamics of menstrual cycling within women over time at various points in the reproductive life span.-Analysis of menstrual diary data across the reproductive life span applicability of the bipartite model approach and the importance of within-woman variance.",0
"In randomized trials of provider-focused clinical interventions, treatment allocation often cannot be blinded to participants, study staff, or providers. The choice of unit of randomization (patient, provider, or clinic) entails tradeoffs in cost, power, and bias. Provider- or clinic-level randomization can minimize contamination, but it incurs the equally problematic potential for referral bias; that is, because arm assignment of future participants generally cannot be concealed, differences between arms may arise in the types of patients enrolled. Pseudo-cluster randomization is a novel study design that balances these competing validity threats. Providers are randomly assigned to an imbalanced proportion of intervention-arm participants (e.g., 80% or 20%). Providers can be masked to the imbalance, avoiding referral bias. Contamination is reduced because only a minority of control-arm participants are treated by majority-intervention providers. Pseudo-cluster randomization was implemented in a randomized trial of a decision support intervention to manage depression among patients receiving human immunodeficiency virus care in the southern United States in 2010-2014. The design appears successful in avoiding referral bias (participants were comparable between arms on important characteristics) and contamination (key depression treatment indicators were comparable between usual care participants managed by majority-intervention and majority-usual care providers and were markedly different compared with intervention participants).-Balancing Contamination and Referral Bias in a Randomized Clinical Trial: An Application of Pseudo-Cluster Randomization.",1
"The instrumental variable (IV) design is a well-known approach for unbiased evaluation of causal effects in the presence of unobserved confounding. In this article, we study the IV approach to account for selection bias in regression analysis with outcome missing not at random. In such a setting, a valid IV is a variable which (i) predicts the nonresponse process, and (ii) is independent of the outcome in the underlying population. We show that under the additional assumption (iii) that the IV is independent of the magnitude of selection bias due to nonresponse, the population regression in view is nonparametrically identified. For point estimation under (i)-(iii), we propose a simple complete-case analysis which modifies the regression of primary interest by carefully incorporating the IV to account for selection bias. The approach is developed for the identity, log and logit link functions. For inferences about the marginal mean of a binary outcome assuming (i) and (ii) only, we describe novel and approximately sharp bounds which unlike Robins-Manski bounds, are smooth in model parameters, therefore allowing for a straightforward approach to account for uncertainty due to sampling variability. These bounds provide a more honest account of uncertainty and allows one to assess the extent to which a violation of the key identifying condition (iii) might affect inferences. For illustration, the methods are used to account for selection bias induced by HIV testing nonparticipation in the evaluation of HIV prevalence in the Zambian Demographic and Health Surveys.-A general instrumental variable framework for regression analysis with outcome missing not at random.",0
"In cluster-randomized trials, groups of individuals (clusters) are randomized to the treatments or interventions to be compared. In many of those trials, the primary objective is to compare the time for an event to occur between randomized groups, and the shared frailty model well fits clustered time-to-event data. Members of the same cluster tend to be more similar than members of different clusters, causing correlations. As correlations affect the power of a trial to detect intervention effects, the clustered design has to be considered in planning the sample size. In this publication, we derive a sample size formula for clustered time-to-event data with constant marginal baseline hazards and correlation within clusters induced by a shared frailty term. The sample size formula is easy to apply and can be interpreted as an extension of the widely used Schoenfeld's formula, accounting for the clustered design of the trial. Simulations confirm the validity of the formula and its use also for non-constant marginal baseline hazards. Findings are illustrated on a cluster-randomized trial investigating methods of disseminating quality improvement to addiction treatment centers in the USA.-Sample size in cluster-randomized trials with time to event as the primary endpoint.",1
"This paper describes methodology for analyzing data from cluster randomized trials with count outcomes, taking indirect effects as well spatial effects into account. Indirect effects are modeled using a novel application of a measure of depth within the intervention arm. Both direct and indirect effects can be estimated accurately even when the proposed model is misspecified. We use spatial regression models with Gaussian random effects, where the individual outcomes have distributions overdispersed with respect to the Poisson, and the corresponding direct and indirect effects have a marginal interpretation. To avoid spatial confounding, we use orthogonal regression, in which random effects represent spatial dependence using a homoscedastic and dimensionally reduced modification of the intrinsic conditional autoregression model. We illustrate the methodology using spatial data from a pair-matched cluster randomized trial against the dengue mosquito vector Aedes aegypti, done in Trujillo,?Venezuela.-Spatial regression and spillover effects in cluster randomized trials with count outcomes",1
"We derived sample size formulae for detecting main effects in group-based randomized clinical trials with different levels of data hierarchy between experimental and control arms. Such designs are necessary when experimental interventions need to be administered to groups of subjects whereas control conditions need to be administered to individual subjects. This type of trial, often referred to as a partially nested or partially clustered design, has been implemented for management of chronic diseases such as diabetes and is beginning to emerge more commonly in wider clinical settings. Depending on the research setting, the level of hierarchy of data structure for the experimental arm can be three or two, whereas that for the control arm is two or one. Such different levels of data hierarchy assume correlation structures of outcomes that are different between arms, regardless of whether research settings require two or three level data structure for the experimental arm. Therefore, the different correlations should be taken into account for statistical modeling and for sample size determinations. To this end, we considered mixed-effects linear models with different correlation structures between experimental and control arms to theoretically derive and empirically validate the sample size formulae with simulation studies.-Sample size determinations for group-based randomized clinical trials with different levels of data hierarchy between experimental and control arms.",2
"In recent years, the number of studies using a cluster-randomized design has grown dramatically. In addition, the cluster-randomized crossover design has been touted as a methodological advance that can increase efficiency of cluster-randomized studies in certain situations. While the cluster-randomized crossover trial has become a popular tool, standards of design, analysis, reporting and implementation have not been established for this emergent design. We address one particular aspect of cluster-randomized and cluster-randomized crossover trial design: estimating statistical power. We present a general framework for estimating power via simulation in cluster-randomized studies with or without one or more crossover periods. We have implemented this framework in the clusterPower software package for R, freely available online from the Comprehensive R Archive Network. Our simulation framework is easy to implement and users may customize the methods used for data analysis. We give four examples of using the software in practice. The clusterPower package could play an important role in the design of future cluster-randomized and cluster-randomized crossover studies. This work is the first to establish a universal method for calculating power for both cluster-randomized and cluster-randomized clinical trials. More research is needed to develop standardized and recommended methodology for cluster-randomized crossover studies.-Empirical power and sample size calculations for cluster-randomized and cluster-randomized crossover studies.",1
"Compared to American Whites, African Americans have a higher prevalence of type 2 diabetes mellitus (T2DM), experiencing poorer metabolic control and greater risks for complications and death. Patient-level factors, such as diabetes knowledge, self-management skills, empowerment, and perceived control, account for &gt;90% of the variance observed in outcomes between these racial groups. There is strong evidence that self-management interventions that include telephone-delivered diabetes education and skills training are effective at improving metabolic control in diabetes. Web-based home telemonitoring systems in conjunction with active care management are also effective ways to lower glycosylated hemoglobin A1c values when compared to standard care, and provide feedback to patients; however, there are no studies in African Americans with poorly controlled T2DM that examine the use of technology-based feedback to tailor or augment diabetes education and skills training. This study provides a unique opportunity to address this gap in the literature. We describe an ongoing 4-year randomized clinical trial, which will test the efficacy of a technology-intensified diabetes education and skills training (TIDES) intervention in African Americans with poorly controlled T2DM. Two hundred male and female AfricanAmerican participants, 21 years of age or older and with a glycosylated hemoglobin A1c level ? 8%, will be randomized into one of two groups for 12 weeks of telephone interventions: (1) TIDES intervention group or (2) a usual-care group. Participants will be followed for 12 months to ascertain the effect of the interventions on glycemic control. Our primary hypothesis is that, among African Americans with poorly controlled T2DM, patients randomized to the TIDES intervention will have significantly greater reduction in glycosylated hemoglobin A1c at 12 months of follow-up compared to the usual-care group. Results from this study will add to the current literature examining how best to deliver diabetes education and skills training and provide important insight into effective strategies to improve metabolic control and hence reduce diabetes complications and mortality rates in African Americans with poorly controlled T2DM. This study was registered with the National Institutes of Health Clinical Trials Registry on 13 March 2014 (ClinicalTrials.gov identifier# NCT02088658).-Technology-Intensified Diabetes Education Study (TIDES) in African Americans with type 2 diabetes: study protocol for a randomized controlled trial.",0
"Noninvasive direct vessel wall (plaque) imaging may provide a good opportunity to study unique aspects of atherosclerotic lesions in different populations. The article published by Esposito et al. provides new insights into our understanding of diabetic atherosclerotic vascular disease by using direct plaque imaging techniques. The findings from this article call for attention to more in vivo imaging to understand the nature of high-risk atherosclerosis, especially in prospective studies in diabetic patients.-MRI plaque imaging and its role in population-based studies.",0
"To compare tracing and contact rates using alternative incentives in a computer-assisted telephone interview (CATI) survey among postpartum women. In a randomized trial of 1,061 postpartum women 18-49 years of age selected from four Iowa counties, we compared the effects of: (1) unconditional $5 telephone card incentive enclosed with the introductory letter followed by $25 incentive conditional upon successful telephone tracing, contact, and completion of CATI (Group 1, n = 530) vs. (2) $30 incentive conditional upon subject completion of CATI (Group 2, n = 531). Overall telephone tracing and contact rates achieved were 67.8% and 66.6%, respectively. Tracing (70.2 vs. 65.4%, P = .09) and contact (68.5 vs. 64.8%, P = .26) rates were consistently higher among subjects assigned the combination of a conditional and an unconditional incentive. The combined incentive type had a greater impact on telephone tracing success rates for subjects on whom we could not initially locate an active telephone number (16.7 vs. 7.3%, P = .07) when compared to subjects for whom we found an active telephone number at the time of mailing the introductory letter (78.9 vs. 75.9%, P = .30). Combining conditional and unconditional recruitment incentives can facilitate telephone tracing efforts in surveys conducted among recently postpartum women.-Combining conditional and unconditional recruitment incentives could facilitate telephone tracing in surveys of postpartum women.",0
"Stepped-wedge cluster randomised trials (SW-CRT) are increasingly being used in health policy and services research, but unless they are conducted and reported to the highest methodological standards, they are unlikely to be useful to decision-makers. Sample size calculations for these designs require allowance for clustering, time effects and repeated measures. We carried out a methodological review of SW-CRTs up to October 2014. We assessed adherence to reporting each of the 9 sample size calculation items recommended in the 2012 extension of the CONSORT statement to cluster trials. We identified 32 completed trials and 28 independent protocols published between 1987 and 2014. Of these, 45 (75%) reported a sample size calculation, with a median of 5.0 (IQR 2.5-6.0) of the 9 CONSORT items reported. Of those that reported a sample size calculation, the majority, 33 (73%), allowed for clustering, but just 15 (33%) allowed for time effects. There was a small increase in the proportions reporting a sample size calculation (from 64% before to 84% after publication of the CONSORT extension, p=0.07). The type of design (cohort or cross-sectional) was not reported clearly in the majority of studies, but cohort designs seemed to be most prevalent. Sample size calculations in cohort designs were particularly poor with only 3 out of 24 (13%) of these studies allowing for repeated measures. The quality of reporting of sample size items in stepped-wedge trials is suboptimal. There is an urgent need for dissemination of the appropriate guidelines for reporting and methodological development to match the proliferation of the use of this design in practice. Time effects and repeated measures should be considered in all SW-CRT power calculations, and there should be clarity in reporting trials as cohort or cross-sectional designs.-Systematic review finds major deficiencies in sample size methodology and reporting for stepped-wedge cluster randomised trials.",3
Observation plans in longitudinal studies with time-varying treatments.,0
"We describe different forms of clustering that may occur in individually randomized trials, where the observed outcomes for different individuals cannot be regarded as independent. We propose random effects models to allow for such clustering, across a range of contexts and trial designs, and investigate their effect on estimation and interpretation of the treatment effect. We apply our proposed models to two individually randomized trials with potential for clustering, a trial of teleconsultation in hospital referral (the main outcome being offer of a further hospital appointment) and a trial of exercise therapy delivered by physiotherapists for low back pain (the outcome being a back pain score). Extensions to the methods include the possibility of explaining heterogeneity between clusters using cluster level characteristics and the potential dilution of cluster effects due to noncompliance. In the teleconsultation trial, the odds ratio was significant (1.52, 95% CI 1.27 to 1.82) when clustering was ignored, but smaller and nonsignificant (1.36, 95% CI 0.85 to 2.13) when clustering by hospital consultant was taken into account. The 95% range of estimated treatment effects across consultants was from 0.21 to 8.76. This variability was only partially explained by the specialty of the consultant. In the back pain trial, although there was an overall benefit of exercise (change of - 0.51 points on the back pain score) and little evidence of clustering, the estimated treatment effects for different physiotherapists ranged from -1.26 to +0.26 points. Clustering is an important issue in many individually randomized trials. Ignoring it can lead to underestimates of the uncertainty and too extreme P-values. Even when there is little apparent heterogeneity across clusters, it can still have a large impact on the estimation and interpretation of the treatment effect.-The use of random effects models to allow for clustering in individually randomized trials.",2
"Researchers commonly collect repeated measures on individuals nested within groups such as students within schools, patients within treatment groups, or siblings within families. Often, it is most appropriate to conceptualize such groups as dynamic entities, potentially undergoing stochastic structural and/or functional changes over time. For instance, as a student progresses through school, more senior students matriculate while more junior students enroll, administrators and teachers may turn over, and curricular changes may be introduced. What it means to be a student within that school may thus differ from 1 year to the next. This article demonstrates how to use multilevel linear models to recover time-varying group effects when analyzing repeated measures data on individuals nested within groups that evolve over time. Two examples are provided. The 1st example examines school effects on the science achievement trajectories of students, allowing for changes in school effects over time. The 2nd example concerns dynamic family effects on individual trajectories of externalizing behavior and depression.-Analyzing repeated measures data on individuals nested within groups: accounting for dynamic group effects",2
"Randomized trials evaluating new cancer screening technologies may underestimate the efficacy of screening to reduce cancer mortality if study participants are noncompliant. Participants may fail to comply with the screening itself or fail to obtain appropriate diagnostic follow-up and treatment. Noncompliance with screening has drawn wide attention, but little attention has been paid to noncompliance with diagnostic follow-up and treatment. To examine the importance of noncompliance with screening, follow-up, and treatment in cancer screening trials. The unique problems associated with noncompliance in screening trials are described and provide an example illustrating the potential impact of noncompliance in a screening trial. I discuss issues that arise with measurement of follow-up and therapeutic noncompliance, and the benefit of collecting information on health system and participant characteristics associated with noncompliance. The estimate of the efficacy of a screening program on cancer mortality can be adjusted for screening, follow-up, and treatment noncompliance. Noncompliance needs to be measured in a rigorous, systematic manner across all arms of the trial. Information on health system and participant characteristics associated with compliance may also be incorporated into statistical models to estimate screening effects with full compliance, plan interventions to increase compliance, and extrapolate results of screening trials from one population to another. Measuring compliance with follow-up and treatment can be difficult when these occur outside the trial, and when there is variation among providers in follow-up and treatment practices. Noncompliance may alter the estimate of a screening effect on cancer mortality in clinical trials. It is possible to adjust screening efficacy estimates for noncompliance using existing statistical techniques. It is important that data describing compliance with screening, follow-up, and treatment are collected as part of standard data collection in cancer screening trials.-Noncompliance in cancer screening trials.",0
"Multimorbidity affects four of ten US adults and eight of ten adults ages 65 years and older, and frequently includes both cardiometabolic conditions and behavioral health concerns. Hispanics/Latinos (hereafter, Latinos) and other ethnic minorities are more vulnerable to these conditions, and face structural, social, and cultural barriers to obtaining quality physical and behavioral healthcare. We report the protocol for a randomized controlled trial that will compare Mi Puente (My Bridge), a cost-efficient care transitions intervention conducted by a specially trained Behavioral Health Nurse and Volunteer Community Mentor team, to usual care or best-practice discharge approaches, in reducing hospital utilization and improving patient reported outcomes in Latino adults with multiple cardiometabolic conditions and behavioral health concerns. The study will examine the degree to which Mi Puente produces superior reductions in hospital utilization at 30 and 180 days (primary aim) and better patient-reported outcomes (quality of life/physical health; barriers to healthcare; engagement with outpatient care; patient activation; resources for chronic disease management), and will examine the cost effectiveness of the Mi Puente intervention relative to usual care. Participants are enrolled as inpatients at a South San Diego safety net hospital, using information from electronic medical records and in-person screenings. After providing written informed consent and completing self-report assessments, participants randomized to usual care receive best-practice discharge processes, which include educational materials, assistance with outpatient appointments, referrals to community-based providers, and other assistance (e.g., with billing, insurance) as required. Those randomized to Mi Puente receive usual-care materials and processes, along with inpatient visits and up to 4 weeks of follow-up phone calls from the intervention team to address their integrated physical-behavioral health needs and support the transition to outpatient care. The Mi Puente Behavioral Health Nurse and Volunteer Community Mentor team intervention is proposed as a cost-effective and culturally appropriate care transitions intervention for Latinos with multimorbidity and behavioral health concerns. If shown to be effective, close linkages with outpatient healthcare and community organizations will help maximize uptake, dissemination, and scaling of the Mi Puente intervention. ClinicalTrials.gov: NCT02723019. Registered on 30 March 2016.-My Bridge (Mi Puente), a care transitions intervention for Hispanics/Latinos with multimorbidity and behavioral health concerns: protocol for a randomized controlled trial.",0
"Advances in antiretroviral therapies have greatly improved the survival of people living with human immunodeficiency virus (HIV) infection (PLWH); yet, PLWH have a higher risk of cardiovascular disease than those without HIV. While numerous genetic loci have been linked to cardiometabolic risk in the general population, genetic predictors of the excessive risk in PLWH are largely unknown. We screened for common and HIV-specific genetic variants associated with variation in lipid levels in 6284 PLWH (3095 European Americans [EA] and 3189 African Americans [AA]), from the Centers for AIDS Research Network of Integrated Clinical Systems cohort. Genetic hits found exclusively in the PLWH cohort were tested for association with other traits. We then assessed the predictive value of a series of polygenic risk scores (PRS) recapitulating the genetic burden for lipid levels, type 2 diabetes (T2D), and myocardial infarction (MI) in EA and AA PLWH. We confirmed the impact of previously reported lipid-related susceptibility loci in PLWH. Furthermore, we identified PLWH-specific variants in genes involved in immune cell regulation and previously linked to HIV control, body composition, smoking, and alcohol consumption. Moreover, PLWH at the top of European-based PRS for T2D distribution demonstrated a &gt; 2-fold increased risk of T2D compared to the remaining 95% in EA PLWH but to a much lesser degree in AA. Importantly, while PRS for MI was not predictive of MI risk in AA PLWH, multiethnic PRS significantly improved risk stratification for T2D and MI. Our findings suggest that genetic loci involved in the regulation of the immune system and predisposition to risky behaviors contribute to dyslipidemia in the presence of HIV infection. Moreover, we demonstrate the utility of the European-based and multiethnic PRS for stratification of PLWH at a high risk of cardiometabolic diseases who may benefit from preventive therapies.-Genetic architecture of cardiometabolic risks in people living with HIV.",0
"Psychological Methods celebrated its 20-year anniversary recently, having published its first quarterly issue in March 1996. It seemed time to provide a brief overview of the history, the highlights over the years, and the current state of the journal, along with tips for submissions. The article is organized to discuss (a) the background and development of the journal; (b) the top articles, authors, and topics over the years; (c) an overview of the journal today; and (d) a summary of the features of successful articles that usually entail rigorous and novel methodology described in clear and understandable writing and that can be applied in meaningful and relevant areas of psychological research. (PsycINFO Database Record-The making of Psychological Methods.",0
"In this article, we propose a new generalization of the Weibull distribution, which incorporates the exponentiated Weibull distribution introduced by Mudholkar and Srivastava (IEEE Trans. Reliab. 1993; 42:299-302) as a special case. We refer to the new family of distributions as the beta-Weibull distribution. We investigate the potential usefulness of the beta-Weibull distribution for modeling censored survival data from biomedical studies. Several other generalizations of the standard two-parameter Weibull distribution are compared with regards to maximum likelihood inference of the cumulative incidence function, under the setting of competing risks. These Weibull-based parametric models are fit to a breast cancer data set from the National Surgical Adjuvant Breast and Bowel Project. In terms of statistical significance of the treatment effect and model adequacy, all generalized models lead to similar conclusions, suggesting that the beta-Weibull family is a reasonable candidate for modeling survival data.-A new generalization of Weibull distribution with application to a breast cancer data set.",0
"Adaptive designs allow planned modifications based on data accumulating within a study. The promise of greater flexibility and efficiency stimulates increasing interest in adaptive designs from clinical, academic, and regulatory parties. When adaptive designs are used properly, efficiencies can include a smaller sample size, a more efficient treatment development process, and an increased chance of correctly answering the clinical question of interest. However, improper adaptations can lead to biased studies. A broad definition of adaptive designs allows for countless variations, which creates confusion as to the statistical validity and practical feasibility of many designs. Determining properties of a particular adaptive design requires careful consideration of the scientific context and statistical assumptions. We first review several adaptive designs that garner the most current interest. We focus on the design principles and research issues that lead to particular designs being appealing or unappealing in particular applications. We separately discuss exploratory and confirmatory stage designs in order to account for the differences in regulatory concerns. We include adaptive seamless designs, which combine stages in a unified approach. We also highlight a number of applied areas, such as comparative effectiveness research, that would benefit from the use of adaptive designs. Finally, we describe a number of current barriers and provide initial suggestions for overcoming them in order to promote wider use of appropriate adaptive designs. Given the breadth of the coverage all mathematical and most implementation details are omitted for the sake of brevity. However, the interested reader will find that we provide current references to focused reviews and original theoretical sources which lead to details of the current state of the art in theory and practice.-Adaptive trial designs: a review of barriers and opportunities.",0
"We propose a method to estimate the regression coefficients in a competing risks model where the cause-specific hazard for the cause of interest is related to covariates through a proportional hazards relationship and when cause of failure is missing for some individuals. We use multiple imputation procedures to impute missing cause of failure, where the probability that a missing cause is the cause of interest may depend on auxiliary covariates, and combine the maximum partial likelihood estimators computed from several imputed data sets into an estimator that is consistent and asymptotically normal. A consistent estimator for the asymptotic variance is also derived. Simulation results suggest the relevance of the theory in finite samples. Results are also illustrated with data from a breast cancer study.-Multiple imputation methods for estimating regression coefficients in the competing risks model with missing cause of failure.",0
"Sexual concerns are distressing for breast cancer survivors and interfere with their intimate relationships. This study evaluates the efficacy of a four-session couple-based intervention delivered via telephone, called Intimacy Enhancement (IE). The IE intervention is grounded in social cognitive theory and integrates evidence-based techniques from cognitive behavioral couple therapy and sex therapy to address survivors' sexual concerns and enhance their and their partners' sexual, relationship, and psychological outcomes. This trial is designed to evaluate the efficacy of the IE intervention in improving survivors' sexual function, the primary study outcome. Secondary outcomes include survivors' sexual distress, partners' sexual function, and survivors' and partners' relationship intimacy and quality as well as psychological distress (depressive symptoms and anxiety symptoms). Additional aims are to examine whether treatment effects on patient sexual function are mediated by sexual communication and self-efficacy for coping with sexual concerns and to explore whether survivor age and race/ethnicity moderate intervention effects on survivors' sexual function. Eligible adult female breast cancer survivors reporting sexual concerns and their intimate partners are recruited from two academic sites in the USA and are randomized to either the IE intervention or to a control condition of equal length offering education and support around breast cancer-related health topics (Living Healthy Together). The target sample size is 120 couples. Self-report outcome measures are administered to participants in both conditions at baseline (T1), post-treatment (T2), 3 months post-treatment (T3), and 6 months post-treatment (T4). Evidence-based interventions are needed to address sexual concerns for breast cancer survivors and to enhance their and their intimate partners' sexual, relationship, and psychological well-being. This randomized controlled trial will allow us to examine the efficacy of a novel couple-based intervention delivered via telephone for breast cancer survivors experiencing sexual concerns and their intimate partners, in comparison with an attention control. Findings of this study could influence clinical care for women with breast cancer and inform theory guiding cancer-related sexual rehabilitation. ClinicalTrials.gov, NCT03930797. Registered on 24 April 2019.-Evaluating a couple-based intervention addressing sexual concerns for breast cancer survivors: study protocol for a randomized controlled trial.",0
"Despite the fact that numerous major public health problems have plagued American Indian communities for generations, American Indian participation in health research traditionally has been sporadic in many parts of the United States. In 2002, the University of Oklahoma Health Sciences Center (Oklahoma City, Oklahoma) and 5 Oklahoma American Indian research review boards (Oklahoma City Area Indian Health Service, Absentee Shawnee Tribe, Cherokee Nation, Chickasaw Nation, and Choctaw Nation) agreed to participate collectively in a national research trial, the Treatment Options for Type 2 Diabetes in Adolescence and Youth (TODAY) Study. During that process, numerous lessons were learned and processes developed that strengthened the partnerships and facilitated the research. Formal Memoranda of Agreement addressed issues related to community collaboration, venue, tribal authority, preferential hiring of American Indians, and indemnification. The agreements aided in uniting sovereign nations, the Indian Health Service, academics, and public health officials to conduct responsible and ethical research. For more than 10 years, this unique partnership has functioned effectively in recruiting and retaining American Indian participants, respecting cultural differences, and maintaining tribal autonomy through prereview of all study publications and local institutional review board review of all processes. The lessons learned may be of value to investigators conducting future research with American Indian communities.-Partnering in research: a national research trial exemplifying effective collaboration with American Indian Nations and the Indian Health Service.",0
"New health status survey instruments are often described by their psychometric (measurement) properties, such as Validity, Reliability, Effect Size, and Responsiveness. For cluster-randomized trials, another important statistic is the Intraclass Correlation (ICC) for the instrument within clusters. Studies using better instruments can be performed with smaller sample sizes, but better instruments may be more expensive in terms of dollars, opportunity cost, or poorer data quality due to the response burden of longer instruments. We defined the psychometric statistics in terms of a mathematical model, and examined the power of a two-sample test as a function of the test-retest Reliability, Effect Size, Responsiveness, and Intraclass Correlation of the instrument. We examined the ""cost-effectiveness"" of using a one-item versus a five-item measure of mental health status. Under the standard model for measurement error, the psychometric statistics are all functions of the same error term. They are also functions of the setting in which they were estimated. In randomized trials, power is a function of Reliability and sample size, and a less reliable instrument can achieve the desired power if N is increased. In cluster-randomized trials, adequate power may be obtained by increasing the number of clusters per treatment group (and often the number of persons per cluster), as well as by choosing a more reliable instrument. The one-item measure of mental health status may be more cost-effective than the five-item measure in some situations. If the goal is to diagnose or refer individual patients, an instrument with high Validity and Reliability is needed. In settings where the sample sizes are large or can be increased easily, any valid instrument may be cost-effective. It is likely that many published values of psychometric statistics are accurate only in settings similar to that in which they were estimated.-Reliability, effect size, and responsiveness of health status measures in the design of randomized and cluster-randomized trials.",1
"A crucial step in designing a new study is to estimate the required sample size. For a design involving cluster sampling, the appropriate sample size depends on the so-called design effect, which is a function of the average cluster size and the intracluster correlation coefficient (ICC). It is well-known that under the framework of hierarchical and generalized linear models, a reduction in residual error may be achieved by including risk factors as covariates. In this paper we show that the covariate design, indicating whether the covariates are measured at the cluster level or at the within-cluster subject level affects the estimation of the ICC, and hence the design effect. Therefore, the distinction between these two types of covariates should be made at the design stage. In this paper we use the nested-bootstrap method to assess the accuracy of the estimated ICC for continuous and binary response variables under different covariate structures. The codes of two SAS macros are made available by the authors for interested readers to facilitate the construction of confidence intervals for the ICC. Moreover, using Monte Carlo simulations we evaluate the relative efficiency of the estimators and evaluate the accuracy of the coverage probabilities of a 95% confidence interval on the population ICC. The methodology is illustrated using a published data set of blood pressure measurements taken on family members.-Covariate-adjusted confidence interval for the intraclass correlation coefficient.",1
"Back-calculation models, developed to reconstruct the past trend of human immunodeficiency virus (HIV) and to project future acquired immunodeficiency syndrome incidence (AIDS), are usually and unrealistically based on the assumption that the observed AIDS counts are independently distributed according to a Poisson process. In contrast, we argue that a multinomial framework is more suitable to this situation, leading to a natural covariance structure. The ill-conditioned nature of the problem is solved by modelling the HIV parameters according to a cubic spline function to reduce the dimensionality of the parameter space and obtain smoother parameter estimates. We applied a regression spline technique which yields to a computationally stable basis incorporating the incubation period in the new design matrix. We directly incorporate the reporting delay distribution in the AIDS incidence data, leading to a more complex formulation of the variance and covariance model that is adapted to the iteratively reweighted least square (IRLS) algorithm. In this case we obtain more accurate estimates of the standard error of the HIV incidence, especially in the most recent time. Our model, which uses a cubic spline reparameterization based on a multinomial probability distribution, is applied to the AIDS epidemic data in Italy.-Multinomial analysis of smoothed HIV back-calculation models incorporating uncertainty in the AIDS incidence.",0
The review process fails to require appropriate statistical analysis of a group-randomized trial.,1
"Inconsistent results in recent HIV prevention trials of pre-exposure prophylactic interventions may be due to heterogeneity in risk among study participants. Intervention effectiveness is most commonly estimated with the Cox model, which compares event times between populations. When heterogeneity is present, this population-level measure underestimates intervention effectiveness for individuals who are at risk. We propose a likelihood-based Bayesian hierarchical model that estimates the individual-level effectiveness of candidate interventions by accounting for heterogeneity in risk with a compound Poisson-distributed frailty term. This model reflects the mechanisms of HIV risk and allows that some participants are not exposed to HIV and, therefore, have no risk of seroconversion during the study. We assess model performance via simulation and apply the model to data from an HIV prevention trial. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Estimating effectiveness in HIV prevention trials with a Bayesian hierarchical compound Poisson frailty model.",0
"Recent studies in children have demonstrated that frequent occurrence of parasomnias is related to increased sleep disruption, mental disorders, physical harm, sleep disordered breathing, and parental duress. Although there have been several cross-sectional and clinical studies of parasomnias in children, there have been no large, population-based studies using full polysomnography to examine the association between parasomnias and sleep disordered breathing. The Tucson Children's Assessment of Sleep Apnea study is a community-based cohort study designed to investigate the prevalence and correlates of objectively measured sleep disordered breathing (SDB) in pre-adolescent children six to 11 years of age. This paper characterizes the relationships between parasomnias and SDB with its associated symptoms in these children. Parents completed questionnaires pertaining to their child's sleep habits. Children had various physiological measurements completed and then were connected to the Compumedics PS-2 sleep recording system for full, unattended polysomnography in the home. A total of 480 unattended home polysomnograms were completed on a sample that was 50% female, 42.3% Hispanic, and 52.9% between the ages of six and eight years. Children with a Respiratory Disturbance Index of one or greater were more likely to have sleep walking (7.0% versus 2.5%, p &lt; 0.02), sleep talking (18.3% versus 9.0%, p &lt; 0.006), and enuresis (11.3% versus 6.3%, p &lt; 0.08) than children with an Respiratory Disturbance Index of less than one. A higher prevalence of other sleep disturbances as well as learning problems was observed in children with parasomnia. Those with parasomnias associated with arousal were observed to have increased number of stage shifts. Small alterations in sleep architecture were found in those with enuresis. In this population-based cohort study, pre-adolescent school-aged children with SDB experienced more parasomnias than those without SDB. Parasomnias were associated with a higher prevalence of other sleep disturbances and learning problems. Clinical evaluation of children with parasomnias should include consideration of SDB.-Parasomnias and sleep disordered breathing in Caucasian and Hispanic children - the Tucson children's assessment of sleep apnea study.",0
"Tests for regression coefficients such as global, local, and partial F-tests are common in applied research. In the framework of multiple imputation, there are several papers addressing tests for regression coefficients. However, for simultaneous hypothesis testing, the existing methods are computationally intensive because they involve calculation with vectors and (inversion of) matrices. In this paper, we propose a simple method based on the scalar entity, coefficient of determination, to perform (global, local, and partial) F-tests with multiply imputed data. The proposed method is evaluated using simulated data and applied to suicide prevention data.-Partial F-tests with multiply imputed data in the linear regression framework via coefficient of determination.",0
"This study proposes a time-varying effect model for examining group differences in trajectories of zero-inflated count outcomes. The motivating example demonstrates that this zero-inflated Poisson model allows investigators to study group differences in different aspects of substance use (e.g., the probability of abstinence and the quantity of alcohol use) simultaneously. The simulation study shows that the accuracy of estimation of trajectory functions improves as the sample size increases; the accuracy under equal group sizes is only higher when the sample size is small (100). In terms of the performance of the hypothesis testing, the type I error rates are close to their corresponding significance levels under all settings. Furthermore, the power increases as the alternative hypothesis deviates more from the null hypothesis, and the rate of this increasing trend is higher when the sample size is larger. Moreover, the hypothesis test for the group difference in the zero component tends to be less powerful than the test for the group difference in the Poisson component. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-A time-varying effect model for examining group differences in trajectories of zero-inflated count outcomes with applications in substance abuse research.",0
"We explore several approaches for imputing partially observed covariates when the outcome of interest is a censored event time and when there is an underlying subset of the population that will never experience the event of interest. We call these subjects 'cured', and we consider the case where the data are modeled using a Cox proportional hazards (CPH) mixture cure model. We study covariate imputation approaches using fully conditional specification. We derive the exact conditional distribution and suggest a sampling scheme for imputing partially observed covariates in the CPH cure model setting. We also propose several approximations to the exact distribution that are simpler and more convenient to use for imputation. A simulation study demonstrates that the proposed imputation approaches outperform existing imputation approaches for survival data without a cure fraction in terms of bias in estimating CPH cure model parameters. We apply our multiple imputation techniques to a study of patients with head and neck cancer. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Multiple imputation of missing covariates for the Cox proportional hazards cure model.",0
"In randomized trials, pair-matching is an intuitive design strategy to protect study validity and to potentially increase study power. In a common design, candidate units are identified, and their baseline characteristics used to create the best n/2 matched pairs. Within the resulting pairs, the intervention is randomized, and the outcomes measured at the end of follow-up. We consider this design to be adaptive, because the construction of the matched pairs depends on the baseline covariates of all candidate units. As a consequence, the observed data cannot be considered as n/2 independent, identically distributed pairs of units, as common practice assumes. Instead, the observed data consist of n dependent units. This paper explores the consequences of adaptive pair-matching in randomized trials for estimation of the average treatment effect, conditional the baseline covariates of the n study units. By avoiding estimation of the covariate distribution, estimators of this conditional effect will often be more precise than estimators of the marginal effect. We contrast the unadjusted estimator with targeted minimum loss based estimation and show substantial efficiency gains from matching and further gains with adjustment. This work is motivated by the Sustainable East Africa Research in Community Health study, an ongoing community randomized trial to evaluate the impact of immediate and streamlined antiretroviral therapy on HIV incidence in rural East Africa.-Adaptive pair-matching in randomized trials with unbiased and efficient effect estimation.",1
"Cluster randomized trials have become the design of choice for evaluating the effect of selected interventions on well-known health indicators such as neonatal mortality rate, episiotomy rate, and postpartum hemorrhage rate in a community setting. Determining the sample size of a cluster randomized trial requires a reliable estimate of cluster size and the intracluster correlation (ICC), because sample size can be substantially impacted by these parameters. During the design phase of a trial, the investigators may have estimates of the valid range of the health indicator which is the primary outcome variable. Furthermore, investigators often have an estimate of the average cluster size or range of cluster sizes that exist among the proposed samples they are planning to include in the trial. We present in this article a simulation technique to estimate the ICC value and its distribution for known binary outcome variables and a varying number of clusters and cluster sizes. We applied this technique to estimate ICC values and confidence intervals for a multi-country trial assessing the effect of neonatal resuscitation to decrease seven-day neonatal mortality, where communities within a country were clusters. This simulation technique can be used to estimate the possible ranges of the ICC values and to help to design an appropriately powered trial.-A simulation based technique to estimate intracluster correlation for a binary variable.",1
"This paper studies model-based and design-based approaches for the analysis of data arising from a stepped wedge randomized design. Specifically, for different scenarios we compare robustness, efficiency, Type I error rate under the null hypothesis, and power under the alternative hypothesis for the leading analytical options including generalized estimating equations (GEE) and linear mixed model (LMM) based approaches. We find that GEE models with exchangeable correlation structures are more efficient than GEE models with independent correlation structures under all scenarios considered. The model-based GEE Type I error rate can be inflated when applied with a small number of clusters, but this problem can be solved using a design-based approach. As expected, correct model specification is more important for LMM (compared to GEE) since the model is assumed correct when standard errors are calculated. However, in contrast to the model-based results, the design-based Type I error rates for LMM models under scenarios with a random treatment effect show type I error inflation even though the fitted models perfectly match the corresponding data generating scenarios. Therefore, greater robustness can be realized by combining GEE and permutation testing strategies.-A simulation study of statistical approaches to data analysis in the stepped wedge design",3
"The recent biostatistical literature contains a number of methods for handling the bias caused by 'informative censoring', which refers to drop-out from a longitudinal study after a number of visits scheduled at predetermined intervals. The same or related methods can be extended to situations where the missing pattern is intermittent. The pattern of missingness is often assumed to be related to the outcome through random effects which represent unmeasured individual characteristics such as health awareness. To date there is only limited experience with applying the methods for informative censoring in practice, mostly because of complicated modelling and difficult computations. In this paper, we propose an estimation method based on grouping the data. The proposed estimator is asymptotically unbiased in various situations under informative missingness. Several existing methods are reviewed and compared in simulation studies. We apply the methods to data from the Wisconsin Diabetes Registry Project, a longitudinal study tracking glycaemic control and acute and chronic complications from the diagnosis of type I diabetes.-Bias adjustment in analysing longitudinal data with informative missingness.",0
"A fully Bayesian approach to causal mediation analysis for group-randomized designs is presented. A unique contribution of this article is the combination of Bayesian inferential methods with G-computation to address the problem of heterogeneous treatment or mediator effects. A detailed simulation study shows that this approach has excellent frequentist properties, particularly in the case of small sample sizes with accurate informative priors. The simulation study also demonstrates that the proposed approach can take into account heterogeneous treatment or mediator effects without bias. A case study using data from a school-based randomized intervention designed to increase parent social capital leading to improved behavioral and academic outcomes in children is offered to illustrate the Bayesian approach to causal mediation in group-randomized designs.-Bayesian Causal Mediation Analysis for Group Randomized Designs with Homogeneous and Heterogeneous Effects: Simulation and Case Study.",1
"It is common to encounter two-dimensional dose finding in phase I trials, for example, in trials combining multiple drugs, or in single-agent trials that simultaneously search for the maximum tolerated dose (MTD) and the optimal treatment schedule. In these cases, the traditional single-agent dose-finding methods are not directly applicable. We propose a simple and adaptive two-dimensional dose-finding design that can accommodate any type of single-agent dose-finding method. In particular, we convert the two-dimensional dose-finding trial to a series of one-dimensional dose-finding subtrials along shortened line search segments by fixing the dose level of one drug. We then conduct the subtrials sequentially. Based on the MTD obtained from the completed one-dimensional trial, we eliminate the doses that lie outside of the search range based on the partial order, and thereby efficiently shrink the two-dimensional dose-finding space. The proposed design dramatically reduces the sample size and still maintains good performance. We illustrate the design through extensive simulation studies motivated by clinical trials evaluating multiple drugs or dose and schedule combinations.-Sequential continual reassessment method for two-dimensional dose finding.",0
"Comparing findings from separate trials is necessary to choose among treatment options, however differences among study cohorts may impede these comparisons. As a case study, to examine the overlap of study cohorts in two large randomized controlled clinical trials that assess interventions to reduce risk of major cardiovascular disease events in adults with type 2 diabetes in order to explore the feasibility of cross-trial comparisons The Action for Health in Diabetes (Look AHEAD) and The Action to Control Cardiovascular Risk in Diabetes (ACCORD) trials enrolled 5145 and 10,251 adults with type 2 diabetes, respectively. Look AHEAD assesses the efficacy of an intensive lifestyle intervention designed to produce weight loss; ACCORD tests pharmacological therapies for control of glycemia, hyperlipidemia, and hypertension. Incidence of major cardiovascular disease events is the primary outcome for both trials. A sample was constructed to include participants from each trial who appeared to meet eligibility criteria and be appropriate candidates for the other trial's interventions. Demographic characteristics, health status, and outcomes of members and nonmembers of this constructed sample were compared. Nearly 80% of Look AHEAD participants were projected to be ineligible for ACCORD; ineligibility was primarily due to better glycemic control or no early history of cardiovascular disease. Approximately 30% of ACCORD participants were projected to be ineligible for Look AHEAD, often for reasons linked to poorer health. The characteristics of participants projected to be jointly eligible for both trials continued to reflect differences between trials according to factors likely linked to retention, adherence, and study outcomes. Accurate ascertainment of cross-trial eligibility was hampered by differences between protocols. Despite several similarities, the Look AHEAD and ACCORD cohorts represent distinct populations. Even within the subsets of participants who appear to be eligible and appropriate candidates for trials of both modes of intervention, differences remained. Direct comparisons of results from separate trials of lifestyle and pharmacologic interventions are compromised by marked differences in enrolled cohorts.-Constructing common cohorts from trials with overlapping eligibility criteria: implications for comparing effect sizes between trials.",0
"A World Health Organization expert meeting on Ebola vaccines proposed urgent safety and efficacy studies in response to the outbreak in West Africa. One approach to communicable disease control is ring vaccination of individuals at high risk of infection due to their social or geographical connection to a known case. This paper describes the protocol for a novel cluster randomised controlled trial design which uses ring vaccination.In the Ebola ?a suffit ring vaccination trial, rings are randomised 1:1 to (a) immediate vaccination of eligible adults with single dose vaccination or (b) vaccination delayed by 21 days. Vaccine efficacy against disease is assessed in participants over equivalent periods from the day of randomisation. Secondary objectives include vaccine effectiveness at the level of the ring, and incidence of serious adverse events. Ring vaccination trials are adaptive, can be run until disease elimination, allow interim analysis, and can go dormant during inter-epidemic periods.-The ring vaccination trial: a novel cluster randomised controlled trial design to evaluate vaccine efficacy and effectiveness during outbreaks, with special reference to Ebola.",1
"This paper discusses design considerations and the role of randomization-based inference in randomized community intervention trials. We stress that longitudinal follow-up of cohorts within communities often yields useful information on the effects of intervention on individuals, whereas cross-sectional surveys can usefully assess the impact of intervention on group indices of health. We also discuss briefly special design considerations, such as sampling cohorts from targeted subpopulations (for example, heavy smokers), matching the communities, calculating sample size, and other practical issues. We present randomization tests for matched and unmatched cohort designs. As is well known, these tests necessarily have proper size under the strong null hypothesis that treatment has no effect on any community response. It is less well known, however, that the size of randomization tests can exceed nominal levels under the 'weak' null hypothesis that intervention does not affect the average community response. Because this weak null hypothesis is of interest in community intervention trials, we study the size of randomization tests by simulation under conditions in which the weak null hypothesis holds but the strong null hypothesis does not. In unmatched studies, size may exceed nominal levels under the weak null hypothesis if there are more intervention than control communities and if the variance among community responses is larger among control communities than among intervention communities; size may also exceed nominal levels if there are more control than intervention communities and if the variance among community responses is larger among intervention communities. Otherwise, size is likely near nominal levels. To avoid such problems, we recommend use of the same numbers of control and intervention communities in unmatched designs. Pair-matched designs usually have size near nominal levels, even under the weak null hypothesis. We have identified some extreme cases, unlikely to arise in practice, in which even the size of pair-matched studies can exceed nominal levels. These simulations, however, tend to confirm the robustness of randomization tests for matched and unmatched community intervention trials, particularly if the latter designs have equal numbers of intervention and control communities. We also describe adaptations of randomization tests to allow for covariate adjustment, missing data, and application to cross-sectional surveys. We show that covariate adjustment can increase power, but such power gains diminish as the random component of variation among communities increases, which corresponds to increasing intraclass correlation of responses within communities. We briefly relate our results to model-based methods of inference for community intervention trials that include hierarchical models such as an analysis of variance model with random community effects and fixed intervention effects. Although we have tailored this paper to the design of community intervention trials, many of the ideas apply to other experiments in which one allocates groups or clusters of subjects at random to intervention or control treatments.-On design considerations and randomization-based inference for community intervention trials.",1
"This paper extends the survival-adjusted Cochran-Armitage test in order to achieve improved robustness to a variety of tumour onset distributions. The Cochran-Armitage test is routinely applied for detecting a linear trend in the incidence of a tumour of interest across dose groups. To improve the robustness to the effects of differential mortality across groups, Bailer and Portier introduced the poly-3 test by a survival adjustment using a fractional weighting scheme for subjects not at full risk of tumour development. The performance of the poly-3 test depends on how closely it represents the correct specification of the time-at-risk weight in the data. Bailer and Portier further suggested that this test can be improved by using a general k reflecting the shape of the tumour onset distribution. In this paper, we propose a method to estimate k by equating the empirical lifetime tumour incidence rate obtained from the data based on the fractional weighting scheme to a separately estimated cumulative lifetime tumour incidence rate. This poly-k test with the statistically estimated k appears to perform better than the poly-3 test which is conducted without prior knowledge of the tumour onset distribution. Our simulation shows that the proposed method improves the robustness to various tumour onset distributions in addition to the robustness to the effects of mortality achieved by the poly-3 test. Large sample properties are shown via simulations to illustrate the consistency of the proposed method. The proposed methods are applied to analyse two real data sets. One is to find a dose-related linear trend on animal carcinogenicity, and the other is to test an effect of calorie restriction on experimental animals.-Estimation of k for the poly-k test with application to animal carcinogenicity studies.",0
"Cluster randomized controlled trials (RCTs) are increasingly used in economic evaluations of social, educational and health care interventions. Methodological research has, therefore, been spread across several disciplines, with the result that it has taken many years for guidelines on good statistical practice in the design and analysis of such trials to become easily accessible to health service researchers. These guidelines remain incomplete, however, because they do not take account of issues specific to the analysis of cost data. In particular, they fail to recognize that the calculation of confidence intervals around costs needed to inform health care priority setting raises unique methodological issues. If poorly designed trials are to be avoided in future (including those by the authors), then collaboration between triallists and health economists is required. This paper sets out a framework that should facilitate such collaboration and draws attention to problems that must be addressed quickly in the design of cluster-based economic evaluations.-Conceptual issues in the analysis of cost data within cluster randomized trials.",1
"A random-effects regression model is proposed for analysis of clustered data. Unlike ordinary regression analysis of clustered data, random-effects regression models do not assume that each observation is independent but do assume that data within clusters are dependent to some degree. The degree of this dependency is estimated along with estimates of the usual model parameters, thus adjusting these effects for the dependency resulting from the clustering of the data. A maximum marginal likelihood solution is described, and available statistical software for the model is discussed. An analysis of a dataset in which students are clustered within classrooms and schools is used to illustrate features of random-effects regression analysis, relative to both individual-level analysis that ignores the clustering of the data, and classroom-level analysis that aggregates the individual data.-Random-effects regression models for clustered data with an example from smoking prevention research.",1
"Estimation of regression parameters in linear survival models is considered in the clustered data setting. One step updates from an initial consistent estimator are proposed. The updates are based on scores that are functions of ranks of the residuals, and that incorporate weight matrices to improve efficiency. Optimal weights are approximated as the solution to a quadratic programming problem, and asymptotic relative efficiencies to various other weights computed. Except under strong dependence, simpler methods are found to be nearly as efficient as the optimal weights. The performance of several practical estimators based on exchangeable and independence working models is explored in simulations.-Weighted estimating equations for linear regression analysis of clustered failure time data.",1
"Medication errors are an important source of potentially preventable morbidity and mortality. The PINCER study, a cluster randomised controlled trial, is one of the world's first experimental studies aiming to reduce the risk of such medication related potential for harm in general practice. Bayesian analyses can improve the clinical interpretability of trial findings. Experts were asked to complete a questionnaire to elicit opinions of the likely effectiveness of the intervention for the key outcomes of interest--three important primary care medication errors. These were averaged to generate collective prior distributions, which were then combined with trial data to generate bayesian posterior distributions. The trial data were analysed in two ways: firstly replicating the trial reported cohort analysis acknowledging pairing of observations, but excluding non-paired observations; and secondly as cross-sectional data, with no exclusions, but without acknowledgement of the pairing. Frequentist and bayesian analyses were compared. Bayesian evaluations suggest that the intervention is able to reduce the likelihood of one of the medication errors by about 50 (estimated to be between 20% and 70%). However, for the other two main outcomes considered, the evidence that the intervention is able to reduce the likelihood of prescription errors is less conclusive. Clinicians are interested in what trial results mean to them, as opposed to what trial results suggest for future experiments. This analysis suggests that the PINCER intervention is strongly effective in reducing the likelihood of one of the important errors; not necessarily effective in reducing the other errors. Depending on the clinical importance of the respective errors, careful consideration should be given before implementation, and refinement targeted at the other errors may be something to consider.-Bayesian cohort and cross-sectional analyses of the PINCER trial: a pharmacist-led intervention to reduce medication errors in primary care.",1
"To examine the relationship between guideline panel members' conflicts of interest and guideline recommendations on screening mammography in asymptomatic, average-risk women aged 40-49 years. We searched the National Guideline Clearinghouse and MEDLINE for relevant guidelines published between January 2005 and June 2011. We examined the disclosures and specialties of the lead and secondary authors of these guidelines, as well as the publications of the lead authors. Twelve guidelines were identified with a total of 178 physician authors from a broad range of specialties. Of the four guidelines not recommending routine screening, none had a radiologist member, whereas of the eight guidelines recommending routine screening, five had a radiologist member (comparison of the proportions, P=0.05). A guideline with radiologist authors was more likely to recommend routine screening (odds ratio=6.05, 95% confidence interval=0.57-?, P=0.14). The proportion of primary care physicians on guideline panels recommending routine vs. nonroutine screening was significantly different (38% vs. 90% of authors; P=0.01). The odds of a recommendation in favor of routine screening were related to the number of recent publications on breast disease diagnosis and treatment by the lead guideline author (P=0.02). Recommendations regarding mammography screening in this target population may reflect the specialty and intellectual interests of the guideline authors.-Author's specialty and conflicts of interest contribute to conflicting guidelines for screening mammography.",0
"Cluster randomized and multicentre trials evaluate the effect of a treatment on persons nested within clusters, for instance patients within clinics or pupils within schools. Although equal sample sizes per cluster are generally optimal for parameter estimation, they are rarely feasible. This paper addresses the relative efficiency (RE) of unequal versus equal cluster sizes for estimating variance components in cluster randomized trials and in multicentre trials with person randomization within centres, assuming a quantitative outcome. Starting from maximum likelihood estimation, the RE is investigated numerically for a range of cluster size distributions. An approximate formula is presented for computing the RE as a function of the mean and variance of cluster sizes and the intraclass correlation. The accuracy of this approximation is checked and found to be good. It is concluded that the loss of efficiency for variance component estimation due to variation of cluster sizes rarely exceeds 20% and can be compensated by sampling 25% more clusters.-Relative efficiency of unequal cluster sizes for variance component estimation in cluster randomized and multicentre trials.",1
"Siblings and parents of children with neurodevelopmental disorders are at risk of mental health problems and poorer family communication. Some group interventions for siblings exist, but few have clearly described parent components and none are considered evidence-based. We are conducting a randomized controlled trial comparing a five-session manual-based group intervention for siblings (aged 8 to 16 years) and parents of children with neurodevelopmental disorders to a 12-week waitlist, called SIBS-RCT. The intervention comprises three separate sibling and parent group sessions and two joint sessions in which each sibling talks to their parent alone. The intervention aims at improving parent-child communication and covers themes such as siblings' understanding of the neurodevelopmental disorder, siblings' emotions, and perceived family challenges. Participants are recruited through municipal and specialist health centers across Norway. The primary outcome is sibling mental health. Quality of life and family communication are secondary outcomes. Participants are block-randomized to the intervention or 12-week waitlist in groups of six. Measures are collected electronically at pre- and post-intervention/waitlist, as well as 3, 6, and 12?months post-intervention. The main effect to be examined is the difference between the intervention and waitlist at 12?weeks post. All outcomes will also be examined using growth curve analyses. We plan to include 288 siblings and their parents by the end of 2022. SIBS-RCT represents a major contribution to the research and practice field towards establishing an evidence-based intervention for siblings. In the event that intervention and waitlist are no different, the impact of SIBS-RCT is still substantial in that we will aim to identify participant subgroups that show positive response and effective components of the SIBS manual by examining group leader adherence as an outcome predictor. This will allow us to continue to re-engineer the SIBS manual iteratively to improve outcomes, and avoid the promotion of a less-than-optimal intervention. ClinicalTrials.gov NCT04056884 . Registered in August 2019.-Group intervention for siblings and parents of children with chronic disorders (SIBS-RCT): study protocol for a randomized controlled trial.",0
"Tumor shrinkage has been adopted as an end point for evaluating the effectiveness of new anticancer agents. The WHO (World Health Organization) criterion suggested measuring the tumor shrinkage by the change in the product of maximal diameter (MD) and the corresponding largest perpendicular diameter (LPD). The RECIST (Response Evaluation Criteria In Solid Tumor) guideline proposed using the change in MD only, based on the observation that this measure is more linearly related to tumor cell kill than the cross product (MD*LPD). Both criteria classify patients into four categories of response: complete response (CR: total disappearance), partial response (PR), stable disease (SD), and progressive disease (PD) but the criteria used in the definition of PD vary. It was anticipated that patients' actual response categorization would not be considerably affected by utilizing the RECIST criteria instead of WHO. Empirical evidence supporting this fact was provided by retrospective analysis of several large datasets. A statistical simulation is performed to generate tumor measurements and patient response data under meaningful probability distributions with parameters based on data from 130 patients on clinical trials at a cancer center. Concordance measures between the two response criteria (Kappa coefficient and percentage disagreement per response category) are assessed systematically over various combinations of the percentage of elliptical tumors at baseline and the percentage of tumors changing shape from baseline to follow-up. The overall percentage of disagreement between the two methods of response assessment is found to be in the range of 14-20%. The patients categorized by WHO in the PR, SD, and PD groups fall into a different category when assessed by RECIST between 8-16%, 3-12%, and 32-35% of the times, respectively. The kappa coefficient ranges between 0.68-0.77. The proportion of elliptical tumors at baseline does not greatly impact the concordance, but the magnitude of the change in the aspect ratio has a large impact. Response assessment as measured by RECIST, with both a change in the underlying metric and change in definition of progression, often results in different categorization of response compared to WHO. The difference in response categorization may be problematic when new experimental therapies are compared to conventional agents whose response rates have been established in historical trials. The apparent lower rate of disease progression with RECIST may mean that more patients remain on therapy. Higher percentages of patients with SD need to be interpreted cautiously by distinguishing those due to the change in the response criterion as opposed to those induced by drugs using pathways such as angiogenesis where disease stabilization is expected rather than shrinkage of tumor.-A statistical simulation study finds discordance between WHO criteria and RECIST guideline.",0
"Clinical trials in the context of comparative effectiveness research (CER) are often conducted to evaluate health outcomes under real-world conditions and standard health care settings. In such settings, three-level hierarchical study designs are increasingly common. For example, patients may be nested within treating physicians, who in turn are nested within an urgent care center or hospital. While many trials randomize the third-level units (e.g., centers) to intervention, in some cases randomization may occur at lower levels of the hierarchy, such as patients or physicians. In this article, we present and verify explicit closed-form sample size and power formulas for three-level designs assuming randomization is at the first or second level. The formulas are based on maximum likelihood estimates from mixed-effect linear models and verified by simulation studies. Results indicate that even with smaller sample sizes, theoretical power derived with known variances is nearly identical to empirically estimated power for the more realistic setting when variances are unknown. In addition, we show that randomization at the second or first level of the hierarchy provides an increasingly statistically efficient alternative to third-level randomization. Power to detect a treatment effect under second-level randomization approaches that of patient-level randomization when there are few patients within each randomized second-level cluster and, most importantly, when the correlation attributable to second-level variation is a small proportion of the overall correlation between patient outcomes.-Sample size determination for three-level randomized clinical trials with randomization at the first or second level.",1
Relative Risk Estimation in Cluster Randomized Trials: A Comparison of Generalized Estimating Equation Methods,1
"Many stepped wedge trials (SWTs) are analysed by using a mixed-effect model with a random intercept and fixed effects for the intervention and time periods (referred to here as the standard model). However, it is not known whether this model is robust to misspecification. We simulated SWTs with three groups of clusters and two time periods; one group received the intervention during the first period and two groups in the second period. We simulated period and intervention effects that were either common-to-all or varied-between clusters. Data were analysed with the standard model or with additional random effects for period effect or intervention effect. In a second simulation study, we explored the weight given to within-cluster comparisons by simulating a larger intervention effect in the group of the trial that experienced both the control and intervention conditions and applying the three analysis models described previously. Across 500 simulations, we computed bias and confidence interval coverage of the estimated intervention effect. We found up to 50% bias in intervention effect estimates when period or intervention effects varied between clusters and were treated as fixed effects in the analysis. All misspecified models showed undercoverage of 95% confidence intervals, particularly the standard model. A large weight was given to within-cluster comparisons in the standard model. In the SWTs simulated here, mixed-effect models were highly sensitive to departures from the model assumptions, which can be explained by the high dependence on within-cluster comparisons. Trialists should consider including a random effect for time period in their SWT analysis model. ? 2017 The Authors. Statistics in Medicine published by John Wiley &amp; Sons Ltd.-Bias and inference from misspecified mixed-effect models in stepped wedge trial analysis.",3
"HIV prevention programs are typically evaluated using behavioral outcomes. Mathematical models of HIV transmission can be used to translate these behavioral outcomes into estimates of the number of HIV infections averted. Usually, intervention effectiveness is evaluated over a brief assessment period and an infection is considered to be prevented if it does not occur during this period. This approach may overestimate intervention effectiveness if participants continue to engage in risk behaviors. Conversely, this strategy underestimates the true impact of interventions by assuming that behavioral changes persist only until the end of the intervention assessment period. In this article, the authors (a) suggest a simple framework for distinguishing between HIV infections that are truly prevented and those that are merely delayed, (b) illustrate how these outcomes can be estimated, (c) discuss strategies for extrapolating intervention effects beyond the assessment period, and (d) highlight the implications of these findings for HIV prevention decision making.-When is an HIV infection prevented and when is it merely delayed?",0
"To establish current practice of the management of learning and clustering effects, by treating center and surgeon, in the design and analysis of randomized surgical trials. The need for more surgical randomized trials is well recognized, and in recent years conduct has grown. Rigorous design, conduct, and analyses of such studies is important. Two methodological challenges are clustering effects, by center or surgeon, and surgical learning on trial outcomes. Sixteen leading journals were searched for randomized trials published within a two-year period. Data were extracted on considerations for learning and clustering effects. A total of 247 eligible studies were identified. Trials accounted for learning with 2% using an expertise-based design and 39% accounting for expertise by predefining surgeon credentials. One study analyzed learning. Clustering, by site and surgeon, was commonly managed by stratifying randomization, although one-third of center and 40% of surgeon stratified trials did not also adjust analysis. Considerations for surgical learning and clustering effects are often unclear. Methods are varied and demonstrate poor adherence to established reporting guidelines. It is recommended that researchers consider these issues on a trial-by-trial basis, and report methods or justify where not needed to inform interpretation of results.-Randomized trials involving surgery did not routinely report considerations of learning and clustering effects",2
"Network meta-analysis is becoming more popular as a way to compare multiple treatments simultaneously. Here, we develop a new estimation method for fitting models for network meta-analysis with random inconsistency effects. This method is an extension of the procedure originally proposed by DerSimonian and Laird. Our methodology allows for inconsistency within the network. The proposed procedure is semi-parametric, non-iterative, fast and highly accessible to applied researchers. The methodology is found to perform satisfactorily in a simulation study provided that the sample size is large enough and the extent of the inconsistency is not very severe. We apply our approach to two real examples.-Extending DerSimonian and Laird's methodology to perform network meta-analyses with random inconsistency effects.",0
"While research on statistical power for designs with continuous outcomes is extensive, the literature on power for designs with binary outcomes is notably more limited. Because statistical power for continuous outcomes is well known, a natural question is whether power may be estimated in a similar way for the case of binary outcomes. This question involves establishing the appropriate analogy between design parameters in the continuous and binary outcome cases, which consists of an analogy in effect sizes and an analogy in the intraclass correlation coefficient (ICC). This article proposes two possible analogies for the ICC and discusses the challenges in establishing a valid and useful analogy for statistical power in designs with binary outcomes. Using the results from two simulation studies, we compare the power estimates and Type I error rates under two analytic models for binary outcomes and the corresponding values under the ICC analogies. We use the simulation results to assess the implications for power and statistical inference when no useful analogy in ICCs exists for a given study. We provide a discussion on how researchers might think of the analogies using an empirical example on a preschool intervention in Ghana. We conclude with final thoughts on the limitations in establishing an analogy in design parameters and ideas for future research. (PsycINFO Database Record (c) 2019 APA, all rights reserved).-The relationship among design parameters for statistical power between continuous and binomial outcomes in cluster randomized trials.",1
"Encouragement design studies are particularly useful for estimating the effect of an intervention that cannot itself be randomly administered to some and not to others. They require a randomly selected group receive extra encouragement to undertake the treatment of interest, where the encouragement typically takes the form of additional information or incentives. We consider a ""clustered encouragement design"" (CED), where the randomization is at the level of the clusters (e.g. physicians), but the compliance with assignment is at the level of the units (e.g. patients) within clusters. Noncompliance and missing data are particular problems in encouragement design studies, where encouragement to take the treatment, rather than the treatment itself, is randomized. The motivating study looks at whether computer-based care suggestions can improve patient outcomes in veterans with chronic heart failure. Since physician adherence has been inadequate, the original study focused on methods to improve physician adherence, although an equally important question is whether physician adherence improves patient outcomes. Here, we reanalyze the data to determine the effect of physician adherence on patient outcomes. We propose causal inference methodology for the effect of a treatment versus a control in a randomized CED study with all-or-none compliance at the unit level. These methods extend the current approaches to account for nonignorable missing data and use an alternative approach to inference using multiple imputation methods, which have been successfully applied to a wide variety of missing data problems and have recently been applied to the potential outcomes framework of causal inference (Taylor and Zhou, 2009b).-Methods for clustered encouragement design studies with noncompliance and missing data.",1
"Large outbreaks, such as those caused by influenza, put a strain on resources necessary for their control. In particular, children have been shown to play a key role in influenza transmission during recent outbreaks, and targeted interventions, such as school closures, could positively impact the course of emerging epidemics. As an outbreak is unfolding, it is important to be able to estimate reproductive numbers that incorporate this heterogeneity and to use surveillance data that is routinely collected to more effectively target interventions and obtain an accurate understanding of transmission dynamics. There are a growing number of methods that estimate age-group specific reproductive numbers with limited data that build on methods assuming a homogenously mixing population. In this article, we introduce a new approach that is flexible and improves on many aspects of existing methods. We apply this method to influenza data from two outbreaks, the 2009 H1N1 outbreaks in South Africa and Japan, to estimate age-group specific reproductive numbers and compare it to three other methods that also use existing data from social mixing surveys to quantify contact rates among different age groups. In this exercise, all estimates of the reproductive numbers for children exceeded the critical threshold of one and in most cases exceeded those of adults. We introduce a flexible new method to estimate reproductive numbers that describe heterogeneity in the population.-Estimating age-specific reproductive numbers-A comparison of methods.",0
"In the research on complex diseases, gene expression (GE) data have been extensively used for clustering samples. The clusters so generated can serve as the basis for disease subtype identification, risk stratification, and many other purposes. With the small sample sizes of genetic profiling studies and noisy nature of GE data, clustering analysis results are often unsatisfactory. In the most recent studies, a prominent trend is to conduct multidimensional profiling, which collects data on GEs and their regulators (copy number alterations, microRNAs, methylation, etc.) on the same subjects. With the regulation relationships, regulators contain important information on the properties of GEs. We develop a novel assisted clustering method, which effectively uses regulator information to improve clustering analysis using GE data. To account for the fact that not all GEs are informative, we propose a weighted strategy, where the weights are determined data-dependently and can discriminate informative GEs from noises. The proposed method is built on the NCut technique and effectively realized using a simulated annealing algorithm. Simulations demonstrate that it can well outperform multiple direct competitors. In the analysis of TCGA cutaneous melanoma and lung adenocarcinoma data, biologically sensible findings different from the alternatives are made.-Assisted gene expression-based clustering with AWNCut.",0
"To assess the design and statistical methods used in cluster-randomized crossover (CRXO) trials. We undertook a systematic review of CRXO trials. Searches of MEDLINE, EMBASE, and CINAHL Plus; and citation searches of CRXO methodological articles were conducted to December 2014. We extracted data on design characteristics and statistical methods for sample size, data analysis, and handling of missing data. Ninety-one trials including 139 end point analyses met the inclusion criteria. Trials had a median of nine clusters [interquartile range (IQR), 4-21] and median cluster-period size of 30 individuals (IQR, 14-77); 58 (69%) trials had two periods, and 27 trials (30%) included the same individuals in all periods. A rationale for the design was reported in only 25 trials (27%). A sample size justification was provided in 53 (58%) trials. Only nine (10%) trials accounted appropriately for the design in their sample size calculation. Ten of the 12 cluster-level analyses used a method that accounted for the clustering and multiple-period aspects of the design. In contrast, only 4 of the 127 individual-level analyses used a potentially appropriate method. There is a need for improved application of appropriate analysis and sample size methods, and reporting, in CRXO trials.-Appropriate statistical methods were infrequently used in cluster-randomized crossover trials.",1
"There is substantial variation in the design of stepped wedge trials. Many recruit participants continuously over time, although the methodological literature has tended not to differentiate closely between continuous recruitment and discrete sampling. We argue for a deeper understanding of the special features of stepped wedge trials with continuous recruitment. This is a commentary and informal review. We discuss the scheduling of recruitment and implementation in continuous time and how contamination might be avoided. We also offer some suggestions on reporting and terminology for stepped wedge trials with continuous recruitment and comment on issues for analysis. Repeated cross-section and continuous recruitment stepped wedge trials are not the same thing. More work is needed to develop the theory and practice of stepped wedge designs with continuous recruitment. Thoughtful approaches to design and clarity of reporting are vital.-Stepped wedge trials with continuous recruitment require new ways of thinking",3
"The cluster randomised trial (CRT) is commonly used in healthcare research. It is the gold-standard study design for evaluating healthcare policy interventions. A key characteristic of this design is that as more participants are included, in a fixed number of clusters, the increase in achievable power will level off. CRTs with cluster sizes that exceed the point of levelling-off will have excessive numbers of participants, even if they do not achieve nominal levels of power. Excessively large cluster sizes may have ethical implications due to exposing trial participants unnecessarily to the burdens of both participating in the trial and the potential risks of harm associated with the intervention. We explore these issues through the use of two case studies. Where data are routinely collected, available at minimum cost and the intervention poses low risk, the ethical implications of excessively large cluster sizes are likely to be low (case study 1). However, to maximise the social benefit of the study, identification of excessive cluster sizes can allow for prespecified and fully powered secondary analyses. In the second case study, while there is no burden through trial participation (because the outcome data are routinely collected and non-identifiable), the intervention might be considered to pose some indirect risk to patients and risks to the healthcare workers. In this case study it is therefore important that the inclusion of excessively large cluster sizes is justifiable on other grounds (perhaps to show sustainability). In any randomised controlled trial, including evaluations of health policy interventions, it is important to minimise the burdens and risks to participants. Funders, researchers and research ethics committees should be aware of the ethical issues of excessively large cluster sizes in cluster trials.-Ethical implications of excessive cluster sizes in cluster randomised trials.",1
"We examined diabetes management practices among Hispanics, Blacks, and 3 Asian American subgroups in New York City. Compared with Blacks and Hispanics, all 3 Asian American subgroups had lower average rates of diabetes management practices. Compared with Blacks, Chinese and Koreans were significantly less likely to participate in all diabetes management behaviors and practices, whereas Asian Indians were significantly less likely to perform feet checks or undergo an eye examination. Results demonstrated the need for health care provider interventions and training to support diabetes management among Asian Americans.-Disparities in diabetes management in Asian Americans in New York City compared with other racial/ethnic minority groups.",0
"This article discusses marginalization of the regression parameters in mixed models for correlated binary outcomes. As is well known, the regression parameters in such models have the ""subject-specific"" (SS) or conditional interpretation, in contrast to the ""population-averaged"" (PA) or marginal estimates that represent the unconditional covariate effects. We describe an approach using numerical quadrature to obtain PA estimates from their SS counterparts in models with multiple random effects. Standard errors for the PA estimates are derived using the delta method. We illustrate our proposed method using data from a smoking cessation study in which a binary outcome (smoking, Y/N) was measured longitudinally. We compare our estimates to those obtained using GEE and marginalized multilevel models, and present results from a simulation study.-A note on marginalization of regression parameters from mixed models of binary outcomes.",1
"Girardeau, Ravaud and Donner in 2008 presented a formula for sample size calculations for cluster randomised crossover trials, when the intracluster correlation coefficient, interperiod correlation coefficient and mean cluster size are specified in advance. However, in many randomised trials, the number of clusters is constrained in some way, but the mean cluster size is not. We present a version of the Girardeau formula for sample size calculations for cluster randomised crossover trials when the number of clusters is fixed. Formulae are given for the minimum number of clusters, the maximum cluster size and the relationship between the correlation coefficients when there are constraints on both the number of clusters and the cluster size. Our version of the formula may aid the efficient planning and design of cluster randomised crossover trials.-A note on sample size calculations for cluster randomised crossover trials with a fixed number of clusters",1
"Public health interventions usually operate at the level of groups rather than individuals, and cluster randomized controlled trials (RCTs) are one means of evaluating their effectiveness. Using examples from six such trials in Bangladesh, India, Malawi and Nepal, we discuss our experience of the ethical issues that arise in their conduct. We set cluster RCTs in the broader context of public health research, highlighting debates about the need to reconcile individual autonomy with the common good and about the ethics of public health research in low-income settings in general. After a brief introduction to cluster RCTs, we discuss particular challenges we have faced. These include the nature of - and responsibility for - group consent, and the need for consent by individuals within groups to intervention and data collection. We discuss the timing of consent in relation to the implementation of public health strategies, and the problem of securing ethical review and approval in a complex domain. Finally, we consider the debate about benefits to control groups and the standard of care that they should receive, and the issue of post-trial adoption of the intervention under test.-Ethical challenges in cluster randomized controlled trials: experiences from public health interventions in Africa and Asia.",1
"Cluster randomised trials (CRTs) randomise participants in groups, rather than as individuals, and are key tools used to assess interventions in health research where treatment contamination is likely or if individual randomisation is not feasible. Missing outcome data can reduce power in trials, including in CRTs, and is a potential source of bias. The current review focuses on evaluating methods used in statistical analysis and handling of missing data with respect to the primary outcome in CRTs. We will search for CRTs published between August 2013 and July 2014 using PubMed, Web of Science and PsycINFO. We will identify relevant studies by screening titles and abstracts, and examining full-text articles based on our predefined study inclusion criteria. 86 studies will be randomly chosen to be included in our review. Two independent reviewers will collect data from each study using a standardised, prepiloted data extraction template. Our findings will be summarised and presented using descriptive statistics. This methodological systematic review does not need ethical approval because there are no data used in our study that are linked to individual patient data. After completion of this systematic review, data will be immediately analysed, and findings will be disseminated through a peer-reviewed publication and conference presentation.-Statistical analysis and handling of missing data in cluster randomised trials: protocol for a systematic review.",1
"In orthodontics, multiple site observations within patients or multiple observations collected at consecutive time points are often encountered. Clustered designs require larger sample sizes compared to individual randomized trials and special statistical analyses that account for the fact that observations within clusters are correlated. It is the purpose of this study to assess to what degree clustering effects are considered during design and data analysis in the three major orthodontic journals. The contents of the most recent 24 issues of the American Journal of Orthodontics and Dentofacial Orthopedics (AJODO), Angle Orthodontist (AO), and European Journal of Orthodontics (EJO) from December 2010 backwards were hand searched. Articles with clustering effects and whether the authors accounted for clustering effects were identified. Additionally, information was collected on: involvement of a statistician, single or multicenter study, number of authors in the publication, geographical area, and statistical significance. From the 1584 articles, after exclusions, 1062 were assessed for clustering effects from which 250 (23.5 per cent) were considered to have clustering effects in the design (kappa = 0.92, 95 per cent CI: 0.67-0.99 for inter rater agreement). From the studies with clustering effects only, 63 (25.20 per cent) had indicated accounting for clustering effects. There was evidence that the studies published in the AO have higher odds of accounting for clustering effects [AO versus AJODO: odds ratio (OR) = 2.17, 95 per cent confidence interval (CI): 1.06-4.43, P = 0.03; EJO versus AJODO: OR = 1.90, 95 per cent CI: 0.84-4.24, non-significant; and EJO versus AO: OR = 1.15, 95 per cent CI: 0.57-2.33, non-significant). The results of this study indicate that only about a quarter of the studies with clustering effects account for this in statistical data analysis.-Does published orthodontic research account for clustering effects during statistical data analysis?",1
"We propose a sample size calculation approach for testing a proportion using the weighted sign test when binary observations are dependent within a cluster. Sample size formulas are derived with nonparametric methods using three weighting schemes: equal weights to observations, equal weights to clusters, and optimal weights that minimize the variance of the estimator. Sample size formulas are derived incorporating intracluster correlation and the variability in cluster sizes. Simulation studies are conducted to evaluate a finite sample performance of the proposed sample size formulas. Empirical powers are generally close to nominal levels. The number of clusters required increases as the imbalance in cluster size increases and the intracluster correlation increases. The estimator using optimal weights yields the smallest sample size estimate among three estimators. For small values of intracluster correlation the sample size estimates derived from the optimal weight estimator are close to that derived from the estimator assigning equal weights to observations. For large values of intracluster correlation, the optimal weight sample size estimate is close to the sample size estimate assigning equal weights to clusters.-Sample Size Calculation for Clustered Binary Data with Sign Tests Using Different Weighting Schemes.",1
"Cluster randomized controlled trials increasingly are used to evaluate health interventions where patients are nested within larger clusters such as practices, hospitals or communities. Patients within a cluster may be similar to each other relative to patients in other clusters on key variables; therefore, sample size calculations and analyses of results require special statistical methods. The purpose of this study was to illustrate the calculations used for sample size estimation and data analysis and to provide estimates of the intraclass correlation coefficients (ICCs) for several variables using data from the Seniors Medication Assessment Research Trial (SMART), a community-based trial of pharmacists consulting to family physicians to optimize the drug therapy of older patients. The study was a paired cluster randomized trial, where the family physician's practice was the cluster. The sample size calculation was based on a hypothesized reduction of 15% in mean daily units of medication in the intervention group compared with the control group, using an alpha of 0.05 (one-tailed) with 80% power, and an ICC from pilot data of 0.08. ICCs were estimated from the data for several variables. The analyses comparing the two groups used a random effects model for a meta-analysis over pairs. The design effect due to clustering was 2.12, resulting in an inflation in sample size from 340 patients required using individual randomization, to 720 patients using randomization of practices, with 15 patients from each of 48 practices. ICCs for medication use, health care utilization and general health were &lt;0.1; however, the ICC for mean systolic blood pressure over the trial period was 0.199. Compared with individual randomization, cluster randomization may substantially increase the sample size required to maintain adequate statistical power. The differences in ICCs among potential outcome variables reinforce the need for valid estimates to ensure proper study design.-Randomizing patients by family practice: sample size estimation, intracluster correlation and data analysis.",1
"To evaluate the quality of reviews about etanercept (ETN) and infliximab (IFX), two biologic treatments for rheumatoid arthritis (RA). A comprehensive, systematic review, including searches of MEDLINE, EMBASE, and other electronic databases and hand-searches for published and unpublished literature. Two raters independently examined each article and identified systematic reviews as those including either a description of: (1) sources for identification and data retrieval; or (2) search strategy. They applied the quality of reporting of meta-analyses (QUOROM) instrument to systematic reviews. Of 3,620 total citations, 281 were identified as reviews. Of these, 26 (9%) qualified as systematic rather than narrative. Overall, few reviews described selection of sources, critical appraisal, or quantitative summary or synthesis. Systematic reviews most often failed to explain validity assessment. Several articles did not disclose authors' participation in industry-funded clinical trials. Most reviews published in high impact factor and rheumatology journals did not meet many quality standards. Significant associations existed between review type (narrative vs. systematic) and reported funding (P=0.05), conflicts of interest (P=0.005), and country of publication (P&lt;0.0001). More than 90% of the published reviews were narrative and did not report methods and conflicts of interest in sufficient detail, raising concerns about selection and reporting bias.-Poor reporting of search strategy and conflict of interest in over 250 narrative and systematic reviews of two biologic agents in arthritis: a systematic review.",0
"Although several studies have evaluated the relationship between adult height and mortality, their results have not been entirely consistent. Little is known about components of adult height in relation to mortality, particularly in developing countries. We examined the association of adult height and its components (leg and trunk length) with mortality using data from 74 869 Chinese women and 61,333 men in the Shanghai Women's (1996-2008) and Men's (2002-2008) Health Studies. Anthropometric measurements, including standing and sitting height and weight, were taken at baseline by trained interviewers according to a standard protocol. Deaths were ascertained by biennial home visits and linkage with the vital statistics registry. Cox regression models were used to evaluate the associations. Neither height nor its components were associated with all-cause mortality. Height and, less consistently, its components were positively associated with cancer mortality, but inversely associated with cardiovascular disease (CVD) mortality. Hazard ratios (HRs) [95% confidence intervals (CIs)] for cancer mortality per 1-SD increment in height, trunk and leg length were 1.06 (1.01-1.12), 1.07 (1.01-1.12) and 1.03 (0.98-1.08), respectively, in women, and 1.13 (1.05-1.22), 1.09 (1.00-1.19) and 1.10 (1.03-1.16), respectively, in men. The corresponding HRs for CVD mortality were 0.89 (0.84-0.95), 0.93 (0.87-0.99) and 0.91 (0.86-0.98) in women, and 0.93 (0.86-1.02), 0.89 (0.81-0.98) and 0.99 (0.92-1.06) in men. Our results suggest that different mechanisms may be involved in linking height and its components with cancer and CVD mortality.-Associations of adult height and its components with mortality: a report from cohort studies of 135,000 Chinese women and men.",0
"The Community Youth Development Study (CYDS) will evaluate the Communities That Care (CTC) operating system for its effects on alcohol, tobacco, drug use, and other outcomes among adolescents resident in the 24 participating communities. The CYDS employs a combination of both cross-sectional and cohort designs. We use data from an earlier study that included the CYDS communities to estimate power for CYDS intervention effects given several analytic models that might be applied to the multiple baseline and follow-up surveys that define the CYDS cross-sectional design. We compare pre-post mixed-model ANCOVA models against random coefficients models, both in one- and two-stage versions. The two-stage pre-post mixed-model ANCOVA offers the best power for the primary outcomes and will provide adequate power for detection of modest but important intervention effects.-Analysis strategies for a community trial to reduce adolescent ATOD use: a comparison of random coefficient and ANOVA/ANCOVA models.",1
"Generalized estimating equations (GEE (Biometrika 1986; 73(1):13-22) is a general statistical method to fit marginal models for correlated or clustered responses, and it uses a robust sandwich estimator to estimate the variance-covariance matrix of the regression coefficient estimates. While this sandwich estimator is robust to the misspecification of the correlation structure of the responses, its finite sample performance deteriorates as the number of clusters or observations per cluster decreases. To address this limitation, Pan (Biometrika 2001; 88(3):901-906) and Mancl and DeRouen (Biometrics 2001; 57(1):126-134) investigated two modifications to the original sandwich variance estimator. Motivated by the ideas underlying these two modifications, we propose a novel robust variance estimator that combines the strengths of these estimators. Our theoretical and numerical results show that the proposed estimator attains better efficiency and achieves better finite sample performance compared with existing estimators. In particular, when the sample size or cluster size is small, our proposed estimator exhibits lower bias and the resulting confidence intervals for GEE estimates achieve better coverage rates performance. We illustrate the proposed method using data from a dental study.-Modified robust variance estimator for generalized estimating equations with improved small-sample performance.",1
"Given increasing concerns about the relevance of research to policy and practice, there is growing interest in assessing and enhancing the external validity of randomized trials: determining how useful a given randomized trial is for informing a policy question for a specific target population. This article highlights recent advances in assessing and enhancing external validity, with a focus on the data needed to make ex post statistical adjustments to enhance the applicability of experimental findings to populations potentially different from their study sample. We use a case study to illustrate how to generalize treatment effect estimates from a randomized trial sample to a target population, in particular comparing the sample of children in a randomized trial of a supplemental program for Head Start centers (the Research-Based, Developmentally Informed study) to the national population of children eligible for Head Start, as represented in the Head Start Impact Study. For this case study, common data elements between the trial sample and population were limited, making reliable generalization from the trial sample to the population challenging. To answer important questions about external validity, more publicly available data are needed. In addition, future studies should make an effort to collect measures similar to those in other data sets. Measure comparability between population data sets and randomized trials that use samples of convenience will greatly enhance the range of research and policy relevant questions that can be answered.-Generalizing Treatment Effect Estimates From Sample to Population: A Case Study in the Difficulties of Finding Sufficient Data.",0
"Methadone maintenance is an effective treatment for opioid dependence but is rarely initiated in US jails. Patient navigation is a promising approach to improve continuity of care but has not been tested in bridging the gap between jail- and community-based drug treatment programs. This is an open-label randomized clinical trial among 300 adult opioid dependent newly-arrested detainees that will compare three treatment conditions: methadone maintenance without routine counseling (termed Interim Methadone; IM) initiated in jail v. IM and patient navigation v. enhanced treatment-as-usual. The two primary outcomes will be: (1) the rate of entry into treatment for opioid use disorder within 30days from release and (2) frequency of opioid positive urine tests over the 12-month follow-up period. An economic analysis will examine the costs, cost-effectiveness, and cost-benefit ratio of the study interventions. We describe the background and rationale for the study, its aims, hypotheses, and study design. Given the large number of opioid dependent detainees in the US and elsewhere, initiating IM at the time of incarceration could be a significant public health and clinical approach to reducing relapse, recidivism, HIV-risk behavior, and criminal behavior. An economic analysis will be conducted to assist policy makers in determining the utility of adopting this approach. ClinicalTrials.gov: NCT02334215.-Interim methadone and patient navigation in jail: Rationale and design of a randomized clinical trial.",0
"The analysis of covariance provides a common approach to adjusting for a baseline covariate in medical research. With Gaussian errors, adding random covariates does not change either the theory or the computations of general linear model data analysis. However, adding random covariates does change the theory and computation of power analysis. Many data analysts fail to fully account for this complication in planning a study. We present our results in five parts. (i) A review of published results helps document the importance of the problem and the limitations of available methods. (ii) A taxonomy for general linear multivariate models and hypotheses allows identifying a particular problem. (iii) We describe how random covariates introduce the need to consider quantiles and conditional values of power. (iv) We provide new exact and approximate methods for power analysis of a range of multivariate models with a Gaussian baseline covariate, for both small and large samples. The new results apply to the Hotelling-Lawley test and the four tests in the ""univariate"" approach to repeated measures (unadjusted, Huynh-Feldt, Geisser-Greenhouse, Box). The techniques allow rapid calculation and an interactive, graphical approach to sample size choice. (v) Calculating power for a clinical trial of a treatment for increasing bone density illustrates the new methods. We particularly recommend using quantile power with a new Satterthwaite-style approximation.-Adjusting power for a baseline covariate in linear models.",0
"Background/Aims HIV continues to be a major public health threat in the United States, and mathematical modeling has demonstrated that the universal effective use of antiretroviral therapy among all HIV-positive individuals (i.e. the ""test and treat"" approach) has the potential to control HIV. However, to accomplish this, all the steps that define the HIV care continuum must be achieved at high levels, including HIV testing and diagnosis, linkage to and retention in clinical care, antiretroviral medication initiation, and adherence to achieve and maintain viral suppression. The HPTN 065 (Test, Link-to-Care Plus Treat [TLC-Plus]) study was designed to determine the feasibility of the ""test and treat"" approach in the United States. Methods HPTN 065 was conducted in two intervention communities, Bronx, NY, and Washington, DC, along with four non-intervention communities, Chicago, IL; Houston, TX; Miami, FL; and Philadelphia, PA. The study consisted of five components: (1) exploring the feasibility of expanded HIV testing via social mobilization and the universal offer of testing in hospital settings, (2) evaluating the effectiveness of financial incentives to increase linkage to care, (3) evaluating the effectiveness of financial incentives to increase viral suppression, (4) evaluating the effectiveness of a computer-delivered intervention to decrease risk behavior in HIV-positive patients in healthcare settings, and (5) administering provider and patient surveys to assess knowledge and attitudes regarding the use of antiretroviral therapy for prevention and the use of financial incentives to improve health outcomes. The study used observational cohorts, cluster and individual randomization, and made novel use of the existing national HIV surveillance data infrastructure. All components were developed with input from a community advisory board, and pragmatic methods were used to implement and assess the outcomes for each study component. Results A total of 76 sites in Washington, DC, and the Bronx, NY, participated in the study: 37 HIV test sites, including 16 hospitals, and 39 HIV care sites. Between September 2010 and December 2014, all study components were successfully implemented at these sites and resulted in valid outcomes. Our pragmatic approach to the study design, implementation, and the assessment of study outcomes allowed the study to be conducted within established programmatic structures and processes. In addition, it was successfully layered on the ongoing standard of care and existing data infrastructure without disrupting health services. Conclusion The HPTN 065 study demonstrated the feasibility of implementing and evaluating a multi-component ""test and treat"" trial that included a large number of community sites and involved pragmatic approaches to study implementation and evaluation.-Design of the HPTN 065 (TLC-Plus) study: A study to evaluate the feasibility of an enhanced test, link-to-care, plus treat approach for HIV prevention in the United States.",0
"Bayesian methods for cluster randomized trials extend the random-effects formulation by allowing both the use of external evidence on parameters and straightforward relaxation of the standard normality and constant variance assumptions. Care is required in specifying prior distributions on variance components, and a number of different options are explored with implied prior distributions for other parameters given in closed form. Markov chain Monte Carlo (MCMC) methods permit the fitting of very general models and the introduction of parameter uncertainty into power calculations. We illustrate these ideas using a published example in which general practices were randomized to intervention or control, and show that different choices of supposedly 'non-informative' prior distributions can have substantial influence on conclusions. We also illustrate the use of forward simulation methods in power calculations with uncertainty on multiple inputs. Bayesian methods have the potential to be very useful but guidance is required as to appropriate strategies for robust analysis. Our current experience leads us to recommend a standard 'non-informative' prior distribution for the within-cluster sampling variance, and an independent prior on the intraclass correlation coefficient (ICC). The latter may exploit background evidence or, as a reference analysis, be a uniform ICC or a 'uniform shrinkage' prior.-Bayesian methods for cluster randomized trials with continuous responses.",1
"Stratified cluster randomization trials (CRTs) have been frequently employed in clinical and healthcare research. Comparing with simple randomized CRTs, stratified CRTs reduce the imbalance of baseline prognostic factors among different intervention groups. Due to the popularity, there has been a growing interest in methodological development on sample size estimation and power analysis for stratified CRTs; however, existing work mostly assumes equal cluster size within each stratum and uses multilevel models. Clusters are often naturally formed with random sizes in CRTs. With varying cluster size, commonly used ad hoc approaches ignore the variability in cluster size, which may underestimate (overestimate) the required number of clusters for each group per stratum and lead to underpowered (overpowered) clinical trials. We propose closed-form sample size formulas for estimating the required total number of subjects and for estimating the number of clusters for each group per stratum, based on Cochran-Mantel-Haenszel statistic for stratified cluster randomization design with binary outcomes, accounting for both clustering and varying cluster size. We investigate the impact of various design parameters on the relative change in the required number of clusters for each group per stratum due to varying cluster size. Simulation studies are conducted to evaluate the finite-sample performance of the proposed sample size method. A real application example of a pragmatic stratified CRT of a triad of chronic kidney disease, diabetes, and hypertension is presented for illustration.-Sample size considerations for stratified cluster randomization design with binary outcomes and varying cluster size",1
"Clustered binary data occur frequently in biostatistical work. Several approaches have been proposed for the analysis of clustered binary data. In Rosner (1984, Biometrics 40, 1025-1035), a polychotomous logistic regression model was proposed that is a generalization of the beta-binomial distribution and allows for unit- and subunit-specific covariates, while controlling for clustering effects. One assumption of this model is that all pairs of subunits within a cluster are equally correlated. This is appropriate for ophthalmologic work where clusters are generally of size 2, but may be inappropriate for larger cluster sizes. A beta-binomial mixture model is introduced to allow for multiple subclasses within a cluster and to estimate odds ratios relating outcomes for pairs of subunits within a subclass as well as in different subclasses. To include covariates, an extension of the polychotomous logistic regression model is proposed, which allows one to estimate effects of unit-, class-, and subunit-specific covariates, while controlling for clustering using the beta-binomial mixture model. This model is applied to the analysis of respiratory symptom data in children collected over a 14-year period in East Boston, Massachusetts, in relation to maternal and child smoking, where the unit is the child and symptom history is divided into early-adolescent and late-adolescent symptom experience.-Multivariate methods for clustered binary data with multiple subclasses, with application to binary longitudinal data.",1
"Cluster randomized trials (CRTs) are a design used to test interventions where individual randomization is not appropriate. The mixed model for repeated measures (MMRM) is a popular choice for individually randomized trials with longitudinal continuous outcomes. This model's appeal is due to avoidance of model misspecification and its unbiasedness for data missing completely at random or at random. We extended the MMRM to cluster randomized trials by adding a random intercept for the cluster and undertook a simulation experiment to investigate statistical properties when data are missing at random. We simulated cluster randomized trial data where the outcome was continuous and measured at baseline and three post-intervention time points. We varied the number of clusters, the cluster size, the intra-cluster correlation, missingness and the data-generation models. We demonstrate the MMRM-CRT with an example of a cluster randomized trial on cardiovascular disease prevention among diabetics. When simulating a treatment effect at the final time point we found that estimates were unbiased when data were complete and when data were missing at random. Variance components were also largely unbiased. When simulating under the null, we found that type I error was largely nominal, although for a few specific cases it was as high as 0.081. Although there have been assertions that this model is inappropriate when there are more than two repeated measures on subjects, we found evidence to the contrary. We conclude that the MMRM for CRTs is a good analytic choice for cluster randomized trials with a continuous outcome measured longitudinally. ClinicalTrials.gov, ID: NCT02804698.-The mixed model for repeated measures for cluster randomized trials: a simulation study investigating bias and type I error with missing continuous data.",1
"To assess the quality of reporting and accuracy of a priori estimates used in sample size calculations for cluster randomized trials (CRTs). We reviewed 300 CRTs published between 2000 and 2008. The prevalence of reporting sample size elements from the 2004 CONSORT recommendations was evaluated and a priori estimates compared with those observed in the trial. Of the 300 trials, 166 (55%) reported a sample size calculation. Only 36 of 166 (22%) reported all recommended descriptive elements. Elements specific to CRTs were the worst reported: a measure of within-cluster correlation was specified in only 58 of 166 (35%). Only 18 of 166 articles (11%) reported both a priori and observed within-cluster correlation values. Except in two cases, observed within-cluster correlation values were either close to or less than a priori values. Even with the CONSORT extension for cluster randomization, the reporting of sample size elements specific to these trials remains below that necessary for transparent reporting. Journal editors and peer reviewers should implement stricter requirements for authors to follow CONSORT recommendations. Authors should report observed and a priori within-cluster correlation values to enable comparisons between these over a wider range of trials.-Reporting and methodological quality of sample size calculations in cluster randomized trials could be improved: a review.",1
"Composite endpoints can be problematic in the presence of competing risks when a treatment does not affect events comprising the endpoint equally. We conducted secondary analysis of SWOG 8794 trial of adjuvant radiation therapy (RT) for high-risk post-operative prostate cancer. The primary outcome was metastasis-free survival (MFS), defined as time to first occurrence of metastasis or death from any cause (competing mortality (CM)). We developed separate risk scores for time to metastasis and CM using competing risks regression. We estimated treatment effects using Cox models adjusted for risk scores and identified an enriched subgroup of 75 patients at high risk of metastasis and low risk of CM. The mean CM risk score was significantly lower in the RT arm vs. control arm (p=0.001). The effect of RT on MFS (HR 0.70; 95% CI, 0.53-0.92; p=0.010) was attenuated when controlling for metastasis and CM risk (HR 0.76; 95% CI, 0.58-1.00; p=0.049), and the effect of RT on overall survival (HR 0.73; 95% CI, 0.55-0.96; p=0.02) was no longer significant when controlling for metastasis and CM risk (HR 0.80; 95% CI, 0.60-1.06; p=0.12). Compared to the whole sample, the enriched subgroup had the same 10-year incidence of MFS (40%; 95% CI, 22-57%), but a higher incidence of metastasis (30% (95% CI, 15-47%) vs. 20% (95% CI, 15-26%)). A randomized trial in the subgroup would have achieved 80% power with 56% less patients (313 vs. 709, respectively). Stratification on competing event risk may improve the efficiency of clinical trials.-Competing event risk stratification may improve the design and efficiency of clinical trials: secondary analysis of SWOG 8794.",0
"Overdispersion models have been extensively studied for correlated normal and binomial data but much less so for correlated multinomial data. In this work, we describe a multinomial overdispersion model that leads to the specification of the first two moments of the outcome and allows the estimation of the global parameters using generalized estimating equations (GEE). We introduce a Global Blinding Index as a target parameter and illustrate the application of the GEE method to its estimation from (1) a clinical trial with clustering by practitioner and (2) a meta-analysis on psychiatric disorders. We examine the impact of a small number of clusters, high variability in cluster sizes, and the magnitude of the intraclass correlation on the performance of the GEE estimators of the Global Blinding Index using the data simulated from different models. We compare these estimators with the inverse-variance weighted estimators and a maximum-likelihood estimator, derived under the Dirichlet-multinomial model. Our results indicate that the performance of the GEE estimators was satisfactory even in situations with a small number of clusters, whereas the inverse-variance weighted estimators performed poorly, especially for larger values of the intraclass correlation coefficient. Our findings and illustrations may be instrumental for practitioners who analyze clustered multinomial data from clinical trials and/or meta-analysis.-Overdispersion models for correlated multinomial data: Applications to blinding assessment.",0
"Cluster randomized trials (CRTs) offer unique advantages over standard randomized controlled clinical trials (RCTs) and observational methodologies, and may provide a cost-efficient alternative for answering questions about the best treatments for common conditions. To describe health plan leaders' views on CRTs, identify barriers to conducting CRTs, and solicit recommendations for increasing the acceptability of CRTs. Qualitative in-depth telephone interviews with leaders from 8 health plans. : Thirty-four health plan leaders (medical directors, pharmacy directors, Institutional Review Board leaders, ethics leaders, compliance leaders, and others). Qualitative analysis of interview transcripts to identify barriers, factors influencing leaders' views, ethical issues, aspects of CRTs that appeal to leaders, and recommendations for increasing acceptability of CRTs. Multiple barriers were identified, including financial costs, concerns about stakeholders' perceptions of CRTs, impact on physicians' prescribing habits, and formulary changes. Most leaders recognized the potential value of studying the comparative effectiveness of therapeutics, and many stressed the need for head-to-head trials. Leaders' views would be influenced by variations in study design and implementation. Recommendations for increasing acceptability of CRTs included ensuring that the fiscal impact of a CRT be budget neutral, and that researchers educate stakeholders and decision-makers about CRTs. Overall, health plan leaders recognized the need for studies of the comparative effectiveness of therapeutics under real world conditions, and many expressed support for CRTs. However, researchers seeking to conduct CRTs in health plans are likely to face numerous barriers, and preparatory work will be essential.-Cluster randomized trials: opportunities and barriers identified by leaders of eight health plans.",1
"Nesting of patients within care providers in trials of physical and talking therapies creates an additional level within the design. The statistical implications of this are analogous to those of cluster randomised trials, except that the clustering effect may interact with treatment and can be restricted to one or more of the arms. The statistical model that is recommended at the trial level includes a random effect for the care provider but allows the provider and patient level variances to differ across arms. Evidence suggests that, while potentially important, such within-trial clustering effects have rarely been taken into account in trials and do not appear to have been considered in meta-analyses of these trials. This paper describes summary measures and individual-patient-data methods for meta-analysing absolute mean differences from randomised trials with two-level nested clustering effects, contrasting fixed and random effects meta-analysis models. It extends methods for incorporating trials with unequal variances and homogeneous clustering to allow for between-arm and between-trial heterogeneity in intra-class correlation coefficient estimates. The work is motivated by a meta-analysis of trials of counselling in primary care, where the control is no counselling and the outcome is the Beck Depression Inventory. Assuming equal counsellor intra-class correlation coefficients across trials, the recommended random-effects heteroscedastic model gave a pooled absolute mean difference of -2.53 (95% CI -5.33 to 0.27) using summary measures and -2.51 (95% CI -5.35 to 0.33) with the individual-patient-data. Pooled estimates were consistently below a minimally important clinical difference of four to five points on the Beck Depression Inventory.-Meta-analysis of absolute mean differences from randomised trials with treatment-related clustering associated with care providers.",2
"The design and use of cluster randomised controlled trials in evaluating injury prevention interventions: part 1. Rationale, design and informed consent.",1
"For many complex diseases, gene-environment (G-E) interactions have independent contributions beyond the main G and E effects. Despite extensive effort, it still remains challenging to identify G-E interactions. With the long accumulation of experiments and data, for many biomedical problems of common interest, there are existing studies that can be relevant and informative for the identification of G-E interactions and/or main effects. In this study, our goal is to identify G-E interactions (as well as their corresponding main G effects) under a joint statistical modeling framework. Significantly advancing from the existing studies, a quasi-likelihood-based approach is developed to incorporate information mined from the existing literature. A penalization approach is adopted for identification and selection and respects the ""main effects, interactions"" hierarchical structure. Simulation shows that, when the existing information is of high quality, significant improvement can be observed. On the other hand, when the existing information is less informative, the proposed method still performs reasonably (and hence demonstrates a certain degree of ""robustness""). The analysis of The Cancer Genome Atlas (TCGA) data on cutaneous melanoma and glioblastoma multiforme demonstrates the practical applicability of the proposed approach and also leads to sensible findings.-Identifying gene-environment interactions incorporating prior information.",0
"The health care system in the United States is inherently hierarchical. Patients are ""nested"" within physicians who in turn are ""nested"" within practices. Much of the research data gathered in practice-based research networks (PBRNs) also have similar patterns of nesting (clustering). When research data are nested, statistical approaches to the data must account for the multilevel nature of the data or risk errors in interpretation. We illustrate the concept of multilevel structure and provide examples with implications for practice-based research. We present a selection of multilevel (hierarchical) models and contrast them with traditional linear regression models, using an example of a simulated observational study to illustrate increasingly complex statistical approaches, as well as to explore the consequences of ignoring clustering in data. Additionally, we discuss other types of outcome data and designs, and the effects of clustering on sample size and power. Multilevel models demonstrate that the effects of physician-level activities may differ from clinic to clinic as well as between rural and urban settings; this variability would be undetected in traditional linear regression approaches. Study conclusions differed when the data were analyzed with multilevel methods compared with traditional linear regression methods. Clustered data also affected sample size; as the intraclass correlation increased and the patients per cluster increased, the required number of patients increased dramatically. Recognizing and accounting for multilevel structure when analyzing data from PBRN studies can lead to more accurate conclusions, as well as offer opportunities to explore contextual effects and differences across sites. Accommodating multilevel structure in planning research studies can result in more appropriate estimation of required sample size.-Multilevel modeling and practice-based research.",1
The clustered permutation test is a nonparametric method of significance testing for correlated data. It is often used in cluster randomized trials where groups of patients rather than individuals are randomized to either a treatment or control intervention. We describe a flexible and efficient SAS macro that implements the 2-sample clustered permutation test. We discuss the theory and applications behind this test as well as details of the SAS code.-A SAS macro for a clustered permutation test.,1
"Field experiments in education frequently assign entire groups such as schools to treatment or control conditions. These experiments incorporate sometimes a longitudinal component where for example students are followed over time to assess differences in the average rate of linear change, or rate of acceleration. In this study, we provide methods for power analysis in three-level polynomial change models for cluster randomized designs (i.e., treatment assigned to units at the third level). Power computations take into account clustering effects at the second and third levels, the number of measurement occasions, the impact of sample sizes at different levels (e.g., number of schools or students), and covariates effects. An illustrative example that shows how power is influenced by the number of measurement occasions, and sample sizes and covariates at the second or third levels is presented.-Power Analysis for Models of Change in Cluster Randomized Designs.",1
"Recently meta-analysis has been widely utilized to combine information across multiple studies to evaluate a common effect. Integrating data from similar studies is particularly useful in genomic studies where the individual study sample sizes are not large relative to the number of parameters of interest. In this article, we are interested in developing robust prognostic rules for the prediction of t-year survival based on multiple studies. We propose to construct a composite score for prediction by fitting a stratified semiparametric transformation model that allows the studies to have related but not identical outcomes. To evaluate the accuracy of the resulting score, we provide point and interval estimators for the commonly used accuracy measures including the time-specific receiver operating characteristic curves, and positive and negative predictive values. We apply the proposed procedures to develop prognostic rules for the 5-year survival of breast cancer patients based on five breast cancer genomic studies.-Robust prediction of?t-year survival with data from multiple studies.",0
"Randomised trials involving infants from both single and multiple births present unique statistical challenges. A range of methods have been used to analyse such data, including standard methods which treat all infants as independent, and more complex methods which account for the dependence between outcomes of infants from the same pregnancy. Conflicting recommendations have been made regarding if and when this dependence, or clustering, should be taken into account in the analysis. We studied the performance of ordinary logistic regression, which ignores the clustering, compared with logistic generalised estimating equations (GEEs) and mixed effects models (MEMs), which account for the clustering, using real and simulated datasets. Ordinary logistic regression produced appropriate type I error and coverage rates, provided the dependence between outcomes of infants from the same pregnancy was small and the multiple birth rate was low, but performed poorly otherwise. The type I error rate increased and the coverage rate decreased as either the strength of the dependence or the multiple birth rate increased. In contrast, logistic GEEs maintained appropriate type I error and coverage rates across a wide range of settings. The performance of logistic MEMs varied depending on the setting and the estimation procedure used but was often similar to or better than ordinary logistic regression. We recommend using a method which takes the clustering into account when analysing datasets including infants from multiple births.-Analysis of binary outcomes from randomised trials including multiple births: when should clustering be taken into account?",1
"We present an algorithm for randomizing units in blocks for controlled trials when the composition of blocking factors is not known in advance. For example, suppose the desired goal of an intervention study is to randomize units to one of two interventions while blocking on a dichotomous factor (e.g., gender), but the total number of units, and therefore number or composition, of males and females among those units assembled for randomization cannot be determined in advance. This situation arises in randomized trials when subjects are scheduled to be randomized as a group, but not all of the subjects show up for the visit. Since investigators do not know which of the scheduled subjects will or will not show up, a dynamic randomization scheme is required to accommodate the unknown composition of the blocking factor once a group of subjects (units) is assembled for randomization. These settings are further complicated when there is more than one blocking factor. In this paper, we present an algorithm that ensures the integrity of the randomization process in these settings.-A dynamic block-randomization algorithm for group-randomized clinical trials when the composition of blocking factors is not known in advance.",1
"Cluster randomization trials in which families are the unit of allocation are commonly adopted for the evaluation of disease prevention interventions. Sample size estimation for cluster randomization trials depends on parameters that quantify the variability within and between clusters and the variability in cluster size. Accurate advance estimates of these nuisance parameters may be difficult to obtain and misspecification may lead to an underpowered study. Since families are typically recruited over time, we propose using a portion of the data to estimate the nuisance parameters and to re-estimate sample size based on the estimates. This extends the standard internal pilot study methods to the setting of cluster randomization trials. The effect of this design on the power, significance level and sample size is analysed via simulation and is shown to provide a flexible and practical approach to cluster randomization trials.-Sample size re-estimation in cluster randomization trials.",1
"In stepped cluster designs the intervention is introduced into some (or all) clusters at different times and persists until the end of the study. Instances include traditional parallel cluster designs and the more recent stepped-wedge designs. We consider the precision offered by such designs under mixed-effects models with fixed time and random subject and cluster effects (including interactions with time), and explore the optimal choice of uptake times. The results apply both to cross-sectional studies where new subjects are observed at each time-point, and longitudinal studies with repeat observations on the same subjects. The efficiency of the design is expressed in terms of a 'cluster-mean correlation' which carries information about the dependency-structure of the data, and two design coefficients which reflect the pattern of uptake-times. In cross-sectional studies the cluster-mean correlation combines information about the cluster-size and the intra-cluster correlation coefficient. A formula is given for the 'design effect' in both cross-sectional and longitudinal studies. An algorithm for optimising the choice of uptake times is described and specific results obtained for the best balanced stepped designs. In large studies we show that the best design is a hybrid mixture of parallel and stepped-wedge components, with the proportion of stepped wedge clusters equal to the cluster-mean correlation. The impact of prior uncertainty in the cluster-mean correlation is considered by simulation. Some specific hybrid designs are proposed for consideration when the cluster-mean correlation cannot be reliably estimated, using a minimax principle to ensure acceptable performance across the whole range of unknown values. ? 2016 The Authors. Statistics in Medicine published by John Wiley &amp; Sons Ltd.-Statistical efficiency and optimal design for stepped cluster studies under linear mixed effects models.",3
Randomised trials--cluster versus individual randomisation. Primary Care Alliance for Clinical Trials (PACT) network.,1
"A key requirement for a useful power calculation is that the calculation mimic the data analysis that will be performed on the actual data, once that data is observed. Close approximations may be difficult to achieve using analytic solutions, however, and thus Monte Carlo approaches, including both simulation and bootstrap resampling, are often attractive. One setting in which this is particularly true is cluster-randomized trial designs. However, Monte Carlo approaches are useful in many additional settings as well. Calculating power for cluster-randomized trials using analytic or simulation-based methods is frequently unsatisfactory due to the complexity of the data analysis methods to be employed and to the sparseness of data to inform the choice of important parameters in these methods. We propose that among Monte Carlo methods, bootstrap approaches are most likely to generate data similar to the observed data. In bootstrap approaches, real data are resampled to build complete data sets based on real data that resemble the data for the intended analyses. In contrast, simulation methods would use the real data to estimate parameters for the data, and would then simulate data using these parameters. We describe means of implementing bootstrap power calculation. We demonstrate bootstrap power calculation for a cluster-randomized trial with a censored survival outcome and a baseline observation period. Bootstrap power calculation is a natural application of resampling methods. It provides a relatively simple solution to power calculation that is likely to be more accurate than analytic solutions or simulation-based calculations, in the sense that the bootstrap approach does not rely on the assumptions inherent in analytic calculations. This method of calculation has several important strengths. Notably, it is simple to achieve great fidelity to the proposed data analysis method and there is no requirement for parameter estimates, or estimates of their variability, from outside settings. So, for example, for cluster-randomized trials, power calculations need not depend on intracluster correlation coefficient estimates from outside studies. In contrast, bootstrap power calculation requires initial data that resemble data that are to be used in the planned study. We are not aware of bootstrap power calculation being previously proposed or explored for cluster-randomized trials, but it can also be applied for other study designs. We show with a simulation study that bootstrap power calculation can replicate analytic power in cases where analytic power can be accurately calculated. We also demonstrate power calculations for correlated censored survival outcomes in a cluster-randomized trial setting, for which we are unaware of an analytic alternative. The method can easily be used when preliminary data are available, as is likely to be the case when research is performed in health delivery systems or other settings where electronic medical records can be obtained.-Calculating Power by Bootstrap, with an Application to Cluster-Randomized Trials.",1
"Paid media are important resources used to recruit subjects in clinical trials. An index for evaluating which advertising resource has minimal cost and time requirement for patient accrual, for a given study design, has not been previously introduced. In this communication the authors present a new index, the Cost-Time Index, which represents a measure of the average amount of money and time spent, simultaneously, on a given advertising resource to recruit one analyzable subject. This index can be calculated using retrospective data and may be a useful tool for comparing recruitment efficiencies among various resources. The authors demonstrate the utility of the Cost-Time Index and recommend its use as an additional variable in future studies regarding recruitment strategies in clinical trials.-The Cost-Time Index: a new method for measuring the efficiencies of recruitment-resources in clinical trials.",0
Design and Analysis of Cluster Randomization Trials in Health Research,1
"In multicenter studies, center-specific variations in measurements may arise for various reasons, such as low interrater reliability, differences in equipment, deviations from the protocol, sociocultural characteristics, and differences in patient populations due to e.g. local referral patterns. The aim of this research is to derive measures for the degree of clustering. We present a method to detect heavily clustered variables and to identify physicians with outlying measurements. We use regression models with fixed effects to account for patient case-mix and a random cluster intercept to study clustering by physicians. We propose to use the residual intraclass correlation (RICC), the proportion of residual variance that is situated at the cluster level, to detect variables that are influenced by clustering. An RICC of 0 indicates that the variance in the measurements is not due to variation between clusters. We further suggest, where appropriate, to evaluate RICC in combination with R2, the proportion of variance that is explained by the fixed effects. Variables with a high R2 may have benefits that outweigh the disadvantages of clustering in terms of statistical analysis. We apply the proposed methods to a dataset collected for the development of models for ovarian tumor diagnosis. We study the variability in 18 tumor characteristics collected through ultrasound examination, 4 patient characteristics, and the serum marker CA-125 measured by 40 physicians on 2407 patients. The RICC showed large variation between variables: from 2.2% for age to 25.1% for the amount of fluid in the pouch of Douglas. Seven variables had an RICC above 15%, indicating that a considerable part of the variance is due to systematic differences at the physician level, rather than random differences at the patient level. Accounting for differences in ultrasound machine quality reduced the RICC for a number of blood flow measurements. We recommend that the degree of data clustering is addressed during the monitoring and analysis of multicenter studies. The RICC is a useful tool that expresses the degree of clustering as a percentage. Specific applications are data quality monitoring and variable screening prior to the development of a prediction model.-Screening for data clustering in multicenter studies: the residual intraclass correlation.",1
"While designing a trial to evaluate a complex intervention, one may be confronted with the dilemma that randomization at the level of the individual patient risks contamination bias, whereas cluster randomization risks incomparability of study arms and recruitment problems. Literature provides only few solutions to this dilemma and these are not always feasible. As an alternative solution for this dilemma, we developed a new two-stage randomization method called pseudo cluster randomization. In the first stage, the clusters (e.g., recruiting physicians) are randomized into two groups: one group of clusters in which the majority of the participants (e.g., 80%) will receive the experimental treatment; one group of clusters in which the majority will receive the control condition. Following this, the second stage of the randomization involves randomly assigning participants within clusters in the proportions determined by the first stage. This has important advantages. Compared with cluster randomization the potential occurrence of baseline incomparability of treatment arms and poor recruitment is reduced, because the physicians who recruit the participants are unable to know in advance which treatment condition the next participant they recruit will be assigned to. Limiting the exposure of half of the physicians to the innovative intervention lowers risk of contamination bias. When this type of contamination bias is present, pseudo cluster randomization can be more efficient than individual or cluster randomization in that smaller number of study participants is needed to achieve a predefined power.-Pseudo cluster randomization: balancing the disadvantages of cluster and individual randomization.",1
"This work has investigated under what conditions cost-effectiveness data from a cluster randomized trial (CRT) are suitable for analysis using a cluster-adjusted nonparametric bootstrap. The bootstrap's main advantages are in dealing with skewed data and its ability to take correlations between costs and effects into account. However, there are known theoretical problems with a commonly used cluster bootstrap procedure, and the practical implications of these require investigation. Simulations were used to estimate the coverage of confidence intervals around incremental cost-effectiveness ratios from CRTs using two bootstrap methods. The bootstrap gave excessively narrow confidence intervals, but there was evidence to suggest that, when the number of clusters per treatment arm exceeded 24, it might give acceptable results. The method that resampled individuals as well as clusters did not perform well when cost and effectiveness data were correlated. If economic data from such trials are to be analyzed adequately, then there is a need for further investigations of more complex bootstrap procedures. Similarly, further research is required on methods such as the net benefit approach.-Cluster randomized trials: another problem for cost-effectiveness ratios.",1
"The generalized estimating equation (GEE) approach can be used to analyze cluster randomized trial data to obtain population-averaged intervention effects. However, most cluster randomized trials have some missing outcome data and a GEE analysis of available data may be biased when outcome data are not missing completely at random. Although multilevel multiple imputation for GEE (MMI-GEE) has been widely used, alternative approaches such as weighted GEE are less common in practice. Using both simulations and a real data example, we evaluate the performance of inverse probability weighted GEE vs. MMI-GEE for binary outcomes. Simulated data are generated assuming a covariate-dependent missing data pattern across a range of missingness clustering (from none to high), where all covariates are measured at baseline and are fully observed (i.e. a type of missing-at-random mechanism). Two types of weights are estimated and used in the weighted GEE: (1) assuming no clustering of missingness (W-GEE) and (2) accounting for such clustering (CW-GEE). Results show that, even in settings with high missingness clustering, CW-GEE can lead to more bias and lower coverage than W-GEE, whereas W-GEE and MMI-GEE provide comparable results. W-GEE should be considered a viable strategy to account for missing outcomes in cluster randomized trials.-Properties and pitfalls of weighting as an alternative to multilevel multiple imputation in cluster randomized trials with missing binary outcomes under covariate-dependent missingness",1
"Because dental implant failure patterns tend to cluster within subjects, we hypothesized that the risk of implant failure varies among subjects. To address this hypothesis in the setting of clustered, correlated observations, we considered a retrospective cohort study where we identified a cohort having at least one implant placed. The cohort was composed of 677 patients who had 2349 implants placed. To test the hypothesis, we applied an innovative analytic method, i.e., the Cox proportional hazards model with frailty, to account for correlation within subjects and the heterogeneity of risk, i.e., frailty, among subjects for implant failure. Consistent with our hypothesis, risk for implant failure among subjects varied to a statistically significantly degree (p=0.041). In addition, the risk for implant failure is significantly associated with several factors, including tobacco use, implant length, immediate implant placement, staging, well size, and proximity of adjacent implants or teeth.-Frailty approach for the analysis of clustered failure time observations in dental research.",1
"As the discipline of health services research has developed so methods of evaluation have become increasingly sophisticated; where once a 'simple' randomized controlled trial would have been considered the gold standard, now terms such as 'complex interventions' and the 'cluster randomized controlled trial' are hot topics for discussion. The challenges involved in carrying out such studies are rarely presented. In this paper we discuss some of these challenges in relation to ethical and statistical considerations, and illustrate them using a recently completed cluster randomized controlled trial of a decision tool for early labour.-So you want to conduct a cluster randomized controlled trial? Lessons from a national cluster trial of early labour.",1
"We estimate the effects of non-randomized time-varying treatments on the discrete-time hazard, using inverse weighting. We consider the special monotone pattern of treatment that develops over time as subjects permanently discontinue an initial treatment, and assume that treatment selection is sequentially ignorable. We use a propensity score in the hazard model to reduce the potential for finite-sample bias due to inverse weighting. When the number of subjects who discontinue treatment at any given time is small, we impose scientific restrictions on the potentially observable discontinuation hazards to improve efficiency. We use predictive inference to account for the correlation of the potential hazards, when comparing outcomes under different durations of initial treatment.-Using inverse weighting and predictive inference to estimate the effects of time-varying treatments on the discrete-time hazard.",0
"Matched-pair cluster randomization trials are frequently adopted as the design of choice for evaluating an intervention offered at the community level. However, previous research has demonstrated that a strategy of breaking the matches and performing an unmatched analysis may be more efficient than performing a matched analysis on the resulting data, particularly when the total number of communities is small and the matching is judged as relatively ineffective. The research concerning this question has naturally focused on testing the effect of intervention. However, a secondary objective of many community intervention trials is to investigate the effect of individual-level risk factors on one or more outcome variables. Focusing on the case of a continuous outcome variable, we show that the practice of performing an unmatched analysis on data arising from a matched-pair design can lead to bias in the estimated regression coefficient, and a corresponding test of significance which is overly liberal. However, for large-scale community intervention trials, which typically recruit a relatively small number of large clusters, such an analysis will generally be both valid and efficient. We also consider other approaches to testing the effect of an individual-level risk factor in a matched-pair cluster randomization design, including a generalized linear model approach that preserves the matching, a two-stage cluster-level analysis, and an approach based on generalized estimating equations.-The merits of breaking the matches: a cautionary tale.",1
"Mean-based semi-parametric regression models such as the popular generalized estimating equations are widely used to improve robustness of inference over parametric models. Unfortunately, such models are quite sensitive to outlying observations. The Wilcoxon-score-based rank regression (RR) provides more robust estimates over generalized estimating equations against outliers. However, the RR and its extensions do not sufficiently address missing data arising in longitudinal studies. In this paper, we propose a new approach to address outliers under a different framework based on the functional response models. This functional-response-model-based alternative not only addresses limitations of the RR and its extensions for longitudinal data, but, with its rank-preserving property, even provides more robust estimates than these alternatives. The proposed approach is illustrated with both real and simulated data. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-Rank-preserving regression: a more robust rank regression model against outliers.",0
"While randomized controlled trials (RCTs) are the gold standard for research, many research questions cannot be ethically and practically answered using an RCT. Comparative effectiveness research (CER) techniques are often better suited than RCTs to address the effects of an intervention under routine care conditions, an outcome otherwise known as effectiveness. CER research techniques covered in this section include: effectiveness-oriented experimental studies such as pragmatic trials and cluster randomized trials, treatment response heterogeneity, observational and database studies including adjustment techniques such as sensitivity analysis and propensity score analysis, systematic reviews and meta-analysis, decision analysis, and cost effectiveness analysis. Each section describes the technique and covers the strengths and weaknesses of the approach.-Approaches to answering critical CER questions.",1
"Cluster randomized controlled trials are increasingly used to evaluate medical interventions. Research has found that cluster size variability leads to a reduction in the overall effective sample size. Although reporting standards of cluster trials have started to evolve, a far greater degree of transparency is needed to ensure that robust evidence is presented. The use of the numbers of patients recruited to summarize recruitment rate should be avoided in favour of an improved metric that illustrates cumulative power and accounts for cluster variability. Data from four trials is included to show the link between cluster size variability and imbalance. Furthermore, using simulations it is demonstrated that by randomising using a two block randomization strategy and weighting the second by cluster size recruitment, chance imbalance can be minimized.-Cluster size variability and imbalance in cluster randomized controlled trials.",1
"Despite the known health risks of hypertension, many hypertensive patients still have uncontrolled blood pressure. Clinical inertia, the tendency of physicians not to intensify treatment, is a common barrier in controlling chronic diseases. This trial is aimed at determining the impact of activating patients to ask providers to make changes to their care through tailored feedback. Diagnosed hypertensive patients were enrolled in this RCT and randomized to one of two study groups: (1) the intervention condition--Web-based hypertension feedback, based on the individual patient's self-report of health variables and previous BP measurements, to prompt them to ask questions during their next physician's visit about hypertension care (2) the control condition--Web-based preventive health feedback, based on the individual's self-report of receiving preventive care (e.g., pap testing), to prompt them to ask questions during their next physician's visit about preventive care. The primary outcome of the study is change in blood pressure and change in the percentage of patients in each group with controlled blood pressure. Five hundred participants were enrolled and baseline characteristics include a mean age of 60.0 years; 57.6% female; and 77.6% white. Overall 37.7% participants had uncontrolled blood pressure; the mean body mass index (BMI) was in the obese range (32.4) and 21.8% had diabetes. By activating patients to become involved in their own care, we believe the addition of the web-based intervention will improve blood pressure control compared to a control group who receive web-based preventive messages unrelated to hypertension.-A web-based patient activation intervention to improve hypertension care: study design and baseline characteristics in the web hypertension study.",0
"In 2012, Weijer et?al published ""The Ottawa Statement on the ethical design and conduct of cluster randomized trials"". In 2015, we reflected on this statement and argued that three recommendations in this statement need to be further refined. Weijer and Taljaard have responded to our comments in this issue of the journal. They agree with one of the proposed revisions but not with two others. In this commentary, we argue that the main reason why there is disagreement about two of our refinements is that we have different views on the moral and legal status of the health care workers as ""research participants"" in cluster randomized trials (CRTs). In this commentary, we clarify misunderstandings about our view expressed in 2015 and elaborate on the positions of health care workers in CRTs. We argue that there is sufficient reason to doubt whether the rights and interests of health care workers (HCWs) should be protected by means of ethics guidance documents and laws on human subjects research. Their interests are protected in the first place by professional codes of conduct which ensure that they cannot provide substandard care. Furthermore, protection of HCWs by ethics guidance on human subjects research will create an enormous burden for principle investigators and research ethics committees. Further debate is essential to determine how the interests of HCWs in CRTs can be protected best.-The moral and legal status of Health Care Workers in Cluster Randomized Trials: a response to Weijer and Taljaard",1
"The assessment of methods for analyzing over-dispersed zero inflated count outcome has received very little or no attention in stratified cluster randomized trials. In this study, we performed sensitivity analyses to empirically compare eight methods for analyzing zero inflated over-dispersed count outcome from the Vitamin D and Osteoporosis Study (ViDOS) - originally designed to assess the feasibility of a knowledge translation intervention in long-term care home setting. Forty long-term care (LTC) homes were stratified and then randomized into knowledge translation (KT) intervention (19 homes) and control (21 homes) groups. The homes/clusters were stratified by home size (&lt;250/&gt;?=?250) and profit status (profit/non-profit). The outcome of this study was number of falls measured at 6-month post-intervention. The following methods were used to assess the effect of KT intervention on number of falls: i) standard Poisson and negative binomial regression; ii) mixed-effects method with Poisson and negative binomial distribution; iii) generalized estimating equation (GEE) with Poisson and negative binomial; iv) zero inflated Poisson and negative binomial - with the latter used as a primary approach. All these methods were compared with or without adjusting for stratification. A total of 5,478 older people from 40 LTC homes were included in this study. The mean (=1) of the number of falls was smaller than the variance (=6). Also 72% and 46% of the number of falls were zero in the control and intervention groups, respectively. The direction of the estimated incidence rate ratios (IRRs) was similar for all methods. The zero inflated negative binomial yielded the lowest IRRs and narrowest 95% confidence intervals when adjusted for stratification compared to GEE and mixed-effect methods. Further, the widths of the 95% confidence intervals were narrower when the methods adjusted for stratification compared to the same method not adjusted for stratification. The overall conclusion from the GEE, mixed-effect and zero inflated methods were similar. However, these methods differ in terms of effect estimate and widths of the confidence interval. ClinicalTrials.gov: NCT01398527. Registered: 19 July 2011.-An empirical comparison of methods for analyzing over-dispersed zero-inflated count data from stratified cluster randomized trials",1
"This article presents methods and inference for causal estimation in semiparametric transformation models for the prevalent survival data. Through the estimation of the transformation models and covariate distribution, we propose a few analytical procedures to estimate the causal survival function. As the data are observational, the unobserved potential outcome (survival time) may be associated with the treatment assignment, and therefore there may exist a systematic imbalance between the data observed from each treatment arm. Further, due to prevalent sampling, subjects are observed only if they have not experienced the failure event when data collection began, causing the prevalent sampling bias. We propose a unified approach, which simultaneously corrects the bias from the prevalent sampling and balances the systematic differences from the observational data. We illustrate in the simulation study that standard analysis without proper adjustment would result in biased causal inference. Large sample properties of the proposed estimation procedures are established by techniques of empirical processes and examined by simulation studies. The proposed methods are applied to the Surveillance, Epidemiology, and End Results (SEER) and Medicare-linked data for women diagnosed with breast cancer.-Causal estimation using semiparametric transformation models under prevalent sampling.",0
Some methodological issues in the design and analysis of cluster randomised trials.,1
"Implantable defibrillators are considered life-saving therapy in heart failure (CHF) patients. Surprisingly, the recent Sudden Cardiac Death in Heart Failure Trial (SCD-HeFT) reached an opposing conclusion from that of numerous other trials about their survival benefit in patients with advanced CHF. A critical analysis of common control trial design may explain this paradoxical finding, with important implications for future studies. Common control trials compare several intervention groups to a single rather than separate control groups. Though potentially requiring fewer patients than trials using separate controls, variation in the common control group will influence all comparisons and creates correlations between findings. During subgroup analyses, this dependency of outcomes may increase belief in the presence of a real subgroup effect when, in fact, it should increase skepticism. For example, a high (r = 0.92), statistically unlikely (p = 0.052) correlation between comparisons was observed across the subgroups reported in SCD-HeFT. Such concordance between amiodarone and a defibrillator across subgroups was unexpected, given how much the effects of these treatments significantly differed from one another in the main study. This suggests the study's subgroup findings (specifically the absence of benefit from defibrillators in advanced CHF) were not necessarily a consequence of treatment; more likely, they resulted from variation in what the treatments were compared against, the common control. Common control trials can be more efficient than other designs, but induce dependence between treatment comparisons and require cautious interpretation.-Pearls and perils of an implantable defibrillator trial using a common control: implications for the design of future studies.",0
"When multiple biomarkers are available for disease diagnosis, it is desirable to efficiently combine them to form a single index. Making use of the Neyman-Pearson paradigm, we propose a new combination/transformation approach to disease diagnosis that efficiently combines multiple biomarkers. The proposed method does not require that the biomarkers be jointly normally distributed or the covariance matrices for the diseased and the nondiseased are nondifferential. An R package is developed to implement the proposed method. Simulations and two real data examples demonstrate advantages of the new method over existing ones.-A new semiparametric transformation approach to disease diagnosis with multiple biomarkers.",0
"Group randomized trials (GRTs) randomize groups, or clusters, of people to intervention or control arms. To test for the effectiveness of the intervention when subject-level outcomes are binary, and while fitting a marginal model that adjusts for cluster-level covariates and utilizes a logistic link, we develop a pseudo-Wald statistic to improve inference. Alternative Wald statistics could employ bias-corrected empirical sandwich standard error estimates, which have received limited attention in the GRT literature despite their broad utility and applicability in our settings of interest. The test could also be carried out using popular approaches based upon cluster-level summary outcomes. A simulation study covering a variety of realistic GRT settings is used to compare the accuracy of these methods in terms of producing nominal test sizes. Tests based upon the pseudo-Wald statistic and a cluster-level summary approach utilizing the natural log of observed cluster-level odds worked best. Due to weighting, some popular cluster-level summary approaches were found to lead to invalid inference in many settings. Finally, although use of bias-corrected empirical sandwich standard error estimates did not consistently result in nominal sizes, they did work well, thus supporting the applicability of marginal models in GRT settings.-On small-sample inference in group randomized trials with binary outcomes and cluster-level covariates.",1
"Joint modeling of a primary response and a longitudinal process via shared random effects is widely used in many areas of application. Likelihood-based inference on joint models requires model specification of the random effects. Inappropriate model specification of random effects can compromise inference. We present methods to diagnose random effect model misspecification of the type that leads to biased inference on joint models. The methods are illustrated via application to simulated data, and by application to data from a study of bone mineral density in perimenopausal women and data from an HIV clinical trial.-Latent-model robustness in joint models for a primary endpoint and a longitudinal process.",0
"It is well known that the sample correlation coefficient (Rxy ) is the maximum likelihood estimator of the Pearson correlation (?xy ) for independent and identically distributed (i.i.d.) bivariate normal data. However, this is not true for ophthalmologic data where X (e.g., visual acuity) and Y (e.g., visual field) are available for each eye and there is positive intraclass correlation for both X and Y in fellow eyes. In this paper, we provide a regression-based approach for obtaining the maximum likelihood estimator of ?xy for clustered data, which can be implemented using standard mixed effects model software. This method is also extended to allow for estimation of partial correlation by controlling both X and Y for a vector U_ of other covariates. In addition, these methods can be extended to allow for estimation of rank correlation for clustered data by (i) converting ranks of both X and Y to the probit scale, (ii) estimating the Pearson correlation between probit scores for X and Y, and (iii) using the relationship between Pearson and rank correlation for bivariate normally distributed data. The validity of the methods in finite-sized samples is supported by simulation studies. Finally, two examples from ophthalmology and analgesic abuse are used to illustrate the methods. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-Estimation of rank correlation for clustered data.",1
"We present statistical considerations for the design of a 20-community randomized trial. The community intervention aims at multiple cancer prevention health behaviors including reducing dietary fat, increasing fruit and vegetable intake, smoking cessation, and increasing colorectal cancer screening. To better measure the overall impact of the intervention, individual endpoints as well as a global test of multiple endpoints are used. The statistical power analysis takes into account the heterogeneity between communities, the correlation between health behaviors over time, and the correlation between multiple endpoints. The study is being conducted in a relatively confined geographic area. Several measures have been taken to account for potential contamination. These include collecting information in baseline and follow-up surveys on intervention dose and conducting surveys in three similar communities in another part of the state.-Some design issues in a community intervention trial.",1
"This paper discusses causal inference with survival data from cluster randomized trials. It is argued that cluster randomization carries the potential for post-randomization exposures which involve differentially selective compliance between treatment arms, even for an all or nothing exposure at the individual level. Structural models can be employed to account for post-randomization exposures, but should not ignore clustering. We show how marginal modelling and random effects models allow to adapt structural estimators to account for clustering. Our findings are illustrated with data from a vitamin A trial for the prevention of infant mortality in the rural plains of Nepal.-Accounting for correlation and compliance in cluster randomized trials.",1
"The frequency of cluster-randomized trials (CRTs) in peer-reviewed literature has increased exponentially over the past two decades. CRTs are a valuable tool for studying interventions that cannot be effectively implemented or randomized at the individual level. However, some aspects of the design and analysis of data from CRTs are more complex than those for individually randomized controlled trials. One of the key components to designing a successful CRT is calculating the proper sample size (i.e. number of clusters) needed to attain an acceptable level of statistical power. In order to do this, a researcher must make assumptions about the value of several variables, including a fixed mean cluster size. In practice, cluster size can often vary dramatically. Few studies account for the effect of cluster size variation when assessing the statistical power for a given trial. We conducted a simulation study to investigate how the statistical power of CRTs changes with variable cluster sizes. In general, we observed that increases in cluster size variability lead to a decrease in power.-The effect of cluster size variability on statistical power in cluster-randomized trials.",1
"In group-randomized trials, a frequent practical limitation to adopting rigorous research designs is that only a small number of groups may be available, and therefore, simple randomization cannot be relied upon to balance key group-level prognostic factors across the comparison arms. Constrained randomization is an allocation technique proposed for ensuring balance and can be used together with a permutation test for randomization-based inference. However, several statistical issues have not been thoroughly studied when constrained randomization is considered. Therefore, we used simulations to evaluate key issues including the following: the impact of the choice of the candidate set size and the balance metric used to guide randomization; the choice of adjusted versus unadjusted analysis; and the use of model-based versus randomization-based tests. We conducted a simulation study to compare the type I error and power of the F-test and the permutation test in the presence of group-level potential confounders. Our results indicate that the adjusted F-test and the permutation test perform similarly and slightly better for constrained randomization relative to simple randomization in terms of power, and the candidate set size does not substantially affect their power. Under constrained randomization, however, the unadjusted F-test is conservative, while the unadjusted permutation test carries the desired type I error rate as long as the candidate set size is not too small; the unadjusted permutation test is consistently more powerful than the unadjusted F-test and gains power as candidate set size changes. Finally, we caution against the inappropriate specification of permutation distribution under constrained randomization. An ongoing group-randomized trial is used as an illustrative example for the constrained randomization design.-An evaluation of constrained randomization for the design and analysis of group-randomized trials.",1
"To compare three statistical strategies for classifying positive treatment response based on a dimensional measure (Yale Global Tic Severity Scale [YGTSS]) and a categorical measure (Clinical Global Impression-Improvement [CGI-I] scale). Subjects (N=232; 69.4% male; ages 9-69years) with Tourette syndrome or chronic tic disorder participated in one of two 10-week, randomized controlled trials comparing behavioral treatment to supportive therapy. The YGTSS and CGI-I were rated by clinicians blind to treatment assignment. We examined the percent reduction in the YGTSS-Total Tic Score (TTS) against Much Improved or Very Much Improved on the CGI-I, computed a signal detection analysis (SDA) and built a mixture model to classify dimensional response based on the change in the YGTSS-TTS. A 25% decrease on the YGTSS-TTS predicted positive response on the CGI-I during the trial. The SDA showed that a 25% reduction in the YGTSS-TTS provided optimal sensitivity (87%) and specificity (84%) for predicting positive response. Using a mixture model without consideration of the CGI-I, the dimensional response was defined by 23% (or greater) reduction on the YGTSS-TTS. The odds ratio (OR) of positive response (OR=5.68, 95% CI=[2.99, 10.78]) on the CGI-I for behavioral intervention was greater than the dimensional response (OR=2.86, 95% CI=[1.65, 4.99]). A 25% reduction on the YGTSS-TTS is highly predictive of positive response by all three analytic methods. For trained raters, however, tic severity alone does not drive the classification of positive response. Clinicaltrials.gov identifiers: NCT00218777; NCT00231985.-Detecting a clinically meaningful change in tic severity in Tourette syndrome: a comparison of three methods.",0
"A new clinical trial design, designated the two-way enriched design (TED), is introduced, which augments the standard randomized placebo-controlled trial with second-stage enrichment designs in placebo non-responders and drug responders. The trial is run in two stages. In the first stage, patients are randomized between drug and placebo. In the second stage, placebo non-responders are re-randomized between drug and placebo and drug responders are re-randomized between drug and placebo. All first-stage data, and second-stage data from first-stage placebo non-responders and first-stage drug responders, are utilized in the efficacy analysis. The authors developed one, two and three degrees of freedom score tests for treatment effect in the TED and give formulae for asymptotic power and for sample size computations. The authors compute the optimal allocation ratio between drug and placebo in the first stage for the TED and compare the operating characteristics of the design to the standard parallel clinical trial, placebo lead-in and randomized withdrawal designs. Two motivating examples from different disease areas are presented to illustrate the possible design considerations.-A two-way enriched clinical trial design: combining advantages of placebo lead-in and randomized withdrawal.",0
How large are teacher effects?,2
"In Cambodia, HIV prevalence is concentrated in key populations including among female entertainment workers (FEWs) who may engage in direct or indirect sex work. Reaching FEWs with sexual and reproductive health (SRH) services has been difficult because of their hidden and stigmatized nature. Mobile-phone-based interventions may be an effective way to reach this population and connect them with the existing services. This article describes study design and implementation of a randomized controlled trial (RCT) of a mobile health intervention (the Mobile Link) aiming to improve SRH and related outcomes among FEWs in Cambodia. A two-arm RCT will be used to determine the effectiveness of a mobile-phone-based text/voice messaging intervention. The intervention will be developed through a participatory process. Focus group discussions and in-depth interviews have been conducted to inform and tailor behavior change theory-based text and voice messages. During the implementation phase, 600 FEWs will be recruited and randomly assigned into one of the two arms: (1) a control group and (2) a mobile phone message group (either text messages [SMS] or voice messages [VM], a delivery method chosen by participants). Participants in the control group will also receive a weekly monitoring survey, which will provide real-time information to implementing partners to streamline outreach efforts and be able to quickly identify geographic trends. The primary outcome measures will include self-reported HIV and sexually transmitted infections (STI) testing and treatment, condom use, contraceptive use, and gender-based violence (GBV). If the Mobile Link trial is successful, participants will report an increase in condom use, linkages to screening and treatment for HIV and STI, and contraception use as well as a reduction in GBV. This trial is unique in a number of ways. First, the option of participation mode (SMS or VM) allows participants to choose the message medium that best links them to services. Second, this is the first RCT of a mobile-phone-based behavior change intervention using SMS/VMs to support linkage to SRH services in Cambodia. Lastly, we are working with a hidden, hard-to-reach, and dynamic population with which existing methods of outreach have not been fully successful. Clinical trials.gov, NCT03117842 . Registered on 31 March 2017.-Mobile Link - a theory-based messaging intervention for improving sexual and reproductive health of female entertainment workers in Cambodia: study protocol of a randomized controlled trial.",0
"Cluster randomization design is increasingly used for the evaluation of health-care, screening or educational interventions. The intraclass correlation coefficient (ICC) defines the clustering effect and be specified during planning. The aim of this work is to study the influence of the ICC on power in cluster randomized trials. Power contour graphs were drawn to illustrate the loss in power induced by an underestimation of the ICC when planning trials. We also derived the maximum achievable power given a specified ICC. The magnitude of the ICC can have a major impact on power, and with low numbers of clusters, 80% power may not be achievable. Underestimating the ICC during planning cluster randomized trials can lead to a seriously underpowered trial. Publication of a priori postulated and a posteriori estimated ICCs is necessary for a more objective reading: negative trial results may be the consequence of a loss of power due to a mis-specification of the ICC.-A priori postulated and real power in cluster randomized trials: mind the gap.",1
"Futility (inefficacy) interim monitoring is an important component in the conduct of phase III clinical trials, especially in life-threatening diseases. Desirable futility monitoring guidelines allow timely stopping if the new therapy is harmful or if it is unlikely to demonstrate to be sufficiently effective if the trial were to continue to its final analysis. There are a number of analytical approaches that are used to construct futility monitoring boundaries. The most common approaches are based on conditional power, sequential testing of the alternative hypothesis, or sequential confidence intervals. The resulting futility boundaries vary considerably with respect to the level of evidence required for recommending stopping the study. We evaluate the performance of commonly used methods using event histories from completed phase III clinical trials of the Radiation Therapy Oncology Group, Cancer and Leukemia Group B, and North Central Cancer Treatment Group. We considered published superiority phase III trials with survival endpoints initiated after 1990. There are 52 studies available for this analysis from different disease sites. Total sample size and maximum number of events (statistical information) for each study were calculated using protocol-specified effect size, type I and type II error rates. In addition to the common futility approaches, we considered a recently proposed linear inefficacy boundary approach with an early harm look followed by several lack-of-efficacy analyses. For each futility approach, interim test statistics were generated for three schedules with different analysis frequency, and early stopping was recommended if the interim result crossed a futility stopping boundary. For trials not demonstrating superiority, the impact of each rule is summarized as savings on sample size, study duration, and information time scales. For negative studies, our results show that the futility approaches based on testing the alternative hypothesis and repeated confidence interval rules yielded less savings (compared to the other two rules). These boundaries are too conservative, especially during the first half of the study (&lt;50% of information). The conditional power rules are too aggressive during the second half of the study (&gt;50% of information) and may stop a trial even when there is a clinically meaningful treatment effect. The linear inefficacy boundary with three or more interim analyses provided the best results. For positive studies, we demonstrated that none of the futility rules would have stopped the trials. The linear inefficacy boundary futility approach is attractive from statistical, clinical, and logistical standpoints in clinical trials evaluating new anti-cancer agents.-Comparison of futility monitoring guidelines using completed phase III oncology trials.",0
"To use the rs1229984 variant in the alcohol dehydrogenase 1B gene (ADH1B) as an instrument to investigate the causal role of alcohol in cardiovascular disease. Mendelian randomisation meta-analysis of 56 epidemiological studies. 261 991 individuals of European descent, including 20 259 coronary heart disease cases and 10 164 stroke events. Data were available on ADH1B rs1229984 variant, alcohol phenotypes, and cardiovascular biomarkers. Odds ratio for coronary heart disease and stroke associated with the ADH1B variant in all individuals and by categories of alcohol consumption. Carriers of the A-allele of ADH1B rs1229984 consumed 17.2% fewer units of alcohol per week (95% confidence interval 15.6% to 18.9%), had a lower prevalence of binge drinking (odds ratio 0.78 (95% CI 0.73 to 0.84)), and had higher abstention (odds ratio 1.27 (1.21 to 1.34)) than non-carriers. Rs1229984 A-allele carriers had lower systolic blood pressure (-0.88 (-1.19 to -0.56) mm Hg), interleukin-6 levels (-5.2% (-7.8 to -2.4%)), waist circumference (-0.3 (-0.6 to -0.1) cm), and body mass index (-0.17 (-0.24 to -0.10) kg/m(2)). Rs1229984 A-allele carriers had lower odds of coronary heart disease (odds ratio 0.90 (0.84 to 0.96)). The protective association of the ADH1B rs1229984 A-allele variant remained the same across all categories of alcohol consumption (P=0.83 for heterogeneity). Although no association of rs1229984 was identified with the combined subtypes of stroke, carriers of the A-allele had lower odds of ischaemic stroke (odds ratio 0.83 (0.72 to 0.95)). Individuals with a genetic variant associated with non-drinking and lower alcohol consumption had a more favourable cardiovascular profile and a reduced risk of coronary heart disease than those without the genetic variant. This suggests that reduction of alcohol consumption, even for light to moderate drinkers, is beneficial for cardiovascular health.-Association between alcohol and cardiovascular disease: Mendelian randomisation analysis based on individual participant data.",0
"In a stepped wedge study design, study clusters usually start with the baseline treatment and then cross over to the intervention at randomly determined times. Such designs are useful when the intervention must be delivered at the cluster level and are becoming increasingly common in practice. In these trials, if the outcome is death or serious morbidity, one may have an ethical imperative to monitor the trial and stop before maximum enrollment if the new therapy is proven to be beneficial. In addition, because formal monitoring allows for the stoppage of trials when a significant benefit for new therapy has been ruled out, their use can make a research program more efficient. However, use of the stepped wedge cluster randomized study design complicates the implementation of standard group sequential monitoring methods. Both the correlation of observations introduced by the clustered randomization and the timing of crossover from one treatment to the other impact the rate of information growth, an important component of an interim analysis. We simulated cross-sectional stepped wedge study data in order to evaluate the impact of sequential monitoring on the Type I error and power when the true intracluster correlation is unknown. We studied the impact of varying intracluster correlations, treatment effects, methods of estimating the information growth, and boundary shapes. While misspecified information growth can impact both the Type I error and power of a study in some settings, we observed little inflation of the Type I error and only moderate reductions in power across a range of misspecified information growth patterns in our simulations. Taking the study design into account and using either an estimate of the intracluster correlation from the ongoing study or other data in the same clusters should allow for easy implementation of group sequential methods in future stepped wedge designs.-Information growth for sequential monitoring of clinical trials with a stepped wedge cluster randomized design and unknown intracluster correlation",3
"Inferences for the difference between two dependent intraclass correlation coefficients (ICCs) may arise in studies in which a sample of subjects are each assessed several times with a new device and a standard. The ICC estimates for the two devices may then be compared using a test of significance. However, a confidence interval for a difference between two ICCs is more informative since it combines point estimation and hypothesis testing into a single inference statement. We propose a procedure that uses confidence limits for a single ICC to recover variance estimates needed to set confidence limits for the difference. An advantage of this approach is that it provides a confidence interval that reflects the underlying sampling distribution. Simulation results show that this method performs very well in terms of overall coverage percentage and tail errors. Two data sets are used to illustrate this procedure.-Confidence interval construction for a difference between two dependent intraclass correlation coefficients.",1
"A Bayesian hierarchical modelling approach to the analysis of cluster randomized trials has advantages in terms of allowing for full parameter uncertainty, flexible modelling of covariates and variance structure, and use of prior information. Previously, such modelling of binary outcome data required use of a log-odds ratio scale for the treatment effect estimate and an approximation linking the intracluster correlation (ICC) to the between-cluster variance on a log-odds scale. In this paper we develop this method to allow estimation on the absolute risk scale, which facilitates clinical interpretation of both the treatment effect and the between-cluster variance. We describe a range of models and apply them to data from a trial of different interventions to promote secondary prevention of coronary heart disease in primary care. We demonstrate how these models can be used to incorporate prior data about typical ICCs, to derive a posterior distribution for the number needed to treat, and to consider both cluster and individual level covariates. Using these methods, we can benefit from the advantages of Bayesian modelling of binary outcome data at the same time as providing results on a clinically interpretable scale.-Bayesian methods for analysis of binary outcome data in cluster randomized trials on the absolute risk scale.",1
Design and Analysis of Group-Randomized Trials,1
"This study aims to investigate the influence of the amount of clustering [intraclass correlation (ICC) = 0%, 5%, or 20%], the number of events per variable (EPV) or candidate predictor (EPV = 5, 10, 20, or 50), and backward variable selection on the performance of prediction models. Researchers frequently combine data from several centers to develop clinical prediction models. In our simulation study, we developed models from clustered training data using multilevel logistic regression and validated them in external data. The amount of clustering was not meaningfully associated with the models' predictive performance. The median calibration slope of models built in samples with EPV = 5 and strong clustering (ICC = 20%) was 0.71. With EPV = 5 and ICC = 0%, it was 0.72. A higher EPV related to an increased performance: the calibration slope was 0.85 at EPV = 10 and ICC = 20% and 0.96 at EPV = 50 and ICC = 20%. Variable selection sometimes led to a substantial relative bias in the estimated predictor effects (up to 118% at EPV = 5), but this had little influence on the model's performance in our simulations. We recommend at least 10 EPV to fit prediction models in clustered data using logistic regression. Up to 50 EPV may be needed when variable selection is performed.-A simulation study of sample size demonstrated the importance of the number of events per variable to develop prediction models in clustered data.",1
"A threshold effect takes place in situations where the relationship between an outcome variable and a predictor variable changes as the predictor value crosses a certain threshold/change point. Threshold effects are often plausible in a complex biological system, especially in defining immune responses that are protective against infections such as HIV-1, which motivates the current work. We study two hypothesis testing problems in change point models. We first compare three different approaches to obtaining a p-value for the maximum of scores test in a logistic regression model with change point variable as a main effect. Next, we study the testing problem in a logistic regression model with the change point variable both as a main effect and as part of an interaction term. We propose a test based on the maximum of likelihood ratios test statistic and obtain its reference distribution through a Monte Carlo method. We also propose a maximum of weighted scores test that can be more powerful than the maximum of likelihood ratios test when we know the direction of the interaction effect. In simulation studies, we show that the proposed tests have a correct type I error and higher power than several existing methods. We illustrate the application of change point model-based testing methods in a recent study of immune responses that are associated with the risk of mother to child transmission of HIV-1.-Change point testing in logistic regression models with interaction term.",0
"We consider the problem of sample size determination for count data. Such data arise naturally in the context of multicenter (or cluster) randomized clinical trials, where patients are nested within research centers. We consider cluster-specific and population-averaged estimators (maximum likelihood based on generalized mixed-effect regression and generalized estimating equations, respectively) for subject-level and cluster-level randomized designs, respectively. We provide simple expressions for calculating the number of clusters when comparing event rates of two groups in cross-sectional studies. The expressions we derive have closed-form solutions and are based on either between-cluster variation or intercluster correlation for cross-sectional studies. We provide both theoretical and numerical comparisons of our methods with other existing methods. We specifically show that the performance of the proposed method is better for subject-level randomized designs, whereas the comparative performance depends on the rate ratio for the cluster-level randomized designs. We also provide a versatile method for longitudinal studies. Three real data examples illustrate the results.-Sample size determination for clustered count data.",1
"We consider the problem of estimating covariate effects in the marginal Cox proportional hazard model and multilevel associations for child mortality data collected from a vitamin A supplementation trial in Nepal, where the data are clustered within households and villages. For this purpose, a class of multivariate survival models that can be represented by a functional of marginal survival functions and accounts for hierarchical structure of clustering is exploited. Based on this class of models, an estimation strategy involving a within-cluster resampling procedure is proposed, and a model assessment approach is presented. The asymptotic theory for the proposed estimators and lack-of-fit test is established. The simulation study shows that the estimates are approximately unbiased, and the proposed test statistic is conservative under extremely heavy censoring but approaches the size otherwise. The analysis of the Nepal study data shows that the association of mortality is much greater within households than within villages.-Analysis of failure time data with multilevel clustering, with application to the child vitamin a intervention trial in Nepal.",1
"Patients with rheumatoid arthritis spend most of their daily hours in sedentary behavior (sitting), a predisposing factor to poor health-related outcomes and all-cause mortality. Interventions focused on reducing sedentary time could be of novel therapeutic relevance. However, studies addressing this topic remain scarce. We aim to investigate the feasibility and efficacy of a newly developed intervention focused on reducing sedentary time, and potential clinical, physiological, metabolic and molecular effects in rheumatoid arthritis. The Take a STAND for Health study is a 4-month, parallel-group, randomized controlled trial, in which postmenopausal patients with rheumatoid arthritis will set individually tailored, progressive goals to replace their sedentary time with standing and light-intensity activities. Patients will be recruited from the Clinical Hospital (School of Medicine, University of Sao Paulo) and will be assessed at baseline and after a 4-month follow up. Outcomes will include objectively measured sedentary behavior (primary outcome) and physical activity levels, clinical parameters, anthropometric parameters and body composition; aerobic fitness, muscle function, blood pressure, cardiovascular autonomic function, vascular function and structure, health-related quality of life, and food intake. Blood and muscle samples will be collected for assessing potential mechanisms, through targeted and non-targeted approaches. Findings will be of scientific and clinical relevance with the potential to inform new prescriptions focused on reducing sedentary behavior, a modifiable risk factor that thus far has been overlooked in patients with rheumatoid arthritis. ClinicalTrials.gov, NCT03186924. Registered on 14 June 2017.-A randomized controlled trial to reduce sedentary time in rheumatoid arthritis: protocol and rationale of the Take a STAND for Health study.",0
"Osteoporotic hip fractures in the elderly are associated with a high mortality in the first year following fracture and a high incidence of disability among survivors. We study first and second fractures of elderly women using data from the Study of Osteoporotic Fractures. We present a new conceptual framework, stochastic model, and statistical methodology for time to fracture. Our approach gives additional insights into the patterns for first and second fractures and the concomitant risk factors. Our modeling perspective involves a novel time-to-event methodology called threshold regression, which is based on the plausible idea that many events occur when an underlying process describing the health or condition of a person or system encounters a critical boundary or threshold for the first time. In the parlance of stochastic processes, this time to event is a first hitting time of the threshold. The underlying process in our model is a composite of a chronic degradation process for skeletal health combined with a random stream of shocks from external traumas, which taken together trigger fracture events.-A model for time to fracture with a shock stream superimposed on progressive degradation: the Study of Osteoporotic Fractures.",0
"Familial aggregation studies are a common first step in the identification of genetic determinants of disease. If aggregation is found, more refined genetic studies may be undertaken. Complex ascertainment schemes are frequently employed to ensure that the sample contains a sufficient number of families with multiple affected members, as required to detect aggregation. For example, an eligibility criterion for a family might be that both the mother and daughter have disease. Adjustments must be made for ascertainment to avoid bias. We propose adjusting for complex ascertainment schemes through a joint model for the outcomes of disease and ascertainment. This approach improves upon previous simplifying assumptions regarding the ascertainment process.-Analysis of familial aggregation studies with complex ascertainment schemes.",0
"To investigate the extent to which cluster sizes vary in stepped-wedge cluster randomised trials (SW-CRT) and whether any variability is accounted for during the sample size calculation and analysis of these trials. Any, not limited to healthcare settings. Any taking part in an SW-CRT published up to March 2016. The primary outcome is the variability in cluster sizes, measured by the coefficient of variation (CV) in cluster size. Secondary outcomes include the difference between the cluster sizes assumed during the sample size calculation and those observed during the trial, any reported variability in cluster sizes and whether the methods of sample size calculation and methods of analysis accounted for any variability in cluster sizes. Of the 101 included SW-CRTs, 48% mentioned that the included clusters were known to vary in size, yet only 13% of these accounted for this during the calculation of the sample size. However, 69% of the trials did use a method of analysis appropriate for when clusters vary in size. Full trial reports were available for 53 trials. The CV was calculated for 23 of these: the median CV was 0.41 (IQR: 0.22-0.52). Actual cluster sizes could be compared with those assumed during the sample size calculation for 14 (26%) of the trial reports; the cluster sizes were between 29% and 480% of that which had been assumed. Cluster sizes often vary in SW-CRTs. Reporting of SW-CRTs also remains suboptimal. The effect of unequal cluster sizes on the statistical power of SW-CRTs needs further exploration and methods appropriate to studies with unequal cluster sizes need to be employed.-Unequal cluster sizes in stepped-wedge cluster randomised trials: a systematic review.",3
"COMMIT (Community Intervention Trial for Smoking Cessation) is a randomized study employing a matched pairs design. Pairs of communities were selected on the basis of their geographical proximity and were chosen to be matched on variables strongly expected to relate to the outcome variable, the smoking quit rate. However, quantitative information was not available to evaluate the efficiency gain from matching. We have used baseline smoking quit rates in the communities as a surrogate for the outcome measure to evaluate the gain in efficiency from the matching. Our method takes account of the possible imperfection of the surrogate as a representative of the true outcome. The method estimates an efficiency gain of at least 50 per cent using the matched design. We also evaluate the further gains in efficiency which would be made by using the baseline quit rate to balance the randomization.-Assessing the gain in efficiency due to matching in a community intervention study.",1
"The purpose of this paper is to summarize current practices for the design and analysis of group-randomized trials involving cancer-related risk factors or outcomes and to offer recommendations to improve future trials. We searched for group-randomized trials involving cancer-related risk factors or outcomes that were published or online in peer-reviewed journals in 2011-15. During 2016-17, in Bethesda MD, we reviewed 123 articles from 76 journals to characterize their design and their methods for sample size estimation and data analysis. Only 66 (53.7%) of the articles reported appropriate methods for sample size estimation. Only 63 (51.2%) reported exclusively appropriate methods for analysis. These findings suggest that many investigators do not adequately attend to the methodological challenges inherent in group-randomized trials. These practices can lead to underpowered studies, to an inflated type 1 error rate, and to inferences that mislead readers. Investigators should work with biostatisticians or other methodologists familiar with these issues. Funders and editors should ensure careful methodological review of applications and manuscripts. Reviewers should ensure that studies are properly planned and analyzed. These steps are needed to improve the rigor and reproducibility of group-randomized trials. The Office of Disease Prevention (ODP) at the National Institutes of Health (NIH) has taken several steps to address these issues. ODP offers an online course on the design and analysis of group-randomized trials. ODP is working to increase the number of methodologists who serve on grant review panels. ODP has developed standard language for the Application Guide and the Review Criteria to draw investigators' attention to these issues. Finally, ODP has created a new Research Methods Resources website to help investigators, reviewers, and NIH staff better understand these issues.-Design and analysis of group-randomized trials in cancer: A review of current practices.",1
Experimental Social Epidemiology - Controlled Community Trials,1
"We examined the relationship between timing of poverty and risk of first-incidence obesity from ages 3 to 15.5 years. We used the National Institute of Child Health and Human Development Study of Early Child Care and Youth Development (1991-2007) to study 1150 children with repeated measures of income, weight, and height from birth to 15.5 years in 10 US cities. Our dependent variable was the first incidence of obesity (body mass index ? 95th percentile). We measured poverty (income-to-needs ratio &lt; 2) prior to age 2 years and a lagged, time-varying measure of poverty between ages 2 and 12 years. We estimated discrete-time hazard models of the relative risk of first transition to obesity. Poverty prior to age 2 years was associated with risk of obesity by age 15.5 years in fully adjusted models. These associations did not vary by gender. Our findings suggest that there are enduring associations between early life poverty and adolescent obesity. This stage in the life course may serve as a critical period for both poverty and obesity prevention.-Longitudinal associations between poverty and obesity from birth through adolescence.",0
"Medical research studies utilize survey instruments consisting of responses to multiple items combined into one or more scales. These studies can benefit from methods for evaluating those scales. Such an approach is presented for evaluating exploratory and confirmatory factor analysis models with decisions about covariance structure, including the number of factors, the factor extraction procedure, the allocation of survey items to summated scales and the extent of inter-scale dependence, made objectively using a likelihood-based form of cross-validation. This approach is demonstrated through example analyses using baseline data for three survey instruments from a clinical trial involving adolescents with type 1 diabetes.-Factor analysis model evaluation through likelihood cross-validation.",0
"Although previous research has discussed an effect size estimator for partially nested cluster randomized designs, the existing estimator (a) is not efficient when used with primary data, (b) can be biased when the homogeneity of variance assumption is violated, and (c) has not yet been empirically evaluated for its finite sample properties. The present paper addresses these limitations by proposing an alternative maximum likelihood estimator for obtaining standardized mean difference effect size and the corresponding sampling variance for partially nested data, as well as the variants that do not make an assumption of homogeneity of variance. The typical estimator, denoted as d (dW with pooled SD and dC with control arm SD), requires input of summary statistics such as observed means, variances, and the intraclass correlation, and is useful for meta-analyses and secondary data analyses; the newly proposed estimator [Formula: see text] ([Formula: see text] and [Formula: see text]) takes parameter estimates from a correctly specified multilevel model as input and is mainly of interest to researchers doing primary research. The simulation results showed that the two methods (d and [Formula: see text]) produced unbiased point and variance estimates for effect size. As expected, in general, [Formula: see text] was more efficient than d with unequal cluster sizes, especially with large average cluster size and large intraclass correlation. Furthermore, under heterogeneous variances, [Formula: see text] demonstrated a greater relative efficiency with small sample size for the unclustered control arm. Real data examples, one from a youth preventive program and one from an eating disorder intervention, were used to demonstrate the methods presented. In addition, we extend the discussion to a scenario with a three-level treatment arm and an unclustered control arm, and illustrate the procedures for effect size estimation using a hypothetical example of multiple therapy groups of clients clustered within therapists.-Estimating Standardized Effect Sizes for Two- and Three-Level Partially Nested Data.",2
"Cancer has traditionally been studied using the disease site of origin as the organizing framework. However, recent advances in molecular genetics have begun to challenge this taxonomy, as detailed molecular profiling of tumors has led to discoveries of subsets of tumors that have profiles that possess distinct clinical and biological characteristics. This is increasingly leading to research that seeks to investigate whether these subtypes of tumors have distinct etiologies. However, research in this field has been opportunistic and anecdotal, typically involving the comparison of distributions of individual risk factors between tumors classified on the basis of candidate tumor characteristics. The purpose of this article is to place this area of investigation within a more general conceptual and analytic framework, with a view to providing more efficient and practical strategies for designing and analyzing epidemiologic studies to investigate etiologic heterogeneity. We propose a formal definition of etiologic heterogeneity and show how classifications of tumor subtypes with larger etiologic heterogeneities inevitably possess greater disease risk predictability overall. We outline analytic strategies for estimating the degree of etiologic heterogeneity among a set of subtypes and for choosing subtypes that optimize the heterogeneity, and we discuss technical challenges that require further methodologic research. We illustrate the ideas by using a pooled case-control study of breast cancer classified by expression patterns of genes known to define distinct tumor subtypes.-A conceptual and methodological framework for investigating etiologic heterogeneity.",0
"Previous research has compared methods of estimation for fitting multilevel models to binary data, but there are reasons to believe that the results will not always generalize to the ordinal case. This article thus evaluates (a) whether and when fitting multilevel linear models to ordinal outcome data is justified and (b) which estimator to employ when instead fitting multilevel cumulative logit models to ordinal data, maximum likelihood (ML), or penalized quasi-likelihood (PQL). ML and PQL are compared across variations in sample size, magnitude of variance components, number of outcome categories, and distribution shape. Fitting a multilevel linear model to ordinal outcomes is shown to be inferior in virtually all circumstances. PQL performance improves markedly with the number of ordinal categories, regardless of distribution shape. In contrast to binary data, PQL often performs as well as ML when used with ordinal data. Further, the performance of PQL is typically superior to ML when the data include a small to moderate number of clusters (i.e., ? 50 clusters).-Fitting multilevel models with ordinal outcomes: performance of alternative specifications and methods of estimation.",0
"To assess the association of dietary fatty acids with cardiovascular disease mortality and total mortality among patients with type 2 diabetes. Prospective, longitudinal cohort study. Health professionals in the United States. 11 264 participants with type 2 diabetes in the Nurses' Health Study (1980-2014) and Health Professionals Follow-Up Study (1986-2014). Dietary fat intake assessed using validated food frequency questionnaires and updated every two to four years. Total and cardiovascular disease mortality during follow-up. During follow-up, 2502 deaths including 646 deaths due to cardiovascular disease were documented. After multivariate adjustment, intake of polyunsaturated fatty acids (PUFAs) was associated with a lower cardiovascular disease mortality, compared with total carbohydrates: hazard ratios comparing the highest with the lowest quarter were 0.76 (95% confidence interval 0.58 to 0.99; P for trend=0.03) for total PUFAs, 0.69 (0.52 to 0.90; P=0.007) for marine n-3 PUFAs, 1.13 (0.85 to 1.51) for ?-linolenic acid, and 0.75 (0.56 to 1.01) for linoleic acid. Inverse associations with total mortality were also observed for intakes of total PUFAs, n-3 PUFAs, and linoleic acid, whereas monounsaturated fatty acids of animal, but not plant, origin were associated with a higher total mortality. In models that examined the theoretical effects of substituting PUFAs for other fats, isocalorically replacing 2% of energy from saturated fatty acids with total PUFAs or linoleic acid was associated with 13% (hazard ratio 0.87, 0.77 to 0.99) or 15% (0.85, 0.73 to 0.99) lower cardiovascular disease mortality, respectively. A 2% replacement of energy from saturated fatty acids with total PUFAs was associated with 12% (hazard ratio 0.88, 0.83 to 0.94) lower total mortality. In patients with type 2 diabetes, higher intake of PUFAs, in comparison with carbohydrates or saturated fatty acids, is associated with lower total mortality and cardiovascular disease mortality. These findings highlight the important role of quality of dietary fat in the prevention of cardiovascular disease and total mortality among adults with type 2 diabetes.-Dietary fats and mortality among patients with type 2 diabetes: analysis in two population based cohort studies.",0
"This article investigates maximum likelihood estimation with saturated and unsaturated models for correlated exchangeable binary data, when a sample of independent clusters of varying sizes is available. We discuss various parameterizations of these models, and propose using the EM algorithm to obtain maximum likelihood estimates. The methodology is illustrated by applications to a study of familial disease aggregation and to the design of a proposed group randomized cancer prevention trial.-Likelihood inference for exchangeable binary data with varying cluster sizes.",1
"Ovarian cancer is the most lethal gynecologic malignancy in both African-American and white women. Although prevalences of many ovarian cancer risk factors differ markedly between African Americans and whites, there has been little research on how the relative contributions of risk factors may vary between racial/ethnic groups. Using data from a North Carolina case-control study (1999-2008), the authors conducted unconditional logistic regression analyses to calculate odds ratios and 95% confidence intervals for ovarian cancer risk factors in African-American (143 cases, 189 controls) and white (943 cases, 868 controls) women and to test for interactions by race/ethnicity. They also calculated attributable fractions within each racial/ethnic group for the modifiable factors of pregnancy, oral contraceptive use, tubal ligation, and body mass index. Many risk factors showed similar relations across racial/ethnic groups, but tubal ligation and family history of breast or ovarian cancer showed stronger associations among African Americans. Younger age at menarche was associated with risk only in white women. Attributable fractions associated with tubal ligation, oral contraceptive use, and obesity were markedly higher for African Americans. The relative importance of ovarian cancer risk factors may differ for African-American women, but conclusions were limited by the small sample. There is a clear need for further research on etiologic factors for ovarian cancer in African-American women.-Ovarian cancer risk factors in African-American and white women.",0
"For the pathogenesis of complex diseases, gene-environment (G-E) interactions have been shown to have important implications. G-E interaction analysis can be challenging with the need to jointly analyze a large number of main effects and interactions and to respect the ""main effects, interactions"" hierarchical constraint. Extensive methodological developments on G-E interaction analysis have been conducted in recent literature. Despite considerable successes, most of the existing studies are still limited as they cannot accommodate long-tailed distributions/data contamination, make the restricted assumption of linear effects, and cannot effectively accommodate missingness in E variables. To directly tackle these problems, a semiparametric model is assumed to accommodate nonlinear effects, and the Huber loss function and Qn estimator are adopted to accommodate long-tailed distributions/data contamination. A regression-based multiple imputation approach is developed to accommodate missingness in E variables. For model estimation and selection of relevant variables, we adopt an effective sparse boosting approach. The proposed approach is practically well motivated, has intuitive formulations, and can be effectively realized. In extensive simulations, it significantly outperforms multiple direct competitors. The analysis of The Cancer Genome Atlas data on stomach adenocarcinoma and cutaneous melanoma shows that the proposed approach makes sensible discoveries with satisfactory prediction and stability.-Robust semiparametric gene-environment interaction analysis using sparse boosting.",0
"Repeated measurement designs have been widely used in various randomized controlled trials for evaluating long-term intervention efficacies. For some clinical trials, the primary research question is how to compare two treatments at a fixed time, using a t-test. Although simple, robust, and convenient, this type of analysis fails to utilize a large amount of collected information. Alternatively, the mixed-effects model is commonly used for repeated measurement data. It models all available data jointly and allows explicit assessment of the overall treatment effects across the entire time spectrum. In this paper, we propose an analytic strategy for longitudinal clinical trial data where the mixed-effects model is coupled with a model selection scheme. The proposed test statistics not only make full use of all available data but also utilize the information from the optimal model deemed for the data. The performance of the proposed method under various setups, including different data missing mechanisms, is evaluated via extensive Monte Carlo simulations. Our numerical results demonstrate that the proposed analytic procedure is more powerful than the t-test when the primary interest is to test for the treatment effect at the last time point. Simulations also reveal that the proposed method outperforms the usual mixed-effects model for testing the overall treatment effects across time. In addition, the proposed framework is more robust and flexible in dealing with missing data compared with several competing methods. The utility of the proposed method is demonstrated by analyzing a clinical trial on the cognitive effect of testosterone in geriatric men with low baseline testosterone levels.-On model selections for repeated measurement data in clinical studies.",0
"Surprisingly, survival from a diagnosis of lung cancer has been found to be longer for those who experienced a previous cancer than for those with no previous cancer. A possible explanation is lead-time bias, which, by advancing the time of diagnosis, apparently extends survival among those with a previous cancer even when they enjoy no real clinical advantage. We propose a discrete parametric model to jointly describe survival in a no-previous-cancer group (where, by definition, lead-time bias cannot exist) and in a previous-cancer group (where lead-time bias is possible). We model the lead time with a negative binomial distribution and the post-lead-time survival with a linear spline on the logit hazard scale, which allows for survival to differ between groups even in the absence of bias; we denote our model Logit-Spline/Negative Binomial. We fit Logit-Spline/Negative Binomial to a propensity-score matched subset of the Surveillance, Epidemiology, and End Results-Medicare linked data set, conducting sensitivity analyses to assess the effects of key assumptions. With lung cancer-specific death as the end point, the estimated mean lead time is roughly 11 months for stage I&amp;II patients; with overall survival, it is roughly 3.4 months in stage I&amp;II. For patients with higher-stage lung cancers, the mean lead time is 1 month or less for both outcomes. Accounting for lead-time bias reduces the survival advantage of the previous-cancer group when one exists, but it does not nullify it in all cases.-Estimating lead-time bias in lung cancer diagnosis of patients with previous cancers.",0
Comparing the performance of different multiple imputation strategies for missing binary outcomes in cluster randomized trials: a simulation study,1
"Two-stage designs have been widely used in phase II clinical trials. Such designs are desirable because they allow a decision to be made on whether a treatment is effective or not after the accumulation of the data at the end of each stage. Optimal fixed two-stage designs, where the sample size at each stage is fixed in advance, were proposed by Simon when the primary outcome is a binary response. This paper proposes an adaptive two-stage design which allows the sample size at the second stage to depend on the results at the first stage. Using a Bayesian decision-theoretic construct, we derive optimal adaptive two-stage designs; the optimality criterion being minimum expected sample size under the null hypothesis. Comparisons are made between Simon's two-stage fixed design and the new design with respect to this optimality criterion.-Adaptive two-stage designs in phase II clinical trials.",0
"Evaluating vaccine efficacy for protection against colonization with bacterial pathogens is an area of growing interest. Colonization of the nasopharynx is an asymptomatic carrier state responsible for person-to-person transmission. It differs from most clinical outcomes in that it is common, recurrent, and observed only in its prevalent state. To estimate rates of acquisition and clearance of colonization requires repeated active sampling of the same individuals over time, an expensive and invasive undertaking. Motivated by feasibility constraints in efficacy trials with colonization endpoints, investigators have been estimating vaccine efficacy from cross-sectional studies without principled methods. We present two examples of vaccine studies estimating vaccine efficacy from cross-sectional data on nasopharyngeal colonization by Streptococcus pneumoniae (pneumococcus). This study presents a framework for defining and estimating strain-specific and overall vaccine efficacy for susceptibility to acquisition of colonization (VE(acq)) when there is a large number of strains with mutual interactions and recurrent dynamics of colonization. We develop estimators based on one observation of the current status per study subject, evaluate their robustness, and re-analyze the two vaccine trials. Methodologically, the proposed estimators are closely related to case-control studies with prevalent cases, with appropriate consideration of the at-risk time in choosing the controls.-Estimating strain-specific and overall efficacy of polyvalent vaccines against recurrent pathogens from a cross-sectional study.",0
"Training data in a supervised learning problem consist of the class label and its potential predictors for a set of observations. Constructing effective classifiers from training data is the goal of supervised learning. In biomedical sciences and other scientific applications, class labels may be subject to errors. We consider a setting where there are two classes but observations with labels corresponding to one of the classes may in fact be mislabeled. The application concerns the use of protein mass-spectrometry data to discriminate between serum samples from cancer and noncancer patients. The patients in the training set are classified on the basis of tissue biopsy. Although biopsy is 100% specific in the sense that a tissue that shows itself to have malignant cells is certainly cancer, it is less than 100% sensitive. Reference gold standards that are subject to this special type of misclassification due to imperfect diagnosis certainty arise in many fields. We consider the development of a supervised learning algorithm under these conditions and refer to it as partially supervised learning. Boosting is a supervised learning algorithm geared toward high-dimensional predictor data, such as those generated in protein mass-spectrometry. We propose a modification of the boosting algorithm for partially supervised learning. The proposal is to view the true class membership of the samples that are labeled with the error-prone class label as missing data, and apply an algorithm related to the EM algorithm for minimization of a loss function. To assess the usefulness of the proposed method, we artificially mislabeled a subset of samples and applied the original and EM-modified boosting (EM-Boost) algorithms for comparison. Notable improvements in misclassification rates are observed with EM-Boost.-Partially supervised learning using an EM-boosting algorithm.",0
"To investigate the familial clustering of postpartum haemorrhage in the Swedish population, and to quantify the relative contributions of genetic and environmental effects. Register based cohort study. Swedish population (multi-generation and medical birth registers). Postpartum haemorrhage, defined as &gt;1000 mL estimated blood loss. The first two live births to individuals in Sweden in 1997-2009 contributed to clusters representing intact couples (n = 366,350 births), mothers with separate partners (n = 53,292), fathers with separate partners (n = 47,054), sister pairs (n = 97,228), brother pairs (n = 91,168), and mixed sibling pairs (n = 177,944). Familial clustering was quantified through cluster specific tetrachoric correlation coefficients, and the influence of potential sharing of known risk factors was evaluated with alternating logistic regression. Relative contributions of genetic and environmental effects to the variation in liability for postpartum haemorrhage were quantified with generalised linear mixed models. The overall prevalence of postpartum haemorrhage after vaginal deliveries in our sample was 4.6%. Among vaginal deliveries, 18% (95% confidence interval 9% to 26%) of the variation in postpartum haemorrhage liability was attributed to maternal genetic factors, 10% (1% to 19%) to unique maternal environment, and 11% (0% to 26%) to fetal genetic effects. Adjustment for known risk factors only partially explained estimates of familial clustering, suggesting that the observed shared genetic and environmental effects operate in part through pathways independent of known risk factors. There were similar patterns of familial clustering for both of the main subtypes examined (atony and retained placenta), though strongest for haemorrhage after retained placenta. There is a maternal genetic predisposition to postpartum haemorrhage, but more than half of the total variation in liability is attributable to factors that are not shared in families.-Genetic contribution to postpartum haemorrhage in Swedish population: cohort study of 466,686 births.",0
"There has been an increased drive towards Evidence Based Policing in recent years. Unlike in other public sector services, such as health and education, randomised controlled trials in the police setting are relatively rare. This paper discusses some of the methodological and practical challenges of conducting a randomised controlled trial in the police setting in the UK, based on our experience of the Connect trial. This pragmatic, cluster-randomised controlled trial investigated the effectiveness of a face-to-face training intervention for frontline officers in comparison to routine training. The primary outcome was the number of incidents which resulted in a police response reported to North Yorkshire Police control room in a 1-month period up to 6 months after delivery of training. The methodological and practical challenges that we experienced whilst conducting the Connect trial are discussed under six headings: establishing the unit of randomisation; population of interest and sample size; co-production of evidence; time frame; outcomes; and organisational issues. Recommendations on the conduct of future randomised controlled trials in the police setting are made. To understand the context in which research is undertaken, collaboration between police and academia is needed and police officers should be embedded within trial management groups. Engagement with police data analysts to understand what data is available and facilitate obtaining trial data is also recommended. Police forces may wish to review their IT systems and recording practices. Pragmatic trials are encouraged and time frames need to allow for trial set-up and obtaining relevant ethical approvals. ISRCTN Registry, ID: ISRCTN11685602 . Retrospectively registered on 13 May 2016.-Undertaking a randomised controlled trial in the police setting: methodological and practical challenges.",1
"In stepped wedge designs (SWD), clusters are randomized to the time period during which new patients will receive the intervention under study in a sequential rollout over time. By the study's end, patients at all clusters receive the intervention, eliminating ethical concerns related to withholding potentially efficacious treatments. This is a practical option in many large-scale public health implementation settings. Little statistical theory for these designs exists for binary outcomes. To address this, we utilized a maximum likelihood approach and developed numerical methods to determine the asymptotic power of the SWD for binary outcomes. We studied how the power of a SWD for detecting risk differences varies as a function of the number of clusters, cluster size, the baseline risk, the intervention effect, the intra-cluster correlation coefficient, and the time effect. We studied the robustness of power to the assumed form of the distribution of the cluster random effects, as well as how power is affected by variable cluster size. % SWD power is sensitive to neither, in contrast to the parallel cluster randomized design which is highly sensitive to variable cluster size. We also found that the approximate weighted least square approach of Hussey and Hughes (2007, Design and analysis of stepped wedge cluster randomized trials. Contemporary Clinical Trials 28, 182-191) for binary outcomes under-estimates the power in some regions of the parameter spaces, and over-estimates it in others. The new method was applied to the design of a large-scale intervention program on post-partum intra-uterine device insertion services for preventing unintended pregnancy in the first 1.5 years following childbirth in Tanzania, where it was found that the previously available method under-estimated the power.-A maximum likelihood approach to power calculations for stepped wedge designs of binary outcomes.",3
Invited commentary: symposium on community intervention trials.,1
"A method is developed for the Bayesian analysis of rate ratios of repeated events in matched cluster-randomized trials, and applied to the Flies and Eyes trial against trachoma in The Gambia. A non-mechanistic analysis estimates the incidence rate ratio for insecticide spray, relative to no intervention, as 0.59 (95 per cent credible interval 0.37--0.92), and for latrine provision as 0.67 (0.41--1.01). The method also yields incidence and clearance rate ratios for age. In the Flies and Eyes trial, the former rates are estimated to decrease with age, although no clear pattern is seen in the latter. Using clinical active trachoma as a marker of infection, and net catches to measure fly density, a mechanistic model is developed to estimate the proportion of transmission due to flies. However, it was not possible to estimate this proportion accurately, and a lack of association was noted between the magnitudes of reductions in fly density and in trachoma. The lack of knowledge on the relative importance of different transmission routes is a constraint on the current global programme for the elimination of trachoma.-Analysis of incidence rates in cluster-randomized trials of interventions against recurrent infections, with an application to trachoma.",1
"The generalized odds-rate model is a class of semiparametric regression models, which includes the proportional hazards and proportional odds models as special cases. There are few works on estimation of the generalized odds-rate model with interval censored data because of the challenges in maximizing the complex likelihood function. In this paper, we propose a gamma-Poisson data augmentation approach to develop an Expectation Maximization algorithm, which can be used to fit the generalized odds-rate model to interval censored data. The proposed Expectation Maximization algorithm is easy to implement and is computationally efficient. The performance of the proposed method is evaluated by comprehensive simulation studies and illustrated through applications to datasets from breast cancer and hemophilia studies. In order to make the proposed method easy to use in practice, an R package 'ICGOR' was developed. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-An Expectation Maximization algorithm for fitting the generalized odds-rate model to interval censored data.",0
"Experiments involving large social units, such as schools, work sites, or whole cities, are commonly limited in statistical power because the number of randomized units is small, leaving few degrees of freedom for residual (between-unit) error. The authors describe a method for increasing residual degrees of freedom in a community experiment without substantially increasing cost or difficulty. In brief, they propose that the experimental units should be divided into random subsamples (batches). Batch sampling can improve statistical power if the community endpoint means are stable over time or if their temporal variation is comparable in period to the batch-sampling schedule. The authors demonstrate the theoretical advantages of the batch system and illustrate its use with data from the Pawtucket Heart Health Program, in which such a design was implemented.-Batch sampling to improve power in a community trial. Experience from the Pawtucket Heart Health Program.",1
"Dichotomizing a continuous variable is known to result in the loss of information, lower statistical power, and lower reliability. In many epidemiological studies, age is a scaled (continuous) variable prior to statistical analyses; however, despite pleas from methodologists, researchers frequently dichotomize age in their data analysis without an appropriate rationale. Using simulated case-control data, we show that dichotomizing age generally will lead to a biased odds ratio (OR). When age was a confounder (potentially representing common causes of risks and outcomes), including age as a scaled variable (whether the age effect was linear or non-linear in the logit), provided satisfactory control, whereas when age was categorized, the estimated risk factor effect was biased. We also demonstrate that the further the cutpoint is from the median age, the greater the increase in the OR; thus, in cases where age dichotomization is warranted, researchers are cautioned not to allow the size of the empirical OR to influence their choice of cutpoint. Recommendations are made for analysing age in epidemiological data and interpretation of empirical findings.-Biased odds ratios from dichotomization of age.",0
"Censored failure time data with a cured subgroup is frequently encountered in many scientific areas including the cancer screening research, tumorigenicity studies, and sociological surveys. Meanwhile, one may also encounter an extraordinary large number of risk factors in practice, such as patient's demographic characteristics, clinical measurements, and medical history, which makes variable selection an emerging need in the data analysis. Motivated by a medical study on prostate cancer screening, we develop a variable selection method in the semiparametric nonmixture or promotion time cure model when interval-censored data with a cured subgroup are present. Specifically, we propose a penalized likelihood approach with the use of the least absolute shrinkage and selection operator, adaptive least absolute shrinkage and selection operator, or smoothly clipped absolute deviation penalties, which can be easily accomplished via a novel penalized expectation-maximization algorithm. We assess the finite-sample performance of the proposed methodology through extensive simulations and analyze the prostate cancer screening data for illustration.-Variable selection in semiparametric nonmixture cure model with interval-censored failure time data: An application to the prostate cancer screening study.",0
"Stepped-wedge cluster-randomised trials: level of evidence, feasibility and reporting.",3
"Cluster-randomized trials, in which health interventions are allocated randomly to intact clusters or communities rather than to individual subjects, are increasingly being used to evaluate disease control strategies both in industrialized and in developing countries. Sample size computations for such trials need to take into account between-cluster variation, but field epidemiologists find it difficult to obtain simple guidance on such procedures. In this paper, we provide simple formulae for sample size determination for both unmatched and pair-matched trials. Outcomes considered include rates per person-year, proportions and means. For simplicity, formulae are expressed in terms of the coefficient of variation (SD/mean) of cluster rates, proportions or means. Guidance is also given on the estimation of this value, with or without the use of prior data on between-cluster variation. The methods are illustrated using two case studies: an unmatched trial of the impact of impregnated bednets on child mortality in Kenya, and a pair-matched trial of improved sexually-transmitted disease (STD) treatment services for HIV prevention in Tanzania.-Simple sample size calculation for cluster-randomized trials.",1
"We investigated methods of including covariates in two-level models for cluster randomized trials to increase power to detect the treatment effect. We compared multilevel models that included either an observed cluster mean or a latent cluster mean as a covariate, as well as the effect of including Level 1 deviation scores in the model. A Monte Carlo simulation study was performed manipulating effect sizes, cluster sizes, number of clusters, intraclass correlation of the outcome, patterns of missing data, and the squared correlations between Level 1 and Level 2 covariates and the outcome. We found no substantial difference between models with observed means or latent means with respect to convergence, Type I error rates, coverage, and bias. However, coverage could fall outside of acceptable limits if a latent mean is included as a covariate when cluster sizes are small. In terms of statistical power, models with observed means performed similarly to models with latent means, but better when cluster sizes were small. A demonstration is provided using data from a study of the Tools for Getting Along intervention.-The Effects of Including Observed Means or Latent Means as Covariates in Multilevel Models for Cluster Randomized Trials.",1
Consort 2010 statement: extension to cluster randomised trials.,1
"We examined the prevalence and correlates of substance use, dependence, and service utilization among uninsured persons aged 12 to 64 years. We drew study data from the 1998 National Household Survey on Drug Abuse. An estimated 80% of uninsured nonelderly persons reported being uninsured for more than 6 months in the prior year. Only 9% of these uninsured persons who were dependent on alcohol or drugs had received any substance abuse service in the past year. Non-Hispanic Whites were an estimated 3 times more likely than Blacks to receive substance abuse services. Compared with the privately insured, uninsured persons had increased odds of having alcohol/drug dependence and appeared to face substantial barriers to health services for substance use problems.-Substance use, dependence, and service utilization among the US uninsured nonelderly population.",0
"Inferences from multilevel models can be complicated in small samples or complex data structures. When using (restricted) maximum likelihood methods to estimate multilevel models, standard errors and degrees of freedom often need to be adjusted to ensure that inferences for fixed effects are correct. These adjustments do not address problems in estimating variance/covariance components. An alternative to the adjusted likelihood method is to use Bayesian methods, which can produce accurate inferences about fixed effects and variance/covariance parameters. In this article, the authors contrast the benefits and limitations of likelihood and Bayesian methods in the estimation of multilevel models. The issues are discussed in the context of a partially clustered intervention study, a common intervention design that has been shown to require an adjusted likelihood analysis. The authors report a Monte Carlo study that compares the performance of an adjusted restricted maximum likelihood (REML) analysis to a Bayesian analysis. The results suggest that for fixed effects, the models perform equally well with respect to bias, efficiency, and coverage of interval estimates. Bayesian models with a carefully selected gamma prior for the variance components were more biased but also more efficient with respect to estimation of the variance components than the REML model. However, the results also show that the inferences about the variance components in partially clustered studies are sensitive to the prior distribution when sample sizes are small. Finally, the authors compare the results of a Bayesian and adjusted likelihood model using data from a partially clustered intervention trial.-Bayesian methods for the analysis of small sample multilevel data with a complex variance structure.",2
"Publication bias and related biases can lead to overly optimistic conclusions in systematic reviews. The funnel plot, which is frequently used to detect such biases, has not yet been subjected to empirical evaluation as a visual tool. We sought to determine whether researchers can correctly identify publication bias from visual inspection of funnel plots in typical-size systematic reviews. A questionnaire with funnel plots containing 10 studies each (the median number in medical meta-analyses) was completed by 41 medical researchers, including clinical research fellows in a meta-analysis class, faculty in clinical care research, and experienced systematic reviewers. On average, participants correctly identified 52.5% (95% CI 50.6-54.4%) of the plots as being affected or unaffected by publication bias. The weighted mean percent correct, which adjusted for the fact that asymmetric plots are more likely to occur in the presence of publication bias, was also low (48.3 to 62.8%, depending on the presence or absence of publication bias and heterogeneous study effects). Researchers who assess for publication bias using the funnel plot may be misled by its shape. Authors and readers of systematic reviews need to be aware of the limitations of the funnel plot.-In an empirical evaluation of the funnel plot, researchers could not visually identify publication bias.",0
"Cluster-randomized clinical trials (CRT) are trials in which the unit of randomization is not a participant but a group (e.g. healthcare systems or community centers). They are suitable when the intervention applies naturally to the cluster (e.g. healthcare policy); when lack of independence among participants may occur (e.g. nursing home hygiene); or when it is most ethical to apply an intervention to all within a group (e.g. school-level immunization). Because participants in the same cluster receive the same intervention, CRT may approximate clinical practice, and may produce generalizable findings. However, when not properly designed or interpreted, CRT may induce biased results. CRT designs have features that add complexity to statistical estimation and inference. Chief among these is the cluster-level correlation in response measurements induced by the randomization. A critical consideration is the experimental unit of inference; often it is desirable to consider intervention effects at the level of the individual rather than the cluster. Finally, given that the number of clusters available may be limited, simple forms of randomization may not achieve balance between intervention and control arms at either the cluster- or participant-level. In non-clustered clinical trials, balance of key factors may be easier to achieve because the sample can be homogenous by exclusion of participants with multiple chronic conditions (MCC). CRTs, which are often pragmatic, may eschew such restrictions. Failure to account for imbalance may induce bias and reducing validity. This article focuses on the complexities of randomization in the design of CRTs, such as the inclusion of patients with MCC, and imbalances in covariate factors across clusters.-The Method of Randomization for Cluster-Randomized Trials: Challenges of Including Patients with Multiple Chronic Conditions.",1
"Many analyses of observational data are attempts to emulate a target trial. The emulation of the target trial may fail when researchers deviate from simple principles that guide the design and analysis of randomized experiments. We review a framework to describe and prevent biases, including immortal time bias, that result from a failure to align start of follow-up, specification of eligibility, and treatment assignment. We review some analytic approaches to avoid these problems in comparative effectiveness or safety research.-Specifying a target trial prevents immortal time bias and other self-inflicted injuries in observational analyses.",0
"We extend the marginalized transition model of Heagerty to accommodate non-ignorable monotone drop-out. Using a selection model, weakly identified drop-out parameters are held constant and their effects evaluated through sensitivity analysis. For data missing at random (MAR), efficiency of inverse probability of censoring weighted generalized estimating equations (IPCW-GEE) is as low as 40 per cent compared to a likelihood-based marginalized transition model (MTM) with comparable modelling burden. MTM and IPCW-GEE regression parameters both display misspecification bias for MAR and non-ignorable missing data, and both reduce bias noticeably by improving model fit.-Marginalized transition models for longitudinal binary data with ignorable and non-ignorable drop-out.",0
"The debate regarding 'When to Start' antiretroviral therapy has raged since the introduction of zidovudine in 1987. Based on the entry criteria for the original Burroughs Wellcome 002 study, the field has been anchored to CD4 cell counts as the prime metric to indicate treatment initiation for asymptomatic individuals infected with Human Immunodeficiency Virus. The pendulum has swung back and forth based mostly on the relative efficacy, toxicity and convenience of available regimens. In today's world, several factors have converged that compel us to initiate therapy as soon as possible: 1) The biology of viral replication (1 to 10 billion viruses per day) strongly suggests that we should be starting early. 2) Resultant inflammation from unchecked replication is associated with earlier onset of multiple co-morbid conditions. 3) The medications available today are more efficacious and less toxic than years past. 4) Clinical trials have demonstrated benefits for all but the highest CD4 strata (&gt;500 cells/?l). 5) Some cohort studies have demonstrated the clear benefit of antiretroviral therapy at any CD4 count and no cohort studies have demonstrated that early therapy is more detrimental than late therapy at the population level. 6) In addition to the demonstrated and inferred benefits to the individual patient, we now have evidence of a Public Health benefit from earlier intervention: treatment is prevention. From a practical, common sense perspective we are talking about life-long therapy. Whether we start at a CD4 count of 732 cells/?l or 493 cells/?l, the patient will be on therapy for over 40 to 50 years. There does not seem to be much benefit in waiting and there likely is significant long-term harm. Do not wait. Treat early.-When to start antiretroviral therapy: as soon as possible.",0
"Consideration of gene-environment (GxE) interaction is becoming increasingly important in the design of new epidemiologic studies. We present a method for computing required sample size or power to detect GxE interaction in the context of three specific designs: the standard matched case-control; the case-sibling, and the case-parent designs. The method is based on computation of the expected value of the likelihood ratio test statistic, assuming that the data will be analysed using conditional logistic regression. Comparisons of required sample sizes indicate that the family-based designs (case-sibling and case-parent) generally require fewer matched sets than the case-control design to achieve the same power for detecting a GxE interaction. The case-sibling design is most efficient when studying a dominant gene, while the case-parent design is preferred for a recessive gene. Methods are also presented for computing sample size when matched sets are obtained from a stratified population, for example, when the population consists of multiple ethnic groups. A software program that implements the method is freely available, and may be downloaded from the website http://hydra.usc.edu/gxe.-Sample size requirements for matched case-control studies of gene-environment interaction.",0
"This article presents the results of a meta-analysis designed to test the prevailing view that we largely understand why adolescents start to smoke and how to delay it. This view has developed even though none of the major reviews of the last 12 years has adjusted for the important methodological problems that all of those reviews identified as common in the published literature. School-based smoking prevention programs based on peer or social-type programs, published between 1974 and 1991, were included in this meta-analysis. Treatment characteristics were used to predict an effect size after adjustment for study design and population characteristics, and in particular, after a post hoc correction for errors in the original unit of analysis. The results suggest that the average effect for peer or social-type programs is likely to be quite limited in magnitude, and that the reduction in smoking may be only 0.10 standard deviation units, or perhaps 5%. Even under optimal conditions, the reduction in smoking may be only 0.50 to 0.75 standard deviation units, or perhaps 20%-30%.-A meta-analysis of smoking prevention programs after adjustment for errors in the unit of analysis.",1
Response to: Comment About Statistical Analysis of a Cluster-Randomized Trial About Clustering and Nesting (DOI: 10.1089/chi.2019.0142),1
"Serious coronary heart disease (CHD) is a primary outcome in the Whitehall II study, a large epidemiological study of British civil servants. Both fatal (F) and non-fatal (NF) CHD events are of interest and while essentially complete information is available on F events, the observation of NF events is subject to potentially informative censoring. A multi-state model with an unobserved state is introduced for the joint modelling of F and NF events. Two model-based assumptions ensure identifiability of the model and a parameter is introduced to allow sensitivity analyses concerning the assumption linked to informative censoring. Weibull transition rates, which include dependence on explanatory variables, are used in the analysis of Whitehall II data with a particular focus on the relationship between civil service grade and CHD events.-A multi-state model for joint modelling of terminal and non-terminal events with application to Whitehall II.",0
"Studies of HIV dynamics in AIDS research are very important in understanding the pathogenesis of HIV-1 infection and also in assessing the effectiveness of antiviral therapies. Nonlinear mixed-effects (NLME) models have been used for modeling between-subject and within-subject variations in viral load measurements. Mostly, normality of both within-subject random error and random-effects is a routine assumption for NLME models, but it may be unrealistic, obscuring important features of between-subject and within-subject variations, particularly, if the data exhibit skewness. In this paper, we develop a Bayesian approach to NLME models and relax the normality assumption by considering both model random errors and random-effects to have a multivariate skew-normal distribution. The proposed model provides flexibility in capturing a broad range of non-normal behavior and includes normality as a special case. We use a real data set from an AIDS study to illustrate the proposed approach by comparing various candidate models. We find that the model with skew-normality provides better fit to the observed data and the corresponding estimates of parameters are significantly different from those based on the model with normality when skewness is present in the data. These findings suggest that it is very important to assume a model with skew-normal distribution in order to achieve robust and reliable results, in particular, when the data exhibit skewness.-Skew-normal Bayesian nonlinear mixed-effects models with application to AIDS studies.",0
"Data on therapeutic decision making have a multilevel structure that can include patient-, provider-, and facility-level variables. A statistical method is presented for attributing explained variation in patient care to different levels of aggregation in a multilevel model with the aim of prioritizing and targeting quality improvement interventions. The proposed method is used in an analysis of adherence to evidence-based guidelines for the care of patients at risk of osteoporosis. Explained variation from a multilevel model of appropriate care is partitioned across patient-, physician-, and clinic-level factors. The combination of patient, physician, and clinic factors explained 20.0% of the variation in patient care. Individual physician effects explained 14.0% of the variation in the data; however, more than half of this explained variation could have been attributed to the individual clinic effect. Patient fixed effects alone explained 13.4% of the variation in the observed clinical decisions. The proposed approach is an intuitive and statistically valid method for attributing explained variation in a multilevel analysis of therapeutic decision making.-Explained variation in a model of therapeutic decision making is partitioned across patient, physician, and clinic factors.",0
"This paper introduces an improved tool for designing matched-pairs randomized trials. The tool allows the incorporation of clinical and other knowledge regarding the relative importance of variables used in matching and allows for multiple types of missing data. The method is illustrated in the context of a cluster-randomized trial. A Web application and an R package are introduced to implement the method and incorporate recent advances in the area. Reweighted Mahalanobis distance (RMD) matching incorporates user-specified weights and imputed values for missing data. Weight may be assigned to missingness indicators to match on missingness patterns. Three examples are presented, using real data from a cohort of 90 Veterans Health Administration sites that had at least 100 incident metformin users in 2007. Matching is utilized to balance seven factors aggregated at the site level. Covariate balance is assessed for 10,000 randomizations under each strategy: simple randomization, matched randomization using the Mahalanobis distance, and matched randomization using the RMD. The RMD matching achieved better balance than simple randomization or MD randomization. In the first example, simple and MD randomization resulted in a 10% chance of seeing an absolute mean difference of greater than 26% in the percent of nonwhite patients per site; the RMD dramatically reduced that to 6%. The RMD achieved significant improvement over simple randomization even with as much as 20% of the data missing. Reweighted Mahalanobis distance matching provides an easy-to-use tool that incorporates user knowledge and missing data.-Reweighted Mahalanobis distance matching for cluster-randomized trials with missing data.",1
"A reliable and meaningful quantitative index of success is paramount in the trial of any new treatment. However, existing methods for defining response and remission for treatments tested for psychiatric disorders are limited in that they often minimize the variance in change over time among individual patients and generally use arbitrarily chosen levels of functioning at specified times during treatment. To suggest and determine the properties of an alternative measure of treatment success, the Illness Density Index (IDI), that may be more sensitive to fluctuations in symptoms over the course of treatment compared to existing measures. We examined data from 64 depressed patients with multiple assessments of the Hamilton Depression Rating Scale (HDRS) over 12 weeks of randomized treatment in order to compare and contrast varying numerical definitions of response and remission, including percent change and linear slope over time. Examination of the indices comparing the within-sample rank of individual patients revealed that these indices agree in cases where patients have little or no response as well as clear and sustained response, while they differ in patients who have a slow (or late) response as well as relapse during the treatment course. The measure may not be useful for all types of studies, especially short-term treatment trials. The IDI is highly correlated with both categorical (e.g., remission) and continuous (e.g., percent change) definitions of treatment success. Furthermore, it differentiates certain trajectories of change that current definitions do not. Thus, the proposed index may be a valuable addition to current measures of efficacy, especially when trying to identify biological substrates of illness or predictors of long-term outcome.-The Illness Density Index (IDI): A longitudinal measure of treatment efficacy.",0
"Genetic epidemiologists often gather outcome-dependent samples of family data to measure within-family associations of genetic factors with disease outcomes. Generalized linear mixed models provide effective methods to estimate within-family associations but typically require parametric specification of the random effects distribution. Although misspecification of the random effects distribution often leads to little bias in estimated regression coefficients in standard, prospective clustered data settings, some recent studies suggest that such misspecification will impact parameter estimates from outcome-dependent cluster sampling designs. Using analytic results, simulation studies and fits to example data, this study examines the effect of misspecification of random effects distributions on parameter estimates in clustered data settings with outcome-dependent sampling. We show that the effects are consistent with results from prospective cluster sampling settings. In particular, ascertainment corrected mixed model methods that assume normally distributed random intercepts and conditional likelihood approaches provide accurate estimates of within-family covariate effects even under a misspecified random effects distribution.-The effect of misspecification of random effects distributions in clustered data settings with outcome-dependent sampling.",1
"In laboratory validation studies, it is often important to assess agreement between two assays, based on different techniques. Oftentimes, both assays have lower limits of detection and thus measurements are left censored. For example, in studies of Human Immunodeficiency Virus (HIV), the branched DNA (bDNA) assay was developed to quantify HIV-1 RNA concentrations in plasma. Validation of newer assays, such as the RT-PCR (reverse transcriptase polymerase chain reaction) involves assessing agreement of measurements obtained using the two techniques. Both bDNA and RT-PCR assays have lower limits of detection and thus new statistical methods are needed for assessing agreement between two left-censored variables. In this paper, we present maximum likelihood and generalized estimating equations approaches to evaluate agreement between two assays that are subject to lower limits of detection. The concordance correlation coefficient is used as an agreement index. The methodology is illustrated using HIV RNA assay data collected as part of a long-term HIV cohort study.-Assay validation for left-censored data.",0
"The photoactivatable ribonucleoside enhanced cross-linking immunoprecipitation (PAR-CLIP) has been increasingly used for the global mapping of RNA-protein interaction sites. There are two key features of the PAR-CLIP experiments: The sequence read tags are likely to form an enriched peak around each RNA-protein interaction site; and the cross-linking procedure is likely to introduce a specific mutation in each sequence read tag at the interaction site. Several ad hoc methods have been developed to identify the RNA-protein interaction sites using either sequence read counts or mutation counts alone; however, rigorous statistical methods for analyzing PAR-CLIP are still lacking. In this article, we propose an integrative model to establish a joint distribution of observed read and mutation counts. To pinpoint the interaction sites at single base-pair resolution, we developed a novel modeling approach that adopts non-homogeneous hidden Markov models to incorporate the nucleotide sequence at each genomic location. Both simulation studies and data application showed that our method outperforms the ad hoc methods, and provides reliable inferences for the RNA-protein binding sites from PAR-CLIP data.-Bayesian hidden Markov models to identify RNA-protein interaction sites in PAR-CLIP.",0
"Sources of population heterogeneity may or may not be observed. If the sources of heterogeneity are observed (e.g., gender), the sample can be split into groups and the data analyzed with methods for multiple groups. If the sources of population heterogeneity are unobserved, the data can be analyzed with latent class models. Factor mixture models are a combination of latent class and common factor models and can be used to explore unobserved population heterogeneity. Observed sources of heterogeneity can be included as covariates. The different ways to incorporate covariates correspond to different conceptual interpretations. These are discussed in detail. Characteristics of factor mixture modeling are described in comparison to other methods designed for data stemming from heterogeneous populations. A step-by-step analysis of a subset of data from the Longitudinal Survey of American Youth illustrates how factor mixture models can be applied in an exploratory fashion to data collected at a single time point.-Investigating population heterogeneity with factor mixture models.",0
"Published methods for sample size calculation for cluster randomised trials with baseline data are inflexible and primarily assume an equal amount of data collected at baseline and endline, that is, before and after the intervention has been implemented in some clusters. We extend these methods to any amount of baseline and endline data. We explain how to explore sample size for a trial if some baseline data from the trial clusters have already been collected as part of a separate study. Where such data aren't available, we show how to choose the proportion of data collection devoted to the baseline within the trial, when a particular cluster size or range of cluster sizes is proposed. We provide a design effect given the cluster size and correlation parameters, assuming different participants are assessed at baseline and endline in the same clusters. We show how to produce plots to identify the impact of varying the amount of baseline data accounting for the inevitable uncertainty in the cluster autocorrelation. We illustrate the methodology using an example trial. Baseline data provide more power, or allow a greater reduction in trial size, with greater values of the cluster size, intracluster correlation and cluster autocorrelation. Investigators should think carefully before collecting baseline data in a cluster randomised trial if this is at the expense of endline data. In some scenarios, this will increase the sample size required to achieve given power and precision.-Cluster randomised trials with different numbers of measurements at baseline and endline: Sample size and optimal allocation",1
"Mediation analyses have provided a critical platform to assess the validity of theories of action across a wide range of disciplines. Despite widespread interest and development in these analyses, literature guiding the design of mediation studies has been largely unavailable. Like studies focused on the detection of a total or main effect, an important design consideration is the statistical power to detect indirect effects if they exist. Understanding the sensitivity to detect indirect effects is exceptionally important because it directly influences the scale of data collection and ultimately governs the types of evidence group-randomized studies can bring to bear on theories of action. However, unlike studies concerned with the detection of total effects, literature has not established power formulas for detecting multilevel indirect effects in group-randomized designs. In this study, we develop closed-form expressions to estimate the variance of and the power to detect indirect effects in group-randomized studies with a group-level mediator using two-level linear models (i.e., 2-2-1 mediation). The results suggest that when carefully planned, group-randomized designs may frequently be well positioned to detect mediation effects with typical sample sizes. The resulting power formulas are implemented in the R package PowerUpR and the PowerUp!-Mediator software (causalevaluation.org).-Experimental Power for Indirect Effects in Group-randomized Studies with Group-level Mediators.",1
"In their comments on the authors' article, R. C. Serlin, B. E. Wampold, and J. R. Levin and P. Crits-Christoph, X. Tu, and R. Gallop took issue with the authors' suggestion to evaluate therapy studies with nested providers with a fixed model approach. In this rejoinder, the authors' comment on Serlin et al's critique by showing that their arguments do not apply, are based on misconceptions about the purpose and nature of statistical inference, or are based on flawed reasoning. The authors also comment on Crits-Christoph et al's critique by showing that the proposed approach is very similar to, but less inclusive than, their own suggestion.-Assumptions and consequences of treating providers in therapy studies as fixed versus random effects: reply to Crits-Christoph, Tu, and Gallop (2003) and Serlin, Wampold, and Levin (2003)",2
"Nonpharmacologic treatments (including surgery, technical operation, rehabilitation, physiotherapy, education or the use of external technical devices) represent a wide range of treatments for chronic rheumatic disease. Randomized, controlled trials (RCTs) are recognized as the best method for avoiding bias in assessing nonpharmacologic treatments. Designs such as nonrandomized studies, cluster-randomized trials, patient-preference trials, modified Zelen-design trials, tracker trials and expertise-based RCTs could, however, be used to assess such treatments. Assessing nonpharmacologic treatments involves methodologic issues linked to difficulties associated with blinding, duration of the study, main outcomes of the study, difficulties associated with standardizing the intervention and the influence of health-care providers. Hence, these treatments cannot be assessed according to the standards used for pharmacologic treatments. As well, specific instruments such as A CheckList to Evaluate A Report of a NonPharmacological Trial (CLEAR NPT) should be used to assess the quality of reports in this field. Important reporting guidelines that take an evidence-based approach to improve the quality of reports from RCTs, such as the Consolidated Standards of Reporting Trials (CONSORT) statement, should be extended to take these issues into account.-Primer: assessing the efficacy and safety of nonpharmacologic treatments for chronic rheumatic diseases.",1
"To report woman physicians' experiences, in their own words, of discrimination based on their role as a mother. Qualitative analysis of physician mothers' free-text responses to the open question: ""We want to hear your story and experience. Please share"" included in questions about workplace discrimination. Three analysts iteratively formulated a structured codebook, then applied codes after inter-coder reliability scores indicated high concordance. The relationships among themes and sub-themes were organized into a conceptual model illustrated by exemplary quotes. Respondents to an anonymous, voluntary online survey about the health and wellbeing of physician mothers posted on a Facebook group, the Physician Moms Group, an online community of US physicians who identify as mothers. We analyzed 947 free-text responses. Participants provide diverse and vivid descriptions of experiences of maternal discrimination. Gendered job expectations, financial inequalities (including lower pay than equally qualified colleagues and more unpaid work), limited opportunities for advancement, lack of support during the pregnancy and postpartum period, and challenging work-life balance are some of the key themes identified. In addition, participants' quotes show several potential structural drivers of maternal discrimination and describe the downstream consequences of maternal discrimination on the physician herself, her career, family, and the healthcare system. These findings provide a view of maternal discrimination directly from the perspective of those who experience it. Women physicians report a range of previously uncharacterized ways in which they experience maternal discrimination. While certain aspects of these experiences are consistent with those reported by women across other professions, there are unique aspects of medical training and the medical profession that perpetuate maternal discrimination.-Physician mothers' experience of workplace discrimination: a qualitative analysis.",0
"Adjustments of sample size formulas are given for varying cluster sizes in cluster randomized trials with a binary outcome when testing the treatment effect with mixed effects logistic regression using second-order penalized quasi-likelihood estimation (PQL). Starting from first-order marginal quasi-likelihood (MQL) estimation of the treatment effect, the asymptotic relative efficiency of unequal versus equal cluster sizes is derived. A Monte Carlo simulation study shows this asymptotic relative efficiency to be rather accurate for realistic sample sizes, when employing second-order PQL. An approximate, simpler formula is presented to estimate the efficiency loss due to varying cluster sizes when planning a trial. In many cases sampling 14 per cent more clusters is sufficient to repair the efficiency loss due to varying cluster sizes. Since current closed-form formulas for sample size calculation are based on first-order MQL, planning a trial also requires a conversion factor to obtain the variance of the second-order PQL estimator. In a second Monte Carlo study, this conversion factor turned out to be 1.25 at most.-Sample size adjustments for varying cluster sizes in cluster randomized trials with binary outcomes analyzed with second-order PQL mixed logistic regression.",1
"To evaluate the association of kidney function with cardiovascular disease and mortality among apparently healthy women. Prospective cohort study. Women's Health Study, United States. 27 939 female health professionals aged &gt;or=45 who were free of cardiovascular disease and other major disease and who provided a blood sample at study entry. Time to cardiovascular disease (non-fatal stroke, non-fatal myocardial infarction, coronary revascularisation procedures, or death from cardiovascular cause), specific cardiovascular disease events, and all-cause mortality. End points were confirmed after review of medical records and death certificates. Glomerular filtration rate (GFR) was estimated with the abbreviated Modification of Diet in Renal Disease Study equation. At baseline, 1315 (4.7%) women had GFR &lt;60 ml/min/1.73 m(2). During 12 years of follow-up, 1199 incident cardiovascular disease events and 856 deaths (179 from cardiovascular disease) occurred. Compared with women with GFR &gt;or=90 ml/min/1.73 m(2), the multivariable adjusted hazard ratios for any first cardiovascular disease were 0.95 (95% CI 0.83 to 1.08), 0.84 (0.70 to 1.00), and 1.00 (0.79 to 1.27) among women with GFR of 75-89.9, 60-74.9, and &lt;60 ml/min/1.73 m(2), respectively; the equivalent hazard ratios for all cause mortality were 0.93 (0.79 to 1.09), 1.03 (0.85 to 1.26), and 1.09 (0.83 to 1.45). Similar null findings were observed for myocardial infarction, stroke, coronary revascularisation, and non-cardiovascular death. However, an increased risk of death from cardiovascular disease was found among women with GFR &lt;60 ml/min/1.73 m(2) (hazard ratio 1.68 (1.02 to 2.79)). In this large cohort of women, a glomerular filtration rate &lt;60 ml/min/1.73 m(2) was associated with increased risk of cardiovascular disease death but not other cardiovascular disease events or non-cardiovascular disease mortality. We observed no increase in risk of any of the outcomes among women with less severe impairment of kidney function.-Kidney function and risk of cardiovascular disease and mortality in women: a prospective cohort study.",0
"The area under a receiver operating characteristic (ROC) curve (AUC) is a commonly used index for summarizing the ability of a continuous diagnostic test to discriminate between healthy and diseased subjects. If all subjects have their true disease status verified, one can directly estimate the AUC nonparametrically using the Wilcoxon statistic. In some studies, verification of the true disease status is performed only for a subset of subjects, possibly depending on the result of the diagnostic test and other characteristics of the subjects. Because estimators of the AUC based only on verified subjects are typically biased, it is common to estimate the AUC from a bias-corrected ROC curve. The variance of the estimator, however, does not have a closed-form expression and thus resampling techniques are used to obtain an estimate. In this paper, we develop a new method for directly estimating the AUC in the setting of verification bias based on U-statistics and inverse probability weighting (IPW). Closed-form expressions for the estimator and its variance are derived. We also show that the new estimator is equivalent to the empirical AUC derived from the bias-corrected ROC curve arising from the IPW approach.-Direct estimation of the area under the receiver operating characteristic curve in the presence of verification bias.",0
"Random effects models are used in many applications in medical statistics, including meta-analysis, cluster randomized trials and comparisons of health care providers. This paper provides a tutorial on the practical implementation of a flexible random effects model based on methodology developed in Bayesian non-parametrics literature, and implemented in freely available software. The approach is applied to the problem of hospital comparisons using routine performance data, and among other benefits provides a diagnostic to detect clusters of providers with unusual results, thus avoiding problems caused by masking in traditional parametric approaches. By providing code for Winbugs we hope that the model can be used by applied statisticians working in a wide variety of applications.-Flexible random-effects models using Bayesian semi-parametric models: applications to institutional comparisons.",1
"It is now well known that standard statistical procedures become invalidated when applied to cluster randomized trials in which the unit of inference is the individual. A resulting consequence is that researchers conducting such trials are faced with a multitude of design choices, including selection of the primary unit of inference, the degree to which clusters should be matched or stratified by prognostic factors at baseline, and decisions related to cluster subsampling. Moreover, application of ethical principles developed for individually randomized trials may also require modification. We discuss several topics related to these issues, with emphasis on the choices that must be made in the planning stages of a trial and on some potential pitfalls to be avoided.-Pitfalls of and controversies in cluster randomization trials.",1
"Classification and regression tree analyses identify subsets of a sample that differ on an outcome. Discrimination of subsets is performed using recursive binary splitting on a set of covariates, allowing for interactions of variable subgroups not easily captured in standard model building techniques. Using classification and regression tree with epidemiological data can be problematic as there is often a need to adjust for potential confounders and to account for time-varying covariates in the context of right-censored survival data. While classification and regression tree variations exist individually for survival analysis, time-varying covariates and incorporating possible confounders, examples of classification and regression tree using all three together are lacking. We propose a method to identify subsets of time-varying covariate risk factors that affect survival while adjusting for possible confounders. The technique is demonstrated on data from the Bypass Angioplasty Revascularization Investigation 2 Diabetes clinical trial to find combinations of modifiable time-varying cardiac risk factors (e.g. smoking status, blood pressure, lipid levels and HbA1c level) that are associated with time-to-event clinical outcomes.-Tree-based identification of subgroups for time-varying covariate survival data.",0
External validity is also an ethical consideration in cluster-randomised trials of policy changes: the author's reply.,1
"Till B?rnighausen points out the medical risks that two categories of contemporary HIV prevention trials, for ""treatment-as-prevention"" and for ""pre-exposure prophylaxis,"" pose to people who are not study participants. B?rnighausen's compelling case forces reconsideration of the absence of bystanders in the law governing ethical review of health research. It raises the intriguing question: to what legal protection are bystanders morally entitled? The remedy might seem to be to accord bystanders the rights and protections currently accorded to human study participants. We counsel against that remedy on three grounds, inviting colleagues to suggest alternatives.-Ethical complexities of responding to bystander risk in HIV prevention trials.",0
"In many areas of medical research, such as psychiatry and gerontology, latent class variables are used to classify individuals into disease categories, often with the intention of hierarchical modeling. Problems arise when it is not clear how many disease classes are appropriate, creating a need for model selection and diagnostic techniques. Previous work has shown that the Pearson chi 2 statistic and the log-likelihood ratio G2 statistic are not valid test statistics for evaluating latent class models. Other methods, such as information criteria, provide decision rules without providing explicit information about where discrepancies occur between a model and the data. Identifiability issues further complicate these problems. This paper develops procedures for assessing Markov chain Monte Carlo convergence and model diagnosis and for selecting the number of categories for the latent variable based on evidence in the data using Markov chain Monte Carlo techniques. Simulations and a psychiatric example are presented to demonstrate the effective use of these methods.-Latent class model diagnosis.",0
"Matched-pair cluster randomization design is becoming increasingly used in clinical and health behavioral studies. Investigators often encounter incomplete observations in the data collected. Statistical inference for matched-pair cluster randomization design with incomplete observations has been extensively studied in literature. However, sample size method for such study design is sparsely available. We propose a closed-form sample size formula for matched-pair cluster randomization design with continuous outcomes, based on the generalized estimating equation approach by treating incomplete observations as missing data in a marginal linear model. The sample size formula is flexible to accommodate different correlation structures, missing patterns, and magnitude of missingness. In the presence of missing data, the proposed method would lead to a more accurate sample size estimation than the crude adjustment method. Simulation studies are conducted to evaluate the finite-sample performance of the proposed sample size method under various design configurations. We use bias-corrected variance estimators to address the issue of inflated type I error when the number of clusters per group is small. A real application example of physical fitness study in Ecuadorian adolescents is presented for illustration.-Sample size considerations for matched-pair cluster randomization design with incomplete observations of continuous outcomes.",1
"In the Swedish two-county trial women aged 40-74 years from two counties in Sweden were randomised to invitation to mammographic screening for breast cancer. This paper uses random effects logistic regression models to analyse recent data from the trial. The analysis accounts for the structure of the trial, where small geographical units are randomised within larger geographical strata (blocks of two or three small units that are socio-economically similar). Fixed effects and a variety of random effects models show a strong degree of agreement and yield a significant 29% or 30% reduction in breast-cancer mortality. Fixed effects and random effects models agree for this example, because heterogeneity both between strata and within strata between clusters is small and because the effect of treatment does not vary much in different strata.-Some random-effects models for the analysis of matched-cluster randomised trials: application to the Swedish two-county trial of breast-cancer screening.",1
"Common problems to many longitudinal HIV/AIDS, cancer, vaccine, and environmental exposure studies are the presence of a lower limit of quantification of an outcome with skewness and time-varying covariates with measurement errors. There has been relatively little work published simultaneously dealing with these features of longitudinal data. In particular, left-censored data falling below a limit of detection may sometimes have a proportion larger than expected under a usually assumed log-normal distribution. In such cases, alternative models, which can account for a high proportion of censored data, should be considered. In this article, we present an extension of the Tobit model that incorporates a mixture of true undetectable observations and those values from a skew-normal distribution for an outcome with possible left censoring and skewness, and covariates with substantial measurement error. To quantify the covariate process, we offer a flexible nonparametric mixed-effects model within the Tobit framework. A Bayesian modeling approach is used to assess the simultaneous impact of left censoring, skewness, and measurement error in covariates on inference. The proposed methods are illustrated using real data from an AIDS clinical study. .-Bayesian semiparametric mixture Tobit models with left censoring, skewness, and covariate measurement errors.",0
"To compare the performance of a targeted maximum likelihood estimator (TMLE) and a collaborative TMLE (CTMLE) to other estimators in a drug safety analysis, including a regression-based estimator, propensity score (PS)-based estimators, and an alternate doubly robust (DR) estimator in a real example and simulations. The real data set is a subset of observational data from Kaiser Permanente Northern California formatted for use in active drug safety surveillance. Both the real and simulated data sets include potential confounders, a treatment variable indicating use of one of two antidiabetic treatments and an outcome variable indicating occurrence of an acute myocardial infarction (AMI). In the real data example, there is no difference in AMI rates between treatments. In simulations, the double robustness property is demonstrated: DR estimators are consistent if either the initial outcome regression or PS estimator is consistent, whereas other estimators are inconsistent if the initial estimator is not consistent. In simulations with near-positivity violations, CTMLE performs well relative to other estimators by adaptively estimating the PS. Each of the DR estimators was consistent, and TMLE and CTMLE had the smallest mean squared error in simulations.-Targeted maximum likelihood estimation in safety analysis.",0
A stepped wedge cluster randomized trial is preferable for assessing complex health interventions.,3
Should we move from syringe exchange to distribution?,0
"Stillbirth rates in the United Kingdom (UK) are amongst the highest of all developed nations. The association between small-for-gestational-age (SGA) foetuses and stillbirth is well established, and observational studies suggest that improved antenatal detection of SGA babies may halve the stillbirth rate. The Growth Assessment Protocol (GAP) describes a complex intervention that includes risk assessment for SGA and screening using customised fundal-height growth charts. Increased detection of SGA from the use of GAP has been implicated in the reduction of stillbirth rates by 22%, in observational studies of UK regions where GAP uptake was high. This study will be the first randomised controlled trial examining the clinical efficacy, health economics and implementation of the GAP programme in the antenatal detection of SGA. In this randomised controlled trial, clusters comprising a maternity unit (or National Health Service Trust) were randomised to either implementation of the GAP programme, or standard care. The primary outcome is the rate of antenatal ultrasound detection of SGA in infants found to be SGA at birth by both population and customised standards, as this is recognised as being the group with highest risk for perinatal morbidity and mortality. Secondary outcomes include antenatal detection of SGA by population centiles, antenatal detection of SGA by customised centiles, short-term maternal and neonatal outcomes, resource use and economic consequences, and a process evaluation of GAP implementation. Qualitative interviews will be performed to assess facilitators and barriers to implementation of GAP. This study will be the first to provide data and outcomes from a randomised controlled trial investigating the potential difference between the GAP programme compared to standard care for antenatal ultrasound detection of SGA infants. Accurate information on the performance and service provision requirements of the GAP protocol has the potential to inform national policy decisions on methods to reduce the rate of stillbirth. Primary registry and trial identifying number: ISRCTN 67698474 . Registered on 2 November 2016.-The DESiGN trial (DEtection of Small for Gestational age Neonate), evaluating the effect of the Growth Assessment Protocol (GAP): study protocol for a randomised controlled trial.",0
"The sandwich estimator in generalized estimating equations (GEE) approach underestimates the true variance in small samples and consequently results in inflated type I error rates in hypothesis testing. This fact limits the application of the GEE in cluster-randomized trials (CRTs) with few clusters. Under various CRT scenarios with correlated binary outcomes, we evaluate the small sample properties of the GEE Wald tests using bias-corrected sandwich estimators. Our results suggest that the GEE Wald z-test should be avoided in the analyses of CRTs with few clusters even when bias-corrected sandwich estimators are used. With t-distribution approximation, the Kauermann and Carroll (KC)-correction can keep the test size to nominal levels even when the number of clusters is as low as 10 and is robust to the moderate variation of the cluster sizes. However, in cases with large variations in cluster sizes, the Fay and Graubard (FG)-correction should be used instead. Furthermore, we derive a formula to calculate the power and minimum total number of clusters one needs using the t-test and KC-correction for the CRTs with binary outcomes. The power levels as predicted by the proposed formula agree well with the empirical powers from the simulations. The proposed methods are illustrated using real CRT data. We conclude that with appropriate control of type I error rates under small sample sizes, we recommend the use of GEE approach in CRTs with binary outcomes because of fewer assumptions and robustness to the misspecification of the covariance structure.-Small sample performance of bias-corrected sandwich estimators for cluster-randomized trials with binary outcomes.",1
"Prior research has focused primarily on empirically estimating design parameters for cluster-randomized trials (CRTs) of mathematics and reading achievement. Little is known about how design parameters compare across other educational outcomes. This article presents empirical estimates of design parameters that can be used to appropriately power CRTs in science education and compares them to estimates using mathematics and reading. Estimates of intraclass correlations (ICCs) are computed for unconditional two-level (students in schools) and three-level (students in schools in districts) hierarchical linear models of science achievement. Relevant student- and school-level pretest and demographic covariates are then considered, and estimates of variance explained are computed. Subjects: Five consecutive years of Texas student-level data for Grades 5, 8, 10, and 11. Science, mathematics, and reading achievement raw scores as measured by the Texas Assessment of Knowledge and Skills. Results: Findings show that ICCs in science range from .172 to .196 across grades and are generally higher than comparable statistics in mathematics, .163-.172, and reading, .099-.156. When available, a 1-year lagged student-level science pretest explains the most variability in the outcome. The 1-year lagged school-level science pretest is the best alternative in the absence of a 1-year lagged student-level science pretest. Science educational researchers should utilize design parameters derived from science achievement outcomes.-An Empirical Investigation of Variance Design Parameters for Planning Cluster-Randomized Trials of Science Achievement.",1
Cluster randomised trials: useful for interventions delivered to groups: Study design: cluster randomised trials.,1
"In this article we consider the interpretation of regression parameters used to represent 'chronic' or 'long-term' air pollution exposure effects. Although scientific interest typically lies in understanding such effects at the level of the individual, studies have generally employed a semi-ecological design; outcomes and confounder information are collected on individuals while exposure is only available at the aggregate-or group-level. A precise interpretation of results from a semi-ecological design must take into account the aggregated nature, both spatial and temporal, of the exposure measure. The most common analysis approach for assessing chronic exposure effects has been within the Cox proportional hazards model framework; specific analyses are tailored to accommodate the shortcomings of the available exposure information. We revisit the underlying assumptions of the Cox model and discuss the implications of two common aspects of chronic effects studies: time-dependent exposures and time-varying effects. Focusing on the consequences of temporal aggregation of exposure, we show that an estimate obtained from a time-aggregated semi-ecological design can correspond to very different underlying time-varying exposure and risk scenarios. Further, distinguishing which of these is correct is not possible from the semi-ecological data alone. Our goal is to highlight some statistical issues faced by existing studies of chronic air pollution effects, and aid in the development and planning of future studies.-The interpretation of exposure effect estimates in chronic air pollution studies.",0
"Integrative genomics offers a promising approach to more powerful genetic association studies. The hope is that combining outcome and genotype data with other types of genomic information can lead to more powerful SNP detection. We present a new association test based on a statistical model that explicitly assumes that genetic variations affect the outcome through perturbing gene expression levels. It is shown analytically that the proposed approach can have more power to detect SNPs that are associated with the outcome through transcriptional regulation, compared to tests using the outcome and genotype data alone, and simulations show that our method is relatively robust to misspecification. We also provide a strategy for applying our approach to high-dimensional genomic data. We use this strategy to identify a potentially new association between a SNP and a yeast cell's response to the natural product tomatidine, which standard association analysis did not detect.-More powerful genetic association testing via a new statistical framework for integrative genomics.",0
"The severe acute respiratory syndrome (SARS) epidemic, the growing fear of an influenza pandemic and the recent shortage of flu vaccine highlight the need for surveillance systems able to provide early, quantitative predictions of epidemic events. We use dynamic Bayesian networks to discover the interplay among four data sources that are monitored for influenza surveillance. By integrating these different data sources into a dynamic model, we identify in children and infants presenting to the pediatric emergency department with respiratory syndromes an early indicator of impending influenza morbidity and mortality. Our findings show the importance of modelling the complex dynamics of data collected for influenza surveillance, and suggest that dynamic Bayesian networks could be suitable modelling tools for developing epidemic surveillance systems.-A Bayesian dynamic model for influenza surveillance.",0
"While the traditional clinical trial design lays emphasis on testing the treatment effect between randomly assigned groups, it ignores the role of patient preference for a particular treatment in the trial. Yet, for healthcare providers who seek to optimize the patient-centered treatment strategy, the evaluation of a patient's psychology toward each treatment could be a key consideration. The two-stage randomized trial design allows researchers to test patient's preference and selection effects, in addition to the treatment effect. The current methodology for the two-stage design is limited to continuous and binary outcomes; this article extends the model to include count outcomes. The test statistics for preference, selection, and treatment effects are derived. Closed-form sample size formulae are presented for each effect. Simulations are presented to demonstrate the properties of the unstratified and stratified designs. Finally, we apply methods to the use of antimicrobials at the end of life to demonstrate the applicability of the methods.-Two-stage randomized trial design for testing treatment, preference, and self-selection effects for count outcomes.",0
"Background/Aims We sought to optimise the design of stepped wedge trials with an equal allocation of clusters to sequences and explored sample size comparisons with alternative trial designs. Methods We developed a new expression for the design effect for a stepped wedge trial, assuming that observations are equally correlated within clusters and an equal number of observations in each period between sequences switching to the intervention. We minimised the design effect with respect to (1) the fraction of observations before the first and after the final sequence switches (the periods with all clusters in the control or intervention condition, respectively) and (2) the number of sequences. We compared the design effect of this optimised stepped wedge trial to the design effects of a parallel cluster-randomised trial, a cluster-randomised trial with baseline observations, and a hybrid trial design (a mixture of cluster-randomised trial and stepped wedge trial) with the same total cluster size for all designs. Results We found that a stepped wedge trial with an equal allocation to sequences is optimised by obtaining all observations after the first sequence switches and before the final sequence switches to the intervention; this means that the first sequence remains in the control condition and the last sequence remains in the intervention condition for the duration of the trial. With this design, the optimal number of sequences is [Formula: see text], where [Formula: see text] is the cluster-mean correlation, [Formula: see text] is the intracluster correlation coefficient, and m is the total cluster size. The optimal number of sequences is small when the intracluster correlation coefficient and cluster size are small and large when the intracluster correlation coefficient or cluster size is large. A cluster-randomised trial remains more efficient than the optimised stepped wedge trial when the intracluster correlation coefficient or cluster size is small. A cluster-randomised trial with baseline observations always requires a larger sample size than the optimised stepped wedge trial. The hybrid design can always give an equally or more efficient design, but will be at most 5% more efficient. We provide a strategy for selecting a design if the optimal number of sequences is unfeasible. For a non-optimal number of sequences, the sample size may be reduced by allowing a proportion of observations before the first or after the final sequence has switched. Conclusion The standard stepped wedge trial is inefficient. To reduce sample sizes when a hybrid design is unfeasible, stepped wedge trial designs should have no observations before the first sequence switches or after the final sequence switches.-The optimal design of stepped wedge trials with equal allocation to sequences and a comparison to other trial designs.",3
"Selection bias in cluster randomized trials may threaten the validity of the results. This bias may occur either at the level of the cluster or of the individual. We describe measures for maintaining comparability of intervention groups in a cluster randomized trial of a health education package to reduce dietary salt. The setting was 12 villages of the Ashanti region of Ghana. In total, 1896 villagers between 40 and 75 years of age were selected to take part in the trial using stratified random sampling, based on age and sex. Following individuals' consent and baseline measurements in a pair of villages, villages were randomized to intervention or control arms, stratified for locality (semi-urban or rural). Primary outcomes of the trial were reduction in 24-hour urinary sodium and blood pressure. Of the villagers, 1013 individuals agreed to take part, with a response rate of 53%. The groups were comparable with respect to mean (SD) systolic and diastolic blood pressure (125/74 (27/14) mmHg versus 126/75 (25/14) mmHg) and other outcomes. In conclusion, in this study blind recruitment, aided by randomization in small blocks, and stratified random sampling of the subjects within the clusters helped to ensure comparability of intervention groups, which is vital for the validity of the trial results.-Reducing selection bias in a cluster randomized trial in West African villages.",1
"Surgical site infections (SSIs) cause significant patient suffering. Surveillance and feedback of SSI rates is an evidence-based strategy to reduce SSIs, but traditional surveillance methods are slow and prone to bias. The objective of this cluster randomized controlled trial (RCT) is to determine if using optimized statistical process control (SPC) charts for SSI surveillance and feedback lead to a reduction in SSI rates compared to traditional surveillance. The Early 2RIS Trial is a prospective, multicenter cluster RCT using a stepped wedge design. The trial will be performed in 29 hospitals in the Duke Infection Control Outreach Network (DICON) and 105 clusters over 4?years, from March 2016 through February 2020; year one represents a baseline period; thereafter, 8-9 clusters will be randomized to intervention every 3 months over a 3-year period using a stepped wedge randomization design. All patients who undergo one of 13 targeted procedures at study hospitals will be included in the analysis; these procedures will be included in one of six clusters: cardiac, orthopedic, gastrointestinal, OB-GYN, vascular, and spinal. All clusters will undergo traditional surveillance for SSIs; once randomized to intervention, clusters will also undergo surveillance and feedback using optimized SPC charts. Feedback on surveillance data will be provided to all clusters, regardless of allocation or type of surveillance. The primary endpoint is the difference in rates of SSI between the SPC intervention compared to traditional surveillance and feedback alone. The traditional approach for SSI surveillance and feedback has several major deficiencies because SSIs are rare events. First, traditional statistical methods require aggregation of measurements over time, which delays analysis until enough data accumulate. Second, traditional statistical tests and resulting p values are difficult to interpret. Third, analyses based on average SSI rates during predefined time periods have limited ability to rapidly identify important, real-time trends. Thus, standard analytic methods that compare average SSI rates between arbitrarily designated time intervals may not identify an important SSI rate increase on time unless the ""signal"" is very strong. Therefore, novel strategies for early identification and investigation of SSI rate increases are needed to decrease SSI rates. While SPC charts are used throughout industry and healthcare to improve and optimize processes, including other types of healthcare-associated infections, they have not been evaluated as a tool for SSI surveillance and feedback in a randomized trial. ClinicalTrials.gov NCT03075813 , Registered March 9, 2017.-Early recognition and response to increases in surgical site infections using optimized statistical process control charts-the Early 2RIS Trial: a multicenter cluster randomized controlled trial with stepped wedge design.",0
"When well-designed and implemented, cluster randomized trials can meet the high standards federal agencies and other funders increasingly require for evidence on the effectiveness of school health programs and services. However, designing and implementing these studies can present more challenges than at first appears. I reviewed summaries of the methodological literature on cluster randomized trials. I then conducted a search to identify practical applications of these methods in school health research. The review identified 6 key issues or decisions school health researchers must address when designing, conducting, and analyzing data from a cluster randomized trial: (1) reasons to use a clustered design, (2) sample size calculation, (3) the use of matching or stratification, (4) definition of the school and student samples, (5) consent gathering, and (6) analysis methods. School health researchers can take several practical steps to ensure the availability of high-quality research evidence and meet the growing demands for evidence and accountability in education policy and programming. These steps include selecting the right research design for the intervention and evaluation setting, identifying appropriate sample definitions and analysis methods, and developing appropriate procedures for gathering parental permission and student assent.-A Practical Guide to Cluster Randomized Trials in School Health Research",1
"Despite randomization, baseline imbalance and confounding bias may occur in cluster randomized trials (CRTs). Covariate imbalance may jeopardize the validity of statistical inferences if they occur on prognostic factors. Thus, the diagnosis of a such imbalance is essential to adjust statistical analysis if required. We developed a tool based on the c-statistic of the propensity score (PS) model to detect global baseline covariate imbalance in CRTs and assess the risk of confounding bias. We performed a simulation study to assess the performance of the proposed tool and applied this method to analyze the data from 2 published CRTs. The proposed method had good performance for large sample sizes (n =500 per arm) and when the number of unbalanced covariates was not too small as compared with the total number of baseline covariates (?40% of unbalanced covariates). We also provide a strategy for pre selection of the covariates needed to be included in the PS model to enhance imbalance detection. The proposed tool could be useful in deciding whether covariate adjustment is required before performing statistical analyses of CRTs.-Propensity score to detect baseline imbalance in cluster randomized trials: the role of the c-statistic.",1
Cluster randomized trials must be better designed and reported.,1
"In a stepped wedge, cluster randomised trial, clusters receive the intervention at different time points, and the order in which they received it is randomised. Previous systematic reviews of stepped wedge trials have documented a steady rise in their use between 1987 and 2010, which was attributed to the design's perceived logistical and analytical advantages. However, the interventions included in these systematic reviews were often poorly reported and did not adequately describe the analysis and/or methodology used. Since 2010, a number of additional stepped wedge trials have been published. This article aims to update previous systematic reviews, and consider what interventions were tested and the rationale given for using a stepped wedge design. We searched PubMed, PsychINFO, the Cumulative Index to Nursing and Allied Health Literature (CINAHL), the Web of Science, the Cochrane Library and the Current Controlled Trials Register for articles published between January 2010 and May 2014. We considered stepped wedge randomised controlled trials in all fields of research. We independently extracted data from retrieved articles and reviewed them. Interventions were then coded using the functions specified by the Behaviour Change Wheel, and for behaviour change techniques using a validated taxonomy. Our review identified 37 stepped wedge trials, reported in 10 articles presenting trial results, one conference abstract, 21 protocol or study design articles and five trial registrations. These were mostly conducted in developed countries (n = 30), and within healthcare organisations (n = 28). A total of 33 of the interventions were educationally based, with the most commonly used behaviour change techniques being 'instruction on how to perform a behaviour' (n = 32) and 'persuasive source' (n = 25). Authors gave a wide range of reasons for the use of the stepped wedge trial design, including ethical considerations, logistical, financial and methodological. The adequacy of reporting varied across studies: many did not provide sufficient detail regarding the methodology or calculation of the required sample size. The popularity of stepped wedge trials has increased since 2010, predominantly in high-income countries. However, there is a need for further guidance on their reporting and analysis.-Stepped wedge randomised controlled trials: systematic review of studies published between 2010 and 2014.",3
Some aspects of the design and analysis of cluster randomization trials,1
"Mounting evidence suggests that there is frequently considerable variation in the risk of the outcome of interest in clinical trial populations. These differences in risk will often cause clinically important heterogeneity in treatment effects (HTE) across the trial population, such that the balance between treatment risks and benefits may differ substantially between large identifiable patient subgroups; the ""average"" benefit observed in the summary result may even be non-representative of the treatment effect for a typical patient in the trial. Conventional subgroup analyses, which examine whether specific patient characteristics modify the effects of treatment, are usually unable to detect even large variations in treatment benefit (and harm) across risk groups because they do not account for the fact that patients have multiple characteristics simultaneously that affect the likelihood of treatment benefit. Based upon recent evidence on optimal statistical approaches to assessing HTE, we propose a framework that prioritizes the analysis and reporting of multivariate risk-based HTE and suggests that other subgroup analyses should be explicitly labeled either as primary subgroup analyses (well-motivated by prior evidence and intended to produce clinically actionable results) or secondary (exploratory) subgroup analyses (performed to inform future research). A standardized and transparent approach to HTE assessment and reporting could substantially improve clinical trial utility and interpretability.-Assessing and reporting heterogeneity in treatment effects in clinical trials: a proposal.",0
"Many randomized controlled trials (RCTs) employ mortality at a given time as a primary outcome. There are at least three common ways to measure 90-day mortality: first, all-location mortality, that is, all-cause mortality within 90 days of randomization at any location. Second, ARDSnet mortality is death in a healthcare facility of greater intensity than the patient was in prior to the hospitalization during which they were randomized. Finally, in-hospital mortality is death prior to discharge from the primary hospitalization of randomization. Data comparing the impact of these different measurements on sample size are lacking. We evaluated the extent to which event rates vary by mortality definition. This was a retrospective cohort study of 30,691 patients hospitalized at Veterans Affairs (VA) hospitals for sepsis during 2009. 12,727 (41.5%) received care in an ICU setting. For each patient, we measured event rates for three different 90-day mortality outcomes: all-location mortality, ARDSnet mortality, and in-hospital mortality. We also calculated sample sizes necessary to power an example RCT given those event rates. At 90 days, all-location mortality was 26.4% (95% CI 25.9-26.9%), ARDSnet mortality was 19.2% (95% CI 18.8-19.7%), and in-hospital mortality was 13.4% (95% CI 13.0-13.8%) (p &lt; 0.01 all comparisons). These respective event rates result in different required sample sizes to achieve a 20% relative reduction in mortality with 80% power and a 5% false positive rate. Such a trial of VA sepsis patients would require 2080 patients for all-location mortality, 3080 for ARDSnet mortality, and 4796 for in-hospital mortality. Among sepsis patients mechanically ventilated in an ICU, 2438 experienced all-location mortality (46.2% [95% CI 44.8-47.5%]), 2181 experienced ARDSnet mortality (41.3% [95% CI 40.0-42.6%]), and 1894 experienced in-hospital mortality (36.0% [95% CI 34.7-37.3%]). Event rates vary substantially in sepsis patients based on the chosen 90-day mortality definition. This could have important implications for RCT design trade-offs.-Sample size implications of mortality definitions in sepsis: a retrospective cohort study.",0
"In a randomized clinical trial to assess the effectiveness of different strategies for treating low-back pain in a managed-care setting, 681 adult patients presenting with low-back pain were randomized to four treatment groups: medical care with and without physical therapy; and chiropractic care with and without physical modalities. Follow-up information was obtained by questionnaires at two and six weeks, six, 12 and 18 months and by a telephone interview at four weeks. One outcome measurement at each follow-up is the patient's self-report on the perception of low-back pain improvement from the previous survey, recorded as 'A lot better,' 'A little better,' 'About the same' and 'Worse.' Since the patient's perception of improvement may be influenced by past experience, the outcome is analysed using a transition (first-order Markov) model. Although one could collapse categories to the point that logistic regression analysis with repeated measurements could be used, here we allow for multiple categories by relating transition probabilities to covariates and previous outcomes through a polytomous logistic regression model with Markov structure. This approach allows us to assess not only the effects of treatment assignment and baseline characteristics but also the effects of past outcomes in analysing longitudinal categorical data.-Use of a Markov transition model to analyse longitudinal low-back pain data.",0
"Receiver operating characteristic (ROC) curves are useful statistical tools used to assess the precision of diagnostic markers or to compare new diagnostic markers with old ones. The most common index employed for these purposes is the area under the ROC curve (theta) and several statistical tests exist that test the null hypotheses H(0): theta= 0.5 or H(0): theta1=theta2, in the case of two-marker comparisons, against alternatives of interest. In this paper we show that goodness-of-fit of uniformity of the distribution of the false positive (true positive) rates can be used instead of tests based on the area index. A semi-parametric approach is based on a completely specified distribution of marker measurements for either the healthy (F) or diseased (G) subjects, and this is extended to the two-marker case. We then extend to the one- and two-marker case when neither distribution is specified (the non-parametric case). In general, ROC-based tests are more powerful than goodness-of-fit tests for location differences between the distributions of healthy and diseased subjects. However ROC-based tests are less powerful when location-scale differences exist (producing ROC curves that cross the diagonal) and are incapable of discriminating between healthy and diseased samples when theta=0.5 but F not equal G. In these cases, goodness-of-fit tests have a distinct advantage over ROC-based tests. In conclusion, ROC methodology should be used with recognition of its potential limitations and should be replaced by goodness-of-fit tests when appropriate. The latter are a viable alternative and can be used as a 'black box' or as an exploratory first step in the evaluation of novel diagnostic markers.-Assessment of diagnostic markers by goodness-of-fit tests.",0
Randomization of Clusters Versus Randomization of Persons Within Clusters,1
Testing for group membership effects during and after treatment: The example of group therapy for smoking cessation,2
Regarding the prevention of global chronic disease: academic public health's new frontier.,0
"We examine two posterior predictive distribution based approaches to assess model fit for incomplete longitudinal data. The first approach assesses fit based on replicated complete data as advocated in Gelman et al. (2005). The second approach assesses fit based on replicated observed data. Differences between the two approaches are discussed and an analytic example is presented for illustration and understanding. Both checks are applied to data from a longitudinal clinical trial. The proposed checks can easily be implemented in standard software like (Win)BUGS/JAGS/Stan. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-A note on posterior predictive checks to assess model fit for incomplete data.",0
"Evidence that can be used to improve clinical practice patterns and processes is frequently generated through standard, parallel-arms cluster randomized trial (CRT) designs that test interventions implemented at the center-level. Although the primary endpoint of these trials is often a center-level outcome, patient-level factors may vary between centers and, consequently, may influence the center-level outcome. Furthermore, there may be important factors that predict the variation in the center-level outcome and this knowledge can help contextualize the trial results and inform practice patterns. Our symbolic two-step method that applies symbolic data analysis to account for patient-level factors when estimating and testing a center-level effect on both the average center-level outcome and its variation was developed for such settings. Herein, we sought to extend the method to prospectively size a CRT so that the application of our method in data analysis is consistent with the design. Our formulaic approach to sample size planning incorporated predictive factors of the within-center variation and accounted for patient-level characteristics. The sample size approximation performed well in many different pragmatic settings. Our symbolic two-step method provides an alternate approach in the design and analysis of CRTs evaluating novel improvement processes within care delivery research.-Sample size planning in the design and analysis of cluster randomized trials using the symbolic two-step method",1
"The use of cluster randomized trials (CRTs) is increasing, along with the variety in their design and analysis. The simplest approach for their sample size calculation is to calculate the sample size assuming individual randomization and inflate this by a design effect to account for randomization by cluster. The assumptions of a simple design effect may not always be met; alternative or more complicated approaches are required. We summarise a wide range of sample size methods available for cluster randomized trials. For those familiar with sample size calculations for individually randomized trials but with less experience in the clustered case, this manuscript provides formulae for a wide range of scenarios with associated explanation and recommendations. For those with more experience, comprehensive summaries are provided that allow quick identification of methods for a given design, outcome and analysis method. We present first those methods applicable to the simplest two-arm, parallel group, completely randomized design followed by methods that incorporate deviations from this design such as: variability in cluster sizes; attrition; non-compliance; or the inclusion of baseline covariates or repeated measures. The paper concludes with methods for alternative designs. There is a large amount of methodology available for sample size calculations in CRTs. This paper gives the most comprehensive description of published methodology for sample size calculation and provides an important resource for those designing these trials.-Methods for sample size determination in cluster randomized trials.",1
"Comorbidities are often included in risk-factor models for nosocomial antibiotic-resistant bacterial infections, and aggregate comorbidity measures are valuable because they allow one variable to represent many. This study aimed to develop new aggregate comorbidity measures based upon the Chronic Disease Score (CDS) for assessing the comorbidity-attributable risk of methicillin-resistant Staphylococcus aureus (MRSA) and vancomycin-resistant enterococci (VRE) nosocomial infections. For each outcome, two retrospective cohort studies of hospitalized patients were conducted. Outcomes were a first MRSA or VRE positive clinical culture obtained 48 hours or more postadmission. Each cohort was divided into development (July 1998-2001) and validation (August 2001-2003) samples. New comorbidity measures were created for MRSA (CDS-MRSA), VRE (CDS-VRE), or any nosocomial infection outcome (CDS-ID) using logistic regression and subsequently validated. Model discrimination was measured using the c-statistic. Discrimination of the CDS-MRSA (c=0.60), CDS-VRE (c=0.65), and CDS-ID (MRSA: c=0.57; VRE: c=0.64) was greater than that of the original CDS (MRSA: c=0.52; VRE: c=0.57). The CDS-MRSA, CDS-VRE, and CDS-ID are new infectious disease specific comorbidity risk-adjustment measures that will be useful for the quality of future epidemiologic studies of MRSA, VRE, and other infectious diseases.-Comorbidity risk-adjustment measures were developed and validated for studies of antibiotic-resistant infections.",0
"In a cluster randomised controlled trial (CRCT), randomisation units are ""clusters"" such as schools or GP practices. This has methodological implications for study design and statistical analysis, since clustering often leads to correlation between observations which, if not accounted for, can lead to spurious conclusions of efficacy/effectiveness. Bayesian methodology offers a flexible, intuitive framework to deal with such issues, but its use within CRCT design and analysis appears limited. This review aims to explore and quantify the use of Bayesian methodology in the design and analysis of CRCTs, and appraise the quality of reporting against CONSORT guidelines. We sought to identify all reported/published CRCTs that incorporated Bayesian methodology and papers reporting development of new Bayesian methodology in this context, without restriction on publication date or location. We searched Medline and Embase and the Cochrane Central Register of Controlled Trials (CENTRAL). Reporting quality metrics according to the CONSORT extension for CRCTs were collected, as well as demographic data, type and nature of Bayesian methodology used, journal endorsement of CONSORT guidelines, and statistician involvement. Twenty-seven publications were included, six from an additional hand search. Eleven (40.7%) were reports of CRCT results: seven (25.9%) were primary results papers and four (14.8%) reported secondary results. Thirteen papers (48.1%) reported Bayesian methodological developments, the remaining three (11.1%) compared different methods. Four (57.1%) of the primary results papers described the method of sample size calculation; none clearly accounted for clustering. Six (85.7%) clearly accounted for clustering in the analysis. All results papers reported use of Bayesian methods in the analysis but none in the design or sample size calculation. The popularity of the CRCT design has increased rapidly in the last twenty years but this has not been mirrored by an uptake of Bayesian methodology in this context. Of studies using Bayesian methodology, there were some differences in reporting quality compared to CRCTs in general, but this study provided insufficient data to draw firm conclusions. There is an opportunity to further develop Bayesian methodology for the design and analysis of CRCTs in order to expand the accessibility, availability, and, ultimately, use of this approach.-Bayesian statistics in the design and analysis of cluster randomised controlled trials and their reporting quality: a methodological systematic review.",1
"Ye, Lin, and Taylor (2008, Biometrics 64, 1238-1246) proposed a joint model for longitudinal measurements and time-to-event data in which the longitudinal measurements are modeled with a semiparametric mixed model to allow for the complex patterns in longitudinal biomarker data. They proposed a two-stage regression calibration approach that is simpler to implement than a joint modeling approach. In the first stage of their approach, the mixed model is fit without regard to the time-to-event data. In the second stage, the posterior expectation of an individual's random effects from the mixed-model are included as covariates in a Cox model. Although Ye et al. (2008) acknowledged that their regression calibration approach may cause a bias due to the problem of informative dropout and measurement error, they argued that the bias is small relative to alternative methods. In this article, we show that this bias may be substantial. We show how to alleviate much of this bias with an alternative regression calibration approach that can be applied for both discrete and continuous time-to-event data. Through simulations, the proposed approach is shown to have substantially less bias than the regression calibration approach proposed by Ye et al. (2008). In agreement with the methodology proposed by Ye et al. (2008), an advantage of our proposed approach over joint modeling is that it can be implemented with standard statistical software and does not require complex estimation techniques.-On estimating the relationship between longitudinal measurements and time-to-event data using a simple two-stage procedure.",0
"A new statistical methodology is developed for the analysis of spontaneous adverse event (AE) reports from post-marketing drug surveillance data. The method involves both empirical Bayes (EB) and fully Bayes estimation of rate multipliers for each drug within a class of drugs, for a particular AE, based on a mixed-effects Poisson regression model. Both parametric and semiparametric models for the random-effect distribution are examined. The method is applied to data from Food and Drug Administration (FDA)'s Adverse Event Reporting System (AERS) on the relationship between antidepressants and suicide. We obtain point estimates and 95 per cent confidence (posterior) intervals for the rate multiplier for each drug (e.g. antidepressants), which can be used to determine whether a particular drug has an increased risk of association with a particular AE (e.g. suicide). Confidence (posterior) intervals that do not include 1.0 provide evidence for either significant protective or harmful associations of the drug and the adverse effect. We also examine EB, parametric Bayes, and semiparametric Bayes estimators of the rate multipliers and associated confidence (posterior) intervals. Results of our analysis of the FDA AERS data revealed that newer antidepressants are associated with lower rates of suicide adverse event reports compared with older antidepressants. We recommend improvements to the existing AERS system, which are likely to improve its public health value as an early warning system.-Mixed-effects Poisson regression analysis of adverse event reports: the relationship between antidepressants and suicide.",0
"Reviews have repeatedly noted important methodological issues in the conduct and reporting of cluster randomized controlled trials (C-RCTs). These reviews usually focus on whether the intracluster correlation was explicitly considered in the design and analysis of the C-RCT. However, another important aspect requiring special attention in C-RCTs is the risk for imbalance of covariates at baseline. Imbalance of important covariates at baseline decreases statistical power and precision of the results. Imbalance also reduces face validity and credibility of the trial results. The risk of imbalance is elevated in C-RCTs compared to trials randomizing individuals because of the difficulties in recruiting clusters and the nested nature of correlated patient-level data. A variety of restricted randomization methods have been proposed as way to minimize risk of imbalance. However, there is little guidance regarding how to best restrict randomization for any given C-RCT. The advantages and limitations of different allocation techniques, including stratification, matching, minimization, and covariate-constrained randomization are reviewed as they pertain to C-RCTs to provide investigators with guidance for choosing the best allocation technique for their trial.-Allocation techniques for balance at baseline in cluster randomized trials: a methodological review.",1
"It is unclear to what extent the incremental predictive performance of a novel biomarker is impacted by the method used to control for standard predictors. We investigated whether adding a biomarker to a model with a published risk score overestimates its incremental performance as compared to adding it to a multivariable model with individual predictors (or a composite risk score estimated from the sample of interest) and to a null model. We used 1000 simulated datasets (with a range of risk factor distributions and event rates) to compare these methods, using the continuous net reclassification index (NRI), the integrated discrimination index (IDI), and change in the C-statistic as discrimination metrics. The new biomarker was added to the following: null model, model including a published risk score, model including a composite risk score estimated from the sample of interest, and multivariable model with individual predictors. We observed a gradient in the incremental performance of the biomarker, with the null model resulting in the highest predictive performance of the biomarker and the model using individual predictors resulting in the lowest (mean increases in C-statistic between models without and with the biomarker: 0.261, 0.085, 0.030, and 0.031; NRI: 0.767, 0.621, 0.513, and 0.530; IDI: 0.153, 0.093, 0.053 and 0.057, respectively). These findings were supported by the Framingham Study data predicting atrial fibrillation using novel biomarkers. We recommend that authors report the effect of a new biomarker after controlling for standard predictors modeled as individual variables.-Assessing the incremental predictive performance of novel biomarkers over standard predictors.",0
"Cluster randomized controlled trial (RCT), in which groups or clusters of individuals rather than individuals themselves are randomized, are increasingly common. Indeed, for the evaluation of certain types of intervention (such as those used in health promotion and educational interventions) a cluster randomized trial is virtually the only valid approach. However, cluster trials are generally more difficult to design and execute than individually randomized studies, and some design features of a cluster trial may make it particularly vulnerable to a range of threats that can introduce bias. In this paper we discuss the issues that can lead to bias in cluster randomized trials and conclude with some suggestions for avoiding these problems.-Cluster randomized controlled trials.",1
"The relation between alcohol consumption and incident hypertension is unclear, and most observational studies have not accounted for socioeconomic factors. This study examined the association between alcohol consumption in a diverse group of young adults and incident hypertension over 20 years. Participants (n = 4,711) were from the Coronary Artery Risk Development in Young Adults Study cohort, recruited in 1985 (aged 18-30 years) from Birmingham, Alabama; Chicago, Illinois; Minneapolis, Minnesota; and Oakland, California. The 20-year incidence of hypertension for never, former, light, moderate, and at-risk drinkers was 25.1%, 31.8%, 20.9%, 22.2%, and 18.8%, respectively (P &lt; 0.001). Race, gender, age, family history of hypertension, body mass index, income, education, and difficulty paying for basics and medical care were associated with hypertension. Adjustment using Cox proportional hazard models revealed no association between baseline alcohol consumption and incident hypertension, except among European-American women in whom any current alcohol consumption was associated with lower risk of incident hypertension. The lack of association between alcohol and hypertension in the majority of this socioeconomically diverse cohort is not definitive. Future studies should include social factors, such as income and education, and consider additional characteristics that may modify or confound associations between alcohol and blood pressure.-Alcohol consumption in young adults and incident hypertension: 20-year follow-up from the Coronary Artery Risk Development in Young Adults Study.",0
"We used 3 approaches to analyzing clustered data to assess the impact of model choice on interpretation. Approaches 1 and 2 specified random intercept models but differed in standard versus novel specification of covariates, which impacts ability to separate within- and between-cluster effects. Approach 3 was based on standard analysis of paired differences. We applied these methods to data from the National Collaborative Perinatal Project to examine the association between head circumference at birth and intelligence (IQ) at age 7 years. Approach 1, which ignored within- and between-family effects, yielded an overall IQ effect of 1.1 points (95% confidence interval [CI]=0.9, 1.3) for every 1-cm increase in head circumference. Approaches 2 and 3 found comparable within-family effects of 0.6 points (95% CI = 0.4, 0.9) and 0.69 points (95% CI = 0.4, 1.0), respectively. Our findings confirm the importance of applying appropriate analytic methods to clustered data, as well as the need for careful covariate specification in regression modeling. Method choice should be informed by the level of interest in cluster-level effects and item-level effects.-A comparison of regression approaches for analyzing clustered data.",1
"In studies in which a binary response for each subject is observed, the success probability and functions of this quantity are of interest. The use of confidence intervals has been increasingly encouraged as complementary to, and indeed preferable to, p-values as the primary expression of the impact of sampling uncertainty on the findings. The asymptotic confidence interval, based on a normal approximation, is often considered, but this interval can have poor statistical properties when the sample size is small and/or when the success probability is near 0 or 1. In this paper, an estimate of the risk difference based on median unbiased estimates (MUEs) of the two group probabilities is proposed. A corresponding confidence interval is derived using a fully specified bootstrap sample space. The proposed method is compared with Chen's quasi-exact method, Wald intervals and Agresti and Caffo's method with regard to mean square error and coverage probability. For a variety of settings, the MUE-based estimate of risk difference has mean square error uniformly smaller than maximum likelihood estimate within a certain range of risk difference. The fully specified bootstrap had better coverage probability in the tail area than Chen's quasi-exact method, Wald intervals and Agresti and Caffo's intervals.-Fully specified bootstrap confidence intervals for the difference of two independent binomial proportions based on the median unbiased estimator.",0
"Increasingly, researchers are recognizing that there are many situations where the use of a cluster randomized trial may be more appropriate than an individually randomized trial. Similarly, the need for appropriate standards of reporting of cluster trials is more widely acknowledged. In this paper, we describe the results of a survey to inform the appropriate reporting of the intracluster correlation coefficient (ICC)--the statistical measure of the clustering effect associated with a cluster randomized trial. We identified three dimensions that should be considered when reporting an ICC--a description of the dataset (including characteristics of the outcome and the intervention), information on how the ICC was calculated, and information on the precision of the ICC. This paper demonstrates the development of a framework for the reporting of ICCs. If adopted into routine practice, it has the potential to facilitate the interpretation of the cluster trial being reported and should help the development of new trials in the area.-Intracluster correlation coefficients in cluster randomized trials: empirical insights into how should they be reported.",1
"The frailty model is increasingly popular for analyzing multivariate time-to-event data. The most common model is the shared frailty model. Although study design consideration is as important as analysis strategies, sample size determination methodology in studies with multivariate time-to-event data is greatly lacking in the literature. In this article, we develop a sample size determination method for the shared frailty model to investigate the treatment effect on multivariate event times. We analyzed the data using both a parametric model and a piecewise model with unknown baseline hazard, and compare the empirical power with the calculated power. Last, we discuss the formula for testing the treatment effect on recurrent events.-Sample size determination in shared frailty models for multivariate time-to-event data.",1
"Most of the 100 million Americans with persistent pain are treated in primary care clinics, but evidence-based psychosocial approaches targeting pain-related disability are not usually provided in these settings. This manuscript describes the rationale and methods for a protocol to pilot test the feasibility and effectiveness of Acceptance and Commitment Therapy (ACT), an evidence-based psychological treatment for persistent pain, delivered by a Behavioral Health Consultant in primary care. Eligible patients are identified through electronic health record registries and invited to participate via secure messaging, letters and a follow-up phone call. Participants are also recruited with advertising and clinician referral. Patients agreeing to participate are consented and complete initial assessments, with a target of 60 participants. Randomization is stratified based on pain severity with participants assigned to either ACT or Enhanced Treatment as Usual (E-TAU). ACT participants receive one standardized Behavioral Health Consultation visit followed by three ACT-based group visits and one group booster visit. All patients attend six assessment visits, during which the E-TAU patients are provided with educational pain management handouts based on standard cognitive behavioral treatment of pain. The study aims to determine feasibility and effectiveness of brief ACT for persistent pain delivered by an integrated behavioral health clinician in primary care from pre- to post-treatment, and to examine mechanisms of change in ACT participants. This study, in a ""real-world"" setting, will lay groundwork for a larger trial. If effective, it could improve treatment methods and quality of life for patients with persistent pain using a scalable approach.-Rationale and design of a pilot study examining Acceptance and Commitment Therapy for persistent pain in an integrated primary care clinic.",0
"Incomplete and inadequate reporting is an avoidable waste that reduces the usefulness of research. The CONSORT (Consolidated Standards of Reporting Trials) Statement is an evidence-based reporting guideline that aims to improve research transparency and reduce waste. In 2008, the CONSORT Group developed an extension to the original statement that addressed methodological issues specific to trials of nonpharmacologic treatments (NPTs), such as surgery, rehabilitation, or psychotherapy. This article describes an update of that extension and presents an extension for reporting abstracts of NPT trials. To develop these materials, the authors reviewed pertinent literature published up to July 2016; surveyed authors of NPT trials; and conducted a consensus meeting with editors, trialists, and methodologists. Changes to the CONSORT Statement extension for NPT trials include wording modifications to improve readers' understanding and the addition of 3 new items. These items address whether and how adherence of participants to interventions is assessed or enhanced, description of attempts to limit bias if blinding is not possible, and specification of the delay between randomization and initiation of the intervention. The CONSORT extension for abstracts of NPT trials includes 2 new items that were not specified in the original CONSORT Statement for abstracts. The first addresses reporting of eligibility criteria for centers where the intervention is performed and for care providers. The second addresses reporting of important changes to the intervention versus what was planned. Both the updated CONSORT extension for NPT trials and the CONSORT extension for NPT trial abstracts should help authors, editors, and peer reviewers improve the transparency of NPT trial reports.-CONSORT Statement for Randomized Trials of Nonpharmacologic Treatments: A 2017 Update and a CONSORT Extension for Nonpharmacologic Trial Abstracts",2
"In many studies comparing a new 'target treatment' with a control target treatment, the received treatment does not always agree with assigned treatment-that is, the compliance is imperfect. An obvious example arises when ethical or practical constraints prevent even the randomized assignment of receipt of the new target treatment but allow the randomized assignment of the encouragement to receive this treatment. In fact, many randomized experiments where compliance is not enforced by the experimenter (e.g. with non-blinded assignment) may be more accurately thought of as randomized encouragement designs. Moreover, often the assignment of encouragement is at the level of clusters (e.g. doctors) where the compliance with the assignment varies across the units (e.g. patients) within clusters. We refer to such studies as 'clustered encouragement designs' (CEDs) and they arise relatively frequently (e.g. Sommer and Zeger, 1991; McDonald et al., 1992; Dexter et al., 1998) Here, we propose Bayesian methodology for causal inference for the effect of the new target treatment versus the control target treatment in the randomized CED with all-or-none compliance at the unit level, which generalizes the approach of Hirano et al. (2000) in important and surprisingly subtle ways, to account for the clustering, which is necessary for statistical validity. We illustrate our methods using data from a recent study exploring the role of physician consulting in increasing patients' completion of Advance Directive forms.-Clustered encouragement designs with individual noncompliance: bayesian inference with randomization, and application to advance directive forms.",1
"The use of multiple drugs in a single clinical trial or as a therapeutic strategy has become common, particularly in the treatment of cancer. Because traditional trials are designed to evaluate one agent at a time, the evaluation of therapies in combination requires specialized trial designs. In place of the traditional separate phase I and II trials, we propose using a parallel phase I/II clinical trial to evaluate simultaneously the safety and efficacy of combination dose levels, and select the optimal combination dose. The trial is started with an initial period of dose escalation, then patients are randomly assigned to admissible dose levels. These dose levels are compared with each other. Bayesian posterior probabilities are used in the randomization to adaptively assign more patients to doses with higher efficacy levels. Combination doses with lower efficacy are temporarily closed and those with intolerable toxicity are eliminated from the trial. The trial is stopped if the posterior probability for safety, efficacy, or futility crosses a prespecified boundary. For illustration, we apply the design to a combination chemotherapy trial for leukemia. We use simulation studies to assess the operating characteristics of the parallel phase I/II trial design, and compare it to a conventional design for a standard phase I and phase II trial. The simulations show that the proposed design saves sample size, has better power, and efficiently assigns more patients to doses with higher efficacy levels.-A parallel phase I/II clinical trial design for combination therapies.",0
"There are two distinct problems about bystander effects raised by organ donor intervention research. The first is the problem of ""bystander organs""-sometimes called ""non-target organs""-which Kimmelman discusses in his case presentation. How do we treat the recipients of organs that are not the subject of the intervention research but nonetheless might be directly affected by the research? The second problem is not about altering the organ but the pattern of distribution of organs. Each of these cases shows bystander effects that matter for real people. This article examines how research ethics should approach each of these cases.-Organ donor intervention trials and risk to bystanders: An ethical analysis.",0
"Dependent data, such as arise with cluster sampling, typically yield variances of parameter estimates which are larger than would be provided by a simple random sample of the same size. This variance inflation factor is called the design effect of the estimator. Design effects have been derived for cluster sampling designs using simple estimators such as means and proportions, and also for linear regression coefficient estimators. In this paper, we show that a method to derive design effects for linear regression estimators extends to generalized linear models for binary responses. In particular, some simple expressions for design effects in the linear regression model provide accurate approximations for binary regression models such as those based on the logistic, probit and complementary log-log links. We corroborate our findings with two examples and some simulation studies.-Design effects for binary regression models fitted to dependent data.",1
"This article combines procedures for single-level mediational analysis with multilevel modeling techniques in order to appropriately test mediational effects in clustered data. A simulation study compared the performance of these multilevel mediational models with that of single-level mediational models in clustered data with individual- or group-level initial independent variables, individual- or group-level mediators, and individual level outcomes. The standard errors of mediated effects from the multilevel solution were generally accurate, while those from the single-level procedure were downwardly biased, often by 20% or more. The multilevel advantage was greatest in those situations involving group-level variables, larger group sizes, and higher intraclass correlations in mediator and outcome variables. Multilevel mediational modeling methods were also applied to data from a preventive intervention designed to reduce intentions to use steroids among players on high school football teams. This example illustrates differences between single-level and multilevel mediational modeling in real-world clustered data and shows how the multilevel technique may lead to more accurate results.-Multilevel Modeling of Individual and Group Level Mediated Effects.",1
"Suicide remains the 10th-ranked most frequent cause of death in the United States, accounting for over 40,000 deaths per year. Nonfatal suicide attempts lead to over 200,000 hospitalizations and 600,000 emergency department visits annually. Recent evidence indicates that responses to the commonly used Patient Health Questionnaire (PHQ9) can identify outpatients who are at risk of suicide attempt and suicide death and that specific psychotherapy or Care Management programs can prevent suicide attempts in high-risk patients. Motivated by these developments, the NIMH-funded Mental Health Research Network has undertaken a multisite trial of two outreach programs to prevent suicide attempts among outpatients identified by routinely administered PHQ9 questionnaires. Outpatients who are at risk of suicide attempt are automatically identified using data from electronic health records (EHRs). Following a modified Zelen design, all those identified are assigned to continued usual care (i.e., no contact) or to be offered one of two population-based outreach programs. A Care Management intervention includes systematic outreach to assess suicide risk, EHR-based tools to implement risk-based care pathways, and care management to facilitate recommended follow-up. A Skills Training intervention includes interactive online training in Dialectical Behavior Therapy skills, supported by reminder and reinforcement messages from a skills coach. Each intervention supplements, rather than replaces, usual care; participants may receive any other services normally available. Interventions are delivered primarily by secure messaging through EHR patient portals. Suicide attempts and deaths following randomization are identified using state vital statistics data and health system EHR and insurance claim data. Primary evaluation will compare risk of suicide attempt or death over 18?months according to the initial assignment, regardless of intervention participation. Recruitment is underway in three health systems (Group Health Cooperative, HealthPartners, and Kaiser Permanente Colorado). Over 2500 participants have been randomized as of 1 March 2016, with enrollment averaging approximately 100 per week. Assessing the effectiveness of population-based suicide prevention requires adherence to the principles of pragmatic trials: population-based enrollment, accepting variable treatment participation, assessing outcomes using health record data, and analyses based on intent-to-treat. ClinicalTrials.gov registration # NCT02326883 , registered on 23 December 2014.-Population-based outreach versus care as usual to prevent suicide attempt: study protocol for a randomized controlled trial.",0
"Physician-delivered tobacco treatment using the 5As is clinically recommended, yet its use has been limited. Lack of adequate training and confidence to provide tobacco treatment is cited as leading reasons for limited 5A use. Tobacco dependence treatment training while in medical school is recommended, but is minimally provided. The MSQuit trial (Medical Students helping patients Quit tobacco) aims to determine if a multi-modal and theoretically-guided tobacco educational intervention will improve tobacco dependence treatment skills (i.e. 5As) among medical students. 10 U.S. medical schools were pair-matched and randomized in a group-randomized controlled trial to evaluate whether a multi-modal educational (MME) intervention compared to traditional education (TE) will improve observed tobacco treatment skills. MME is primarily composed of TE approaches (i.e. didactics) plus a 1st year web-based course and preceptor-facilitated training during a 3rd year clerkship rotation. The primary outcome measure is an objective score on an Objective Structured Clinical Examination (OSCE) tobacco-counseling smoking case among 3rd year medical students from schools who implemented the MME or TE. MSQuit is the first randomized to evaluate whether a tobacco treatment educational intervention implemented during medical school will improve medical students' tobacco treatment skills. We hypothesize that the MME intervention will better prepare students in tobacco dependence treatment as measured by the OSCE. If a comprehensive tobacco treatment educational learning approach is effective, while also feasible and acceptable to implement, then medical schools may substantially influence skill development and use of the 5As among future physicians.-Teaching tobacco dependence treatment and counseling skills during medical school: rationale and design of the Medical Students helping patients Quit tobacco (MSQuit) group randomized controlled trial.",0
"Dose feasibility is a challenge that may arise in the development of adoptive T cell therapies for cancer. In early-phase clinical trials, dose is quantified either by a fixed or per unit body weight number of cells infused. It may not be feasible, however, to administer a patient's assigned dose due to an insufficient number of cells harvested or functional heterogeneity of the product. The study objective becomes to identify the maximum tolerated dose with high feasibility of being administered. This article describes a new dose-finding method that adaptively accounts for safety and feasibility endpoints in guiding dose allocation. We propose an adaptive dose-finding method that integrates accumulating feasibility and safety data to select doses for participant cohorts in early-phase trials examining adoptive cell immunotherapy. We sequentially model the probability of dose-limiting toxicity and the probability of feasibility using independent beta-binomial models. The probability model for toxicity borrows information across all dose levels using isotonic regression, allowing participants infused at a lower dose than his or her planned dose to contribute safety data to the dose-finding algorithm. We applied the proposed methodology in a single simulated trial and evaluated its operating characteristics through extensive simulation studies. In simulations conducted for a phase I study of adoptive immunotherapy for newly diagnosed glioblastoma, the proposed method demonstrates the ability to identify accurately the feasible maximum tolerated doses and to treat participants at and around these doses. Over 10 hypothesized scenarios studied, the percentage of correctly selecting the true feasible and maximum tolerated dose ranged from 50% to 90% with sample sizes averaging between 21 and 24 participants. A comparison to the only known existing method accounting for safety and feasibility yields competitive performance. We have developed a new practical adaptive dose-finding method to assess feasibility in early-phase adoptive cell therapy trials. A design that incorporates feasibility, as a function of the quantity and quality of the product manufactured, in addition to safety will have an impact on the recommended phase II doses in studies that evaluate patient outcomes.-Adaptive dose-finding based on safety and feasibility in early-phase clinical trials of adoptive cell immunotherapy.",0
"Bias-corrected covariance estimators are introduced in the context of an estimating equations approach for intracluster correlations among binary outcomes. Simulation study results show that the bias-corrected covariance estimators perform better than uncorrected sandwich estimators in terms of bias and coverage probabilities. Additionally, introduction of a matrix-based bias-correction into the estimating equations considerably improves point and interval estimation for the intracluster correlations. The methods are illustrated using data from a nested cross-sectional cluster trial on reducing underage drinking.-Finite sample adjustments in estimating equations and covariance estimators for intracluster correlations.",1
"Generalized linear mixed models with random intercepts and slopes provide useful analyses of clustered and longitudinal data and typically require the specification of the distribution of the random effects. Previous work for models with only random intercepts has shown that misspecifying the shape of this distribution may bias estimates of the intercept, but typically leads to little bias in estimates of covariate effects. Very few papers have examined the effects of misspecifying the joint distribution of random intercepts and slopes. However, simulation results in a recent paper suggest that misspecifying the shape of the random slope distribution can yield severely biased estimates of all model parameters. Using analytic results, simulation studies and fits to example data, this paper examines the bias in parameter estimates due to misspecification of the shape of the joint distribution of random intercepts and slopes. Consistent with results for models with only random intercepts, and contrary to the claims of severe bias in a recent paper, we show that misspecification of the joint distribution typically yields little bias in estimates of covariate effects and is restricted to covariates associated with the misspecified random effects distributions. We also show that misspecification of the distribution of random effects has little effect on confidence interval performance. Coverage rates based on the model-based standard errors from fitted likelihoods were generally quite close to nominal.-Estimation of covariate effects in generalized linear mixed models with a misspecified distribution of?random intercepts and slopes.",1
"Intervention trials of vector control methods often require community level randomization with appropriate inferential methods. For many interventions, the possibility of confounding due to the effects of health-care seeking behavior on disease ascertainment remains a concern. The test-negative design, a variant of the case-control method, was introduced to mitigate this issue in the assessment of the efficacy of influenza vaccination (measured at an individual level) on influenza infection. Here, we introduce a cluster-randomized test-negative design that includes randomization of the intervention at a group level. We propose several methods for estimation and inference regarding the relative risk (RR). The inferential methods considered are based on the randomization distribution induced by permuting intervention assignment across two sets of randomly selected clusters. The motivating example is a current study of the efficacy of randomized releases of Wolbachia-infected Aedes aegypti mosquitoes to reduce the incidence of dengue in Yogyakarta City, Indonesia. Estimation and inference techniques are assessed through a simulation study.-Analysis of cluster-randomized test-negative designs: cluster-level methods.",1
"Biomarkers are playing an increasingly important role in disease screening, early detection, and risk prediction. The two-phase case-control sampling study design is widely used for the evaluation of candidate biomarkers. The sampling probabilities for cases and controls in the second phase can often depend on other covariates (sampling strata). This biased sampling can lead to invalid inference on a biomarker's classification accuracy if not properly accounted for. In this paper, we adopt the idea of inverse probability weighting and develop inverse probability weighting-based estimators for various measures of a biomarker's classification performance, including the points on the receiver operating characteristics (ROCs) curve, the area under the ROC curve (area under the curve), and the partial area under the curve. In particular, we consider classification accuracy estimators using sampling weights estimated conditionally on sampling strata and further improve their efficiency through the use of estimated weights that additionally take into account the auxiliary variables available from the phase-one cohort. We develop asymptotic properties of the proposed estimators and provide analytical variance for making inference. Extensive simulation studies demonstrate excellent performance of the proposed weighted estimators, while the traditional empirical estimator can be severely biased. We also investigate the advantages in efficiency gain for estimating various classification accuracy estimators through the use of auxiliary variables in addition to sampling strata and apply the proposed method to examples from a renal artery stenosis study and a prostate cancer study.-Evaluating classification performance of biomarkers in two-phase case-control studies.",0
"Only 40-60% of patients with generalized anxiety disorder experience long-lasting improvement with gold standard psychosocial interventions. Identifying neurobehavioral factors that predict treatment success might provide specific targets for more individualized interventions, fostering more optimal outcomes and bringing us closer to the goal of ""personalized medicine."" Research suggests that reward and threat processing (approach/avoidance behavior) and cognitive control may be important for understanding anxiety and comorbid depressive disorders and may have relevance to treatment outcomes. This study was designed to determine whether approach-avoidance behaviors and associated neural responses moderate treatment response to exposure-based versus behavioral activation therapy for generalized anxiety disorder. We are conducting a randomized controlled trial involving two 10-week group-based interventions: exposure-based therapy or behavioral activation therapy. These interventions focus on specific and unique aspects of threat and reward processing, respectively. Prior to and after treatment, participants are interviewed and undergo behavioral, biomarker, and neuroimaging assessments, with a focus on approach and avoidance processing and decision-making. Primary analyses will use mixed models to examine whether hypothesized approach, avoidance, and conflict arbitration behaviors and associated neural responses at baseline moderate symptom change with treatment, as assessed using the Generalized Anxiety Disorder-7 item scale. Exploratory analyses will examine additional potential treatment moderators and use data reduction and machine learning methods. This protocol provides a framework for how studies may be designed to move the field toward neuroscience-informed and personalized psychosocial treatments. The results of this trial will have implications for approach-avoidance processing in generalized anxiety disorder, relationships between levels of analysis (i.e., behavioral, neural), and predictors of behavioral therapy outcome. The study was retrospectively registered within 21 days of first participant enrollment in accordance with FDAAA 801 with ClinicalTrials.gov, NCT02807480. Registered on June 21, 2016, before results.-Protocol for a randomized controlled trial examining multilevel prediction of response to behavioral activation and exposure-based therapy for generalized anxiety disorder.",0
"During the course of a clinical trial, subjects may experience treatment failure. For ethical reasons, it is necessary to administer emergency or rescue medications for such subjects. However, the rescue medications may bias the set of response measurements. This bias is of particular concern if a subject has been randomized to the control group, and the rescue medications improve the subject's condition. The standard approach to analysing data from a clinical trial is to perform an intent-to-treat (ITT) analysis, wherein the data are analysed according to treatment randomization. Supplementary analyses may be performed in addition to the ITT analysis to account for the effect of treatment failures and rescue medications. A Bayesian, counterfactual approach, which uses the data augmentation (DA) algorithm, is proposed for supplemental analysis. A simulation study is conducted to compare the operating characteristics of this procedure with a likelihood-based, counterfactual approach based on the EM algorithm. An example from the Asthma Clinical Research Network (ACRN) is used to illustrate the Bayesian procedure.-Bayesian inference for randomized clinical trials with treatment failures.",0
"This study assesses psychometric properties of the Modified Mini-Mental State Exam (3MS) and present population norms and demographic risk factors for low 3MS scores. The subjects were 885 persons aged 65 and older who took the 3MS as part of the Stirling County Study, a population-based longitudinal study of adult residents of a county in Atlantic Canada. 3MS scores were not dependent on the specific rater who scored the test; thus, the 3MS is free of rater bias. Interrater reliability was high (intraclass correlation coefficient=0.98), as was internal consistency (coefficient alpha=0.91). Test-retest reliability over 3 years was 0.78. One third of subjects tested as cognitively impaired. Risk factors for low scores include older age, less education, male gender, and examination in French. The correlation between 3MS and Mini Mental State Exam scores was 0.95. The 3MS can be used as an epidemiologic measure of global cognitive performance among elderly persons.-Characteristics of the Modified Mini-Mental State Exam among elderly persons.",0
"Most statistical methods that adjust analyses for measurement error assume that the target exposure T is a fixed quantity for each individual. However, in many applications, the value of T for an individual varies with time. We develop a model that accounts for such variation, describing the model within the framework of a meta-analysis of validation studies of dietary self-report instruments, where the reference instruments are biomarkers. We demonstrate that in this application, the estimates of the attenuation factor and correlation with true intake, key parameters quantifying the accuracy of the self-report instrument, are sometimes substantially modified under the time-varying exposure model compared with estimates obtained under a traditional fixed-exposure model. We conclude that accounting for the time element in measurement error problems is potentially important.-A statistical model for measurement error that incorporates variation over time in the target measure, with application to nutritional epidemiology.",0
"Power, sample size and sampling costs for clustered data",1
"In clinical trials, the statistical concepts of significance and power are used in the determination of sample size for trials. The trialist must provide an estimate of standard deviation and a hypothetical population difference to be detected. This must be modified to deal with the designs encountered in guideline research. These are cluster randomized trials, because the patients of a single doctor or practice form a cluster. The trialist must be able to provide information about the effects of clustering, in the form of an intraclass correlation coefficient.-Sample size in guidelines trials.",1
"We propose a nonparametric approach for cumulative incidence estimation when causes of failure are unknown or missing for some subjects. Under the missing at random assumption, we estimate the cumulative incidence function using multiple imputation methods. We develop asymptotic theory for the cumulative incidence estimators obtained from multiple imputation methods. We also discuss how to construct confidence intervals for the cumulative incidence function and perform a test for comparing the cumulative incidence functions in two samples with missing cause of failure. Through simulation studies, we show that the proposed methods perform well. The methods are illustrated with data from a randomized clinical trial in early stage breast cancer.-Multiple imputation methods for nonparametric inference on cumulative incidence with missing cause of failure.",0
"Hospital mortality outcomes for acute myocardial infarction (AMI) patients are a focus of quality improvement programs conducted by government agencies. AMI mortality risk-adjustment models using administrative data typically adjust for baseline differences in mortality risk with a limited set of common and definite comorbidities. In this study, we present an AMI mortality risk-adjustment model that adjusts for comorbid disease and for AMI severity using information from secondary diagnoses reported as present at admission for California hospital patients. AMI patients were selected from California hospital administrative data for 1996 through 1999 according to criteria used by the California Hospital Outcomes Project Report on Heart Attack Outcomes, a state-mandated public report that compares hospital mortality outcomes. We compared results for the new model to two mortality risk-adjustment models used to assess hospital AMI mortality outcomes by the state of California, and to two other models used in prior research. The model using present-at-admission diagnoses obtained substantially better discrimination between predicted survival and inpatient death than the other models we considered. AMI mortality risk-adjustment methods can be meaningfully improved using present-at-admission diagnoses to identify comorbid disease and conditions related closely to AMI.-Present-at-admission diagnoses improved mortality risk adjustment among acute myocardial infarction patients.",0
"MIXREGLS is a program which provides estimates for a mixed-effects location scale model assuming a (conditionally) normally-distributed dependent variable. This model can be used for analysis of data in which subjects may be measured at many observations and interest is in modeling the mean and variance structure. In terms of the variance structure, covariates can by specified to have effects on both the between-subject and within-subject variances. Another use is for clustered data in which subjects are nested within clusters (e.g., clinics, hospitals, schools, etc.) and interest is in modeling the between-cluster and within-cluster variances in terms of covariates. MIXREGLS was written in Fortran and uses maximum likelihood estimation, utilizing both the EM algorithm and a Newton-Raphson solution. Estimation of the random effects is accomplished using empirical Bayes methods. Examples illustrating stand-alone usage and features of MIXREGLS are provided, as well as use via the SAS and R software packages.-MIXREGLS: A Program for Mixed-Effects Location Scale Analysis.",1
"In longitudinal cluster randomized clinical trials (cluster-RCT), subjects are nested within a higher level unit such as clinics and are evaluated for outcome repeatedly over the study period. This study design results in a three level hierarchical data structure. When the primary goal is to test the hypothesis that an intervention has an effect on the rate of change in the outcome over time and the between-subject variation in slopes is substantial, the subject-specific slopes are often modeled as random coefficients in a mixed-effects linear model. In this paper, we propose approaches for determining the samples size for each level of a 3-level hierarchical trial design based on ordinary least squares (OLS) estimates for detecting a difference in mean slopes between two intervention groups when the slopes are modeled as random. Notably, the sample size is not a function of the variances of either the second or the third level random intercepts and depends on the number of second and third level data units only through their product. Simulation results indicate that the OLS-based power and sample sizes are virtually identical to the empirical maximum likelihood based estimates even with varying cluster sizes. Sample sizes for random versus fixed slope models are also compared. The effects of the variance of the random slope on the sample size determinations are shown to be enormous. Therefore, when between-subject variations in outcome trends are anticipated to be significant, sample size determinations based on a fixed slope model can result in a seriously underpowered study.-Sample size requirements to detect an intervention by time interaction in longitudinal cluster randomized clinical trials with random slopes.",1
"We consider inference procedures on intraclass correlations for unbalanced data from several multivariate normal populations. We derive several tests, including ones based on Fisher's variance stabilizing transformation and Neyman's score functions, to test the homogeneity of intraclass correlations. We illustrate the methodology with an example that uses arterial blood pressure data collected by Miall and Oldham and we compare the procedures in terms of their empirical levels and powers with a Monte Carlo simulation study. We recommend the use of Neyman's C(alpha) test and a test based on the ANOVA estimators of the intraclass correlations as they hold their significance levels and give consistently higher powers.-Statistical analysis of intraclass correlations from multiple samples with applications to arterial blood pressure data.",1
"Comparisons of baseline covariates in randomised controlled trials whilst often undertaken is regarded by many as an exercise in futility. Because of randomisation the null hypothesis is true for baseline comparisons and therefore any differences will occur by chance. However, this is only the case if allocations are not known in advance of recruitment. If this occurs then selection bias at randomisation may be present and it is possible that the statistical testing of covariates may unveil selection bias. In this paper we show that this is particularly the case for cluster randomised trials when post-randomised recruitment often occurs and can lead to selection bias. We take a recently published cluster randomised trial that has suffered from selection bias due to differential recruitment and calculate baseline p values. We show that statistically significant imbalances of p &lt; 0.0001 occurred in 5 of the 10 covariates. In comparison for an individually randomised trial that had no evidence of selection bias only 1 p value of p &lt; 0.05 out of 20 tests was observed. Had baseline p values for the cluster trial been presented to journal editors, reviewers and readers then the results of the trial might have been treated with more caution. We argue that the blanket ban of baseline testing as advocated by some may reduce the chance of identifying deficient cluster randomised trials and this opposition should be reconsidered for cluster trials.-Baseline testing in cluster randomised controlled trials: should this be done?",1
"To examine the association between sickness absence and mortality compared with associations between established health indicators and mortality. Prospective cohort study. Medical examination and questionnaire survey conducted in 1985-8; sickness absence records covered the period 1985-98. 20 civil service departments in London. 6895 male and 3413 female civil servants aged 35-55 years. All cause mortality until the end of 1999. After adjustment for age and grade, men and women who had more than five medically certified absences (spells &gt; 7 days) per 10 years had a mortality 4.8 (95% confidence interval 3.3 to 6.9) and 2.7 (1.5 to 4.9) times greater than those with no such absence. Poor self rated health, presence of longstanding illness, and a measure of common clinical conditions comprising diabetes, diagnosed heart disease, abnormalities on electrocardiogram, hypertension, and respiratory illness were all associated with mortality--relative rates between 1.3 and 1.9. In a multivariate model including all the above health indicators and additional health risk factors, medically certified sickness absence remained a significant predictor of mortality. No linear association existed between self certified absence (spells 1-7 days) and mortality, but the findings suggest that a small amount of self certified absence is protective. Evidence linking sickness absence to mortality indicates that routinely collected sickness absence data could be used as a global measure of health differentials between employees. However, such approaches should focus on medically certified (or long term) absences rather than self certified absences.-Sickness absence as a global measure of health: evidence from mortality in the Whitehall II prospective cohort study.",0
"Cluster randomized trials (CRTs) are increasingly used to evaluate the effectiveness of health-care interventions. A key feature of CRTs is that the observations on individuals within clusters are correlated as a result of between-cluster variability. Sample size formulae exist which account for such correlations, but they make different assumptions regarding the between-cluster variability in the intervention arm of a trial, resulting in different sample size estimates. We explore the relationship for binary outcome data between two common measures of between-cluster variability: k, the coefficient of variation and rho, the intracluster correlation coefficient. We then assess how the assumptions of constant k or rho across treatment arms correspond to different assumptions about intervention effects. We assess implications for sample size estimation and present a simple solution to the problems outlined.-Measures of between-cluster variability in cluster randomized trials with binary outcomes.",1
"Neuroblastoma is a childhood cancer with patients experiencing heterogeneous survival outcomes despite aggressive treatment. Disease outcomes range from early death to spontaneous regression of the tumor followed by cure. Owing to this heterogeneity, it is of interest to identify patients with similar types of neuroblastoma so that specific types of treatment can be developed. Oncologists are especially interested in identifying patients who will be cured so that the minimum amount of a potentially toxic treatment can be given to this group of patients. We analyze a large cohort of neuroblastoma patients and develop a finite mixture model that uses covariates to predict the probability of being in a cure group or other (one or more) risk groups. A prediction method is developed that uses the estimated probabilities to assign a patient to different risk groups. The robustness of the model and the prediction method is examined via simulation by looking at misclassification rates under misspecified models.-A finite mixture survival model to characterize risk groups of neuroblastoma.",0
"In clinical trials, a surrogate outcome ( S) can be measured before the outcome of interest ( T) and may provide early information regarding the treatment ( Z) effect on T. Many methods of surrogacy validation rely on models for the conditional distribution of T given Z and S. However, S is a post-randomization variable, and unobserved, simultaneous predictors of S and T may exist, resulting in a non-causal interpretation. Frangakis and Rubin developed the concept of principal surrogacy, stratifying on the joint distribution of the surrogate marker under treatment and control to assess the association between the causal effects of treatment on the marker and the causal effects of treatment on the clinical outcome. Working within the principal surrogacy framework, we address the scenario of an ordinal categorical variable as a surrogate for a censored failure time true endpoint. A Gaussian copula model is used to model the joint distribution of the potential outcomes of T, given the potential outcomes of S. Because the proposed model cannot be fully identified from the data, we use a Bayesian estimation approach with prior distributions consistent with reasonable assumptions in the surrogacy assessment setting. The method is applied to data from a colorectal cancer clinical trial, previously analyzed by Burzykowski et?al.-Surrogacy assessment using principal stratification and a Gaussian copula model.",0
"The decision to randomize by clusters of subjects such as a classroom or clinic versus individual randomization where some contamination may occur is examined within the framework of sample size issues. Estimates for background rates and intraclass correlations are also provided for adolescent tobacco and alcohol outcomes derived from a recent study using cluster randomization. A ratio of adjusted sample sizes is derived which is a function of the intraclass correlation and cluster size for cluster randomization and total amount of contamination for individual randomization. Using estimated incidence rates and intraclass correlations, we provide a comparison of sample sizes for two plausible study outcomes. Small clusters such as a family or small classroom tend to have stronger within cluster dependence and cluster randomization would be clearly favoured over individual randomization. For moderately sized clusters, if contamination levels are likely to be high then cluster randomization would be a better choice. However in some situations where lower levels of contamination are expected, individual randomization may be preferred. With larger clusters, individual randomization should be considered when contamination rates are expected to be low. Investigators must carefully consider the choice of cluster randomization versus individual randomization in the context of likely contamination. In this paper we provided a basis for making this decision as well as examples to illustrate these decisions, and parameter estimates that will be especially useful for investigators in adolescent tobacco and alcohol studies.-Cluster versus individual randomization in adolescent tobacco and alcohol studies: illustrations for design decisions.",1
"We present a rank-based test statistic for the identification of differentially expressed genes using a distance measure. The proposed test statistic is highly robust against extreme values and does not assume the distribution of parent population. Simulation studies show that the proposed test is more powerful than some of the commonly used methods, such as paired t-test, Wilcoxon signed rank test, and significance analysis of microarray (SAM) under certain non-normal distributions. The asymptotic distribution of the test statistic, and the p-value function are discussed. The application of proposed method is shown using a real-life data set.-Finding differentially expressed genes in high dimensional data: Rank based test statistic via a distance measure.",0
"Measurement of serum biomarkers by multiplex assays may be more variable as compared to single biomarker assays. Measurement error in these data may bias parameter estimates in regression analysis, which could mask true associations of serum biomarkers with an outcome. The Least Absolute Shrinkage and Selection Operator (LASSO) can be used for variable selection in these high-dimensional data. Furthermore, when the distribution of measurement error is assumed to be known or estimated with replication data, a simple measurement error correction method can be applied to the LASSO method. However, in practice the distribution of the measurement error is unknown and is expensive to estimate through replication both in monetary cost and need for greater amount of sample which is often limited in quantity. We adapt an existing bias correction approach by estimating the measurement error using validation data in which a subset of serum biomarkers are re-measured on a random subset of the study sample. We evaluate this method using simulated data and data from the Tucson Epidemiological Study of Airway Obstructive Disease (TESAOD). We show that the bias in parameter estimation is reduced and variable selection is improved.-Measurement error correction in the least absolute shrinkage and selection operator model when validation data are available.",0
"We examined the impact of weighting the generalized estimating equation (GEE) by the inverse of the number of sex acts on the magnitude of association for factors predictive of recent condom use. Data were analyzed from a cross-sectional survey on condom use reported during vaginal intercourse during the past year among male students attending two Georgia universities. The usual GEE model was fit to the data predicting the binary act-specific response indicating whether a condom was used. A second cluster-weighted GEE model (i.e., weighting the GEE score equation by the inverse of the number of sex acts) was also fit to predict condom use. Study participants who engaged in a greater frequency of sex acts were less likely to report condom use, resulting in nonignorable cluster-size data. The GEE analysis weighted by sex act (usual GEE) and the GEE analysis weighted by study subject (cluster-weighted GEE) produced different estimates of the association between the covariates and condom use in last year. For example, the cluster-weighted GEE analysis resulted in a marginally significant relationship between age and condom use (odds ratio of 0.49 with 95% confidence interval (0.23-1.03) for older versus younger participants) versus a nonsignificant relationship with the usual GEE model (odds ratio of 0.67 with a 95% confidence interval of 0.28-1.60). The two ways of weighting the GEE score equation, by the sex act or by the respondent, may produce different results and a different interpretation of the parameters in the presence of nonignorable cluster size.-Weighting condom use data to account for nonignorable cluster size.",1
Selected methodological issues in evaluating community-based health promotion and disease prevention programs.,1
"The Pediatric HIV/AIDS Cohort Study's Surveillance Monitoring of ART Toxicities Study is a prospective cohort study conducted at 22 US sites between 2007 and 2011 that was designed to evaluate the safety of in utero antiretroviral drug exposure in children not infected with human immunodeficiency virus who were born to mothers who were infected. This ongoing study uses a ""trigger-based"" design; that is, initial assessments are conducted on all children, and only those meeting certain thresholds or ""triggers"" undergo more intensive evaluations to determine whether they have had an adverse event (AE). The authors present the estimated rates of AEs for each domain of interest in the Surveillance Monitoring of ART Toxicities Study. They also evaluated the efficiency of this trigger-based design for estimating AE rates and for testing associations between in utero exposures to antiretroviral drugs and AEs. The authors demonstrate that estimated AE rates from the trigger-based design are unbiased after correction for the sensitivity of the trigger for identifying AEs. Even without correcting for bias based on trigger sensitivity, the trigger approach is generally more efficient for estimating AE rates than is evaluating a random sample of the same size. Minor losses in efficiency when comparing AE rates between persons exposed and unexposed in utero to particular antiretroviral drugs or drug classes were observed under most scenarios.-A trigger-based design for evaluating the safety of in utero antiretroviral exposure in uninfected children of human immunodeficiency virus-infected mothers.",0
"This article studies the effect of attrition in the cluster randomized crossover trial. The focus is on the two-treatment two-period AB/BA design where attrition occurs during the washout period. Attrition may occur at either the subject level or the cluster level. In the latter case, clusters drop out entirely and provide no measurements in the second period. Subject attrition can only occur in the cohort design, where each subject receives both treatments. Cluster attrition can also occur in the cross-sectional design, where different subjects are measured in the two time periods. Furthermore, this article explores two different strategies to account for potential levels of attrition: increasing sample size and replacing those subjects who drop out by others. The statistical model that takes into account the nesting of subjects within clusters, and the nesting of repeated measurements within subjects is presented. The effect of attrition is evaluated on the basis of the efficiency of the treatment effect estimator. Matrix algebra is used to derive the relation between efficiency, the degree of attrition, cluster size and the intraclass correlations: the within-cluster within-period correlation, the within-cluster between-period correlation and (in the case of a cohort design) the within-subject correlation. The methodology is implemented in two Shiny Apps. Attrition in a cluster randomized crossover trial implies a loss of efficiency. Efficiency decreases with an increase of the attrition rate. The loss of efficiency due to attrition of subjects in a cohort design is largest for small number of subjects per cluster-period, but it may be repaired to a large degree by increasing the number of subjects per cluster-period or by replacing those subjects who drop out by others. Attrition of clusters results in a larger loss of efficiency, but this loss does not depend on the number of subjects per cluster-period. Repairing for this loss requires a large increase in the number of subjects per cluster-period. The methodology of this article is illustrated by an example on the effect of lavender scent on dental patients' anxiety. This article provides the methodology of exploring the effect of attrition in cluster randomized crossover trials, and to repair for attrition. As such, it helps researchers plan their trial in an appropriate way and avoid underpowered trials. To use the methodology, prior estimates of the degree of attrition and intraclass correlation coefficients are needed. It is advocated that researchers clearly report the estimates of these quantities to help facilitate planning future trials.-The cluster randomized crossover trial: The effects of attrition in the AB/BA design and how to account for it in sample size calculations",1
"This paper studies summary measures of the predictive power of a generalized linear model, paying special attention to a generalization of the multiple correlation coefficient from ordinary linear regression. The population value is the correlation between the response and its conditional expectation given the predictors, and the sample value is the correlation between the observed response and the model predicted value. We compare four estimators of the measure in terms of bias, mean squared error and behaviour in the presence of overparameterization. The sample estimator and a jack-knife estimator usually behave adequately, but a cross-validation estimator has a large negative bias with large mean squared error. One can use bootstrap methods to construct confidence intervals for the population value of the correlation measure and to estimate the degree to which a model selection procedure may provide an overly optimistic measure of the actual predictive power.-Summarizing the predictive power of a generalized linear model.",0
"Transfusion research seeks to improve survival for severely injured and hemorrhaging patients using optimal plasma and platelet ratios over red blood cells (RBCs). However, most published studies comparing different ratios are plagued with serious bias and ignore time-varying effects. We applied joint recurrent event frailty models to increase validity and clinical utility. Using the PRospective Observational Multicenter Major Trauma Transfusion study data, our joint random-effects models estimated the association of (1) clinical covariates with transfusion rate intensities and (2) varying plasma:RBC and platelet:RBC ratios with survival over the 24?hours after hospital admission. Along with survival time, baseline patient vital signs, laboratory values, and longitudinal data on types and volumes of transfusions were included. Baseline systolic blood pressure, heart rate, pH, and hemoglobin were significantly associated with RBC transfusion rates. Increased transfusion rates (per hour) of plasma (P?=?0.05), platelets (P?&lt;?0.001), or RBCs were associated with increased 24-hour mortality. Higher ratios of plasma:RBC (P?=?0.107) and platelet:RBC (P?&lt;?0.001) were associated with reduced mortality in a time-varying pattern (P?&lt;?0.001). The proposed joint analysis of transfusion rates and ratios offers a more valid statistical approach to evaluate survival effects in the presence of informative censoring by early death.-Recurrent event frailty models reduced time-varying and other biases in?evaluating transfusion protocols for traumatic hemorrhage.",0
"Cluster randomised controlled trials (CRCTs) are frequently used in health service evaluation. Assuming an average cluster size, required sample sizes are readily computed for both binary and continuous outcomes, by estimating a design effect or inflation factor. However, where the number of clusters are fixed in advance, but where it is possible to increase the number of individuals within each cluster, as is frequently the case in health service evaluation, sample size formulae have been less well studied. We systematically outline sample size formulae (including required number of randomisation units, detectable difference and power) for CRCTs with a fixed number of clusters, to provide a concise summary for both binary and continuous outcomes. Extensions to the case of unequal cluster sizes are provided. For trials with a fixed number of equal sized clusters (k), the trial will be feasible provided the number of clusters is greater than the product of the number of individuals required under individual randomisation (nI) and the estimated intra-cluster correlation (?). So, a simple rule is that the number of clusters (k) will be sufficient provided: [formula in text]. Where this is not the case, investigators can determine the maximum available power to detect the pre-specified difference, or the minimum detectable difference under the pre-specified value for power. Designing a CRCT with a fixed number of clusters might mean that the study will not be feasible, leading to the notion of a minimum detectable difference (or a maximum achievable power), irrespective of how many individuals are included within each cluster.-Sample size calculations for cluster randomised controlled trials with a fixed number of clusters.",1
"The method of quadratic inference functions (QIF) is an increasingly popular method for the analysis of correlated data because of its multiple advantages over generalized estimating equations (GEE). One advantage is that it is more efficient for parameter estimation when the working covariance structure for the data is misspecified. In the QIF literature, the asymptotic covariance formula is used to obtain standard errors. We show that in small to moderately sized samples, these standard error estimates can be severely biased downward, therefore inflating test size and decreasing coverage probability. We propose adjustments to the asymptotic covariance formula that eliminate finite-sample biases and, as shown via simulation, lead to substantial improvements in standard error estimates, inference, and coverage. The proposed method is illustrated in application to a cluster randomized trial and a longitudinal study. Furthermore, QIF and GEE are contrasted via simulation and these applications.-A bias-corrected covariance estimate for improved inference with quadratic inference functions.",1
"In contrast to prior observational studies, hormone replacement therapy (HRT) did not prevent coronary heart disease in the Women's Health Initiative Randomized Controlled Trial (WHI RCT). To assess the validity of a novel observational study design, we compared the WHI RCT with a simulation using data from the United Kingdom General Practice Research Database (GPRD). A cohort from GPRD was used to simulate the WHI RCT by replicating, to the extent possible, all aspects of the RCT except randomization. The study included 37,730 Unexposed and 13,658 Exposed women treated with estrogen and norgestrel. Myocardial infarction (adjusted hazard ratio 0.95 [0.78-1.16]) was not decreased significantly in the GPRD Exposed group. Similar to the WHI RCT, stroke, venous thromboembolic events, and breast cancer were increased; and colorectal cancer was decreased. Although death appeared to decrease in the total cohort, it was unaltered in a subset of subjects without missing data on baseline covariates. A structured comparison using data from GPRD was largely concordant with the WHI RCT and did not show a cardioprotective effect of HRT. These findings further generalize the results of WHI and reinforce the potential utility of this analytic approach.-A simulation using data from a primary care practice database closely replicated the women's health initiative trial.",0
"This paper discusses the choice of randomization tests for inferences from cluster-randomized trials that have been designed to ensure a balanced allocation of clusters to treatments. Methods for covariate-adjusted randomization tests are reviewed and their application to balanced cluster-randomized trials discussed. Two cluster-randomized trials with balanced designs are used to illustrate the choices that can be made in selecting a randomization test, and methods for obtaining confidence intervals for treatment effects are illustrated. The balance imposed by the randomization in these trials makes adjustment for covariates less beneficial than for an unbalanced design. However, the adjusted analyses do not appear generally to have worse properties than the unadjusted ones, and may provide protection against any imbalance that has not been controlled for in the design. The only case when adjustment for covariates may result in worse precision is when a large number of cluster-level covariates are included in the analysis. An expression is provided that allows the size of this effect to be calculated for any given set of cluster-level covariates.-Randomization inference for balanced cluster-randomized trials.",1
"Interventions to alleviate stigma are demonstrating effectiveness across a range of conditions, though few move beyond the pilot phase, especially in low- and middle-income countries (LMICs). Implementation science offers tools to study complex interventions, understand barriers to implementation, and generate evidence of affordability, scalability, and sustainability. Such evidence could be used to convince policy-makers and donors to invest in implementation. However, the utility of implementation research depends on its rigor and replicability. Our objectives were to systematically review implementation studies of health-related stigma reduction interventions in LMICs and critically assess the reporting of implementation outcomes and intervention descriptions. PubMed, CINAHL, PsycINFO, and EMBASE were searched for evaluations of stigma reduction interventions in LMICs reporting at least one implementation outcome. Study- and intervention-level characteristics were abstracted. The quality of reporting of implementation outcomes was assessed using a five-item rubric, and the comprehensiveness of intervention description and specification was assessed using the 12-item Template for Intervention Description and Replication (TIDieR). A total of 35 eligible studies published between 2003 and 2017 were identified; of these, 20 (57%) used qualitative methods, 32 (91%) were type 1 hybrid effectiveness-implementation studies, and 29 (83%) were evaluations of once-off or pilot implementations. No studies adopted a formal theoretical framework for implementation research. Acceptability (20, 57%) and feasibility (14, 40%) were the most frequently reported implementation outcomes. The quality of reporting of implementation outcomes was low. The 35 studies evaluated 29 different interventions, of which 18 (62%) were implemented across sub-Saharan Africa, 20 (69%) focused on stigma related to HIV/AIDS, and 28 (97%) used information or education to reduce stigma. Intervention specification and description was uneven. Implementation science could support the dissemination of stigma reduction interventions in LMICs, though usage to date has been limited. Theoretical frameworks and validated measures have not been used, key implementation outcomes like cost and sustainability have rarely been assessed, and intervention processes have not been presented in detail. Adapted frameworks, new measures, and increased LMIC-based implementation research capacity could promote the rigor of future stigma implementation research, helping the field deliver on the promise of stigma reduction interventions worldwide.-Implementation science and stigma reduction interventions in low- and middle-income countries: a systematic review.",0
"Catastrophe theory (Thom, 1972, 1993) is the study of the many ways in which continuous changes in a system's parameters can result in discontinuous changes in 1 or several outcome variables of interest. Catastrophe theory-inspired models have been used to represent a variety of change phenomena in the realm of social and behavioral sciences. Despite their promise, widespread applications of catastrophe models have been impeded, in part, by difficulties in performing model fitting and model comparison procedures. We propose a new modeling framework for testing 1 kind of catastrophe model-the cusp catastrophe model-as a mixture structural equation model (MSEM) when cross-sectional data are available; or alternatively, as an MSEM with regime-switching (MSEM-RS) when longitudinal panel data are available. The proposed models and the advantages offered by this alternative modeling framework are illustrated using 2 empirical examples and a simulation study.-The cusp catastrophe model as cross-sectional and longitudinal mixture structural equation models.",0
"What is the extent and effect of excessive testing for glycated hemoglobin (HbA1c) among adults with controlled type 2 diabetes? A retrospective analysis of data from a national administrative claims database included commercially insured individuals in the USA, 2001-13. Study patients were aged 18 years or older, had type 2 diabetes with stable glycemic control (two consecutive tests showing HbA1c&lt;7.0% within 24 months), did not use insulin, had no history of severe hypoglycemia or hyperglycemia, and were not pregnant. HbA1c testing frequency was measured within 24 months after the second (index) HbA1c test, and classified as guideline recommended (? 2 times/year), frequent (3-4 times/year), and excessive (? 5 times/year). Changes in treatment regimen were ascertained within three months of the index test. Of 31,545 patients in the study cohort (mean age 58 years; mean index HbA1c 6.2%), HbA1c testing frequency was excessive in 6% and frequent in 55%. Despite good glycemic control at baseline, treatment was further intensified by addition of glucose lowering drugs or insulin in 8.4% of patients (comprising 13%, 9%, and 7% of those tested excessively, frequently, and per guidelines, respectively; P&lt;0.001). Compared with guideline recommended testing, excessive testing was associated with treatment intensification (odds ratio 1.35 (95% confidence interval 1.22 to 1.50)). Excessive testing rates remained unchanged in 2001-08, but fell significantly after 2009. The odds of excessive testing was 46% lower in 2011 than in 2001-02. The study population is not representative of all US patients with type 2 diabetes because it was restricted to commercially insured adults with stable and controlled diabetes not receiving insulin treatment. The study design did not capture the underuse of HbA1c testing. In this US cohort of adults with stable and controlled type 2 diabetes, more than 60% received too many HbA1c tests, a practice associated with potential overtreatment with hypoglycemic drugs. Excessive testing contributes to the growing problem of waste in healthcare and increased patient burden in diabetes management. NDS and RGM are funded partly by the Agency for Healthcare Research and Quality (R18HS18339) and AcademyHealth Delivery System Science Fellowship (2013), respectively. No competing interests declared. Additional data are available from mccoy.rozalina@mayo.edu.-HbA1c overtesting and overtreatment among US adults with controlled type 2 diabetes, 2001-13: observational population based study.",0
"Group testing is an indispensable tool for laboratories when testing high volumes of clinical specimens for infectious diseases. An important decision that needs to be made prior to implementation is determining what group sizes to use. In best practice, an objective function is chosen and then minimized to determine an optimal set of these group sizes, known as the optimal testing configuration (OTC). There are a few options for objective functions, and they differ based on how the expected number of tests, assay characteristics, and testing constraints are taken into account. These varied options have led to a recent controversy in the literature regarding which of two different objective functions is better. In our paper, we examine these objective functions over a number of realistic situations for infectious disease testing. We show that this controversy may be much ado about nothing because the OTCs and corresponding results (eg, number of tests and accuracy) are largely the same for standard testing algorithms in a wide variety of situations.-The objective function controversy for group testing: Much ado about nothing?",0
"Combining different treatment regimens provides an effective approach to induce a synergistic treatment effect and overcome resistance to monotherapy. The challenge is that, given the large number of existing monotherapies, the number of possible combinations is huge and new potentially more efficacious compounds may become available any time during drug development. To address this challenge, we propose a flexible Bayesian drug combination platform design with adaptive shrinkage (ComPAS), which allows for dropping futile combinations, graduating effective combinations, and adding new combinations during the course of the trial. A new adaptive shrinkage method is developed to adaptively borrow information across combinations and efficiently identify the efficacious combinations based on Bayesian model selection and hierarchical models. Simulation studies show that ComPAS identifies the effective combinations with higher probability than some existing designs. ComPAS provides an efficient and flexible platform to accelerate drug development in a seamless and timely fashion.-ComPAS: A Bayesian drug combination platform trial design with adaptive shrinkage.",0
Optimal permutation tests for the analysis of group randomized trials,1
"Eating disorders (EDs) are common amongst women; however, no research has specifically investigated the lifetime/12-month prevalence of eating disorders amongst women in mid-life (i.e., fourth and fifth decade of life) and the relevant longitudinal risk factors. We aimed to investigate the lifetime and 12-month prevalence of EDs and lifetime health service use and to identify childhood, parenting, and personality risk factors. This is a two-phase prevalence study, nested within an existing longitudinal community-based sample of women in mid-life. A total of 5658 women from the UK Avon Longitudinal Study of Parents and Children (ALSPAC; enrolled 20?years earlier) participated. ED diagnoses were obtained using validated structured interviews. Weighted analyses were carried out accounting for the two-phase methodology to obtain prevalence figures and to carry out risk factor regression analyses. By mid-life, 15.3% (95% confidence intervals, 13.5-17.4%) of women had met criteria for a lifetime ED. The 12-month prevalence of EDs was 3.6%. Childhood sexual abuse was prospectively associated with all binge/purge type disorders and an external locus of control was associated with binge-eating disorder. Better maternal care was protective for bulimia nervosa. Childhood life events and interpersonal sensitivity were associated with all EDs. By mid-life a significant proportion of women will experience an ED, and few women accessed healthcare. Active EDs are common in mid-life, both due to new onset and chronic disorders. Increased awareness of the full spectrum of EDs in this stage of life and adequate service provision is important. This is the first study to investigate childhood and personality risk factors for full threshold and sub-threshold EDs and to identify common predictors for full and sub-threshold EDs. Further research should clarify the role of preventable risk factors on both full and sub-threshold EDs.-Lifetime and 12-month prevalence of eating disorders amongst women in mid-life: a population-based study of diagnoses and risk factors.",0
"Standard sample size calculation formulas for stepped wedge cluster randomized trials (SW-CRTs) assume that cluster sizes are equal. When cluster sizes vary substantially, ignoring this variation may lead to an under-powered study. We investigate the relative efficiency of a SW-CRT with varying cluster sizes to equal cluster sizes, and derive variance estimators for the intervention effect that account for this variation under a mixed effects model-a commonly used approach for analyzing data from cluster randomized trials. When cluster sizes vary, the power of a SW-CRT depends on the order in which clusters receive the intervention, which is determined through randomization. We first derive a variance formula that corresponds to any particular realization of the randomized sequence and propose efficient algorithms to identify upper and lower bounds of the power. We then obtain an ""expected"" power based on a first-order approximation to the variance formula, where the expectation is taken with respect to all possible randomization sequences. Finally, we provide a variance formula for more general settings where only the cluster size arithmetic mean and coefficient of variation, instead of exact cluster sizes, are known in the design stage. We evaluate our methods through simulations and illustrate that the average power of a SW-CRT decreases as the variation in cluster sizes increases, and the impact is largest when the number of clusters is small.-Power calculation for cross-sectional stepped wedge cluster randomized trials with variable cluster sizes",3
"In trials of physical and talking therapies, nesting of patients within therapists has statistical implications analogous to those of cluster randomised trials. Nevertheless, the clustering effect may be more complex, as it interacts with treatment. For some therapies, individual patients may receive care from multiple therapists of the same type, so that patients are no longer strictly nested within therapists, creating a 'multiple-membership' relationship between patients and therapists. This paper considers methods of analysis and sample size estimation for trials with multiple-membership clustering effects. It is motivated by a trial of a psychotherapy for the treatment of adolescent depression with cognitive behavioural therapy. We tested methods and issues in a Monte Carlo simulation study, simulating trials with multiple membership. Results demonstrate satisfactory performance in terms of convergence and give estimates of the intra-cluster correlation coefficient and empirical test size similar to a simple hierarchical design. We derive formulae for sample size and power for multiple-membership trial designs. We then compare estimates of power from this formula with empirical power derived from the simulation study. Finally, we show that we can easily extend formulae for sample size and power to allow consideration of power and sample size for certain types of more complex interventions. These include situations where therapists of different types deliver separate components of the intervention, creating a cross-classified relationship, or where several therapists deliver a group-administered treatment, creating further levels.-Design and analysis of non-pharmacological treatment trials with multiple therapists per patient.",2
Response to Keriel-Gascou et?al. Addressing assumptions on the stepped wedge randomized trial design.,3
"We propose a flexible model for correlated medical cost data with several appealing features. First, the mean function is partially linear. Second, the distributional form for the response is not specified. Third, the covariance structure of correlated medical costs has a semiparametric form. We use extended generalized estimating equations to simultaneously estimate all parameters of interest. B-splines are used to estimate unknown functions, and a modification to Akaike information criterion is proposed for selecting knots in spline bases. We apply the model to correlated medical costs in the Medical Expenditure Panel Survey dataset. Simulation studies are conducted to assess the performance of our method.-A flexible model for correlated medical costs, with application to medical expenditure panel survey data.",0
"Robotic assistance during laparoscopic surgery for pelvic organ prolapse rapidly disseminated across the United States without level I data to support its benefit over traditional open and laparoscopic approaches [1]. This manuscript describes design and methodology of the Abdominal Colpopexy: Comparison of Endoscopic Surgical Strategies (ACCESS) Trial. ACCESS is a randomized comparative effectiveness trial enrolling patients at two academic teaching facilities, UCLA (Los Angeles, CA) and Loyola University (Chicago, IL). The primary aim is to compare costs of robotic assisted versus pure laparoscopic abdominal sacrocolpopexy (RASC vs LASC). Following a clinical decision for minimally-invasive abdominal sacrocolpopexy (ASC) and research consent, participants with symptomatic stage?II pelvic organ prolapse are randomized to LASC or RASC on the day of surgery. Costs of care are based on each patient's billing record and equipment costs at each hospital. All costs associated with surgical procedure including costs for robot and initial hospitalization and any re-hospitalization in the first 6weeks are compared between groups. Secondary outcomes include post-operative pain, anatomic outcomes, symptom severity and quality of life, and adverse events. Power calculation determined that 32 women in each arm would provide 95% power to detect a $2500 difference in total charges, using a two-sided two sample t-test with a significance level of 0.05. Enrollment was completed in May 2011. The 12-month follow-up was completed in May 2012. This is a multi-center study to assess cost as a primary outcome in a comparative effectiveness trial of LASC versus RASC.-Abdominal Colpopexy: Comparison of Endoscopic Surgical Strategies (ACCESS).",0
"Clinical studies of dental caries experience generate multiple outcome data for each participant, with information collected for each individual tooth surface. This paper investigates multilevel modelling as a method of analysis for dental caries data, allowing for full use of the data collected at surface level. Data from a clinical trial of a caries preventive agent in adolescents are modelled. The effect of tooth position within the mouth on the development of dental caries is investigated, with the results showing the importance of differentiating between the upper and lower arches, when modelling the probabilities of caries developing on teeth. Calculation of the intracluster correlation using the threshold model is suggested for use in multilevel logistic regression modelling of caries data. This model, which assumes that a dichotomous outcome is based on an underlying continuous variable with a threshold point where the outcome changes from zero to one, is identified to be appropriate for the analysis of caries which is a continuous process, but is only identified as present in a clinical trial when it has reached a certain level of severity.-The application of multilevel modelling to dental caries data.",1
"A great deal of educational and social data arises from cluster sampling designs where clusters involve schools, classrooms, or communities. A mistake that is sometimes encountered in the analysis of such data is to ignore the effect of clustering and analyse the data as if it were based on a simple random sample. This typically leads to an overstatement of the precision of results and too liberal conclusions about precision and statistical significance of mean differences. This paper gives simple corrections to the test statistics that would be computed in an analysis of variance if clustering were (incorrectly) ignored. The corrections are multiplicative factors depending on the total sample size, the cluster size, and the intraclass correlation structure. For example, the corrected F statistic has Fisher's F distribution with reduced degrees of freedom. The corrected statistic reduces to the F statistic computed by ignoring clustering when the intraclass correlations are zero. It reduces to the F statistic computed using cluster means when the intraclass correlations are unity, and it is in between otherwise. A similar adjustment to the usual statistic for testing a linear contrast among group means is described.-Correcting an analysis of variance for clustering.",1
"We sought to determine the relationship of fibroids to pregnancy loss in a prospective cohort in which fibroid status was uniformly documented in early pregnancy. Participants had an intake interview, transvaginal ultrasonography, computer-assisted telephone interview, and follow-up assessment of outcomes. We recruited diverse participants for the Right From the Start study from 8 metropolitan areas in 3 states in the United States during 2000-2012. Participants were at least 18 years of age, trying to become pregnant or at less than 12 weeks' gestation, not using fertility treatments, fluent in English or Spanish, and available for telephone interviews. Miscarriage was defined as loss before 20 weeks' gestation. Fibroid presence, number, type, and volume were assessed using standardized ultrasonography methods. We used proportional hazards models to estimate associations. Among 5,512 participants, 10.4% had at least 1 fibroid, and 10.8% experienced a miscarriage. Twenty-three percent had experienced a prior miscarriage and 52% prior births. Presence of fibroids was associated with miscarriage in models without adjustments. Adjusting for key confounders indicated no increase in risk (adjusted hazard ratio?=?0.83, 95% confidence interval: 0.63, 1.08). No characteristic of fibroids was associated with risk. Prior evidence attributing miscarriage to fibroids is potentially biased. These findings imply that surgical removal of fibroids to reduce risk of miscarriage deserves careful scrutiny.-Prospective Cohort Study of Uterine Fibroids and Miscarriage Risk.",0
"High-throughput screening (HTS) is a large-scale hierarchical process in which a large number of chemicals are tested in multiple stages. Conventional statistical analyses of HTS studies often suffer from high testing error rates and soaring costs in large-scale settings. This article develops new methodologies for false discovery rate control and optimal design in HTS studies. We propose a two-stage procedure that determines the optimal numbers of replicates at different screening stages while simultaneously controlling the false discovery rate in the confirmatory stage subject to a constraint on the total budget. The merits of the proposed methods are illustrated using both simulated and real data. We show that, at the expense of a limited budget, the proposed screening procedure effectively controls the error rate and the design leads to improved detection power.-Optimal design for high-throughput screening via false discovery rate control.",0
The stepped wedge cluster randomized trial and its potential for child health services research: a narrative review.,3
"In some trials, the intervention is delivered to individuals in groups, for example, groups that exercise together. The group structure of such trials has to be taken into consideration in the analysis and has an impact on the power of the trial. Our aim was to provide optimal methods for the design and analysis of such trials. We described various treatment allocation methods and presented a new allocation algorithm: optimal batchwise minimization (OBM). We carried out a simulation study to evaluate the performance of unrestricted randomization, stratification, permuted block randomization, deterministic minimization, and OBM. Furthermore, we described appropriate analysis methods and derived a formula to calculate the study size. Stratification, deterministic minimization, and OBM had considerably less risk of imbalance than unrestricted randomization and permuted block randomization. Furthermore, OBM led to unpredictable treatment allocation. The sample size calculation and the analysis of the study must be based on a multilevel model that takes the group structure of the trial into account. Trials evaluating interventions that are carried out in subsequent groups require adapted treatment allocation, power calculation, and analysis methods. From the perspective of obtaining overall balance, we conclude that minimization is the method of choice. When the number of prognostic factors is low, stratification is an excellent alternative. OBM leads to better balance within the batches, but it is more complicated. It is probably most worthwhile in trials with many prognostic factors. From the perspective of predictability, a treatment allocation method, such as OBM, that allocates several subjects at the same time, is superior to other methods because it leads to the lowest possible predictability.-Studies with group treatments required special power calculations, allocation methods, and statistical analyses.",2
"Studies of individuals sampled in unbalanced clusters have become common in health services and epidemiological research, but available tools for power/sample size estimation and optimal design are currently limited. This paper presents and illustrates power estimation formulas for t-test comparisons of effect of an exposure at the cluster level on continuous outcomes in unbalanced studies with unequal numbers of clusters and/or unequal numbers of subjects per cluster in each exposure arm. Iterative application of these power formulas obtains minimal sample size needed and/or minimal detectable difference. SAS subroutines to implement these algorithms are given in the Appendices. When feasible, power is optimized by having the same number of clusters in each arm k(A) = k(B) and (irrespective of numbers of clusters in each arm) the same total number of subjects in each arm n(A)k(A) = n(B)k(B). Cost beneficial upper limits for numbers of subjects per cluster may be approximately (5/rho) - 5 or less where rho is the intraclass correlation. The methods presented here for simple cluster designs may be extended to some settings involving complex hierarchical weighted cluster samples.-Power for T-test comparisons of unbalanced cluster exposure studies.",1
"The use of hierarchical data (also called multilevel data or clustered data) is common in behavioural and psychological research when data of lower-level units (e.g., students, clients, repeated measures) are nested within clusters or higher-level units (e.g., classes, hospitals, individuals). Over the past 25?years we have seen great advances in methods for computing the sample sizes needed to obtain the desired statistical properties for such data in experimental evaluations. The present research provides closed-form and iterative formulas for sample size determination that can be used to ensure the desired width of confidence intervals for hierarchical data. Formulas are provided for a four-level hierarchical linear model that assumes slope variances and inclusion of covariates under both balanced and unbalanced designs. In addition, we address several mathematical properties relating to sample size determination for hierarchical data via the standard errors of experimental effect estimates. These include the relative impact of several indices (e.g., random intercept or slope variance at each level) on standard errors, asymptotic standard errors, minimum required values at the highest level, and generalized expressions of standard errors for designs with any-level randomization under any number of levels. In particular, information on the minimum required values will help researchers to minimize the risk of conducting experiments that are statistically unlikely to show the presence of an experimental effect.-Confidence interval-based sample size determination formulas and some mathematical properties for hierarchical data",1
"This article describes the protocol for a Hybrid Type I cost-effectiveness and implementation study of interpersonal psychotherapy (IPT) for men and women prisoners with major depressive disorder (MDD). The goal is to promote uptake of evidence-based treatments in criminal justice settings by conducting a randomized effectiveness study that collects implementation data, including a full cost-effectiveness analysis. More than 2.3 million people are incarcerated in the United States on any given day. MDD is the most common severe mental illness among incarcerated individuals. Despite the prevalence and consequences of MDD among incarcerated populations, this study will be the first fully-powered randomized trial of any treatment for MDD in an incarcerated population. Given the politically charged nature of the justice system, advantageous health outcomes are often not enough to get an intervention implemented in prisons. To increase the policy impact of this trial, we sought advice from prison providers and administrators about outcomes that would be persuasive to policy-makers and defensible to the public. In this trial, effectiveness questions will be answered using a randomized clinical trial design comparing IPT plus prison treatment as usual (TAU) to TAU alone, with outcomes including depressive symptoms (primary), suicidality, and in prison functioning (enrollment and completion of correctional programs; disciplinary and incident reports; aggression/victimization; social support). Implementation outcomes will include cost-effectiveness; feasibility and acceptability of IPT to clients, providers, and administrators; prison provider intervention fidelity, attitudes, and competencies; and barriers and facilitators of implementation assessed through surveys, interviews, and process notes.-Study protocol: Hybrid Type I cost-effectiveness and implementation study of interpersonal psychotherapy (IPT) for men and women prisoners with major depression.",0
"The statistical issues in clinical trials where clusters, communities or groups rather than individuals are randomized are often not fully appreciated. In this paper we discuss the design and analysis of trials in which pairs of clusters are randomized in the context of one recent trial, the British Family Heart Study. Both sample size calculations and the analysis strategy need to take account of the between-cluster component of variance. The analysis can be considered as a random effects meta-analysis across cluster pairs, and can usefully be presented as such. Techniques developed in the context of meta-analysis can then be used in the analysis, for example using a profile likelihood method to derive a confidence interval for the overall treatment effect which takes into account the variability in the estimate of the between-cluster variance. The methods presented here are contrasted with previously published methods for cluster randomized trials.-The design and analysis of paired cluster randomized trials: an application of meta-analysis techniques.",1
"Sample size calculations for a continuous outcome require specification of the anticipated variance; inaccurate specification can result in an underpowered or overpowered study. For this reason, adaptive methods whereby sample size is recalculated using the variance of a subsample have become increasingly popular. The first proposal of this type (Stein, 1945, Annals of Mathematical Statistics 16, 243-258) used all of the data to estimate the mean difference but only the first stage data to estimate the variance. Stein's procedure is not commonly used because many people perceive it as ignoring relevant data. This is especially problematic when the first stage sample size is small, as would be the case if the anticipated total sample size were small. A more naive approach uses in the denominator of the final test statistic the variance estimate based on all of the data. Applying the Helmert transformation, we show why this naive approach underestimates the true variance and how to construct an unbiased estimate that uses all of the data. We prove that the type I error rate of our procedure cannot exceed alpha.-An improved double sampling procedure based on the variance.",0
"Free to Grow: Head Start Partnerships to Promote Substance-free Communities (FTG) was a national initiative in which local Head Start (HS) agencies, in partnership with other community organizations, implemented a mix of evidence-based family-strengthening and community-strengthening strategies. The evaluation of FTG used a quasi-experimental design to compare 14 communities that participated in the FTG intervention with 14 matched comparison communities. Telephone surveys were conducted with two cohorts of the primary caregivers of children in HS at baseline and then annually for 2 years. The survey was also administered to repeated cross-sectional samples of primary caregivers of young children who were not enrolled in HS. No consistent evidence was found in changes in family functioning or neighborhood conditions when the 14 FTG sites were compared to 14 matched sites. However, caregivers of young children who were not in HS in three high-implementing FTG sites showed evidence of improvements in neighborhood organization, neighborhood norms against substance abuse, and child disciplinary practices. Results provide highly limited support for the concept that family and neighborhood conditions that are likely to affect child development and well-being can be changed through organized efforts implemented by local HS programs.-Evaluation of free to grow: head start partnerships to promote substance-free communities.",0
"This article is part of a series of papers examining ethical issues in cluster randomized trials (CRTs) in health research. In the introductory paper in this series, we set out six areas of inquiry that must be addressed if the cluster trial is to be set on a firm ethical foundation. This paper addresses the second of the questions posed, namely, from whom, when, and how must informed consent be obtained in CRTs in health research? The ethical principle of respect for persons implies that researchers are generally obligated to obtain the informed consent of research subjects. Aspects of CRT design, including cluster randomization, cluster level interventions, and cluster size, present challenges to obtaining informed consent. Here we address five questions related to consent and CRTs: How can a study proceed if informed consent is not possible? Is consent to randomization always required? What information must be disclosed to potential subjects if their cluster has already been randomized? Is passive consent a valid substitute for informed consent? Do health professionals have a moral obligation to participate as subjects in CRTs designed to improve professional practice?We set out a framework based on the moral foundations of informed consent and international regulatory provisions to address each of these questions. First, when informed consent is not possible, a study may proceed if a research ethics committee is satisfied that conditions for a waiver of consent are satisfied. Second, informed consent to randomization may not be required if it is not possible to approach subjects at the time of randomization. Third, when potential subjects are approached after cluster randomization, they must be provided with a detailed description of the interventions in the trial arm to which their cluster has been randomized; detailed information on interventions in other trial arms need not be provided. Fourth, while passive consent may serve a variety of practical ends, it is not a substitute for valid informed consent. Fifth, while health professionals may have a moral obligation to participate as subjects in research, this does not diminish the necessity of informed consent to study participation.-When is informed consent required in cluster randomized trials in health research?",1
"Although dense neighborhood built environments support increased physical activity and lower obesity, these features may also disturb sleep. Therefore, we sought to understand the association between the built environment and objectively measured sleep. From 2010 to 2013, we analyzed data from examination 5 of the Multi-Ethnic Study of Atherosclerosis, a diverse population from 6 US cities. We fit multilevel models that assessed the association between the built environment (Street Smart Walk Score, social engagement destinations, street intersections, and population density) and sleep duration or efficiency from 1-week wrist actigraphy in 1,889 individuals. After adjustment for covariates, a 1-standard-deviation increase in Street Smart Walk Score was associated with 23% higher odds of short sleep duration (?6 hours; odds ratio = 1.2, 95% confidence interval: 1.0, 1.4), as well as shorter average sleep duration (mean difference = -8.1 minutes, 95% confidence interval: -12.1, -4.2). Results were consistent across other built environment measures. Associations were attenuated after adjustment for survey-based measure of neighborhood noise. Dense neighborhood development may have multiple health consequence. In promoting denser neighborhoods to increase walkability, it is important to also implement strategies that reduce the adverse impacts of this development on sleep, such as noise reductions efforts.-Associations Between the Built Environment and Objective Measures of Sleep: The Multi-Ethnic Study of Atherosclerosis.",0
"The aim of many research investigations is to compare the proportion of individuals in each of several groups that have a certain characteristic. The unit of allocation for such investigations is often an intact social unit, as in randomizing families, medical practices, schools, or entire communities, to different intervention groups. Standard statistical methods are not appropriate for these designs, since they do not take into account the dependencies among individuals within the same cluster. The authors review the strengths and weaknesses of several approaches for dealing with this problem, using data from a school-based smoking cessation trial. A principal conclusion is that the choice of method should depend on whether or not random allocation is used in the assignment of interventions.-Methods for comparing event rates in intervention studies when the unit of allocation is a cluster.",1
"Despite the increasing use of data derived from randomized controlled trials (RCTs) to perform observational studies, little is known about the validity of this approach. We compared inferences from studies that were performed using Global Utilization of Streptokinase and t-PA for Occluded Coronary Arteries (GUSTO) RCT data with those derived from studies using data from the population-based Cooperative Cardiovascular Project (CCP). We performed a systematic review. Articles were included if similar study questions were addressed by at least one manuscript that used GUSTO data and one that used CCP data. GUSTO findings were disparate from CCP data regarding absolute rates of specific process or outcome measures, such as thrombolysis-associated intracranial hemorrhage (ICH) (0.65% versus 1.43%, respectively), atrial fibrillation (10% versus 21%, respectively), or use of beta-blockers (58% versus 37%, respectively). However, many important relations noted in GUSTO were corroborated by studies using CCP data. Both data sets identified similar predictors of ICH and presentation delay. The degree of variability in beta-blocker use (across geographic region) and angiography use (between genders) was remarkably similar when studied using CCP or GUSTO data. Inferences derived from GUSTO about treatment variations and risk factors for outcomes were generalizable to community patients.-The generalizability of observational data to elderly patients was dependent on the research question in a systematic review.",0
"Attrition is a common occurrence in cluster randomised trials which leads to missing outcome data. Two approaches for analysing such trials are cluster-level analysis and individual-level analysis. This paper compares the performance of unadjusted cluster-level analysis, baseline covariate adjusted cluster-level analysis and linear mixed model analysis, under baseline covariate dependent missingness in continuous outcomes, in terms of bias, average estimated standard error and coverage probability. The methods of complete records analysis and multiple imputation are used to handle the missing outcome data. We considered four scenarios, with the missingness mechanism and baseline covariate effect on outcome either the same or different between intervention groups. We show that both unadjusted cluster-level analysis and baseline covariate adjusted cluster-level analysis give unbiased estimates of the intervention effect only if both intervention groups have the same missingness mechanisms and there is no interaction between baseline covariate and intervention group. Linear mixed model and multiple imputation give unbiased estimates under all four considered scenarios, provided that an interaction of intervention and baseline covariate is included in the model when appropriate. Cluster mean imputation has been proposed as a valid approach for handling missing outcomes in cluster randomised trials. We show that cluster mean imputation only gives unbiased estimates when missingness mechanism is the same between the intervention groups and there is no interaction between baseline covariate and intervention group. Multiple imputation shows overcoverage for small number of clusters in each intervention group.-Missing continuous outcomes under covariate dependent missingness in cluster randomised trials.",1
"Factorial experimental designs have many applications in the behavioral sciences. In the context of intervention development, factorial experiments play a critical role in building and optimizing high-quality, multicomponent behavioral interventions. One challenge in implementing factorial experiments in the behavioral sciences is that individuals are often clustered in social or administrative units and may be more similar to each other than to individuals in other clusters. This means that data are dependent within clusters. Power planning resources are available for factorial experiments in which the multilevel structure of the data is due to individuals' membership in groups that existed before experimentation. However, in many cases clusters are generated in the course of the study itself. Such experiment-induced clustering (EIC) requires different data analysis models and power planning resources from those available for multilevel experimental designs in which clusters exist prior to experimentation. Despite the common occurrence of both experimental designs with EIC and factorial designs, a bridge has yet to be built between EIC and factorial designs. Therefore, resources are limited or nonexistent for planning factorial experiments that involve EIC. This article seeks to bridge this gap by extending prior models for EIC, developed for single-factor experiments, to factorial experiments involving various types of EIC. We also offer power formulas to help investigators decide whether a particular experimental design involving EIC is feasible. We demonstrate that factorial experiments can be powerful and feasible even with EIC. We discuss design considerations and directions for future research. (PsycINFO Database Record-Multilevel factorial designs with experiment-induced clustering.",1
"In randomized clinical trials, subjects are recruited at multiple study centres. Factors that vary across centres may exert a powerful independent influence on study outcomes. A common problem is how to incorporate these centre effects into the analysis of censored time-to-event data. We survey various methods and find substantial advantages in the gamma frailty model. This approach compares favourably with competing methods and appears minimally affected by violation of the assumption of a gamma-distributed frailty. Recent computational advances make use of the gamma frailty model a practical and appealing tool for addressing centre effects in the analysis of multicentre trials.-Modelling clustered survival data from multicentre clinical trials.",1
"This article explores the possibility of randomly assigning groups (or clusters) of individuals to a program or a control group to estimate the impacts of programs designed to affect whole groups. This cluster assignment approach maintains the primary strength of random assignment--the provision of unbiased impact estimates--but has less statistical power than random assignment of individuals, which usually is not possible for programs focused on whole groups. To explore the statistical implications of cluster assignment, the authors (a) outline the issues involved, (b) present an analytic framework for studying these issues, and (c) apply this framework to assess the potential for using the approach to evaluate education programs targeted on whole schools. The findings suggest that cluster assignment of schools holds some promise for estimating the impacts of education programs when it is possible to control for the average performance of past student cohorts or the past performance of individual students.-Using cluster random assignment to measure program impacts. Statistical implications for the evaluation of education programs.",1
"Propensity scores for the analysis of observational data are typically estimated using logistic regression. Our objective in this review was to assess machine learning alternatives to logistic regression, which may accomplish the same goals but with fewer assumptions or greater accuracy. We identified alternative methods for propensity score estimation and/or classification from the public health, biostatistics, discrete mathematics, and computer science literature, and evaluated these algorithms for applicability to the problem of propensity score estimation, potential advantages over logistic regression, and ease of use. We identified four techniques as alternatives to logistic regression: neural networks, support vector machines, decision trees (classification and regression trees [CART]), and meta-classifiers (in particular, boosting). Although the assumptions of logistic regression are well understood, those assumptions are frequently ignored. All four alternatives have advantages and disadvantages compared with logistic regression. Boosting (meta-classifiers) and, to a lesser extent, decision trees (particularly CART), appear to be most promising for use in the context of propensity score analysis, but extensive simulation studies are needed to establish their utility in practice.-Propensity score estimation: neural networks, support vector machines, decision trees (CART), and meta-classifiers as alternatives to logistic regression.",0
"Dental caries is a highly prevalent disease affecting the tooth's hard tissues by acid-forming bacteria. The past and present caries status of a tooth is characterized by a response called caries experience (CE). Several epidemiological studies have explored risk factors for CE. However, the detection of CE is prone to misclassification because some cases are neither clearly carious nor noncarious, and this needs to be incorporated into the epidemiological models for CE data. From a dentist's point of view, it is most appealing to analyze CE on the tooth's surface, implying that the multilevel structure of the data (surface-tooth-mouth) needs to be taken into account. In addition, CE data are spatially referenced, that is, an active lesion on one surface may impact the decay process of the neighboring surfaces, and that might also influence the process of scoring CE. In this paper, we investigate two hypotheses: that is, (i) CE outcomes recorded at surface level are spatially associated; and (ii) the dental examiners exhibit some spatial behavior while scoring CE at surface level, by using a spatially referenced multilevel autologistic model, corrected for misclassification. These hypotheses were tested on the well-known Signal Tandmobiel? study on dental caries, and simulation studies were conducted to assess the effect of misclassification and strength of spatial dependence on the autologistic model parameters. Our results indicate a substantial spatial dependency in the examiners' scoring behavior and also in the prevalence of CE at surface level.-A multilevel model for spatially correlated binary data in the presence of misclassification: an application in oral health research.",0
"Cox's regression model is the standard regression tool for survival analysis in most applications. Often, however, the model only provides a rough summary of the effect of some covariates. Therefore, if the aim is to give a detailed description of covariate effects and to consequently calculate predicted probabilities, more flexible models are needed. In another article, Scheike and Zhang (2002, Scandinavian Journal of Statistics 29, 75-88), we suggested a flexible extension of Cox's regression model, which aimed at extending the Cox model only for those covariates where additional flexibility are needed. One important advantage of the suggested approach is that even though covariates are allowed a nonparametric effect, the hassle and difficulty of finding smoothing parameters are not needed. We show how the extended model also leads to simple formulae for predicted probabilities and their standard errors, for example, in the competing risk framework.-Extensions and applications of the Cox-Aalen survival model.",0
"Model-based inference procedures for the kappa statistic have developed rapidly over the last decade. However, no method has yet been developed for constructing a confidence interval about a difference between independent kappa statistics that is valid in samples of small to moderate size. In this article, we propose and evaluate two such methods based on an idea proposed by Newcombe (1998, Statistics in Medicine, 17, 873-890) for constructing a confidence interval for a difference between independent proportions. The methods are shown to provide very satisfactory results in sample sizes as small as 25 subjects per group. Sample size requirements that achieve a prespecified expected width for a confidence interval about a difference of kappa statistic are also presented.-Interval estimation for a difference between intraclass kappa statistics.",1
"The ability of cluster-randomized trials to capture mass or indirect effects is one reason for their increasing use to test interventions against vector-borne diseases such as malaria and dengue. For the same reason, however, the independence of clusters may be compromised if the distances between clusters is too small to ensure independence. In other words they may be subject to spillover effects. We distinguish two types of spatial spillover effect: between-cluster dependence in outcomes, or spillover dependence; and modification of the intervention effect according to distance to the intervention arm, or spillover indirect effect. We estimate these effects in trial of insecticide-treated materials against the dengue mosquito vector, Aedes aegypti, in Venezuela, the endpoint being the Breteau index. We use a novel random effects Poisson spatial regression model. Spillover dependence is incorporated via an orthogonalized intrinsic conditional autoregression (ICAR) model. Spillover indirect effects are incorporated via the number of locations within a certain radius, set at 200m, that are in the intervention arm. From the model with ICAR spatial dependence, and the degree of surroundedness, the intervention effect is estimated as 0.74-favouring the intervention-with a 95% credible interval of 0.34 to 1.69. The point estimates are stronger with increasing surroundedness within intervention locations. In this trial there is some evidence of a spillover indirect effect of the intervention, with the Breteau index tending to be lower in locations which are more surrounded by locations in the intervention arm.-Spatial spillover analysis of a cluster-randomized trial against dengue vectors in Trujillo, Venezuela",1
"Cluster randomized trial designs are growing in popularity in, for example, cardiovascular medicine research and other clinical areas and parallel statistical developments concerned with the design and analysis of these trials have been stimulated. Nevertheless, reviews suggest that design issues associated with cluster randomized trials are often poorly appreciated and there remain inadequacies in, for example, describing how the trial size is determined and the associated results are presented. In this paper, our aim is to provide pragmatic guidance for researchers on the methods of calculating sample sizes. We focus attention on designs with the primary purpose of comparing two interventions with respect to continuous, binary, ordered categorical, incidence rate and time-to-event outcome variables. Issues of aggregate and non-aggregate cluster trials, adjustment for variation in cluster size and the effect size are detailed. The problem of establishing the anticipated magnitude of between- and within-cluster variation to enable planning values of the intra-cluster correlation coefficient and the coefficient of variation are also described. Illustrative examples of calculations of trial sizes for each endpoint type are included.-Sample size calculations for the design of cluster randomized trials: A summary of methodology.",1
"Background/aims In the conduct of phase I trials, the limited use of innovative model-based designs in practice has led to an introduction of a class of ""model-assisted"" designs with the aim of effectively balancing the trade-off between design simplicity and performance. Prior to the recent surge of these designs, methods that allocated patients to doses based on isotonic toxicity probability estimates were proposed. Like model-assisted methods, isotonic designs allow investigators to avoid difficulties associated with pre-trial parametric specifications of model-based designs. The aim of this work is to take a fresh look at an isotonic design in light of the current landscape of model-assisted methods. Methods The isotonic phase I method of Conaway, Dunbar, and Peddada was proposed in 2004 and has been regarded primarily as a design for dose-finding in drug combinations. It has largely been overlooked in the single-agent setting. Given its strong simulation performance in application to more complex dose-finding problems, such as drug combinations and patient heterogeneity, as well as the recent development of user-friendly software to accompany the method, we take a fresh look at this design and compare it to a current model-assisted method. We generated operating characteristics of the Conaway-Dunbar-Peddada method using a new web application developed for simulating and implementing the design and compared it to the recently proposed Keyboard design that is based on toxicity probability intervals. Results The Conaway-Dunbar-Peddada method has better performance in terms of accuracy of dose recommendation and safety in patient allocation in 17 of 20 scenarios considered. The Conaway-Dunbar-Peddada method also allocated fewer patients to doses above the maximum tolerated dose than the Keyboard method in many of scenarios studied. Overall, the performance of the Conaway-Dunbar-Peddada method is strong when compared to the Keyboard method, making it a viable simple alternative to the model-assisted methods developed in recent years. Conclusion The Conaway-Dunbar-Peddada method does not rely on the specification and fitting of a parametric model for the entire dose-toxicity curve to estimate toxicity probabilities as other model-based designs do. It relies on a similar set of pre-trial specifications to toxicity probability interval-based methods, yet unlike model-assisted methods, it is able to borrow information across all dose levels, increasing its efficiency. We hope this concise study of the Conaway-Dunbar-Peddada method, and the availability of user-friendly software, will augment its use in practice.-Revisiting isotonic phase I design in the era of model-assisted dose-finding.",0
"We develop two estimators of a common odds ratio psi for designs in which the investigator randomly assigns each of two clusters to interventions within strata. The estimators rely on an empirical adjustment for clustering to provide improved estimators of psi relative to the standard Woolf and Mantel-Haenszel estimators, respectively. The results of a simulation study show that the suggested adjustment improves the accuracy of both of these well-known estimators under conditions likely to arise in practice. We find the clustered Woolf estimator as particularly effective in terms of mean squared error reduction. We also discuss interval estimation.-Estimation of a common odds ratio in paired-cluster randomization designs.",1
"Repeated cross-sectional cluster randomization trials are cluster randomization trials in which the response variable is measured on a sample of subjects from each cluster at baseline and on a different sample of subjects from each cluster at follow-up. One can estimate the effect of the intervention on the follow-up response alone, on the follow-up responses after adjusting for baseline responses, or on the change in the follow-up response from the baseline response. We used Monte Carlo simulations to determine the relative statistical power of different methods of analysis. We examined methods of analysis based on generalized estimating equations (GEE) and a random effects model to account for within-cluster homogeneity. We also examined cluster-level analyses that treated the cluster as the unit of analysis. We found that the use of random effects models to estimate the effect of the intervention on the change in the follow-up response from the baseline response had lower statistical power compared to the other competing methods across a wide range of scenarios. The other methods tended to have similar statistical power in many settings. However, in some scenarios, those analyses that adjusted for the baseline response tended to have marginally greater power than did methods that did not account for the baseline response.-A comparison of the statistical power of different methods for the analysis of repeated cross-sectional cluster randomization trials with binary outcomes.",1
"We consider the problem of estimating the effect of exposure on multiple continuous outcomes, when the outcomes are measured on different scales and are nested within multiple outcome classes, or ""domains."" Our Bayesian model extends the linear mixed models approach to allow the exposure effect to differ across domains and across outcomes within domains. Our model can be parameterized to allow shrinkage of the effects within the different levels of nesting, or to allow fixed domain-specific effects with no shrinkage. Our model also allows covariate effects to differ across outcomes and domains. Our methodology is applied to data on prenatal methylmercury exposure and multiple outcomes in four domains measured at 9 years of age on children enrolled in the Seychelles Child Development Study. We use three different priors and found that our main conclusions were not sensitive to the choice of prior. Simulation studies examine the model performance under alternative scenarios. Our results demonstrate that a sizeable increase in power is possible.-Bayesian models for multiple outcomes nested in domains.",0
"Numerous estimates for the intracluster correlation coefficient (ICC) are available in research databases and publications. When planning a cluster randomized trial, an anticipated value for the ICC is required; currently, researchers base their choice informally on the magnitude of previous ICC estimates. In this paper, we make use of the wealth of ICC information by formally constructing informative prior distributions, while acknowledging the varying relevance and precision of the estimates available. Typically, for a planned trial in a given clinical setting, multiple relevant ICC estimates are available from each of several completed studies. Our preferred model allows for the imprecision in each ICC estimate around its underlying true value and, separately, allows for the similarity of ICC values from the same study. The relevance of each previous estimate to the planned clinical setting is considered, and estimates corresponding to less relevant outcomes or population types are given less influence. We find that such downweighting can increase the precision of the anticipated ICC. In trial design, the prior distribution constructed allows uncertainty about the ICC to be acknowledged, and we describe how to choose a design that provides adequate power across the range of likely ICC values. Prior information on the ICC can also be incorporated in analysis of the trial data, when taking a Bayesian approach. The methods proposed enable available ICC information to be summarised appropriately by an informative prior distribution, which is of direct practical use in cluster randomized trials.-Prior distributions for the intracluster correlation coefficient, based on multiple previous estimates, and their application in cluster randomized trials.",1
"Limited controlled data exist to guide treatment choices for clinicians caring for patients with major depressive disorder (MDD). Although many putative predictors of treatment response have been reported, most were identified through retrospective analyses of existing datasets and very few have been replicated in a manner that can impact clinical practice. One major confound in previous studies examining predictors of treatment response is the patient's treatment history, which may affect both the predictor of interest and treatment outcomes. Moreover, prior treatment history provides an important source of selection bias, thereby limiting generalizability. Consequently, we initiated a randomized clinical trial designed to identify factors that moderate response to three treatments for MDD among patients never treated previously for the condition. Treatment-na?ve adults aged 18 to 65 years with moderate-to-severe, non-psychotic MDD are randomized equally to one of three 12-week treatment arms: (1) cognitive behavior therapy (CBT, 16 sessions); (2) duloxetine (30-60 mg/d); or (3) escitalopram (10-20 mg/d). Prior to randomization, patients undergo multiple assessments, including resting state functional magnetic resonance imaging (fMRI), immune markers, DNA and gene expression products, and dexamethasone-corticotropin-releasing hormone (Dex/CRH) testing. Prior to or shortly after randomization, patients also complete a comprehensive personality assessment. Repeat assessment of the biological measures (fMRI, immune markers, and gene expression products) occurs at an early time-point in treatment, and upon completion of 12-week treatment, when a second Dex/CRH test is also conducted. Patients remitting by the end of this acute treatment phase are then eligible to enter a 21-month follow-up phase, with quarterly visits to monitor for recurrence. Non-remitters are offered augmentation treatment for a second 12-week course of treatment, during which they receive a combination of CBT and antidepressant medication. Predictors of the primary outcome, remission, will be identified for overall and treatment-specific effects, and a statistical model incorporating multiple predictors will be developed to predict outcomes. The PReDICT study's evaluation of biological, psychological, and clinical factors that may differentially impact treatment outcomes represents a sizeable step toward developing personalized treatments for MDD. Identified predictors should help guide the selection of initial treatments, and identify those patients most vulnerable to recurrence, who thus warrant maintenance or combination treatments to achieve and maintain wellness.-Predictors of remission in depression to individual and combined treatments (PReDICT): study protocol for a randomized controlled trial.",0
"For administrative convenience or cost efficiency, we may often employ a cluster randomized trial (CRT), in which randomized units are clusters of patients rather than individual patients. Furthermore, because of ethical reasons or patient's decision, it is not uncommon to encounter data in which there are patients not complying with their assigned treatments. Thus, the development of a sample size calculation procedure for a CRT with noncompliance is important and useful in practice. Under the exclusion restriction model, we have developed an asymptotic test procedure using a tanh(-1)(x) transformation for testing equality between two treatments among compliers for a CRT with noncompliance. We have further derived a sample size formula accounting for both noncompliance and the intraclass correlation for a desired power 1 - ? at a nominal ? level. We have employed Monte Carlo simulation to evaluate the finite-sample performance of the proposed test procedure with respect to type I error and the accuracy of the derived sample size calculation formula with respect to power in a variety of situations. Finally, we use the data taken from a CRT studying vitamin A supplementation to reduce mortality among preschool children to illustrate the use of sample size calculation proposed here.-Sample size determination for testing equality in a cluster randomized trial with noncompliance.",1
"There are many benefits of data sharing, including the promotion of new research from effective use of existing data, replication of findings through re-analysis of pooled data files, meta-analysis using individual patient data, and reinforcement of open scientific inquiry. A randomized controlled trial is considered as the 'gold standard' for establishing treatment effectiveness, but clinical trial research is very costly, and sharing data is an opportunity to expand the investment of the clinical trial beyond its original goals at minimal costs. We describe the goals, developments, and usage of the Data Share website (http://www.ctndatashare.org) for the National Drug Abuse Treatment Clinical Trials Network (CTN) in the United States, including lessons learned, limitations, and major revisions, and considerations for future directions to improve data sharing. Data management and programming procedures were conducted to produce uniform and Health Insurance Portability and Accountability Act (HIPAA)-compliant de-identified research data files from the completed trials of the CTN for archiving, managing, and sharing on the Data Share website. Since its inception in 2006 and through October 2012, nearly 1700 downloads from 27 clinical trials have been accessed from the Data Share website, with the use increasing over the years. Individuals from 31 countries have downloaded data from the website, and there have been at least 13 publications derived from analyzing data through the public Data Share website. Minimal control over data requests and usage has resulted in little information and lack of control regarding how the data from the website are used. Lack of uniformity in data elements collected across CTN trials has limited cross-study analyses. The Data Share website offers researchers easy access to de-identified data files with the goal to promote additional research and identify new findings from completed CTN studies. To maximize the utility of the website, ongoing collaborative efforts are needed to standardize the core measures used for data collection in the CTN studies with the goal to increase their comparability and to facilitate the ability to pool data files for cross-study analyses.-The national drug abuse treatment clinical trials network data share project: website design, usage, challenges, and future directions.",0
"Background/aims The use of the stepped wedge cluster randomized design is rapidly increasing. This design is commonly used to evaluate health policy and service delivery interventions. Stepped wedge cluster randomized trials have unique characteristics that complicate their ethical interpretation. The 2012 Ottawa Statement provides comprehensive guidance on the ethical design and conduct of cluster randomized trials, and the 2010 CONSORT extension for cluster randomized trials provides guidelines for reporting. Our aims were to assess the adequacy of the ethical conduct and reporting of stepped wedge trials to date, focusing on research ethics review and informed consent. Methods We conducted a systematic review of stepped wedge cluster randomized trials in health research published up to 2014 in English language journals. We extracted details of study intervention and data collection procedures, as well as reporting of research ethics review and informed consent. Two reviewers independently extracted data from each trial; discrepancies were resolved through discussion. We identified the presence of any research participants at the cluster level and the individual level. We assessed ethical conduct by tabulating reporting of research ethics review and informed consent against the presence of research participants. Results Of 32 identified stepped wedge trials, only 24 (75%) reported review by a research ethics committee, and only 16 (50%) reported informed consent from any research participants-yet, all trials included research participants at some level. In the subgroup of 20 trials with research participants at cluster level, only 4 (20%) reported informed consent from such participants; in 26 trials with individual-level research participants, only 15 (58%) reported their informed consent. Interventions (regardless of whether targeting cluster- or individual-level participants) were delivered at the group level in more than two-thirds of trials; nine trials (28%) had no identifiable data collected from any research participants. Overall, only three trials (9%) indicated that a waiver of consent had been granted by a research ethics committee. When considering the combined requirement of research ethics review and informed consent (or a waiver), only one in three studies were compliant. Conclusion The ethical conduct and reporting of key ethical protections in stepped wedge trials, namely, research ethics review and informed consent, are inadequate. We recommend that stepped wedge trials be classified as research and reviewed and approved by a research ethics committee. We also recommend that researchers appropriately identify research participants (which may include health professionals), seek informed consent or appeal to an ethics committee for a waiver of consent, and include explicit details of research ethics approval and informed consent in the trial report.-Inadequacy of ethical conduct and reporting of stepped wedge cluster randomized trials: Results from a systematic review.",3
"Randomised trials including a mixture of independent and paired data arise in many areas of health research, yet methods for determining the sample size for such trials are lacking. We derive design effects algebraically assuming clustering because of paired data will be taken into account in the analysis using generalised estimating equations with either an independence or exchangeable working correlation structure. Continuous and binary outcomes are considered, along with three different methods of randomisation: cluster randomisation, individual randomisation and randomisation to opposite treatment groups. The design effect is shown to depend on the intracluster correlation coefficient, proportion of observations belonging to a pair, working correlation structure, type of outcome and method of randomisation. The derived design effects are validated through simulation and example calculations are presented to illustrate their use in sample size planning. These design effects will enable appropriate sample size calculations to be performed for future randomised trials including both independent and paired data. Copyright ? 2017 John Wiley &amp; Sons, Ltd.-Sample size calculations for randomised trials including both independent and paired data.",1
"The maximum likelihood estimator, rhoM, of a common intraclass correlation, rho, is derived in two independent samples drawn from multivariate normal populations. A likelihood ratio statistic for testing the assumption of a common rho is also derived and examples are given. Comparisons are made with a procedure based on Fisher's transformation.-Inferences concerning a common intraclass correlation coefficient.",1
"Given the growing attention to quality improvement, comparative effectiveness research, and pragmatic trials embedded within learning health systems, the use of the cluster randomization design is bound to increase. The number of clusters available for randomization is often limited in such trials. Designs that incorporate pre-intervention measurements (e.g. cluster cross-over, repeated parallel arm, and stepped wedge designs) can substantially reduce the required numbers of clusters by decreasing between-cluster sources of variation. However, there are substantial risks associated with few clusters, including increased probability of chance imbalances and type I and type II error, limited perceived or actual generalizability, and fewer options for statistical analysis. Furthermore, current sample size methods for the stepped wedge design make a strong underlying assumption with respect to the correlation structure-in particular, that the intracluster and inter-period correlations are equal. This is in contrast with methods for the cluster cross-over design that explicitly allow for a smaller inter-period correlation. Failing to similarly allow for the inter-period correlation in the design of a stepped wedge trial may yield perilously low sample sizes. Further methodological and empirical work is required to inform sample size methods and guidance for the stepped wedge trial and to provide minimum thresholds for this design.-Substantial risks associated with few clusters in cluster randomized and stepped wedge designs.",3
"The cluster randomised crossover (CRXO) design provides an opportunity to conduct randomised controlled trials to evaluate low risk interventions in the intensive care setting. Our aim is to provide a tutorial on how to perform a sample size calculation for a CRXO trial, focusing on the meaning of the elements required for the calculations, with application to intensive care trials. We use all-cause in-hospital mortality from the Australian and New Zealand Intensive Care Society Adult Patient Database clinical registry to illustrate the sample size calculations. We show sample size calculations for a two-intervention, two 12-month period, cross-sectional CRXO trial. We provide the formulae, and examples of their use, to determine the number of intensive care units required to detect a risk ratio (RR) with a designated level of power between two interventions for trials in which the elements required for sample size calculations remain constant across all ICUs (unstratified design); and in which there are distinct groups (strata) of ICUs that differ importantly in the elements required for sample size calculations (stratified design). The CRXO design markedly reduces the sample size requirement compared with the parallel-group, cluster randomised design for the example cases. The stratified design further reduces the sample size requirement compared with the unstratified design. The CRXO design enables the evaluation of routinely used interventions that can bring about small, but important, improvements in patient care in the intensive care setting.-Sample size calculations for cluster randomised crossover trials in Australian and New Zealand intensive care research.",1
"Dynamic prediction models make use of patient-specific longitudinal data to update individualized survival probability predictions based on current and past information. Colonoscopy (COL) and fecal occult blood test (FOBT) results were collected from two Australian surveillance studies on individuals characterized as high-risk based on a personal or family history of colorectal cancer. Motivated by a Poisson process, this paper proposes a generalized nonlinear model with a complementary log-log link as a dynamic prediction tool that produces individualized probabilities for the risk of developing advanced adenoma or colorectal cancer (AAC). This model allows predicted risk to depend on a patient's baseline characteristics and time-dependent covariates. Information on the dates and results of COLs and FOBTs were incorporated using time-dependent covariates that contributed to patient risk of AAC for a specified period following the test result. These covariates serve to update a person's risk as additional COL, and FOBT test information becomes available. Model selection was conducted systematically through the comparison of Akaike information criterion. Goodness-of-fit was assessed with the use of calibration plots to compare the predicted probability of event occurrence with the proportion of events observed. Abnormal COL results were found to significantly increase risk of AAC for 1 year following the test. Positive FOBTs were found to significantly increase the risk of AAC for 3 months following the result. The covariates that incorporated the updated test results were of greater significance and had a larger effect on risk than the baseline variables.-A prediction model for colon cancer surveillance data.",0
"To examine the prevalence of a risk of bias associated with the design and conduct of cluster randomised controlled trials among a sample of recently published studies. Retrospective review of cluster randomised trials published in the BMJ, Lancet, and New England Journal of Medicine from January 1997 to October 2002. Prevalence of secure randomisation of clusters, identification of participants before randomisation (to avoid foreknowledge of allocation), differential recruitment between treatment arms, differential application of inclusion and exclusion criteria, and differential attrition. Of the 36 trials identified, 24 were published in the BMJ,11 in the Lancet, and a single trial in the New England Journal of Medicine. At the cluster level, 15 (42%) trials provided evidence for secure allocation and 25 (69%) used stratified allocation. Few trials showed evidence of imbalance at the cluster level. However, some evidence of susceptibility to risk of bias at the individual level existed in 14 (39%) studies. Some recently published cluster randomised trials may not have taken adequate precautions to guard against threats to the internal validity of their design.-Evidence for risk of bias in cluster randomised trials: review of recent trials published in three general medical journals.",1
"Cardiovascular outcome trials, among others, aim to assess the beneficial effects of a treatment on multiple event-time outcomes, such as the time to a myocardial infarction and the time to a stroke. The traditional approach is to conduct a simple analysis of a composite outcome defined as the time to the first component event using a logrank test or the Cox Proportional Hazards regression model. This ignores information from other component events after the first. The composite outcome analysis also treats all initial outcome events as equally important, for example, non-fatal myocardial infarction is as important as cardiovascular death. Herein, we describe the application of the Wei-Lachin multivariate one-sided (or one-directional) test to the analysis of multiple event-time outcomes. The test is based on the unweighted mean of the treatment group coefficients from individual Cox proportional hazards models fit to the outcomes, where the covariance of the set of coefficients is obtained from a partitioning of the information sandwich estimate. A weighted test is also described, weighing the outcomes by a scoring of their clinical importance. These and other methods are compared with application to the Prevention of Events with Angiotensin-Converting Enzyme Inhibition cardiovascular outcome study. The Wei-Lachin test provides an inference with strong control of the type 1 error probability on the difference between groups for the set of outcomes considered. However, it does not provide an inference on the individual components specifically with control of the overall type 1 error probability. By direct computation of relative efficiency and by simulation, we show that the power of the Wei-Lachin one-directional test can be greater than that of the traditional composite outcome analysis based on the time to the first observed component event. The Wei-Lachin multivariate one-directional test may be more powerful than the traditional analysis of a composite outcome defined as the time to the first component outcomes experienced by each subject.-Application of the Wei-Lachin multivariate one-directional test to multiple event-time outcomes.",0
"An adaptive treatment strategy (ATS) is an outcome-guided algorithm that allows personalized treatment of complex diseases based on patients' disease status and treatment history. Conditions such as AIDS, depression, and cancer usually require several stages of treatment because of the chronic, multifactorial nature of illness progression and management. Sequential multiple assignment randomized (SMAR) designs permit simultaneous inference about multiple ATSs, where patients are sequentially randomized to treatments at different stages depending upon response status. The purpose of the article is to develop a sample size formula to ensure adequate power for comparing two or more ATSs. Based on a Wald-type statistic for comparing multiple ATSs with a continuous endpoint, we develop a sample size formula and test it through simulation studies. We show via simulation that the proposed sample size formula maintains the nominal power. The proposed sample size formula is not applicable to designs with time-to-event endpoints but the formula will be useful for practitioners while designing SMAR trials to compare adaptive treatment strategies.-Design of sequentially randomized trials for testing adaptive treatment strategies.",0
"A major methodological reason to use cluster randomization is to avoid the contamination that would arise in an individually randomized design. However, when patient recruitment cannot be completed before randomization of clusters, the non-blindedness of recruiters and patients may cause selection bias, while in the control clusters, it may slow recruitment due to patient or recruiter preferences for the intervention. As a compromise, pseudo cluster randomization has been proposed. Because no insight is available into the relative performance of methods to analyse data obtained from this design, we compared the type I and II error rates of mixed models, generalized estimating equations (GEE) and a paired t-test to those of the estimator originally proposed in this design. The bias in the point estimate and its standard error were also incorporated into this comparison. Furthermore, we evaluated the effect of the weighting scheme and the accuracy of the sample size formula that have been described previously. Power levels of the originally proposed estimator and the unweighted mixed models were in agreement with the sample size formula, but the power of paired t-test fell short. GEE produced too large type I errors, unless the number of clusters was large (&gt;30-40 per arm). The use of the weighting scheme generally enhanced the power, but at the cost of increasing the type I error in mixed models and GEE. We recommend unweighted mixed models as the best compromise between feasibility and power to analyse data from a pseudo cluster randomized trial.-A comparison of methods to analyse continuous data from pseudo cluster randomized trials.",1
"There are many methodological challenges in the conduct and analysis of cluster randomised controlled trials, but one that has received little attention is that of post-randomisation changes to cluster composition. To illustrate this, we focus on the issue of cluster merging, considering the impact on the design, analysis and interpretation of trial outcomes. We explored the effects of merging clusters on study power using standard methods of power calculation. We assessed the potential impacts on study findings of both homogeneous cluster merges (involving clusters randomised to the same arm of a trial) and heterogeneous merges (involving clusters randomised to different arms of a trial) by simulation. To determine the impact on bias and precision of treatment effect estimates, we applied standard methods of analysis to different populations under analysis. Cluster merging produced a systematic reduction in study power. This effect depended on the number of merges and was most pronounced when variability in cluster size was at its greatest. Simulations demonstrate that the impact on analysis was minimal when cluster merges were homogeneous, with impact on study power being balanced by a change in observed intracluster correlation coefficient (ICC). We found a decrease in study power when cluster merges were heterogeneous, and the estimate of treatment effect was attenuated. Examples of cluster merges found in previously published reports of cluster randomised trials were typically homogeneous rather than heterogeneous. Simulations demonstrated that trial findings in such cases would be unbiased. However, simulations also showed that any heterogeneous cluster merges would introduce bias that would be hard to quantify, as well as having negative impacts on the precision of estimates obtained. Further methodological development is warranted to better determine how to analyse such trials appropriately. Interim recommendations include avoidance of cluster merges where possible, discontinuation of clusters following heterogeneous merges, allowance for potential loss of clusters and additional variability in cluster size in the original sample size calculation, and use of appropriate ICC estimates that reflect cluster size.-Changing cluster composition in cluster randomised controlled trials: design and analysis considerations.",1
"Urolithiasis (kidney stones) is a common reason for Emergency Department (ED) visits, accounting for nearly 1% of all visits in the United States. Computed tomography (CT) has become the most common imaging test for these patients but there are few comparative effectiveness data to support its use in comparison to ultrasound. This paper describes the rationale and methods of STONE (Study of Tomography Of Nephrolithiasis Evaluation), a pragmatic randomized comparative effectiveness trial comparing different imaging strategies for patients with suspected urolithiasis. STONE is a multi-center, non-blinded pragmatic randomized comparative effectiveness trial of patients between ages 18 and 75 with suspected nephrolithiasis seen in an ED setting. Patients were randomized to one of three initial imaging examinations: point-of-care ultrasound, ultrasound performed by a radiologist or CT. Participants then received diagnosis and treatment per usual care. The primary aim is to compare the rate of severe SAEs (Serious Adverse Events) between the three arms. In addition, a broad range of secondary outcomes was assessed at baseline and regularly for six months post-baseline using phone, email and mail questionnaires. Excluding 17 patients who withdrew after randomization, a total of 2759 patients were randomized and completed a baseline questionnaire (n=908, 893 and 958 in the point-of-care ultrasound, radiology ultrasound and radiology CT arms, respectively). Follow-up is complete, and full or partial outcomes were assessed on over 90% of participants. The detailed methodology of STONE will provide a roadmap for comparative effectiveness studies of diagnostic imaging conducted in an ED setting.-Study of Tomography Of Nephrolithiasis Evaluation (STONE): methodology, approach and rationale.",0
"Selection bias is a concern when designing cluster randomised controlled trials (c-RCT). Despite addressing potential issues at the design stage, bias cannot always be eradicated from a trial design. The application of bias analysis presents an important step forward in evaluating whether trial findings are credible.?The aim of this paper is to give an example of the technique to quantify potential selection bias in c-RCTs. This analysis uses data from the Primary care Osteoarthritis Screening Trial (POST). The primary aim of this trial was to test whether screening for anxiety and depression, and providing appropriate care for patients consulting their GP with osteoarthritis would improve clinical outcomes. Quantitative bias analysis is a seldom-used technique that can quantify types of bias present in studies. Due to lack of information on the selection probability, probabilistic bias analysis with a range of triangular distributions was also used, applied at all three follow-up time points; 3, 6, and 12 months post consultation. A simple bias analysis was also applied to the study. Worse pain outcomes were observed among intervention participants than control participants (crude odds ratio at 3, 6, and 12 months: 1.30 (95% CI 1.01, 1.67), 1.39 (1.07, 1.80), and 1.17 (95% CI 0.90, 1.53), respectively). Probabilistic bias analysis suggested that the observed effect became statistically non-significant if the selection probability ratio was between 1.2 and 1.4. Selection probability ratios of &gt; 1.8 were needed to mask a statistically significant benefit of the intervention. The use of probabilistic bias analysis in this c-RCT suggested that worse outcomes observed in the intervention arm could plausibly be attributed to selection bias. A very large degree of selection of bias was needed to mask a beneficial effect of intervention making this interpretation less plausible.-Applying quantitative bias analysis to estimate the plausible effects of selection bias in a cluster randomised controlled trial: secondary analysis of the Primary care Osteoarthritis Screening Trial (POST).",1
"In many studies on the spatial risk of disease, investigators use geographic locations at the time of disease diagnosis in spatial models to search for individual areas of elevated risk. However, these studies often fail to find a significant spatial signal. This may be due to the misspecification of the timing and location of pertinent exposures. Environmental exposures related to cancer risk vary over space and time, and many cancers have long latencies. When these factors are considered in conjunction with a mobile population, it is likely that the spatial signal related to relevant historic environmental exposures is obscured. To investigate this hypothesis, we conducted simulation studies to characterize the effect of residential mobility on the ability of generalized additive models to detect areas of significantly elevated historic environmental exposure. We generated data based on the residential histories of participants in the National Cancer Institute Surveillance, Epidemiology, and End Results non-Hodgkin lymphoma study, and varied the duration and intensity of the environmental exposure. Results showed that the probability of detection, mean spatial sensitivity, and mean spatial specificity of models decreased steadily as the time since relevant exposure increased. This suggests that for diseases with long latencies, spatial areas of high risk due to high-intensity exposure of relatively short duration will be difficult to detect over time when using residential locations at the time of diagnosis in mobile study populations.-The timing of geographic power.",0
"C-reactive protein (CRP) has been shown to predict cardiovascular disease. Whether predictions differ across risk factor strata and for short and long-term follow-up has not been clearly examined. The purpose of this report is to assess the relation between CRP and the development of myocardial infarction (MI) over a 20-year period in men in the Honolulu Heart Program. Subjects were aged 48 to 70 years and free of prevalent disease at the time when CRP levels were measured and follow-up began. Using a case-control design, 369 cases of MI were compared with 1,348 control subjects. After risk factor adjustment, the odds of an MI rose with increasing levels of CRP as early as 5 years into follow-up (P = 0.009). Associations appeared to persist beyond this time, but after 15 years, effects became modest. Adverse effects of an elevated CRP level were observed in middle-aged men (&lt; or =55 years), in men without hypertension or diabetes, and in those who were nonsmokers (P &lt; 0.05). Although positive effects were also observed in those who were hypertensive and smoking at the time of CRP measurement, findings suggest that in clinically healthy men, atherosclerosis could have origins more closely linked with inflammation than with other processes.-C-reactive protein and myocardial infarction.",0
"To calculate the intracluster correlation coefficient (ICC) for emergency department (ED) shock rate, early trauma death (ie, death during the first 24 h after arrival at hospital), and in-hospital trauma death rate for multicenter childhood injuries. The National Trauma Data Bank (5th revision), the largest multicenter trauma registry in the US, was used. Data from 80 trauma centers were used to calculate the ICC for in-hospital trauma death rate. Thirty three states provided data for calculation of the ICC for ED shock and early trauma death rate. From 2000 to 2004, 13% of the 952 242 patients in the National Trauma Data Bank were &lt;15 years old. Approximately 17 000 of these children had injuries with an injury severity score &gt;15, of whom 84% (14 095 subjects) were hospitalized at 80 level I or II trauma centers in 33 states. The ICCs for ED shock rate, early trauma death rate, and in-hospital death rate were 0.005 (95% CI 0.000 to 0.010), 0.014 (95% CI 0.004 to 0.024), and 0.023 (95% CI 0.013 to 0.033), respectively. These ICCs were calculated for boys and girls and also for blunt and penetrating injuries. Clustered childhood trauma studies that aim to compare different aspects of pre-hospital and hospital trauma care should incorporate these ICCs for sample calculation. When cluster randomized clinical trials are mounted, if sample sizes are calculated without adjustment for ICC, then the planned trial is likely to be seriously underpowered.-Intracluster correlation coefficient in multicenter childhood trauma studies.",1
"Field experiments with nested structures are becoming increasingly common, especially designs that assign randomly entire clusters such as schools to a treatment and a control group. In such large-scale cluster randomized studies the challenge is to obtain sufficient power of the test of the treatment effect. The objective is to maximize power without adding many clusters that make the study much more expensive. In this article I discuss how power estimates of tests of treatment effects in balanced cluster randomized designs are affected by covariates at different levels. I use third-grade data from Project STAR, a field experiment about class size, to demonstrate how covariates that explain a considerable proportion of variance in outcomes increase power significantly. When lower level covariates are group-mean centered and clustering effects are larger, top-level covariates increase power more than lower level covariates. In contrast, when clustering effects are smaller and lower level covariates are grand-mean centered or uncentered, lower level covariates increase power more than top-level covariates.-The Impact of Covariates on Statistical Power in Cluster Randomized Designs: Which Level Matters More?",1
"Cluster randomisation is commonly used to evaluate educational and organisational interventions in primary care. We conducted a study where 66 general practices in the North East of Scotland were randomised to receive guidelines and access to a fast-track investigation service for two common urological conditions. Patients were identified from referral letters and recruited upon referral to secondary care. Although these urological conditions are common in secondary care, the number of referrals per general practice can be small; this created a number of issues for the analysis. Three general approaches in the analysis of cluster randomised trials; the adjustment of standard tests; analysis at cluster level; and advanced statistical techniques (random effects models and generalised estimating equations) were applied to data from the above trial. The effect of the intervention on both a continuous and a dichotomous outcome was investigated. Spuriously low P values were obtained when conventional tests (which do not account for clustering in the data) were applied. Cluster level analysis of the dichotomous outcome with no account for cluster size resulted in a different conclusion compared with cluster level analysis with weighting, standard tests with adjustment and advanced statistical methods. Cluster randomised trials are becoming increasingly common in primary care. Where recruitment of individual patients is generated by referral from primary to secondary care it is likely that the trial will suffer from inherent weaknesses: not all clusters randomised contribute to the analysis; there is the likelihood of single size clusters and variable cluster sizes. Our analysis indicated that the different approaches produced consistent results across continuous outcomes, but for dichotomous outcomes in the cluster level analysis, failure to weight observations would have resulted in a different conclusion.-Comparison of analytical methods for cluster randomised trials: an example from a primary care setting.",1
"Modelling HIV dynamics has played an important role in understanding the pathogenesis of HIV infection in the past several years. Non-linear parametric models, derived from the mechanisms of HIV infection and drug action, have been used to fit short-term clinical data from AIDS clinical trials. However, it is found that the parametric models may not be adequate to fit long-term HIV dynamic data. To preserve the meaningful interpretation of the short-term HIV dynamic models as well as to characterize the long-term dynamics, we introduce a class of semi-parametric non-linear mixed-effects (NLME) models. The models are non-linear in population characteristics (fixed effects) and individual variations (random effects), both of which are modelled semi-parametrically. A basis-based approach is proposed to fit the models, which transforms a general semi-parametric NLME model into a set of standard parametric NLME models, indexed by the bases used. The bases that we employ are natural cubic splines for easy implementation. The resulting standard NLME models are low-dimensional and easy to solve. Statistical inferences that include testing parametric against semi-parametric mixed-effects are investigated. Innovative bootstrap procedures are developed for simulating the empirical distributions of the test statistics. Small-scale simulation and bootstrap studies show that our bootstrap procedures work well. The proposed approach and procedures are applied to long-term HIV dynamic data from an AIDS clinical study.-The study of long-term HIV dynamics using semi-parametric non-linear mixed-effects models.",0
"This paper evaluates methods for unadjusted analyses of binary outcomes in cluster randomized trials (CRTs). Under the generalized estimating equations (GEE) method the identity, log and logit link functions may be specified to make inferences on the risk difference, risk ratio and odds ratio scales, respectively. An alternative, 'cluster-level', method applies the t-test to summary statistics calculated for each cluster, using proportions, log proportions and log odds, to make inferences on the respective scales. Simulation was used to estimate the bias of the unadjusted intervention effect estimates and confidence interval coverage, generating data sets with different combinations of number of clusters, number of participants per cluster, intra-cluster correlation coefficient rho and intervention effect. When the identity link was specified, GEE had little bias and good coverage, performing slightly better than the log and logit link functions. The cluster-level method provided unbiased point estimates when proportions were used to summarize the clusters. When the log proportion and log odds were used, however, the method often had markedly large bias for two reasons: (i) bias in the modified summary statistic used for cluster-level estimation when a cluster has zero cases with the outcome of interest (arising when the number of participants sampled per cluster is small and the outcome prevalence is low) and (ii) asymptotically, the method estimates the ratio of geometric means of the cluster proportions or odds, respectively, between the trial arms rather than the ratio of arithmetic means.-Comparison of the risk difference, risk ratio and odds ratio scales for quantifying the unadjusted intervention effect in cluster randomized trials.",1
"Vaccine trials are performed to measure the efficacy of vaccines or vaccination strategies. The statistical aspects pertain both to design and analysis of such trials. In experimental trials, the design should ensure that the groups to be compared are equal in all respects except for the factor to be assessed. This is achieved by proper randomisation procedures. The diagnosis of cases should as far as possible be carried out without knowledge of vaccination status (blinding). If the administration of a vaccine per se is considered to influence the outcome, a similar administration of placebo in the control group should be considered. The required sample size will depend on a specified significance level, a specified power, and on the magnitude of the effect one wants to detect. If the cumulative numbers of cases in vaccinated and unvaccinated groups are registered at the end of the observation period, the effect can be measured by the risk ratio, risk difference or the relative percent survival. If the estimation is based on a fish-time approach and the presumption of constant rate of outcome over time is fulfilled, the rate ratio or the rate difference can be estimated. If the rate is not constant, the life table would be a method of choice. In the statistical analysis of the vaccination effect it is important to consider the assumptions on which the statistical tests are based. Proper choice of the experimental and statistical units in the trials is crucial. If a trial is set up with the individual fish as the statistical unit, adjusting for dependence (cluster effect) between the units may be indicated. If possible, the vaccine effect should be given by the point estimate and the confidence intervals. The statistical aspects of fish vaccination trials are based on general principles for controlled clinical trials, but they are also influenced by characteristics connected to the fish population and the experimental conditions.-Statistical aspects of fish vaccination trials.",1
A fully parametric copula model for symmetric dependent clustered categorical data is discussed. The model accommodates any marginal regression models of interest and admits a broad range of within-cluster association. The form of the distribution is independent of cluster size and may be used to model data with varying cluster sizes. The model contains an association parameter that is estimated from the data to give a measure of strength of the within-cluster association and also a test of independence. Two examples are given to illustrate methods.-A parametric model for cluster correlated categorical data.,1
"Randomised trials evaluation of surgical interventions are often designed and analysed as if the outcome of individual patients is independent of the surgeon providing the intervention. There is reason to expect outcomes for patients treated by the same surgeon tend to be more similar than those under the care of another surgeon due to previous experience, individual practice, training, and infrastructure. Such a phenomenon is referred to as the clustering effect and potentially impacts on the design and analysis adopted and thereby the required sample size. The aim of this work was to inform trial design by quantifying clustering effects (at both centre and surgeon level) for various outcomes using a database of surgical trials. Intracluster correlation coefficients (ICCs) were calculated for outcomes from a set of 10 multicentre surgical trials for a range of outcomes and different time points for clustering at both the centre and surgeon level. ICCs were calculated for 198 outcomes across the 10 trials at both centre and surgeon cluster levels. The number of cases varied from 138 to 1370 across the trials. The median (range) average cluster size was 32 (9 to 51) and 6 (3 to 30) for centre and surgeon levels respectively. ICC estimates varied substantially between outcome type though uncertainty around individual ICC estimates was substantial, which was reflected in generally wide confidence intervals. This database of surgical trials provides trialists with valuable information on how to design surgical trials. Our data suggests clustering of outcome is more of an issue than has been previously acknowledged. We anticipate that over time the addition of ICCs from further surgical trial datasets to our database will further inform the design of surgical trials.-Clustering in surgical trials--database of intracluster correlations.",1
"We have developed methods for modeling discrete or grouped time, right-censored survival data collected from correlated groups or clusters. We assume that the marginal hazard of failure for individual items within a cluster is specified by a linear log odds survival model and the dependence structure is based on a gamma frailty model. The dependence can be modeled as a function of cluster-level covariates. Likelihood equations for estimating the model parameters are provided. Generalized estimating equations for the marginal hazard regression parameters and pseudolikelihood methods for estimating the dependence parameters are also described. Data from two clinical trials are used for illustration purposes.-Modeling clustered, discrete, or grouped time survival data with covariates.",1
"Three-level cluster randomized trials (CRTs) are increasingly used in implementation science, where 2fold-nested-correlated data arise. For example, interventions are randomly assigned to practices, and providers within the same practice who provide care to participants are trained with the assigned intervention. Teerenstra et al proposed a nested exchangeable correlation structure that accounts for two levels of clustering within the generalized estimating equations (GEE) approach. In this article, we utilize GEE models to test the treatment effect in a two-group comparison for continuous, binary, or count data in three-level CRTs. Given the nested exchangeable correlation structure, we derive the asymptotic variances of the estimator of the treatment effect for different types of outcomes. When the number of clusters is small, researchers have proposed bias-corrected sandwich estimators to improve performance in two-level CRTs. We extend the variances of two bias-corrected sandwich estimators to three-level CRTs. The equal provider and practice sizes were assumed to calculate number of practices for simplicity. However, they are not guaranteed in practice. Relative efficiency (RE) is defined as the ratio of variance of the estimator of the treatment effect for equal to unequal provider and practice sizes. The expressions of REs are obtained from both asymptotic variance estimation and bias-corrected sandwich estimators. Their performances are evaluated for different scenarios of provider and practice size distributions through simulation studies. Finally, a percentage increase in the number of practices is proposed due to efficiency loss from unequal provider and/or practice sizes.-Sample size calculation in three-level cluster randomized trials using generalized estimating equation models",1
"With heighted interest in causal inference based on real-world evidence, this empirical study sought to understand differences between the results of observational analyses and long-term randomized clinical trials. We hypothesized that patients deemed ""eligible"" for clinical trials would follow a different survival trajectory from those deemed ""ineligible"" and that this factor could partially explain results. In a large observational registry dataset, we estimated separate survival trajectories for hypothetically trial-eligible vs ineligible patients under both coronary artery bypass surgery (CABG) and percutaneous coronary intervention (PCI). We also explored whether results would depend on the causal inference method (inverse probability of treatment weighting vs optimal full propensity matching) or the approach to combine propensity scores from multiple imputations (the ""across"" vs ""within"" approaches). We found that, in this registry population of PCI/CABG multivessel patients, 32.5% would have been eligible for contemporaneous RCTs, suggesting that RCTs enroll selected populations. Additionally, we found treatment selection bias with different distributions of propensity scores between PCI and CABG patients. The different methodological approaches did not result in different conclusions. Overall, trial-eligible patients appeared to demonstrate at least marginally better survival than ineligible patients. Treatment comparisons by eligibility depended on disease severity. Among trial-eligible three-vessel diseased and trial-ineligible two-vessel diseased patients, CABG appeared to have at least a slight advantage with no treatment difference otherwise. In conclusion, our analyses suggest that RCTs enroll highly selected populations, and our findings are generally consistent with RCTs but less pronounced than major registry findings.-Empirical use of causal inference methods to evaluate survival differences in a real-world registry vs those found in randomized clinical trials.",0
"Dose--response experiments are crucial in biomedical studies. There are usually multiple objectives in such experiments and among the goals is the estimation of several percentiles on the dose--response curve. Here we present the first non-parametric adaptive design approach to estimate several percentiles simultaneously via generalized P?lya urns. Theoretical properties of these designs are investigated and their performance is gaged by the locally compound optimal designs. As an example, we re-investigated a psychophysical experiment where one of the goals was to estimate the three quartiles. We show that these multiple-objective adaptive designs are more efficient than the original single-objective adaptive design targeting the median only. We also show that urn designs which target the optimal designs are slightly more efficient than those which target the desired percentiles directly. Guidelines are given as to when to use which type of design. Overall we are pleased with the efficiency results and hope compound adaptive designs proposed in this work or their variants may prove to be a viable non-parametric alternative in multiple-objective dose--response studies.-Adaptive urn designs for estimating several percentiles of a dose--response curve.",0
"Studies of human immunodeficiency virus dynamics in acquired immuno deficiency syndrome (AIDS) research are very important in evaluating the effectiveness of antiretroviral (ARV) therapies. The potency of ARV agents in AIDS clinical trials can be assessed on the basis of a viral response such as viral decay rate or viral load change in plasma. Following ARV treatment, the profile of each subject's viral load tends to follow a 'broken stick'-like dynamic trajectory, indicating multiple phases of decline and increase in viral loads. Such multiple-phases (change-points) can be described by a random change-point model with random subject-specific parameters. One usually assumes a normal distribution for model error. However, this assumption may be unrealistic, obscuring important features of within- and among-subject variations. In this article, we propose piecewise linear mixed-effects models with skew-elliptical distributions to describe the time trend of a response variable under a Bayesian framework. This methodology can be widely applied to real problems for longitudinal studies. A real data analysis, using viral load data from an AIDS study, is carried out to illustrate the proposed method by comparing various candidate models. Biologically important findings are reported, and these findings also suggest that it is very important to assume a model with skew distribution in order to achieve reliable results, in particular, when the data exhibit skewness.-Piecewise mixed-effects models with skew distributions for evaluating viral load changes: A Bayesian approach.",0
"In constructing predictive models, investigators frequently assess the incremental value of a predictive marker by comparing the ROC curve generated from the predictive model including the new marker with the ROC curve from the model excluding the new marker. Many commentators have noticed empirically that a test of the two ROC areas often produces a non-significant result when a corresponding Wald test from the underlying regression model is significant. A recent article showed using simulations that the widely used ROC area test produces exceptionally conservative test size and extremely low power. In this article, we demonstrate that both the test statistic and its estimated variance are seriously biased when predictions from nested regression models are used as data inputs for the test, and we examine in detail the reasons for these problems. Although it is possible to create a test reference distribution by resampling that removes these biases, Wald or likelihood ratio tests remain the preferred approach for testing the incremental contribution of a new marker.-Comparing ROC curves derived from regression models.",0
"Although the negative impact of sleep apnea on the clinical course of acute ischemic stroke (AIS) is well known, data regarding non-invasive ventilation in acute patients are scarce. Several studies have shown its tolerability and safety, yet no controlled randomized sequential phase studies exist that aim to establish the efficacy of early non-invasive ventilation in AIS patients. We decided to examine our hypothesis that early non-invasive ventilation with auto-titrating bilevel positive airway pressure (auto-BPAP) positively affects short-term clinical outcomes in AIS patients. We perform a multicenter, prospective, randomized, controlled, third rater- blinded, parallel-group trial. Patients with AIS with proximal arterial obstruction and clinically suspected sleep apnea will be randomized to standard stroke care alone or standard stroke care plus auto-BPAP. Auto-BPAP will be initiated within 24?hours of stroke onset and performed for a maximum of 48?hours during diurnal and nocturnal sleep. Patients will undergo unattended cardiorespiratory polygraphy between days three and five to assess sleep apnea. Our primary endpoint will be any early neurological improvement on the NIHSS at 72?hours from randomization. Safety, tolerability, short-term and three-months functional outcomes will be assessed as secondary endpoints by un-blinded and blinded observers respectively. We expect that this study will advance our understanding of how early treatment with non-invasive ventilation can counterbalance, or possibly reverse, the deleterious effects of sleep apnea in the acute phase of ischemic stroke. The study will provide preliminary data to power a subsequent phase III study. Clinicaltrials.gov Identifier: NCT01812993.-Reversal of the neurological deficit in acute stroke with the signal of efficacy trial of auto-BPAP to limit damage from suspected sleep apnea (Reverse-STEAL): study protocol for a randomized controlled trial.",0
"In standard regression analyses of clustered data, one typically assumes that the expected value of the response is independent of cluster size. However, this is often false. For example, in studies of surgical interventions, investigators have frequently found surgery volume and outcomes to be related to the skill level of the surgeons. This paper examines the effect of ignoring response-dependent, informative, cluster sizes on standard analytical methods such as mixed-effects models and conditional likelihood methods using analytic calculations, simulation studies and an example from a study of periodontal disease. We consider the case in which cluster sizes and responses share random effects which we assume to be independent of the covariates. Our focus is on maximum likelihood methods that ignore informative cluster sizes, and we show that they exhibit little bias in estimating covariate effects that are uncorrelated with the random effects associated with cluster sizes. However, estimation of covariate effects that are associated with the random effects can be biased. In particular, for models with random intercepts only, ignoring informative cluster sizes can yield biased estimators of the intercept but little bias in estimation of all covariate effects.-Estimation of covariate effects in generalized linear mixed models with informative cluster sizes.",1
Hypoxic ischaemic encephalopathy in infants.,0
"It is common in epidemiological and clinical studies that each subject has repeated measurements on a single common variable, while the subjects are also 'clustered'. To compute sample size or power of a test, we have to consider two types of correlation: correlation among repeated measurements within the same subject, and correlation among subjects in the same cluster. We develop, based on generalized estimating equations, procedures for computing sample size and power with clustered repeated measurements. Explicit formulae are derived for comparing two means, two slopes and two proportions, under several simple correlation structures.-Sample size and power determination for clustered repeated measurements.",1
"The cumulative incidence function is widely reported in competing risks studies, with group differences assessed by an extension of the log-rank test. However, simple, interpretable summaries of group differences are not available. An adaptation of the proportional hazards model to the cumulative incidence function is often employed, but the interpretation of the hazard ratio may be somewhat awkward, unlike the usual survival set-up. We propose nonparametric inferences for general summary measures, which may be time-varying, and for time-averaged versions of the measures. Theoretical justification is provided using counting process techniques. A real data example illustrates the practical utility of the methods.-Summarizing differences in cumulative incidence functions.",0
"To explore the relation between hospital orthopaedic specialisation and postoperative outcomes after total hip or knee replacement surgery. Retrospective analysis of US Medicare data, 2001-5. 3818 US hospitals carrying out total joint replacement. Population 1 273 081 Medicare beneficiaries age 65 and older who underwent primary or revision hip or knee replacement. Hospitals were stratified into fifths on the basis of their degree of orthopaedic specialisation (lowest fifth, least specialised; highest fifth, most specialised). The primary outcome was defined as a composite representing the occurrence of one or more of pulmonary embolism, deep vein thrombosis, haemorrhage, infection, myocardial infarction, or death within 90 days of the index surgery. As hospital orthopaedic specialisation increased from the lowest fifth to highest fifth, the proportion of people admitted who were women or black, or who had diabetes or heart failure progressively decreased (P&lt;0.001), whereas procedural volume increased. Compared with the most specialised hospitals (highest fifth), after adjustment for patient characteristics and hospital volume, the odds of adverse outcomes increased progressively with decreased hospital specialisation: lowest fifth (odds ratio 1.59, 95% confidence interval 1.53 to 1.65), second fifth (1.32, 1.28 to 1.36), third fifth (1.24, 1.21 to 1.28), and fourth fifth (1.10, 1.07 to 1.13). Increased hospital orthopaedic specialisation is associated with improved patient outcomes after adjusting for both patient characteristics and hospital procedural volume. These results should be interpreted with caution because the possibility that other unmeasured confounders related to socioeconomic status or different factors are responsible for the improved patient outcomes rather than hospital specialisation can not be excluded. The findings suggest that hospital specialisation may capture different components of hospital quality than the components captured by hospital volume.-Relation between hospital orthopaedic specialisation and outcomes in patients aged 65 and older: retrospective analysis of US Medicare data.",0
"Characterization of HIV viral rebound after the discontinuation of antiretroviral therapy is central to HIV cure research. We propose a parametric nonlinear mixed effects model for the viral rebound trajectory, which often has a rapid rise to a peak value followed by a decrease to a viral load set point. We choose a flexible functional form that captures the shapes of viral rebound trajectories and can also provide biological insights regarding the rebound process. Each parameter can incorporate a random effect to allow for variation in parameters across individuals. Key features of viral rebound trajectories such as viral set points are represented by the parameters in the model, which facilitates assessment of intervention effects and identification of important pretreatment interruption predictors for these features. We employ a stochastic expectation-maximization (StEM) algorithm to incorporate HIV-1 RNA values that are below the lower limit of assay quantification. We evaluate the performance of our model in simulation studies and apply the proposed model to longitudinal HIV-1 viral load data from five AIDS Clinical Trials Group treatment interruption studies.-A flexible nonlinear mixed effects model for HIV viral load rebound after treatment interruption.",0
"Hispanic cancer patients are underrepresented in clinical trials; research suggests lack of knowledge and language barriers contribute to low accrual. Multimedia materials offer advantages to Hispanic populations because they have high acceptability, are easy to disseminate, and can be viewed with family. Hispanic cancer patients and caregivers participated in focus groups to aid in developing a Spanish-language multimedia intervention to educate Hispanic cancer patients about clinical trials. We explored the feasibility of delivering the intervention in medical oncology clinics. A total of 35 patients were randomized to either the multimedia intervention group (n = 18) or a control group (n = 17) who were asked to read the National Cancer Institute's Spanish-language clinical trials brochure. Self-reported data on knowledge about and attitudes toward clinical trials, self-efficacy for participating in a clinical trial, intention to participate in a clinical trial if asked, and receptivity to information about a clinical trial were collected at baseline and 10 days later. Delivery of the multimedia presentation in oncology clinics was feasible. The intervention group had more knowledge about clinical trials at follow-up than the control group; scores for intention to participate in a clinical trial by participants in the intervention group increased from 3.8 to 4.0 of a possible 5, but declined in the control group from 4.5 to 4.1. No statistically significant difference was detected between groups in scores for attitudes or self-efficacy for making a decision to participate in a clinical trial. Our sample size was inadequate to identify differences between the informational methods. Although all patients were asked about their willingness to participate in a clinical trial, this decision was hypothetical. In addition, the study was conducted with a sample of Spanish-speaking Hispanic cancer patients at a comprehensive cancer center in Florida. Thus, the results may not generalize to other Hispanic populations. In the pilot project, we demonstrated the feasibility of delivering multimedia information to patients in medical oncology clinics. Because delivery in a clinical setting was found to be feasible, a larger study should be conducted to evaluate the efficacy of the multimedia intervention with respect to promoting accrual of Hispanic patients to clinical trials.-Feasibility trial of a Spanish-language multimedia educational intervention.",0
"Most trials of interventions are designed to address the traditional null hypothesis of no benefit. VOICE, a phase 2B HIV prevention trial funded by NIH and conducted in Africa, is designed to assess if the intervention will prevent a substantial fraction of infections. Planned interim analysis may provide conclusive evidence against the traditional null hypothesis without establishing substantial benefit. At this interim point, the Data and Safety Monitoring Board would then face the dilemma of knowing the product has some positive effect, but perhaps not as great an effect as the protocol has declared necessary. In March 2008, NIH program staff recommended that the VOICE protocol team discuss the stopping rules with stakeholders prior to initiating the protocol. The goals of the workshop were to inform community representatives about the potential ethical dilemma associated with stopping rules and engage in dialogue about these issues. We describe the resulting community consultation and summarize the outcomes. A 2-day workshop was convened with the goal of having a clear and transparent consultation with the stakeholders around the question, 'Given emerging evidence that a product could prevent some infections, would the community support a decision to continue accruing to the trial?' Participants included research staff and community stakeholders. Lectures with visual aids, discussions, and exercises using interactive learning tasks were used, with a focus on statistics and interpreting data from trials, particularly interim data. Results of oral and written evaluations by participants were reviewed. The feedback was mostly positive, with some residual confusion regarding statistical concepts. However, discussions with attendees later revealed that not all felt prepared to engage fully in the workshop. This was the presenters' first experience facilitating a formal discussion with an audience that had no advanced science, research, or mathematics training. Community representatives' concern regarding speaking for their communities without consulting them also created a challenge for the workshop. Open discussion around trial stopping rules requires that all discussants have an understanding of trial design concepts and feel a sense of empowerment to ask and answer questions. The VOICE CWG workshop was a first step toward the goal of open discussion regarding trial stopping rules and interim results for the study; however, ongoing education and dialogue must occur to ensure that all stakeholders fully participate in the process.-Fostering community understanding of sufficient benefit and early stopping for a phase 2B HIV prevention clinical trial in Africa.",0
"When designing studies that have a binary outcome as the primary endpoint, the hypothesized proportion of patients in each population experiencing the endpoint of interest (i.e., ? 1,? 2) plays an important role in sample size and power calculations. Point estimates for ? 1 and ? 2 are often calculated using historical data. However, the uncertainty in these estimates is rarely addressed. This paper presents a hybrid classical and Bayesian procedure that formally integrates prior information on the distributions of ? 1 and ? 2 into the study's power calculation. Conditional expected power (CEP), which averages the traditional power curve using the prior distributions of ? 1 and ? 2 as the averaging weight conditional on the presence of a positive treatment effect (i.e., ? 2&gt;? 1), is used, and the sample size is found that equates the pre-specified frequentist power (1-?) and the conditional expected power of the trial. Notional scenarios are evaluated to compare the probability of achieving a target value of power with a trial design based on traditional power and a design based on CEP. We show that if there is uncertainty in the study parameters and a distribution of plausible values for ? 1 and ? 2, the performance of the CEP design is more consistent and robust than traditional designs based on point estimates for the study parameters. Traditional sample size calculations based on point estimates for the hypothesized study parameters tend to underestimate the required sample size needed to account for the uncertainty in the parameters. The greatest marginal benefit of the proposed method is achieved when the uncertainty in the parameters is not large. Through this procedure, we are able to formally integrate prior information on the uncertainty and variability of the study parameters into the design of the study while maintaining a frequentist framework for the final analysis. Solving for the sample size that is necessary to achieve a high level of CEP given the available prior information helps protect against misspecification of hypothesized treatment effect and provides a substantiated estimate that forms the basis for discussion about the study's feasibility during the design phase.-Sample size determination for a binary response in a superiority clinical trial using a hybrid classical and Bayesian procedure.",0
"Multiple imputation (MI) has been proposed for handling missing data in cost-effectiveness analyses (CEAs). In CEAs that use cluster randomized trials (CRTs), the imputation model, like the analysis model, should recognize the hierarchical structure of the data. This paper contrasts a multilevel MI approach that recognizes clustering, with single-level MI and complete case analysis (CCA) in CEAs that use CRTs. We consider a multilevel MI approach compatible with multilevel analytical models for CEAs that use CRTs. We took fully observed data from a CEA that evaluated an intervention to improve diagnosis of active labor in primiparous women using a CRT (2078 patients, 14 clusters). We generated scenarios with missing costs and outcomes that differed, for example, according to the proportion with missing data (10%-50%), the covariates that predicted missing data (individual, cluster-level), and the missingness mechanism: missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR). We estimated incremental net benefits (INBs) for each approach and compared them with the estimates from the fully observed data, the ""true"" INBs. When costs and outcomes were assumed to be MCAR, the INBs for each approach were similar to the true estimates. When data were MAR, the point estimates from the CCA differed from the true estimates. Multilevel MI provided point estimates and standard errors closer to the true values than did single-level MI across all settings, including those in which a high proportion of observations had cost and outcome data MAR and when data were MNAR. Multilevel MI accommodates the multilevel structure of the data in CEAs that use cluster trials and provides accurate cost-effectiveness estimates across the range of circumstances considered.-Multiple imputation methods for handling missing data in cost-effectiveness analyses that use data from hierarchical studies: an application to cluster randomized trials.",1
"To examine whether cluster randomized trials (1) produce baseline imbalances between intervention and control conditions; (2) give results that are substantially different individually randomized trials; and (3) give different results when adjusted for unit of analysis error. We used 14 cluster randomized trials and 20 individualized trials of the same intervention (collaborative care for depression). We conducted a random effects meta-analysis to examine imbalance in baseline depression scores. We used meta-regression to test for differential effect size and heterogeneity between clustered and individualized studies. Unit of analysis error was corrected using a range of plausible published intraclass correlation coefficients (ICCs). There were no baseline imbalances in either cluster randomized (P=0.837) or individually randomized (P=0.737) studies. Cluster randomized studies gave almost identical estimates of effect size when compared to individually randomized studies (standardized mean difference, SMDcluster=0.25, 95% confidence interval [CI]: 0.17, 0.33; SMDindividual=0.24; 95% CI: 0.13, 0.36). Adjustment for clustering had minimal effect on clinical and statistical significance (pooled SMDICC 0.02=0.249 [95% CI: 0.174, 0.325] to SMDICC 0.05=0.258 [95% CI: 0.172, 0.345]). The additional effort and expense involved in cluster randomized trials needs to be justified when individualized studies might produce robust and believable results.-Cluster randomized trials produced similar results to individually randomized trials in a meta-analysis of enhanced care for depression.",1
"Covariate measurement error in regression is typically assumed to act in an additive or multiplicative manner on the true covariate value. However, such an assumption does not hold for the measurement error of sleep-disordered breathing (SDB) in the Wisconsin Sleep Cohort Study (WSCS). The true covariate is the severity of SDB, and the observed surrogate is the number of breathing pauses per unit time of sleep, which has a nonnegative semicontinuous distribution with a point mass at zero. We propose a latent variable measurement error model for the error structure in this situation and implement it in a linear mixed model. The estimation procedure is similar to regression calibration but involves a distributional assumption for the latent variable. Modeling and model-fitting strategies are explored and illustrated through an example from the WSCS.-A longitudinal measurement error model with a semicontinuous covariate.",0
"Research about relationships between place characteristics and racial/ethnic inequities in health has largely ignored conceptual advances about race and place within the discipline of geography. Research has also almost exclusively quantified these relationships using effect estimates (e.g., odds ratios), statistics that fail to adequately capture the full impact of place characteristics on inequities and thus undermine our ability to translate research into action. We draw on geography to further develop the concept of ""racialized risk environments,"" and we argue for the routine calculation of race/ethnicity-specific population-attributable risk percentages.-Population-Attributable Risk Percentages for Racialized Risk Environments.",0
"As a result of previous large, multipoint linkage studies there is a substantial amount of existing marker data. Due to the increased sample size, genetic maps estimated from these data could be more accurate than publicly available maps. However, current methods for map estimation are restricted to data sets containing pedigrees with a small number of individuals, or cannot make full use of marker data that are observed at several loci on members of large, extended pedigrees. In this article, a maximum likelihood (ML) method for map estimation that can make full use of the marker data in a large, multipoint linkage study is described. The method is applied to replicate sets of simulated marker data involving seven linked loci, and pedigree structures based on the real multipoint linkage study of Abkevich et al. (2003, American Journal of Human Genetics 73, 1271-1281). The variance of the ML estimate is accurately estimated, and tests of both simple and composite null hypotheses are performed. An efficient procedure for combining map estimates over data sets is also suggested.-Improving estimates of genetic maps: a maximum likelihood approach.",0
"We examined the association of language proficiency vs language preference with self-rated health among Asian American immigrants. We also examined whether modeling preference or proficiency as continuous or categorical variables changed our inferences. Data came from the 2002-2003 National Latino and Asian American Study (n = 1639). We focused on participants' proficiency in speaking, reading, and writing English and on their language preference when thinking or speaking with family or friends. We examined the relation between language measures and self-rated health with ordered and binary logistic regression. All English proficiency measures were associated with self-rated health across all models. By contrast, associations between language preference and self-rated health varied by the model considered. Although many studies create composite scores aggregated across measures of English proficiency and language preference, this practice may not always be conceptually or empirically warranted.-English proficiency and language preference: testing the equivalence of two measures.",0
"The stepped wedge cluster randomized design has received increasing attention in pragmatic clinical trials and implementation science research. The key feature of the design is the unidirectional crossover of clusters from the control to intervention conditions on a staggered schedule, which induces confounding of the intervention effect by time. The stepped wedge design first appeared in the Gambia hepatitis study in the 1980s. However, the statistical model used for the design and analysis was not formally introduced until 2007 in an article by Hussey and Hughes. Since then, a variety of mixed-effects model extensions have been proposed for the design and analysis of these trials. In this article, we explore these extensions under a unified perspective. We provide a general model representation and regard various model extensions as alternative ways to characterize the secular trend, intervention effect, as well as sources of heterogeneity. We review the key model ingredients and clarify their implications for the design and analysis. The article serves as an entry point to the evolving statistical literatures on stepped wedge designs.-Mixed-effects models for the design and analysis of stepped wedge cluster randomized trials: An overview",3
"Cluster randomized controlled trials (cRCTs; also known as group randomized trials and community-randomized trials) are multilevel experiments in which units that are randomly assigned to experimental conditions are sets of grouped individuals, whereas outcomes are recorded at the individual level. In human cRCTs, clusters that are randomly assigned are typically families, classrooms, schools, worksites, or counties. With growing interest in community-based, public health, and policy interventions to reduce obesity or improve nutrition, the use of cRCTs has increased. Errors in the design, analysis, and interpretation of cRCTs are unfortunately all too common. This situation seems to stem in part from investigator confusion about how the unit of randomization affects causal inferences and the statistical procedures required for the valid estimation and testing of effects. In this article, we provide a brief introduction and overview of the importance of cRCTs and highlight and explain important considerations for the design, analysis, and reporting of cRCTs by using published examples.-Best (but oft-forgotten) practices: designing, analyzing, and reporting cluster randomized controlled trials.",1
"'Tissue banks' are created, at least in part, to help medical scientists learn about disease biology on the basis of samples provided by patients on treatment protocols that were competed years earlier. The bank inventory consists of precious non-renewable patient material (such as frozen diagnostic blood or bone marrow), which can be linked to both clinical data and long term follow-up information. Case-control studies, where cases represent clinical failures and controls clinical successes, are ideal for rapidly learning if a laboratory marker might have prognostic significance. While group sequential (multi-stage) methods are widely used in clinical trials, they have rarely been applied in case-control studies. Further, unlike clinical trials where safety and efficiency may actually be in conflict, case-control studies can focus on efficiency. Hence, minimizing the expected sample size is a desirable goal in such a setting. Since the true effect size is never known, and since no prior distribution can be postulated for the effect size, we have opted for the minimax solution. A strategy is developed to determine amongst all two-stage designs with given type I and type II errors, the one for which the maximum expected sample size is minimized. The user is provided with simple tables, whereby one can determine everything necessary to conduct the study from the corresponding calculation for a single-stage design. A matched pair example is given where the suggested design can be modified, to obtain a superior 'two-plus' stage design. The basic idea is to conduct the first stage as planned, but use the estimate of variance to redesign the study, without using the estimate of effect size in the redesign.-Minimax two-stage-designs with applications to tissue banking case-control studies.",0
"To provide information concerning the magnitude of the intraclass correlation coefficient (ICC) for cluster-based studies set in primary care. Reanalysis of data from 31 cluster-based studies in primary care to estimate intraclass correlation coefficients from random effects models using maximum likelihood estimation. ICCs were estimated for 1,039 variables. The median ICC was 0.010 (interquartile range [IQR] 0 to 0.032, range 0 to 0.840). After adjusting for individual- and cluster-level characteristics, the median ICC was 0.005 (IQR 0 to 0.021). A given measure showed widely varying ICC estimates in different datasets. In six datasets, the ICCs for SF-36 physical functioning scale ranged from 0.001 to 0.055 and for SF-36 general health from 0 to 0.072. In four datasets, the ICC for systolic blood pressure ranged from 0 to 0.052 and for diastolic blood pressure from 0 to 0.108. The precise magnitude of between-cluster variation for a given measure can rarely be estimated in advance. Studies should be designed with reference to the overall distribution of ICCs and with attention to features that increase efficiency.-Patterns of intra-cluster correlation from primary care research to inform study design and analysis.",1
"Dynamic prediction uses longitudinal biomarkers for real-time prediction of an individual patient's prognosis. This is critical for patients with an incurable disease such as cancer. Biomarker trajectories are usually not linear, nor even monotone, and vary greatly across individuals. Therefore, it is difficult to fit them with parametric models. With this consideration, we propose an approach for dynamic prediction that does not need to model the biomarker trajectories. Instead, as a trade-off, we assume that the biomarker effects on the risk of disease recurrence are smooth functions over time. This approach turns out to be computationally easier. Simulation studies show that the proposed approach achieves stable estimation of biomarker effects over time, has good predictive performance, and is robust against model misspecification. It is a good compromise between two major approaches, namely, (i) joint modeling of longitudinal and survival data and (ii) landmark analysis. The proposed method is applied to patients with chronic myeloid leukemia. At any time following their treatment with tyrosine kinase inhibitors, longitudinally measured BCR-ABL gene expression levels are used to predict the risk of disease progression. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-A two-stage approach for dynamic prediction of time-to-event distributions.",0
Letter to the Editor: Is the R coefficient of interest in cluster randomized trials with a binary outcome?,1
"Concordance measures are frequently used for assessing the discriminative ability of risk prediction models. The interpretation of estimated concordance at external validation is difficult if the case-mix differs from the model development setting. We aimed to develop a concordance measure that provides insight into the influence of case-mix heterogeneity and is robust to censoring of time-to-event data. We first derived a model-based concordance (mbc) measure that allows for quantification of the influence of case-mix heterogeneity on discriminative ability of proportional hazards and logistic regression models. This mbc can also be calculated including a regression slope that calibrates the predictions at external validation (c-mbc), hence assessing the influence of overall regression coefficient validity on discriminative ability. We derived variance formulas for both mbc and c-mbc. We compared the mbc and the c-mbc with commonly used concordance measures in a simulation study and in two external validation settings. The mbc was asymptotically equivalent to a previously proposed resampling-based case-mix corrected c-index. The c-mbc remained stable at the true value with increasing proportions of censoring, while Harrell's c-index and to a lesser extent Uno's concordance measure increased unfavorably. Variance estimates of mbc and c-mbc were well in agreement with the simulated empirical variances. We conclude that the mbc is an attractive closed-form measure that allows for a straightforward quantification of the expected change in a model's discriminative ability due to case-mix heterogeneity. The c-mbc also reflects regression coefficient validity and is a censoring-robust alternative for the c-index when the proportional hazards assumption holds. Copyright ? 2016 John Wiley &amp; Sons, Ltd.-A new concordance measure for risk prediction models in external validation settings.",0
"To develop and evaluate a prototype measure (OA-DISABILITY-CAT) for osteoarthritis research using item response theory (IRT) and computer-adaptive test (CAT) methodologies. We constructed an item bank consisting of 33 activities commonly affected by lower extremity (LE) osteoarthritis. A sample of 323 adults with LE osteoarthritis reported their degree of limitation in performing everyday activities, and completed the Health Assessment Questionnaire-II (HAQ-II). We used confirmatory factor analyses to assess scale unidimensionality and IRT methods to calibrate the items and examine the fit of the data. Using CAT simulation analyses, we examined the performance of OA-DISABILITY-CATs of different lengths compared with the full-item bank and the HAQ-II. One distinct disability domain was identified. The 10-item OA-DISABILITY-CAT demonstrated a high degree of accuracy compared with the full-item bank (r=0.99). The item bank and the HAQ-II scales covered a similar estimated scoring range. In terms of reliability, 95% of OA-DISABILITY reliability estimates were over 0.83 vs. 0.60 for the HAQ-II. Except at the highest scores, the 10-item OA-DISABILITY-CAT demonstrated superior precision to the HAQ-II. The prototype OA-DISABILITY-CAT demonstrated promising measurement properties compared with the HAQ-II, and is recommended for use in LE osteoarthritis research.-A computer-adaptive disability instrument for lower extremity osteoarthritis research demonstrated promising breadth, precision, and reliability.",0
"Cluster randomized trials are increasingly popular. In many of these trials, cluster sizes are unequal. This can affect trial power, but standard sample size formulae for these trials ignore this. Previous studies addressing this issue have mostly focused on continuous outcomes or methods that are sometimes difficult to use in practice. We show how a simple formula can be used to judge the possible effect of unequal cluster sizes for various types of analyses and both continuous and binary outcomes. We explore the practical estimation of the coefficient of variation of cluster size required in this formula and demonstrate the formula's performance for a hypothetical but typical trial randomizing UK general practices. The simple formula provides a good estimate of sample size requirements for trials analysed using cluster-level analyses weighting by cluster size and a conservative estimate for other types of analyses. For trials randomizing UK general practices the coefficient of variation of cluster size depends on variation in practice list size, variation in incidence or prevalence of the medical condition under examination, and practice and patient recruitment strategies, and for many trials is expected to be approximately 0.65. Individual-level analyses can be noticeably more efficient than some cluster-level analyses in this context. When the coefficient of variation is &lt;0.23, the effect of adjustment for variable cluster size on sample size is negligible. Most trials randomizing UK general practices and many other cluster randomized trials should account for variable cluster size in their sample size calculations.-Sample size for cluster randomized trials: effect of coefficient of variation of cluster size and analysis method.",1
"Accurate estimates of the intraclass correlation coefficient (ICC) are important for calculating appropriate sample sizes for cluster-randomized trials. The ICC and hence the sample size may be reduced through adjustment for baseline covariates. A method exists for calculating adjusted ICCs for binary outcomes based on the logit link, used to calculate odds ratios. Recent interest in presenting relative risks rather than odds ratios indicates that a method based on the log link is needed. To determine and evaluate a method for calculating adjusted ICCs based on the log link, and to provide and compare unadjusted and adjusted ICCs from a cluster-randomized trial in primary care based on the logit and log link. Two methods are proposed for calculating adjusted ICCs for the log link based on a first-order Taylor series expansion and properties of the lognormal distribution. The methods are evaluated by simulation. Unadjusted and adjusted ICCs are calculated for binary outcomes from the Point of Care Testing (PoCT) Trial using the logit and log link. The methods for calculating adjusted ICCs for the log link produced similar results unless the between cluster variance was large. Unadjusted ICCs for the PoCT Trial ranged from 0.001 to 0.048. The impact of adjustment on the ICC varied between outcomes and link functions, ranging from a 59% reduction to an 89% increase. The true ICC was unknown for the simulation study. Adjustment was made for age and gender only for the PoCT Trial. The method for calculating adjusted ICCs for binary outcomes depends on the link function. For the log link, the method based on the lognormal distribution is recommended. This method will be useful for cluster-randomized trials where the relative risk, rather than the odds ratio, is the effect measure of interest.-Adjusted intraclass correlation coefficients for binary data: methods and estimates from a cluster-randomized trial in primary care.",1
"The authors report the effect of active parental consent on sample bias among rural seventh graders participating in a drug abuse prevention trial. Students obtaining active consent from their parents to complete the survey were of higher academic standing, missed fewer days of school, and were less likely to participate in the special education program at their school as compared to students who did not return a parental consent form. However, students with consent were not significantly different from students whose parents actively declined. The sample obtained under active parental consent represents students less at risk for problem behaviors than would have been obtained under passive consent procedures.-The effect of active parental consent on the ability to generalize the results of an alcohol, tobacco, and other drug prevention trial to rural adolescents.",0
"There is limited guidance on the design of stepped wedge cluster randomised trials. Current methodological literature focuses mainly on trials with cross-sectional data collection at discrete times, yet many recent stepped wedge trials do not follow this design. In this article, we present a typology to characterise the full range of stepped wedge designs, and offer guidance on several other design aspects. We developed a framework to define and report the key characteristics of a stepped wedge trial, including cluster allocation and individual participation. We also considered the relative strengths and weaknesses of trials according to this framework. We classified recently published stepped wedge trials using this framework and identified illustrative case studies. We identified key design choices and developed guidance for each. We identified three main stepped wedge designs: those with a closed cohort, an open cohort, and a continuous recruitment short exposure design. In the first two designs, many individuals experience both control and intervention conditions. In the final design, individuals are recruited in continuous time as they become eligible and experience either the control or intervention condition, but not both, and then provide an outcome measurement at follow-up. While most stepped wedge trials use simple randomisation, stratification and restricted randomisation are often feasible and may be useful. Some recent studies collect outcome information from individuals exposed a long time before or after the rollout period, but this contributes little to the primary analysis. Incomplete designs should be considered when the intervention cannot be implemented quickly. Carry-over effects can arise in stepped wedge trials with closed and open cohorts. Stepped wedge trial designs should be reported more clearly. Researchers should consider the use of stratified and/or restricted randomisation. Trials should generally not commit resources to collect outcome data from individuals exposed a long time before or after the rollout period. Though substantial carry-over effects are uncommon in stepped wedge trials, researchers should consider their possibility before conducting a trial with closed or open cohorts.-Designing a stepped wedge trial: three main designs, carry-over effects and randomisation approaches.",3
"Group-randomized study designs are useful when individually-randomized designs either are not possible, or will not be able to estimate the parameters of interest. Group-randomized trials often have small number of experimental units or groups and strong geographically-induced between-unit correlation, thereby increasing the chance of obtaining a ""bad"" randomization outcome. It has been suggested to highly constrain the design through restriction to those allocations that meet specified criteria based on certain covariates available at the baseline. We describe a SAS macro that allocates treatment conditions in a two-arm stratified group-randomized design that ensures balance on relevant covariates. The application of the macro is illustrated using two examples of group-randomized designs.-A SAS macro for constrained randomization of group-randomized designs.",1
"Group Randomized Trials (GRTs) randomize groups of people to treatment or control arms instead of individually randomizing subjects. When each subject has a binary outcome, over-dispersed binomial data may result, quantified as an intra-cluster correlation (ICC). Typically, GRTs have a small number, bin, of independent clusters, each of which can be quite large. Treating the ICC as a nuisance parameter, inference for a treatment effect can be done using quasi-likelihood with a logistic link. A Wald statistic, which, under standard regularity conditions, has an asymptotic standard normal distribution, can be used to test for a marginal treatment effect. However, we have found in our setting that the Wald statistic may have a variance less than 1, resulting in a test size smaller than its nominal value. This problem is most apparent when marginal probabilities are close to 0 or 1, particularly when n is small and the ICC is not negligible. When the ICC is known, we develop a method for adjusting the estimated standard error appropriately such that the Wald statistic will approximately have a standard normal distribution. We also propose ways to handle non-nominal test sizes when the ICC is estimated. We demonstrate the utility of our methods through simulation results covering a variety of realistic settings for GRTs.-Improving small-sample inference in group randomized trials with binary outcomes.",1
"Researchers planning cluster-randomized controlled trials (cRCTs) require estimates of the intra-cluster correlation coefficient (ICC) from previous studies for sample size calculations. This paper fills a persistent gap in the literature by providing estimates of ICCs for many key HIV-related clinical outcomes. Data from HIV-positive patients from 47 HIV care and treatment clinics in Dar es Salaam, Tanzania were used to calculate ICCs by site of enrollment or site of ART initiation for various clinical outcomes using cross-sectional and longitudinal data. ICCs were estimated using linear mixed models where either clinic of enrollment or clinic of ART initiation served as the random effect. ICCs ranged from 0 to 0.0706 (95% CI: 0.0447, 0.1098). For most outcomes, the ICCs were large enough to meaningfully affect sample size calculations. For binary outcomes, the ICCs for event prevalence at baseline tended to be larger than the ICCs for later cumulative incidences. For continuous outcomes, the ICCs for baseline values tended to be larger than the ICCs for the change in values from baseline. The ICCs for HIV-related outcomes cannot be ignored when calculating sample sizes for future cluster-randomized trials. The differences between ICCs calculated from baseline data alone and ICCs calculated using longitudinal data demonstrate the importance of selecting an ICC that reflects a study's intended design and duration for sample size calculations. While not generalizable to all contexts, these estimates provide guidance for future researchers seeking to design adequately powered cRCTs in Sub-Saharan African HIV treatment and care clinics.-Intra-Cluster Correlation Estimates for HIV-related Outcomes from Care and Treatment Clinics in Dar es Salaam, Tanzania.",1
"In cluster-randomized trials, it is commonly assumed that the magnitude of the correlation among subjects within a cluster is constant across clusters. However, the correlation may in fact be heterogeneous and depend on cluster characteristics. Accurate modeling of the correlation has the potential to improve inference. We use second-order generalized estimating equations to model heterogeneous correlation in cluster-randomized trials. Using simulation studies we show that accurate modeling of heterogeneous correlation can improve inference when the correlation is high or varies by cluster size. We apply the methods to a cluster-randomized trial of an intervention to promote breast cancer screening.-Using second-order generalized estimating equations to model heterogeneous intraclass correlation in cluster-randomized trials.",1
"Cluster randomization (CR) is often used for program evaluation when simple random assignment is inappropriate or infeasible. Pairwise cluster random (PCR) assignment is a more efficient alternative, but evaluators seemed to be deterred from PCR because of bias and identification problems. This article explains the problems, argues that they can be mitigated through design choices, and demonstrates that the suitability of PCR can be tested using Monte Carlo procedures. The article presents simple formulas showing how the PCR estimator is biased and explains why its standard error is not identified. Formal derivations appear in a longer companion article. Using those formulas, this article discusses how good design can mitigate the problems with bias and identification. Using Monte Carol simulation, this article also shows how to choose between CR and PCR at the design stage. This article advocates for wider use of the PCR design. PCR loses its appeal when the investigator lacks baseline data for matching the clusters. Its use is less compelling when there are a large number of clusters. But when the evaluator is working with a fairly small number of clusters-26 in the running example used in this article-PCR is an attractive alternative to CR.-Pairwise cluster randomization: an exposition.",1
"Stepped wedge designed trials are a type of cluster-randomized study in which the intervention is introduced to each cluster in a random order over time. This design is often used to assess the effect of a new intervention as it is rolled out across a series of clinics or communities. Based on a permutation argument, we derive a closed-form expression for an estimate of the intervention effect, along with its standard error, for a stepped wedge design trial. We show that these estimates are robust to misspecification of both the mean and covariance structure of the underlying data-generating mechanism, thereby providing a robust approach to inference for the intervention effect in stepped wedge designs. We use simulations to evaluate the type 1 error and power of the proposed estimate and to compare the performance of the proposed estimate to the optimal estimate when the correct model specification is known. The limitations, possible extensions, and open problems regarding the method are discussed.-Robust inference for the stepped wedge design",3
"The cluster randomized trial, in which groups rather than individuals are allocated to different interventions, is an increasingly popular design. In cluster trials observations on individuals within the same cluster may be correlated, and this lack of independence must be taken into account when designing a new trial. We present intraclass correlation coefficients derived from the Medical Research Council Trial of the Assessment and Management of Older People in the Community. This is a UK-based randomized trial comparing different methods of multidimensional screening for people aged 75 years and over. One hundred six general practices and over 30,000 individuals are taking part. Estimates of the intraclass correlation coefficients were obtained using one-way analysis of variance. This is by far the broadest collection of intraclass correlation coefficients for older people at the level of the primary care clinic published to date. The intraclass correlation coefficients presented will be useful in calculating sample sizes for cluster randomized trials and surveys at the primary care clinic level. In conjunction with other papers presenting collections of intraclass correlation coefficients, this paper should help to improve the quality of cluster randomized trials and hence help lead to more reliable estimates of the effectiveness of health care interventions.-Intraclass correlation coefficients for cluster randomized trials in primary care: data from the MRC Trial of the Assessment and Management of Older People in the Community.",1
"The stepped wedge cluster randomised trial (SW-CRT) is increasingly being used to evaluate policy or service delivery interventions. However, there is a dearth of trials literature addressing analytical approaches to the SW-CRT. Perhaps as a result, a significant number of published trials have major methodological shortcomings, including failure to adjust for secular trends at the analysis stage. Furthermore, the commonly used analytical framework proposed by Hussey and Hughes makes several assumptions. We highlight the assumptions implicit in the basic SW-CRT analytical model proposed by Hussey and Hughes. We consider how simple modifications of the basic model, using both random and fixed effects, can be used to accommodate deviations from the underlying assumptions. We consider the implications of these modifications for the intracluster correlation coefficients. In a case study, the importance of adjusting for the secular trend is illustrated. The basic SW-CRT model includes a fixed effect for time, implying a common underlying secular trend across steps and clusters. It also includes a single term for treatment, implying a constant shift in this trend under the treatment. When these assumptions are not realistic, simple modifications can be implemented to allow the secular trend to vary across clusters and the treatment effect to vary across clusters or time. In our case study, the na?ve treatment effect estimate (adjusted for clustering but unadjusted for time) suggests a beneficial effect. However, after adjusting for the underlying secular trend, we demonstrate a reversal of the treatment effect. Due to the inherent confounding of the treatment effect with time, analysis of a SW-CRT should always account for secular trends or risk-biased estimates of the treatment effect. Furthermore, the basic model proposed by Hussey and Hughes makes a number of important assumptions. Consideration needs to be given to the appropriate model choice at the analysis stage. We provide a Stata code to implement the proposed analyses in the illustrative case study.-Analysis of cluster randomised stepped wedge trials with repeated cross-sectional samples.",3
